"project_acronym","project","main_researcher","institution","details_project","budget_project","date_start_project","date_end_project"
"1D-Engine","1D-electrons coupled to dissipation: a novel approach for understanding and engineering superconducting materials and devices","Adrian KANTIAN","UPPSALA UNIVERSITET","Correlated electrons are at the forefront of condensed matter theory. Interacting quasi-1D electrons have seen vast progress in analytical and numerical theory, and thus in fundamental understanding and quantitative prediction. Yet, in the 1D limit fluctuations preclude important technological use, particularly of superconductors. In contrast, high-Tc superconductors in 2D/3D are not precluded by fluctuations, but lack a fundamental theory, making prediction and engineering of their properties, a major goal in physics, very difficult. This project aims to combine the advantages of both areas by making major progress in the theory of quasi-1D electrons coupled to an electron bath, in part building on recent breakthroughs (with the PIs extensive involvement) in simulating 1D and 2D electrons with parallelized density matrix renormalization group (pDMRG) numerics. Such theory will fundamentally advance the study of open electron systems, and show how to use 1D materials as elements of new superconducting (SC) devices and materials: 1) It will enable a new state of matter, 1D electrons with true SC order. Fluctuations from the electronic liquid, such as graphene, could also enable nanoscale wires to appear SC at high temperatures. 2) A new approach for the deliberate engineering of a high-Tc superconductor. In 1D, how electrons pair by repulsive interactions is understood and can be predicted. Stabilization by reservoir - formed by a parallel array of many such 1D systems - offers a superconductor for which all factors setting Tc are known and can be optimized. 3) Many existing superconductors with repulsive electron pairing, all presently not understood, can be cast as 1D electrons coupled to a bath. Developing chain-DMFT theory based on pDMRG will allow these materials SC properties to be simulated and understood for the first time. 4) The insights gained will be translated to 2D superconductors to study how they could be enhanced by contact with electronic liquids.","1491013","2018-10-01","2023-09-30"
"1st-principles-discs","A First Principles Approach to Accretion Discs","Martin Elias Pessah","KOBENHAVNS UNIVERSITET","Most celestial bodies, from planets, to stars, to black holes; gain mass during their lives by means of an accretion disc. Understanding the physical processes that determine the rate at which matter accretes and energy is radiated in these discs is vital for unraveling the formation, evolution, and fate of almost every type of object in the Universe. Despite the fact that magnetic fields have been known to be crucial in accretion discs since the early 90’s, the majority of astrophysical questions that depend on the details of how disc accretion proceeds are still being addressed using the “standard” accretion disc model (developed in the early 70’s), where magnetic fields do not play an explicit role. This has prevented us from fully exploring the astrophysical consequences and observational signatures of realistic accretion disc models, leading to a profound disconnect between observations (usually interpreted with the standard paradigm) and modern accretion disc theory and numerical simulations (where magnetic turbulence is crucial). The goal of this proposal is to use several complementary approaches in order to finally move beyond the standard paradigm. This program has two main objectives: 1) Develop the theoretical framework to incorporate magnetic fields, and the ensuing turbulence, into self-consistent accretion disc models, and investigate their observational implications. 2) Investigate transport and radiative processes in collision-less disc regions, where non-thermal radiation originates, by employing a kinetic particle description of the plasma. In order to achieve these goals, we will use, and build upon, state-of-the-art magnetohydrodynamic and particle-in-cell codes in conjunction with theoretical modeling. This framework will make it possible to address fundamental questions on stellar and planet formation, binary systems with a compact object, and supermassive black hole feedback in a way that has no counterpart within the standard paradigm.","1793697","2013-02-01","2018-01-31"
"2-3-AUT","Surfaces, 3-manifolds and automorphism groups","Nathalie Wahl","KOBENHAVNS UNIVERSITET","The scientific goal of the proposal is to answer central questions related to diffeomorphism groups of manifolds of dimension 2 and 3, and to their deformation invariant analogs, the mapping class groups. While the classification of surfaces has been known for more than a century, their automorphism groups have yet to be fully understood. Even less is known about diffeomorphisms of 3-manifolds despite much interest, and the objects here have only been classified recently, by the breakthrough work of Perelman on the Poincar\&apos;e and geometrization conjectures. In dimension 2, I will focus on the relationship between mapping class groups and topological conformal field theories, with applications to Hochschild homology. In dimension 3, I propose to compute the stable homology of classifying spaces of diffeomorphism groups and mapping class groups, as well as study the homotopy type of the space of diffeomorphisms. I propose moreover to establish homological stability theorems in the wider context of automorphism groups and more general families of groups. The project combines breakthrough methods from homotopy theory with methods from differential and geometric topology. The research team will consist of 3 PhD students, and 4 postdocs, which I will lead.","724992","2009-11-01","2014-10-31"
"2D-4-CO2","DESIGNING 2D NANOSHEETS FOR CO2 REDUCTION AND INTEGRATION INTO vdW HETEROSTRUCTURES FOR ARTIFICIAL PHOTOSYNTHESIS","Damien VOIRY","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","CO2 reduction reaction (CO2RR) holds great promise for conversion of the green-house gas carbon dioxide into chemical fuels. The absence of catalytic materials demonstrating high performance and high selectivity currently hampers practical demonstration. CO2RR is also limited by the low solubility of CO2 in the electrolyte solution and therefore electrocatalytic reactions in gas phase using gas diffusion electrodes would be preferred. 2D materials have recently emerged as a novel class of electrocatalytic materials thanks to their rich structures and electronic properties. The synthesis of novel 2D catalysts and their implementation into photocatalytic systems would be a major step towards the development of devices for storing solar energy in the form of chemical fuels. With 2D-4-CO2, I propose to: 1) develop novel class of CO2RR catalysts based on conducting 2D nanosheets and 2) demonstrate photocatalytic conversion of CO2 into chemical fuels using structure engineered gas diffusion electrodes made of 2D conducting catalysts. To reach this goal, the first objective of 2D-4-CO2 is to provide guidelines for the development of novel cutting-edge 2D catalysts towards CO2 conversion into chemical fuel. This will be possible by using a multidisciplinary approach based on 2D materials engineering, advanced methods of characterization and novel designs of gas diffusion electrodes for the reduction of CO2 in gas phase. The second objective is to develop practical photocatalytic systems using van der Waals (vdW) heterostructures for the efficient conversion of CO2 into chemical fuels. vdW heterostructures will consist in rational designs of 2D materials and 2D-like materials deposited by atomic layer deposition in order to achieve highly efficient light conversion and prolonged stability. This project will not only enable a deeper understanding of the CO2RR but it will also provide practical strategies for large-scale application of CO2RR for solar fuel production.","1499931","2019-01-01","2023-12-31"
"2D-PnictoChem","Chemistry and Interface Control of Novel 2D-Pnictogen Nanomaterials","Gonzalo ABELLAN SAEZ","UNIVERSITAT DE VALENCIA","2D-PnictoChem aims at exploring the Chemistry of a novel class of graphene-like 2D layered
elemental materials of group 15, the pnictogens: P, As, Sb, and Bi. In the last few years, these materials
have taken the field of Materials Science by storm since they can outperform and/or complement graphene
properties. Their strongly layer-dependent unique properties range from semiconducting to metallic,
including high carrier mobilities, tunable bandgaps, strong spin-orbit coupling or transparency. However,
the Chemistry of pnictogens is still in its infancy, remaining largely unexplored. This is the niche that
2D-PnictoChem aims to fill. By mastering the interface chemistry, we will develop the assembly of 2Dpnictogens
in complex hybrid heterostructures for the first time. Success will rely on a cross-disciplinary
approach combining both Inorganic- and Organic Chemistry with Solid-state Physics, including: 1)
Synthetizing and exfoliating high quality ultra-thin layer pnictogens, providing reliable access down to
the monolayer limit. 2) Achieving their chemical functionalization via both non-covalent and covalent
approaches in order to tailor at will their properties, decipher reactivity patterns and enable controlled
doping avenues. 3) Developing hybrid architectures through a precise chemical control of the interface,
in order to promote unprecedented access to novel heterostructures. 4) Exploring novel applications
concepts achieving outstanding performances. These are all priorities in the European Union agenda
aimed at securing an affordable, clean energy future by developing more efficient hybrid systems for
batteries, electronic devices or applications in catalysis. The opportunity is unique to reduce Europe’s
dependence on external technology and the PI’s background is ideally suited to tackle these objectives,
counting as well on a multidisciplinary team of international collaborators.","1499419","2018-11-01","2023-10-31"
"2D-TOPSENSE","Tunable optoelectronic devices by strain engineering of 2D semiconductors","Andres CASTELLANOS","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","The goal of 2D-TOPSENSE is to exploit the remarkable stretchability of two-dimensional semiconductors to fabricate optoelectronic devices where strain is used as an external knob to tune their properties.

While bulk semiconductors tend to break under strains larger than 1.5%, 2D semiconductors (such as MoS2) can withstand deformations of up to 10-20% before rupture. This large breaking strength promises a great potential of 2D semiconductors as ‘straintronic’ materials, whose properties can be adjusted by applying a deformation to their lattice. In fact, recent theoretical works predicted an interesting physical phenomenon: a tensile strain-induced semiconductor-to-metal transition in 2D semiconductors. By tensioning single-layer MoS2 from 0% up to 10%, its electronic band structure is expected to undergo a continuous transition from a wide direct band-gap of 1.8 eV to a metallic behavior. This unprecedented large strain-tunability will undoubtedly have a strong impact in a wide range of optoelectronic applications such as photodetectors whose cut-off wavelength is tuned by varying the applied strain or atomically thin light modulators. 

To date, experimental works on strain engineering have been mostly focused on fundamental studies, demonstrating part of the potential of 2D semiconductors in straintronics, but they have failed to exploit strain engineering to add extra functionalities to optoelectronic devices. In 2D-TOPSENSE I will go beyond the state of the art in straintronics by designing and fabricating optoelectronic devices whose properties and performance can be tuned by means of applying strain. 2D-TOPSENSE will focus on photodetectors with a tunable bandwidth and detectivity, light emitting devices whose emission wavelength can be adjusted, light modulators based on 2D semiconductors such as transition metal dichalcogenides or black phosphorus and solar funnels capable of directing the photogenerated charge carriers towards a specific position.","1930437","2018-03-01","2023-02-28"
"2DMATER","Controlled Synthesis of Two-Dimensional Nanomaterials for Energy Storage and Conversion","Xinliang Feng","TECHNISCHE UNIVERSITAET DRESDEN","""Two-dimensional (2D) nanosheets, which possess a high degree of anisotropy with nanoscale thickness and infinite length in other dimensions, hold enormous promise as a novel class of ultrathin 2D nanomaterials with various unique functionalities and properties, and exhibit great potential in energy storage and conversion systems that are substantially different from their respective 3D bulk forms. Here I propose a strategy for the synthesis and processing of various 2D nanosheets across a broad range of inorganic, organic and polymeric materials with molecular-level or thin thickness through both the top-down exfoliation of layered materials and the bottom-up assembly of available molecular building blocks. Further, I aim to develop the synthesis of various 2D-nanosheet based composite materials with thickness of less than 100 nm and the assembly of 2D nanosheets into novel hierarchal superstrucutures (like aerogels, spheres, porous particles, nanotubes, multi-layer films). The structural features of these 2D nanomaterials will be controllably tailored by both the used layered precursors and processing methodologies. The consequence is that I will apply and combine defined functional components as well as assembly protocols to create novel 2D nanomaterials for specific purposes in energy storage and conversion systems. Their unique characters will include the good electrical conductivity, excellent mechanical flexibility, high surface area, high chemical stability, fast electron transport and ion diffusion etc. Applications will be mainly demonstrated for the construction of lithium ion batteries (anode and cathode), supercapacitors (symmetric and asymmetric) and fuel cells. As the key achievements, I expect to establish the delineation of reliable structure-property relationships and improved device performance of 2D nanomaterials.""","1500000","2012-09-01","2017-08-31"
"2DNANOCAPS","Next Generation of 2D-Nanomaterials: Enabling Supercapacitor Development","Valeria Nicolosi","THE PROVOST, FELLOWS, FOUNDATION SCHOLARS & THE OTHER MEMBERS OF BOARD OF THE COLLEGE OF THE HOLY & UNDIVIDED TRINITY OF QUEEN ELIZABETH NEAR DUBLIN","Climate change and the decreasing availability of fossil fuels require society to move towards sustainable and renewable resources. 2DNanoCaps will focus on electrochemical energy storage, specifically supercapacitors. In terms of performance supercapacitors fill up the gap between batteries and the classical capacitors. Whereas batteries possess a high energy density but low power density, supercapacitors possess high power density but low energy density. Efforts are currently dedicated to move supercapacitors towards high energy density and high power density performance. Improvements have been achieved in the last few years due to the use of new electrode nanomaterials and the design of new hybrid faradic/capacitive systems. We recognize, however, that we are reaching a newer limit beyond which we will only see small incremental improvements. The main reason for this being the intrinsic difficulty in handling and processing materials at the nano-scale and the lack of communication across different scientific disciplines. I plan to use a multidisciplinary approach, where novel nanomaterials, existing knowledge on nano-scale processing and established expertise in device fabrication and testing will be brought together to focus on creating more efficient supercapacitor technologies. 2DNanoCaps will exploit liquid phase exfoliated two-dimensional nanomaterials such as transition metal oxides, layered metal chalcogenides and graphene as electrode materials. Electrodes will be ultra-thin (capacitance and thickness of the electrodes are inversely proportional), conductive, with high dielectric constants. Intercalation of ions between the assembled 2D flakes will be also achievable, providing pseudo-capacitance. The research here proposed will be initially based on fundamental laboratory studies, recognising that this holds the key to achieving step-change in supercapacitors, but also includes scaling-up and hybridisation as final objectives.","1501296","2011-10-01","2016-09-30"
"2DNANOPTICA","Nano-optics on flatland: from quantum nanotechnology to nano-bio-photonics","Pablo Alonso-González","UNIVERSIDAD DE OVIEDO","Ubiquitous in nature, light-matter interactions are of fundamental importance in science and all optical technologies. Understanding and controlling them has been a long-pursued objective in modern physics. However, so far, related experiments have relied on traditional optical schemes where, owing to the classical diffraction limit, control of optical fields to length scales below the wavelength of light is prevented. Importantly, this limitation impedes to exploit the extraordinary fundamental and scaling potentials of nanoscience and nanotechnology. A solution to concentrate optical fields into sub-diffracting volumes is the excitation of surface polaritons –coupled excitations of photons and mobile/bound charges in metals/polar materials (plasmons/phonons)-. However, their initial promises have been hindered by either strong optical losses or lack of electrical control in metals, and difficulties to fabricate high optical quality nanostructures in polar materials. 
With the advent of two-dimensional (2D) materials and their extraordinary optical properties, during the last 2-3 years the visualization of both low-loss and electrically tunable (active) plasmons in graphene and high optical quality phonons in monolayer and multilayer h-BN nanostructures have been demonstrated in the mid-infrared spectral range, thus introducing a very encouraging arena for scientifically ground-breaking discoveries in nano-optics.  Inspired by these extraordinary prospects, this ERC project aims to make use of our knowledge and unique expertise in 2D nanoplasmonics, and the recent advances in nanophononics, to establish a technological platform that, including coherent sources, waveguides, routers, and efficient detectors, permits an unprecedented active control and manipulation (at room temperature) of light and light-matter interactions on the nanoscale, thus laying experimentally the foundations of a 2D nano-optics field.","1459219","2017-01-01","2021-12-31"
"2DTHERMS","Design of new thermoelectric devices based on layered and field modulated nanostructures of strongly correlated electron systems","Jose Francisco Rivadulla Fernandez","UNIVERSIDAD DE SANTIAGO DE COMPOSTELA","Design of new thermoelectric devices based on layered and field modulated nanostructures of strongly correlated electron systems","1427190","2010-11-01","2015-10-31"
"2D–SYNETRA","Two-dimensional colloidal nanostructures - Synthesis and electrical transport","Christian Klinke","UNIVERSITAET HAMBURG","We propose to develop truly two-dimensional continuous materials and two-dimensional monolayer films composed of individual nanocrystals by the comparatively fast, inexpensive, and scalable colloidal synthesis method. The materials’ properties will be studied in detail, especially regarding their (photo-) electrical transport. This will allow developing new types of device structures, such as Coulomb blockade and field enhancement based transistors.

Recently, we demonstrated the possibility to synthesize in a controlled manner truly two-dimensional colloidal nanostructures. We will investigate their formation mechanism, synthesize further materials as “nanosheets”, develop methodologies to tune their geometrical properties, and study their (photo-) electrical properties.

Furthermore, we will use the Langmuir-Blodgett method to deposit highly ordered monolayers of monodisperse nanoparticles. Such structures show interesting transport properties governed by Coulomb blockade effects known from individual nanoparticles. This leads to semiconductor-like behavior in metal nanoparticle films. The understanding of the electric transport in such “multi-tunnel devices” is still very limited. Thus, we will investigate this concept in detail and take it to its limits. Beside improvement of quality and exchange of material we will tune the nanoparticles’ size and shape in order to gain a deeper understanding of the electrical properties of supercrystallographic assemblies. Furthermore, we will develop device concepts for diode and transistor structures which take into account the novel properties of the low-dimensional assemblies.

Nanosheets and monolayers of nanoparticles truly follow the principle of building devices by the bottom-up approach and allow electric transport measurements in a 2D regime. Highly ordered nanomaterial systems possess easy and reliably to manipulate electronic properties what make them interesting for future (inexpensive) electronic devices.","1497200","2013-02-01","2019-01-31"
"2F4BIODYN","Two-Field Nuclear Magnetic Resonance Spectroscopy for the Exploration of Biomolecular Dynamics","Fabien Ferrage","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The paradigm of the structure-function relationship in proteins is outdated. Biological macromolecules and supramolecular assemblies are highly dynamic objects. Evidence that their motions are of utmost importance to their functions is regularly identified. The understanding of the physical chemistry of biological processes at an atomic level has to rely not only on the description of structure but also on the characterization of molecular motions.
The investigation of protein motions will be undertaken with a very innovative methodological approach in nuclear magnetic resonance relaxation. In order to widen the ranges of frequencies at which local motions in proteins are probed, we will first use and develop new techniques for a prototype shuttle system for the measurement of relaxation at low fields on a high-field NMR spectrometer. Second, we will develop a novel system: a set of low-field NMR spectrometers designed as accessories for high-field spectrometers. Used in conjunction with the shuttle, this system will offer (i) the sensitivity and resolution (i.e. atomic level information) of a high-field spectrometer (ii) the access to low fields of a relaxometer and (iii) the ability to measure a wide variety of relaxation rates with high accuracy. This system will benefit from the latest technology in homogeneous permanent magnet development to allow a control of spin systems identical to that of a high-resolution probe. This new apparatus will open the way to the use of NMR relaxation at low fields for the refinement of protein motions at an atomic scale.
Applications of this novel approach will focus on the bright side of protein dynamics: (i) the largely unexplored dynamics of intrinsically disordered proteins, and (ii) domain motions in large proteins. In both cases, we will investigate a series of diverse protein systems with implications in development, cancer and immunity.","1462080","2012-01-01","2017-12-31"
"2O2ACTIVATION","Development of Direct Dehydrogenative Couplings mediated by Dioxygen","Frederic William Patureau","RHEINISCH-WESTFAELISCHE TECHNISCHE HOCHSCHULE AACHEN","The field of C-H bond activation has evolved at an exponential pace in the last 15 years. What appeals most in those novel synthetic techniques is clear: they bypass the pre-activation steps usually required in traditional cross-coupling chemistry by directly metalating C-H bonds. Many C-H bond functionalizations today however, rely on poorly atom and step efficient oxidants, leading to significant and costly chemical waste, thereby seriously undermining the overall sustainability of those methods. As restrictions in sustainability regulations will further increase, and the cost of certain chemical commodities will rise, atom efficiency in organic synthesis remains a top priority for research. 

The aim of 2O2ACTIVATION is to develop novel technologies utilizing O2 as sole terminal oxidant in order to allow useful, extremely sustainable, thermodynamically challenging, dehydrogenative C-N and C-O bond forming coupling reactions. However, the moderate reactivity of O2 towards many catalysts constitutes a major challenge. 2O2ACTIVATION will pioneer the design of new catalysts based on the ultra-simple propene motive, capable of direct activation of O2 for C-H activation based cross-couplings. The project is divided into 3 major lines: O2 activation using propene and its analogues (propenoids), 1) without metal or halide, 2) with hypervalent halide catalysis, 3) with metal catalyzed C-H activation. 
The philosophy of 2O2ACTIVATION is to focus C-H functionalization method development on the oxidative event.

Consequently, 2O2ACTIVATION breakthroughs will dramatically shortcut synthetic routes through the use of inactivated, unprotected, and readily available building blocks; and thus should be easily scalable. This will lead to a strong decrease in the costs related to the production of many essential chemicals, while preserving the environment (water as terminal by-product). The resulting novels coupling methods will thus have a lasting impact on the chemical industry.","1489823","2017-03-01","2022-02-28"
"3D-FABRIC","3D Flow Analysis in Bijels Reconfigured for Interfacial Catalysis","Martin F. HAASE","UNIVERSITEIT UTRECHT","The objective of this proposal is to determine the unknown criteria for convective cross-flow in bicontinuous interfacially jammed emulsion gels (bijels). Based on this, we will answer the question: Can continuously operated interfacial catalysis be realized in bijel cross-flow reactors? Demonstrating this potential will introduce a broadly applicable chemical technology, replacing wasteful chemical processes that require organic solvents. We will achieve our objective in three steps: 

(a) Control over bijel structure and properties. Bijels will be formed with a selection of functional inorganic colloidal particles. Nanoparticle surface modifications will be developed and extensively characterized. General principles for the parameters determining bijel structures and properties will be established based on confocal and electron microscopy characterization. These principles will enable unprecedented control over bijel formation and will allow for designing desired properties.

(b) Convective flow in bijels. The mechanical strength of bijels will be tailored and measured. With mechanically robust bijels, the influence of size and organization of oil/water channels on convective mass transfer in bijels will be investigated. To this end, a bijel mass transfer apparatus fabricated by 3d-printing of bijel fibers and soft photolithography will be introduced. In conjunction with the following objective, the analysis of convective flows in bijels will facilitate a thorough description of their structure/function relationships.

(c) Biphasic chemical reactions in STrIPS bijel cross-flow reactors. First, continuous extraction in bijels will be realized. Next, conditions to carry out continuously-operated, phase transfer catalysis of well-known model reactions in bijels will be determined. Both processes will be characterized in-situ and in 3-dimensions by confocal microscopy of fluorescent phase transfer reactions in transparent bijels.","1905000","2019-06-01","2024-05-31"
"3D-FIREFLUC","Taming the particle transport in magnetized plasmas via perturbative fields","Eleonora VIEZZER","UNIVERSIDAD DE SEVILLA","Wave-particle interactions are ubiquitous in nature and play a fundamental role in astrophysical and fusion plasmas. In solar plasmas, magnetohydrodynamic (MHD) fluctuations are thought to be responsible for the heating of the solar corona and the generation of the solar wind. In magnetically confined fusion (MCF) devices, enhanced particle transport induced by MHD fluctuations can deteriorate the plasma confinement, and also endanger the device integrity. MCF devices are an ideal testbed to verify current models and develop mitigation / protection techniques.

The proposed project paves the way for providing active control techniques to tame the MHD induced particle transport in a fusion plasma. A solid understanding of the interaction between energetic particles and MHD instabilities in the presence of electric fields and plasma currents is required to develop such techniques. I will pursue this goal through innovative diagnosis techniques with unprecedented spatio-temporal resolution. Combined with state-of-the-art hybrid MHD codes, a deep insight into the underlying physics mechanism will be gained. The outcome of this research project will have a major impact for next-step MCF devices as I will provide ground-breaking control techniques for mitigating MHD induced particle transport in magnetized plasmas.

The project consists of 3 research lines which follow a bottom-up approach: 
(1) Cutting-edge instrumentation, aiming at the new generation of energetic particle and edge current diagnostics. 
(2) Unravel the dynamics of energetic particles, electric fields, edge currents and MHD fluctuations. 
(3) From lab to space weather: The developed models will revolutionize our understanding of the observed particle acceleration and transport in the solar corona.

Based on this approach, the project represents a gateway between the fusion, astrophysics and space communities opening new avenues for a common basic understanding.","1512250","2019-05-01","2024-04-30"
"3D-FM","Taking Force Microscopy into the Third Dimension","Tjerk Hendrik Oosterkamp","UNIVERSITEIT LEIDEN","I propose to pursue two emerging Force Microscopy techniques that allow measuring structural properties below the surface of the specimen. Whereas Force Microscopy (most commonly known under the name AFM) is usually limited to measuring the surface topography and surface properties of a specimen, I will demonstrate that Force Microscopy can achieve true 3D images of the structure of the cell nucleus. In Ultrasound Force Microscopy, an ultrasound wave is launched from below towards the surface of the specimen. After the sound waves interact with structures beneath the surface of the specimen, the local variations in the amplitude and phase shift of the ultrasonic surface motion is collected by the Force Microscopy tip. Previously, measured 2D maps of the surface response have shown that the surface response is sensitive to structures below the surface. In this project I will employ miniature AFM cantilevers and nanotube tips that I have already developed in my lab. This will allow me to quickly acquire many such 2D maps at a much wider range of ultrasound frequencies and from these 2D maps calculate the full 3D structure below the surface. I expect this technique to have a resolving power better than 10 nm in three dimensions as far as 2 microns below the surface. In parallel I will introduce a major improvement to a technique based on Nuclear Magnetic Resonance (NMR). Magnetic Resonance Force Microscopy measures the interaction of a rotating nuclear spin in the field gradient of a magnetic Force Microscopy tip. However, these forces are so small that they pose an enormous challenge. Miniature cantilevers and nanotube tips, in combination with additional innovations in the detection of the cantilever motion, can overcome this problem. I expect to be able to measure the combined signal of 100 proton spins or fewer, which will allow me to measure proton densities with a resolution of 5 nm, but possibly even with atomic resolution.","1794960","2008-08-01","2013-07-31"
"3D-FNPWriting","Unprecedented spatial control of porosity and functionality in nanoporous membranes through 3D printing and microscopy for polymer writing","Annette ANDRIEU-BRUNSEN","TECHNISCHE UNIVERSITAT DARMSTADT","Membranes are key materials in our life. Nature offers high performance membranes relying on a parallel local regulation of nanopore structure, functional placement, membrane composition and architecture. Existing technological membranes are key materials in separation, recycling, sensing, energy conversion, being essential components for a sustainable future. But their performance is far away from their natural counterparts. One reason for this performance gap is the lack of 3D nanolocal control in membrane design. This applies to each individual nanopore but as well to the membrane architecture. This proposal aims to implement 3D printing (additive manufacturing, top down) and complex near-field and total internal reflection (TIR) high resolution microscopy induced polymer writing (bottom up) to nanolocally control in hierarchical nanoporous membranes spatially and independent of each other: porosity, pore functionalization, membrane architecture, composition. This disruptive technology platform will make accessible to date unachieved, highly accurate asymmetric nanopores and multifunctional, hierarchical membrane architecture/ composition and thus highly selective, directed, transport with tuneable rates. 3D-FNPWriting will demonstrate this for the increasing class of metal nanoparticle/ salt pollutants aiming for tuneable, selective, directed transport based monitoring and recycling instead of size-based filtration, accumulation into sewerage and distribution into nature. Specifically, the potential of this disruptive technology with respect to transport design will be demonstrated for a) a 3D-printed in-situ functionalized nanoporous fiber architecture and b) a printed, nanolocally near-field and TIR-microscopy polymer functionalized membrane representing a thin separation layer. This will open systematic understanding of nanolocal functional control on transport and new perspectives in water/ energy management for future smart industry/ homes.","1499844","2019-04-01","2024-03-31"
"3D-nanoMorph","Label-free 3D morphological nanoscopy for studying sub-cellular dynamics in live cancer cells with high spatio-temporal resolution","Krishna AGARWAL","UNIVERSITETET I TROMSOE - NORGES ARKTISKE UNIVERSITET","Label-free optical nanoscopy, free from photobleaching and photochemical toxicity of fluorescence labels and yielding 3D morphological resolution of <50 nm, is the future of live cell imaging. 3D-nanoMorph breaks the diffraction barrier and shifts the paradigm in label-free nanoscopy, providing isotropic 3D resolution of <50 nm. To achieve this, 3D-nanoMorph performs non-linear inverse scattering for the first time in nanoscopy and decodes scattering between sub-cellular structures (organelles).

3D-nanoMorph innovatively devises complementary roles of light measurement system and computational nanoscopy algorithm. A novel illumination system and a novel light collection system together enable measurement of only the most relevant intensity component and create a fresh perspective about label-free measurements. A new computational nanoscopy approach employs non-linear inverse scattering. Harnessing non-linear inverse scattering for resolution enhancement in nanoscopy opens new possibilities in label-free 3D nanoscopy.

I will apply 3D-nanoMorph to study organelle degradation (autophagy) in live cancer cells over extended duration with high spatial and temporal resolution, presently limited by the lack of high-resolution label-free 3D morphological nanoscopy. Successful 3D mapping of nanoscale biological process of autophagy will open new avenues for cancer treatment and showcase 3D-nanoMorph for wider applications.

My cross-disciplinary expertise of 14 years spanning inverse problems, electromagnetism, optical microscopy, integrated optics and live cell nanoscopy paves path for successful implementation of 3D-nanoMorph.","1499999","2019-07-01","2024-06-30"
"3D-PXM","3D Piezoresponse X-ray Microscopy","Hugh SIMONS","DANMARKS TEKNISKE UNIVERSITET","Polar materials, such as piezoelectrics and ferroelectrics are essential to our modern life, yet they are mostly developed by trial-and-error. Their properties overwhelmingly depend on the defects within them, the majority of which are hidden in the bulk. The road to better materials is via mapping these defects, but our best tool for it – piezoresponse force microscopy (PFM) – is limited to surfaces. 3D-PXM aims to revolutionize our understanding by measuring the local structure-property correlations around individual defects buried deep in the bulk.
 
This is a completely new kind of microscopy enabling 3D maps of local strain and polarization (i.e. piezoresponse) with 10 nm resolution in mm-sized samples. It is novel, multi-scale and fast enough to capture defect dynamics in real time. Uniquely, it is a full-field method that uses a synthetic-aperture approach to improve both resolution and recover the image phase. This phase is then quantitatively correlated to local polarization and strain via a forward model. 3D-PXM combines advances in X-Ray optics, phase recovery and data analysis to create something transformative. In principle, it can achieve spatial resolution comparable to the best coherent X-Ray microscopy methods while being faster, used on larger samples, and without risk of radiation damage.
 
For the first time, this opens the door to solving how defects influence bulk properties under real-life conditions. 3D-PXM focuses on three types of defects prevalent in polar materials: grain boundaries, dislocations and polar nanoregions. Individually they address major gaps in the state-of-the-art, while together making great strides towards fully understanding defects. This understanding is expected to inform a new generation of multi-scale models that can account for a material’s full heterogeneity. These models are the first step towards abandoning our tradition of trial-and-error, and with this comes the potential for a new era of polar materials.","1496941","2019-01-01","2023-12-31"
"3D-QUEST","3D-Quantum Integrated Optical Simulation","Fabio Sciarrino","UNIVERSITA DEGLI STUDI DI ROMA LA SAPIENZA","""Quantum information was born from the merging of classical information and quantum physics. Its main objective consists of understanding the quantum nature of information and learning how to process it by using physical systems which operate by following quantum mechanics laws. Quantum simulation is a fundamental instrument to investigate phenomena of quantum systems dynamics, such as quantum transport, particle localizations and energy transfer, quantum-to-classical transition, and even quantum improved computation, all tasks that are hard to simulate with classical approaches. Within this framework integrated photonic circuits have a strong potential to realize quantum information processing by optical systems.

The aim of 3D-QUEST is to develop and implement quantum simulation by exploiting 3-dimensional integrated photonic circuits. 3D-QUEST is structured to demonstrate the potential of linear optics to implement a computational power beyond the one of a classical computer. Such """"hard-to-simulate"""" scenario is disclosed when multiphoton-multimode platforms are realized. The 3D-QUEST research program will focus on three tasks of growing difficulty.
A-1. To simulate bosonic-fermionic dynamics with integrated optical systems acting on 2 photon entangled states.
A-2. To pave the way towards hard-to-simulate, scalable quantum linear optical circuits by investigating m-port interferometers acting on n-photon states with n>2.
A-3. To exploit 3-dimensional integrated structures for the observation of new quantum optical phenomena and for the quantum simulation of more complex scenarios.

3D-QUEST will exploit the potential of the femtosecond laser writing integrated waveguides. This technique will be adopted to realize 3-dimensional capabilities and high flexibility, bringing in this way the optical quantum simulation in to new regime.""","1474800","2012-08-01","2017-07-31"
"3DICE","3D Interstellar Chemo-physical Evolution","Valentine Wakelam","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","At the end of their life, stars spread their inner material into the diffuse interstellar medium. This diffuse medium gets locally denser and form dark clouds (also called dense or molecular clouds) whose innermost part is shielded from the external UV field by the dust, allowing for molecules to grow and get more complex. Gravitational collapse occurs inside these dense clouds, forming protostars and their surrounding disks, and eventually planetary systems like (or unlike) our solar system. The formation and evolution of molecules, minerals, ices and organics from the diffuse medium to planetary bodies, their alteration or preservation throughout this cosmic chemical history set the initial conditions for building planets, atmospheres and possibly the first bricks of life. The current view of interstellar chemistry is based on fragmental works on key steps of the sequence that are observed.  The objective of this proposal is to follow the fractionation of the elements between the gas-phase and the interstellar grains, from the most diffuse medium to protoplanetary disks, in order to constrain the chemical composition of the material in which planets are formed. The potential outcome of this project is to get a consistent and more accurate description of the chemical evolution of interstellar matter. To achieve this objective, I will improve our chemical model by adding new processes on grain surfaces relevant under the diffuse medium conditions. This upgraded gas-grain model will be coupled to 3D dynamical models of the formation of dense clouds from diffuse medium and of protoplanetary disks from dense clouds. The computed chemical composition will also be used with 3D radiative transfer codes to study the chemical tracers of the physics of protoplanetary disk formation. The robustness of the model predictions will be studied with sensitivity analyses. Finally, model results will be confronted to observations to address some of the current challenges.","1166231","2013-09-01","2018-08-31"
"3DMOSHBOND","Three-Dimensional Mapping Of a Single Hydrogen Bond","Adam Marc SWEETMAN","UNIVERSITY OF LEEDS","All properties of matter are ultimately governed by the forces between single atoms, but our knowledge of interatomic, and intermolecular, potentials is often derived indirectly. 

In 3DMOSHBOND, I outline a program of work designed to create a paradigm shift in the direct measurement of complex interatomic potentials via a fundamental reimagining of how atomic resolution imaging, and force measurement, techniques are applied. 

To provide a clear proof of principle demonstration of the power of this concept, I propose to map the strength, shape and extent of single hydrogen bonding (H-bonding) interactions in 3D with sub-Angstrom precision. H-bonding is a key component governing intermolecular interactions, particularly for biologically important molecules. Despite its critical importance, H-bonding is relatively poorly understood, and the IUPAC definition of the H-bond was changed as recently as 2011- highlighting the relevance of a new means to engage with these fundamental interactions. 

Hitherto unprecedented resolution and accuracy will be achieved via a creation of a novel layer of vertically oriented H-bonding molecules, functionalisation of the tip of a scanning probe microscope with a single complementary H-bonding molecule, and by complete characterisation of the position of all atoms in the junction. This will place two H-bonding groups “end on” and map the extent, and magnitude, of the H-bond with sub-Angstrom precision for a variety of systems. This investigation of the H-bond will present us with an unparalleled level of information regarding its properties. 

Experimental results will be compared with ab initio density functional theory (DFT) simulations, to investigate the extent to which state-of-the-art simulations are able to reproduce the behaviour of the H-bonding interaction. The project will create a new generalised probe for the study of single atomic and molecular interactions.","1971468","2018-01-01","2022-12-31"
"3DWATERWAVES","Mathematical aspects of three-dimensional water waves with vorticity","Erik Torsten Wahlén","LUNDS UNIVERSITET","The goal of this project is to develop a mathematical theory for steady three-dimensional water waves with vorticity. The mathematical model consists of the incompressible Euler equations with a free surface, and vorticity is important for modelling the interaction of surface waves with non-uniform currents. In the two-dimensional case, there has been a lot of progress on water waves with vorticity in the last decade. This progress has mainly been based on the stream function formulation, in which the problem is reformulated as a nonlinear elliptic free boundary problem. An analogue of this formulation is not available in three dimensions, and the theory has therefore so far been restricted to irrotational flow. In this project we seek to go beyond this restriction using two different approaches. In the first approach we will adapt methods which have been used to construct three-dimensional ideal flows with vorticity in domains with a fixed boundary to the free boundary context (for example Beltrami flows). In the second approach we will develop methods which are new even in the case of a fixed boundary, by performing a detailed study of the structure of the equations close to a given shear flow using ideas from infinite-dimensional bifurcation theory. This involves handling infinitely many resonances.","1203627","2016-03-01","2021-02-28"
"3FLEX","Three-Component Fermi Gas Lattice Experiment","Selim Jochim","RUPRECHT-KARLS-UNIVERSITAET HEIDELBERG","Understanding the many-body physics of strongly correlated systems has always been a major challenge for theoretical and experimental physics. The recent advances in the field of ultracold quantum gases have opened a completely new way to study such strongly correlated systems. It is now feasible to use ultracold gases as quantum simulators for such diverse systems such as the Hubbard model or the BCS-BEC crossover. The objective of this project is to study a three-component Fermi gas in an optical lattice, a system with rich many-body physics. With our experiments we aim to contribute to the understanding of exotic phases which are discussed in the context of QCD and condensed matter physics.","1469040","2011-08-01","2016-07-31"
"4DVIDEO","4DVideo: 4D spatio-temporal modeling of real-world events from video streams","Marc Pollefeys","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","The focus of this project is the development of algorithms that allow one to capture and analyse dynamic events taking place in the real world. For this, we intend to develop smart camera networks that can perform a multitude of observation tasks, ranging from surveillance and tracking to high-fidelity, immersive reconstructions of important dynamic events (i.e. 4D videos). There are many fundamental questions in computer vision associated with these problems. Can the geometric, topologic and photometric properties of the camera network be obtained from live images? What is changing about the environment in which the network is embedded? How much information can be obtained from dynamic events that are observed by the network? What if the camera network consists of a random collection of sensors that happened to observe a particular event (think hand-held cell phone cameras)? Do we need synchronization? Those questions become even more challenging if one considers active camera networks that can adapt to the vision task at hand. How should resources be prioritized for different tasks? Can we derive optimal strategies to control camera parameters such as pan, tilt and zoom, trade-off resolution, frame-rate and bandwidth? More fundamentally, seeing cameras as points that sample incoming light rays and camera networks as a distributed sensor, how does one decide which rays should be sampled? Many of those issues are particularly interesting when we consider time-varying events. Both spatial and temporal resolution are important and heterogeneous frame-rates and resolution can offer advantages. Prior knowledge or information obtained from earlier samples can be used to restrict the possible range of solutions (e.g. smoothness assumption and motion prediction). My goal is to obtain fundamental answers to many of those question based on thorough theoretical analysis combined with practical algorithms that are proven on real applications.","1757422","2008-08-01","2013-11-30"
"4SUNS","4-Colours/2-Junctions of III-V semiconductors on Si to use in electronics devices and solar cells","María Nair LOPEZ MARTINEZ","UNIVERSIDAD AUTONOMA DE MADRID","It was early predicted by M. Green and coeval colleagues that dividing the solar spectrum into narrow ranges of colours is the most efficient manner to convert solar energy into electrical power. Multijunction solar cells are the current solution to this challenge, which have reached over 30% conversion efficiencies by stacking 3 junctions together. However, the large fabrication costs and time hinders their use in everyday life. It has been shown that highly mismatched alloy (HMA) materials provide a powerful playground to achieve at least 3 different colour absorption regions that enable optimised energy conversion with just one junction. Combining HMA-based junctions with standard Silicon solar cells will rocket solar conversion efficiency at a reduced price. To turn this ambition into marketable devices, several efforts are still needed and few challenges must be overcome.
4SUNS is a revolutionary approach for the development of HMA materials on Silicon technology, which will bring highly efficient multi-colour solar cells costs below current multijunction devices. The project will develop the technology of HMA materials on Silicon via material synthesis opening a new technology for the future. The understanding and optimization of highly mismatched alloy materials-using GaAsNP alloy- will provide building blocks for the fabrication of laboratory-size 4-colours/2-junctions solar cells.
Using a molecular beam epitaxy system, 4SUNS will grow 4-colours/2-junctions structure as well as it will manufacture the final devices. Structural and optoelectronic characterizations will carry out to determine the quality of the materials and the solar cells characteristic to obtain a competitive product. These new solar cells are competitive products to breakthrough on the solar energy sector solar cells and allowing Europe to take leadership on high efficiency solar cells.","1499719","2018-02-01","2023-01-31"
"4TH-NU-AVENUE","Search for a fourth neutrino with a PBq anti-neutrino source","Thierry Michel René Lasserre","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","Several observed anomalies in neutrino oscillation data can be explained by a hypothetical fourth neutrino separated from the three standard neutrinos by a squared mass difference of a few eV2. This hypothesis can be tested with a PBq (ten kilocurie scale) 144Ce antineutrino beta-source deployed at the center of a large low background liquid scintillator detector, such like Borexino, KamLAND, and SNO+. In particular, the compact size of such a source could yield an energy-dependent oscillating pattern in event spatial distribution that would unambiguously determine neutrino mass differences and mixing angles.

The proposed program aims to perform the necessary research and developments to produce and deploy an intense antineutrino source in a large liquid scintillator detector. Our program will address the definition of the production process of the neutrino source as well as its experimental characterization, the detailed physics simulation of both signal and backgrounds, the complete design and the realization of the thick shielding, the preparation of the interfaces with the antineutrino detector, including the safety and security aspects.","1500000","2012-10-01","2018-09-30"
"5D-NanoTrack","Five-Dimensional Localization Microscopy for Sub-Cellular Dynamics","Yoav SHECHTMAN","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","The sub-cellular processes that control the most critical aspects of life occur in three-dimensions (3D), and are intrinsically dynamic. While super-resolution microscopy has revolutionized cellular imaging in recent years, our current capability to observe the dynamics of life on the nanoscale is still extremely limited, due to inherent trade-offs between spatial, temporal and spectral resolution using existing approaches.

We propose to develop and demonstrate an optical microscopy methodology that would enable live sub-cellular observation in unprecedented detail. Making use of multicolor 3D point-spread-function (PSF) engineering, a technique I have recently developed, we will be able to simultaneously track multiple markers inside live cells, at high speed and in five-dimensions (3D, time, and color). 

Multicolor 3D PSF engineering holds the potential of being a uniquely powerful method for 5D tracking. However, it is not yet applicable to live-cell imaging, due to significant bottlenecks in optical engineering and signal processing, which we plan to overcome in this project. Importantly, we will also demonstrate the efficacy of our method using a challenging biological application: real-time visualization of chromatin dynamics - the spatiotemporal organization of DNA. This is a highly suitable problem due to its fundamental importance, its role in a variety of cellular processes, and the lack of appropriate tools for studying it. 
  
The project is divided into 3 aims:
1. Technology development: diffractive-element design for multicolor 3D PSFs. 
2. System design: volumetric tracking of dense emitters.
3. Live-cell measurements: chromatin dynamics. 

Looking ahead, here we create the imaging tools that pave the way towards the holy grail of chromatin visualization: dynamic observation of the 3D positions of the ~3 billion DNA base-pairs in a live human cell. Beyond that, our results will be applicable to numerous 3D micro/nanoscale tracking applications.","1802500","2018-11-01","2023-10-31"
"a SMILE","analyse Soluble + Membrane complexes with Improved LILBID Experiments","Nina Morgner","JOHANN WOLFGANG GOETHE-UNIVERSITATFRANKFURT AM MAIN","Crucial processes within cells depend on specific non-covalent interactions which mediate the assembly of proteins and other biomolecules. Deriving structural information to understand the function of these complex systems is the primary goal of Structural Biology.
In this application, the recently developed LILBID method (Laser Induced Liquid Bead Ion Desorption) will be optimized for investigation of macromolecular complexes with a mass accuracy two orders of magnitude better than in 1st generation spectrometers.
Controlled disassembly of the multiprotein complexes in the mass spectrometric analysis while keeping the 3D structure intact, will allow for the determination of complex stoichiometry and connectivity of the constituting proteins. Methods for such controlled disassembly will be developed in two separate units of the proposed LILBID spectrometer, in a collision chamber and in a laser dissociation chamber, enabling gas phase dissociation of protein complexes and removal of excess water/buffer molecules. As a third unit, a chamber allowing determination of ion mobility (IM) will be integrated to determine collisional cross sections (CCS). From CCS, unique information regarding the spatial arrangement of proteins in complexes or subcomplexes will then be obtainable from LILBID.
The proposed design of the new spectrometer will offer fundamentally new possibilities for the investigation of non-covalent RNA, soluble and membrane protein complexes, as well as broadening the applicability of non-covalent MS towards supercomplexes.","1264477","2014-02-01","2019-01-31"
"A-LIFE","Absorbing aerosol layers in a changing climate: aging, lifetime and dynamics","Bernadett Barbara Weinzierl","UNIVERSITAT WIEN","Aerosols (i.e. tiny particles suspended in the air) are regularly transported in huge amounts over long distances impacting air quality, health, weather and climate thousands of kilometers downwind of the source. Aerosols affect the atmospheric radiation budget through scattering and absorption of solar radiation and through their role as cloud/ice nuclei.
 
In particular, light absorption by aerosol particles such as mineral dust and black carbon (BC; thought to be the second strongest contribution to current global warming after CO2) is of fundamental importance from a climate perspective because the presence of absorbing particles (1) contributes to solar radiative forcing, (2) heats absorbing aerosol layers, (3) can evaporate clouds and (4) change atmospheric dynamics.

Considering this prominent role of aerosols, vertically-resolved in-situ data on absorbing aerosols are surprisingly scarce and aerosol-dynamic interactions are poorly understood in general. This is, as recognized in the last IPCC report, a serious barrier for taking the accuracy of climate models and predictions to the next level. To overcome this barrier, I propose to investigate aging, lifetime and dynamics of absorbing aerosol layers with a holistic end-to-end approach including laboratory studies, airborne field experiments and numerical model simulations.

Building on the internationally recognized results of my aerosol research group and my long-term experience with airborne aerosol measurements, the time seems ripe to systematically bridge the gap between in-situ measurements of aerosol microphysical and optical properties and the assessment of dynamical interactions of absorbing particles with aerosol layer lifetime through model simulations.

The outcomes of this project will provide fundamental new understanding of absorbing aerosol layers in the climate system and important information for addressing the benefits of BC emission controls for mitigating climate change.","1987980","2015-10-01","2020-09-30"
"A-LIFE","The asymmetry of life: towards a unified view of the emergence of biological homochirality","Cornelia MEINERT","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","What is responsible for the emergence of homochirality, the almost exclusive use of one enantiomer over its mirror image? And what led to the evolution of life’s homochiral biopolymers, DNA/RNA, proteins and lipids, where all the constituent monomers exhibit the same handedness? 

Based on in-situ observations and laboratory studies, we propose that this handedness occurs when chiral biomolecules are synthesized asymmetrically through interaction with circularly polarized photons in interstellar space. The ultimate goal of this project will be to demonstrate how the diverse set of heterogeneous enantioenriched molecules, available from meteoritic impact, assembles into homochiral pre-biopolymers, by simulating the evolutionary stages on early Earth. My recent research has shown that the central chiral unit of RNA, ribose, forms readily under simulated comet conditions and this has provided valuable new insights into the accessibility of precursors of genetic material in interstellar environments. The significance of this project arises due to the current lack of experimental demonstration that amino acids, sugars and lipids can simultaneously and asymmetrically be synthesized by a universal physical selection process.

A synergistic methodology will be developed to build a unified theory for the origin of all chiral biological building blocks and their assembly into homochiral supramolecular entities. For the first time, advanced analyses of astrophysical-relevant samples, asymmetric photochemistry triggered by circularly polarized synchrotron and laser sources, and chiral amplification due to polymerization processes will be combined. Intermediates and autocatalytic reaction kinetics will be monitored and supported by quantum calculations to understand the underlying processes. A unified theory on the asymmetric formation and self-assembly of life’s biopolymers is groundbreaking and will impact the whole conceptual foundation of the origin of life.","1500000","2019-04-01","2024-03-31"
"AAATSI","Advanced Antenna Architecture for THZ Sensing Instruments","Andrea Neto","TECHNISCHE UNIVERSITEIT DELFT","The Tera-Hertz portion of the spectrum presents unique potentials for advanced applications. Currently the THz spectrum is revealing the mechanisms at the origin of our universe and provides the means to monitor the health of our planet via satellite based sensing of critical gases. Potentially time domain sensing of the THz spectrum will be the ideal tool for a vast variety of medical and security applications.

Presently, systems in the THz regime are extremely expensive and consequently the THz spectrum is still the domain of only niche (expensive) scientific applications. The main problems are the lack of power and sensitivity. The wide unused THz spectral bandwidth is, herself, the only widely available resource that in the future can compensate for these problems. But, so far, when scientists try to really use the bandwidth, they run into an insurmountable physical limit: antenna dispersion. Antenna dispersion modifies the signal’s spectrum in a wavelength dependent manner in all types of radiation, but is particularly deleterious to THz signals because the spectrum is too wide and with foreseeable technology it cannot be digitized.

The goal of this proposal is to introduce break-through antenna technology that will eliminate the dispersion bottle neck and revolutionize Time Domain sensing and Spectroscopic Space Science. Achieving these goals the project will pole vault THz imaging technology into the 21-th century and develop critically important enabling technologies which will satisfy the electrical engineering needs of the next 30 years and in the long run will enable multi Tera-bit wireless communications.

In order to achieve these goals, I will first build upon two major breakthrough radiation mechanisms that I pioneered: Leaky Lenses and Connected Arrays. Eventually, ultra wide band imaging arrays constituted by thousands of components will be designed on the bases of the new theoretical findings and demonstrated.","1499487","2011-11-01","2017-10-31"
"AArteMIS","Aneurysmal Arterial Mechanics: Into the Structure","Pierre Joseph Badel","ASSOCIATION POUR LA RECHERCHE ET LE DEVELOPPEMENT DES METHODES ET PROCESSUS INDUSTRIELS","The rupture of an Aortic Aneurysm (AA), which is often lethal, is a mechanical phenomenon that occurs when the wall stress state exceeds the local strength of the tissue. Our current understanding of arterial rupture mechanisms is poor, and the physics taking place at the microscopic scale in these collagenous structures remains an open area of research. Understanding, modelling, and quantifying the micro-mechanisms which drive the mechanical response of such tissue and locally trigger rupture represents the most challenging and promising pathway towards predictive diagnosis and personalized care of AA.
The PI's group was recently able to detect, in advance, at the macro-scale, rupture-prone areas in bulging arterial tissues. The next step is to get into the details of the arterial microstructure to elucidate the underlying mechanisms.
Through the achievements of AArteMIS, the local mechanical state of the fibrous microstructure of the tissue, especially close to its rupture state, will be quantitatively analyzed from multi-photon confocal microscopy and numerically reconstructed to establish quantitative micro-scale rupture criteria. AArteMIS will also address developing micro-macro models which are based on the collected quantitative data. 
The entire project will be completed through collaboration with medical doctors and engineers, experts in all required fields for the success of AArteMIS.
AArteMIS is expected to open longed-for pathways for research in soft tissue mechanobiology which focuses on cell environment and to enable essential clinical applications for the quantitative assessment of AA rupture risk. It will significantly contribute to understanding fatal vascular events and improving cardiovascular treatments. It will provide a tremendous source of data and inspiration for subsequent applications and research by answering the most fundamental questions on AA rupture behaviour enabling ground-breaking clinical changes to take place.","1499783","2015-04-01","2020-03-31"
"AAS","Approximate algebraic structure and applications","Ben Green","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","This project studies several mathematical topics with a related theme, all of them part of the relatively new discipline known as additive combinatorics.

We look at approximate, or rough, variants of familiar mathematical notions such as group, polynomial or homomorphism. In each case we seek to describe the structure of these approximate objects, and then to give applications of the resulting theorems. This endeavour has already lead to groundbreaking results in the theory of prime numbers, group theory and combinatorial number theory.","1000000","2011-10-01","2016-09-30"
"ABINITIODGA","Ab initio Dynamical Vertex Approximation","Karsten Held","TECHNISCHE UNIVERSITAET WIEN","Some of the most fascinating physical phenomena are experimentally observed in strongly correlated electron systems and, on the theoretical side, only poorly understood hitherto. The aim of the ERC project AbinitioDGA is the development, implementation and application of a new, 21th century  method for the ab initio calculation of materials with such strong electronic correlations.  AbinitioDGA includes strong electronic correlations on all time and length scales and hence is a big step beyond the state-of-the-art methods, such as the local density approximation, dynamical mean field theory, and the GW approach (Green function G times screened interaction W). It has the potential for an extraordinary high impact not only in the field of computational materials science but also for a better understanding of quantum critical heavy fermion systems,  high-temperature superconductors, and transport through nano- and heterostructures.  These four physical problems and related materials will be studied within the ERC project, besides the methodological development.

On the technical side,  AbinitioDGA realizes Hedin's  idea to include vertex corrections beyond the GW approximation. All vertex corrections which can be traced back to a fully irreducible local vertex and the bare non-local Coulomb interaction are included. This way, AbinitioDGA does not only contain the GW physics of screened exchange and the strong local correlations of dynamical mean field theory but also  non-local  correlations beyond on all length scales. Through the latter, AbinitioDGA can prospectively describe phenomena such as quantum criticality, spin-fluctuation mediated superconductivity, and weak localization corrections to the conductivity. Nonetheless, the computational effort is still manageable even for realistic materials calculations, making the considerable effort to implement AbinitioDGA worthwhile.","1491090","2013-01-01","2018-07-31"
"ABIOS","ABIOtic Synthesis of RNA: an investigation on how life started before biology existed","Guillaume STIRNEMANN","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The emergence of life is one of the most fascinating and yet largely unsolved questions in the natural sciences, and thus a significant challenge for scientists from many disciplines. There is growing evidence that ribonucleic acid (RNA) polymers, which are capable of genetic information storage and self-catalysis, were involved in the early forms of life. But despite recent progress, RNA synthesis without biological machineries is very challenging. The current project aims at understanding how to synthesize RNA in abiotic conditions. I will solve problems associated with three critical aspects of RNA formation that I will rationalize at a molecular level: (i) accumulation of precursors, (ii) formation of a chemical bond between RNA monomers, and (iii) tolerance for alternative backbone sugars or linkages. Because I will study problems ranging from the formation of chemical bonds up to the stability of large biopolymers, I propose an original computational multi-scale approach combining techniques that range from quantum calculations to large-scale all-atom simulations, employed together with efficient enhanced-sampling algorithms, forcefield improvement, cutting-edge analysis methods and model development. 
My objectives are the following:
1 • To explain why the poorly-understood thermally-driven process of thermophoresis can contribute to the accumulation of dilute precursors. 
2 • To understand why linking RNA monomers with phosphoester bonds is so difficult, to understand the molecular mechanism of possible catalysts and to suggest key improvements. 
3 • To rationalize the molecular basis for RNA tolerance for alternative backbone sugars or linkages that have probably been incorporated in abiotic conditions.   
This unique in-silico laboratory setup should significantly impact our comprehension of life’s origin by overcoming major obstacles to RNA abiotic formation, and in addition will reveal significant orthogonal outcomes for (bio)technological applications.","1497031","2018-02-01","2023-01-31"
"ABLASE","Advanced Bioderived and Biocompatible Lasers","Malte Christian Gather","THE UNIVERSITY COURT OF THE UNIVERSITY OF ST ANDREWS","Naturally occurring optical phenomena attract great attention and transform our ability to study biological processes, with “the discovery and development of the green fluorescent protein (GFP)” (Nobel Prize in Chemistry 2008) being a particularly successful example. Although found only in very few species in nature, most organisms can be genetically programmed to produce the brightly fluorescent GFP molecules. Combined with modern fluorescence detection schemes, this has led to entirely new ways of monitoring biological processes. The applicant now demonstrated a biological laser – a completely novel, living source of coherent light based on a single biological cell bioengineered to produce GFP. Such a laser is intrinsically biocompatible, thus offering unique properties not shared by any existing laser. However, the physical processes involved in lasing from GFP remain poorly understood and so far biological lasers rely on bulky, impractical external resonators for optical feedback. Within this project, the applicant and his team will develop for the first time an understanding of stimulated emission in GFP and related proteins and create an unprecedented stand-alone single-cell biolaser based on intracellular optical feedback. These lasers will be deployed as microscopic and biocompatible imaging probes, thus opening in vivo microscopy to dense wavelength-multiplexing and enabling unmatched sensing of biomolecules and mechanical pressure. The evolutionarily evolved nano-structure of GFP will also enable novel ways of studying strong light-matter coupling and will bio-inspire advances of synthetic emitters. The proposed project is inter-disciplinary by its very nature, bridging photonics, genetic engineering and material science. The applicant’s previous pioneering work and synergies with work on other lasers developed at the applicant’s host institution provide an exclusive competitive edge. ERC support would transform this into a truly novel field of research.","1499875","2015-06-01","2020-05-31"
"ACCLAIM","Aerosols effects on convective clouds and climate","Philip Stier","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Clouds play a key role in the climate system. Small anthropogenic perturbations of the cloud system potentially have large radiative effects. Aerosols perturb the global radiation budget directly, by scattering and absorption, as well as indirectly, by the modification of cloud properties and occurrence. The applicability of traditional conceptual models of indirect aerosol effects to convective clouds is disputed as cloud dynamics complicates the picture.

Strong evidence for numerous aerosol effects on convection has been established in individual disciplines: through remote sensing and in-situ observations as well as by cloud resolving and global modelling. However, a coherent scientific view of the effects of aerosols on convection has yet to be established.

The primary objective of ACCLAIM is to recast the effects of aerosols on convective clouds as basis for improved global estimates of anthropogenic climate effects. Specific objectives include: i) to unravel the governing principles of aerosol effects on convective clouds; ii) provide quantitative constraints on satellite-retrieved relationships between convective clouds and aerosols; and ultimately iii) to enable global climate models to represent the full range of anthropogenic climate perturbations and quantify the climate response to aerosol effects on convective clouds.

I have developed the research strategy of ACCLAIM to overcome disciplinary barriers in this frontier research area and seek five years of funding to establish an interdisciplinary, physics focused, research group consisting of two PostDocs, two PhD students and myself. ACCLAIM will be centred around global aerosol-convection climate modelling studies, complemented by research constraining aerosol-convection interactions through remote sensing and a process focused research strand, advancing fundamental  understanding and global model parameterisations through high resolution aerosol-cloud modelling in synergy with in-situ observations.","1429243","2011-09-01","2017-02-28"
"ACCORD","Algorithms for Complex Collective Decisions on Structured Domains","Edith Elkind","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Algorithms for Complex Collective Decisions on Structured Domains. 
The aim of this proposal is to substantially advance the field of Computational Social Choice, by developing new tools and methodologies that can be used for making complex group decisions in rich and structured environments. We consider settings where each member of a decision-making body has preferences over a finite set of alternatives, and the goal is to synthesise a collective preference over these alternatives, which may take the form of a partial order over the set of alternatives with a predefined structure: examples include selecting a fixed-size set of alternatives, a ranking of the alternatives, a winner and up to two runner-ups, etc. We will formulate desiderata that apply to such preference aggregation procedures, design specific procedures that satisfy as many of these desiderata as possible, and develop efficient algorithms for computing them. As the latter step may be infeasible on general preference domains, we will focus on identifying the least restrictive domains that enable efficient computation, and use real-life preference data to verify whether the associated restrictions are likely to be satisfied in realistic preference aggregation scenarios. Also, we will determine whether our preference aggregation procedures are computationally resistant to malicious behavior. To lower the cognitive burden on the decision-makers, we will extend our procedures to accept partial rankings as inputs. Finally, to further contribute towards bridging the gap between theory and practice of collective decision making, we will provide open-source software implementations of our procedures, and reach out to the potential users to obtain feedback on their practical applicability.","1395933","2015-07-01","2020-06-30"
"ACDC","Algorithms and Complexity of Highly Decentralized Computations","Fabian Daniel Kuhn","ALBERT-LUDWIGS-UNIVERSITAET FREIBURG","""Many of today's and tomorrow's computer systems are built on top of large-scale networks such as, e.g., the Internet, the world wide web, wireless ad hoc and sensor networks, or peer-to-peer networks. Driven by technological advances, new kinds of networks and applications have become possible and we can safely assume that this trend is going to continue. Often modern systems are envisioned to consist of a potentially large number of individual components that are organized in a completely decentralized way. There is no central authority that controls the topology of the network, how nodes join or leave the system, or in which way nodes communicate with each other. Also, many future distributed applications will be built using wireless devices that communicate via radio.

The general objective of the proposed project is to improve our understanding of the algorithmic and theoretical foundations of decentralized distributed systems.  From an algorithmic point of view, decentralized networks and computations pose a number of fascinating and unique challenges that are not present in sequential or more standard distributed systems. As communication is limited and mostly between nearby nodes, each node of a large network can only maintain a very restricted view of the global state of the system. This is particularly true if the network can change dynamically, either by nodes joining or leaving the system or if the topology changes over time, e.g., because of the mobility of the devices in case of a wireless network. Nevertheless, the nodes of a network need to coordinate in order to achieve some global goal.

In particular, we plan to study algorithms and lower bounds for basic computation and information dissemination tasks in such systems. In addition, we are particularly interested in the complexity of distributed computations in dynamic and wireless networks.""","1148000","2013-11-01","2018-10-31"
"ACOULOMODE","Advanced coupling of low order combustor simulations with thermoacoustic modelling and controller design","Aimee Morgans","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","""Combustion is essential to the world’s energy generation and transport needs, and will remain so for the foreseeable future. Mitigating its impact on the climate and human health, by reducing its associated emissions, is thus a priority. One significant challenge for gas-turbine combustion is combustion instability, which is currently inhibiting reductions in NOx emissions (these damage human health via a deterioration in air quality). Combustion instability is caused by a two-way coupling between unsteady combustion and acoustic waves - the large pressure oscillations that result can cause substantial mechanical damage. Currently, the lack of fast, accurate modelling tools for combustion instability, and the lack of reliable ways of suppressing it are severely hindering reductions in NOx emissions.
This proposal aims to make step improvements in both fast, accurate modelling of combustion instability, and in developing reliable active control strategies for its suppression. It will achieve this by coupling low order combustor models (these are fast, simplified models for simulating combustion instability) with advances in analytical modelling, CFD simulation, reduced order modelling and control theory tools. In particular:
*  important advances in accurately incorporating the effect of entropy waves (temperature variations resulting from unsteady combustion) and non-linear flame models will be made;
*  new active control strategies for achieving reliable suppression of combustion instability, including from within limit cycle oscillations, will be developed;
*  an open-source low order combustor modelling tool will be developed and widely disseminated, opening access to researchers worldwide and improving communications between the fields of thermoacoustics and control theory.
Thus the proposal aims to use analytical and computational methods to contribute to achieving low NOx gas-turbine combustion, without the penalty of damaging combustion instability.""","1489309","2013-01-01","2017-12-31"
"ACrossWire","A Cross-Correlated Approach to Engineering Nitride Nanowires","Hannah Jane JOYCE","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Nanowires based on group III–nitride semiconductors exhibit outstanding potential for emerging applications in energy-efficient lighting, optoelectronics and solar energy harvesting. Nitride nanowires, tailored at the nanoscale, should overcome many of the challenges facing conventional planar nitride materials, and also add extraordinary new functionality to these materials. However, progress towards III–nitride nanowire devices has been hampered by the challenges in quantifying nanowire electrical properties using conventional contact-based measurements. Without reliable electrical transport data, it is extremely difficult to optimise nanowire growth and device design. This project aims to overcome this problem through an unconventional approach: advanced contact-free electrical measurements. Contact-free measurements, growth studies, and device studies will be cross-correlated to provide unprecedented insight into the growth mechanisms that govern nanowire electronic properties and ultimately dictate device performance. A key contact-free technique at the heart of this proposal is ultrafast terahertz conductivity spectroscopy: an advanced technique ideal for probing nanowire electrical properties. We will develop new methods to enable the full suite of contact-free (including terahertz, photoluminescence and cathodoluminescence measurements) and contact-based measurements to be performed with high spatial resolution on the same nanowires. This will provide accurate, comprehensive and cross-correlated feedback to guide growth studies and expedite the targeted development of nanowires with specified functionality. We will apply this powerful approach to tailor nanowires as photoelectrodes for solar photoelectrochemical water splitting. This is an application for which nitride nanowires have outstanding, yet unfulfilled, potential. This project will thus harness the true potential of nitride nanowires and bring them to the forefront of 21st century technology.","1499195","2017-04-01","2022-03-31"
"ACTAR TPC","Active Target and Time Projection Chamber","Geoffrey Fathom Grinyer","GRAND ACCELERATEUR NATIONAL D'IONS LOURDS","The active target and time projection chamber (ACTAR TPC) is a novel gas-filled detection system that will permit new studies into the structure and decays of the most exotic nuclei.  The use of a gas volume that acts as a sensitive detection medium and as the reaction target itself (an “active target”) offers considerable advantages over traditional nuclear physics detectors and techniques.  In high-energy physics, TPC detectors have found profitable applications but their use in nuclear physics has been limited.  With the ACTAR TPC design, individual detection pad sizes of 2 mm are the smallest ever attempted in either discipline but is a requirement for high-efficiency and high-resolution nuclear spectroscopy.  The corresponding large number of electronic channels (16000 from a surface of only 25×25 cm) requires new developments in high-density electronics and data-acquisition systems that are not yet available in the nuclear physics domain.  New experiments in regions of the nuclear chart that cannot be presently contemplated will become feasible with ACTAR TPC.","1290000","2014-02-01","2019-01-31"
"ActiveBioFluids","Origins of Collective Motion in Active Biofluids","Daniel TAM","TECHNISCHE UNIVERSITEIT DELFT","The emergence of coherent behaviour is ubiquitous in the natural world and has long captivated biologists and physicists alike. One area of growing interest is the collective motion and synchronization arising within and between simple motile organisms. My goal is to develop and use a novel experimental approach to unravel the origins of spontaneous coherent motion in three model systems of biofluids: (1) the synchronization of the two flagella of green algae Chlamydomonas Rheinhardtii, (2) the metachronal wave in the cilia of protist Paramecium and (3) the collective motion of swimming microorganisms in active suspensions. Understanding the mechanisms leading to collective motion is of tremendous importance because it is crucial to many biological processes such as mechanical signal transduction, embryonic development and biofilm formation.

Up till now, most of the work has been theoretical and has led to the dominant view that hydrodynamic interactions are the main driving force for synchronization and collective motion. Recent experiments have challenged this view and highlighted the importance of direct mechanical contact.  New experimental studies are now crucially needed. The state-of-the-art of experimental approaches consists of observations of unperturbed cells. The key innovation in our approach is to dynamically interact with microorganisms in real-time, at the relevant time and length scales. I will investigate the origins of coherent motion by reproducing synthetically the mechanical signatures of physiological flows and direct mechanical interactions and track precisely the response of the organism to the perturbations.  Our new approach will incorporate optical tweezers to interact with motile cells, and a unique μ-Tomographic PIV setup to track their  3D micron-scale motion. 

This proposal tackles a timely question in biophysics and will yield new insight into the fundamental principles underlying collective motion in active biological matter.","1500000","2017-04-01","2022-03-31"
"ACTIVENP","Active and low loss nano photonics (ActiveNP)","Thomas Arno Klar","UNIVERSITAT LINZ","This project aims at designing novel hybrid nanophotonic devices comprising metallic nanostructures and active elements such as dye molecules or colloidal quantum dots. Three core objectives, each going far beyond the state of the art, shall be tackled: (i) Metamaterials containing gain materials: Metamaterials introduce magnetism to the optical frequency range and hold promise to create entirely novel devices for light manipulation. Since present day metamaterials are extremely absorptive, it is of utmost importance to fight losses. The ground-breaking approach of this proposal is to incorporate fluorescing species into the nanoscale metallic metastructures in order to compensate losses by stimulated emission. (ii) The second objective exceeds the ansatz of compensating losses and will reach out for lasing action. Individual metallic nanostructures such as pairs of nanoparticles will form novel and unusual nanometre sized resonators for laser action. State of the art microresonators still have a volume of at least half of the wavelength cubed. Noble metal nanoparticle resonators scale down this volume by a factor of thousand allowing for truly nanoscale coherent light sources. (iii) A third objective concerns a substantial improvement of nonlinear effects. This will be accomplished by drastically sharpened resonances of nanoplasmonic devices surrounded by active gain materials. An interdisciplinary team of PhD students and a PostDoc will be assembled, each scientist being uniquely qualified to cover one of the expertise fields: Design, spectroscopy, and simulation. The project s outcome is twofold: A substantial expansion of fundamental understanding of nanophotonics and practical devices such as nanoscopic lasers and low loss metamaterials.","1494756","2010-10-01","2015-09-30"
"ActiveWindFarms","Active Wind Farms: Optimization and Control of Atmospheric Energy Extraction in Gigawatt Wind Farms","Johan Meyers","KATHOLIEKE UNIVERSITEIT LEUVEN","With the recognition that wind energy will become an important contributor to the world’s energy portfolio, several wind farms with a capacity of over 1 gigawatt are in planning phase. In the past, engineering of wind farms focused on a bottom-up approach, in which atmospheric wind availability was considered to be fixed by climate and weather. However, farms of gigawatt size slow down the Atmospheric Boundary Layer (ABL) as a whole, reducing the availability of wind at turbine hub height. In Denmark’s large off-shore farms, this leads to underperformance of turbines which can reach levels of 40%–50% compared to the same turbine in a lone-standing case. For large wind farms, the vertical structure and turbulence physics of the flow in the ABL become crucial ingredients in their design and operation. This introduces a new set of scientific challenges related to the design and control of large wind farms. The major ambition of the present research proposal is to employ optimal control techniques to control the interaction between large wind farms and the ABL, and optimize overall farm-power extraction. Individual turbines are used as flow actuators by dynamically pitching their blades using time scales ranging between 10 to 500 seconds. The application of such control efforts on the atmospheric boundary layer has never been attempted before, and introduces flow control on a physical scale which is currently unprecedented. The PI possesses a unique combination of expertise and tools enabling these developments: efficient parallel large-eddy simulations of wind farms, multi-scale turbine modeling, and gradient-based optimization in large optimization-parameter spaces using adjoint formulations. To ensure a maximum impact on the wind-engineering field, the project aims at optimal control, experimental wind-tunnel validation, and at including multi-disciplinary aspects, related to structural mechanics, power quality, and controller design.","1499241","2012-10-01","2017-09-30"
"ACTIVIA","Visual Recognition of Function and Intention","Ivan Laptev","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","""Computer vision is concerned with the automated interpretation of images and video streams. Today's research is (mostly) aimed at answering queries such as """"Is this a picture of a dog?"""", (classification) or sometimes """"Find the dog in this photo"""" (detection). While categorisation and detection are useful for many tasks, inferring correct class labels is not the final answer to visual recognition. The categories and locations of objects do not provide direct understanding of their function i.e., how things work, what they can be used for, or how they can act and react. Such an understanding, however, would be highly desirable to answer currently unsolvable queries such as """"Am I in danger?"""" or """"What can happen in this scene?"""". Solving such queries is the aim of this proposal.

My goal is to uncover the functional properties of objects and the purpose of actions by addressing visual recognition from a different and yet unexplored perspective. The main novelty of this proposal is to leverage observations of people, i.e., their actions and interactions to automatically learn the use, the purpose and the function of objects and scenes from visual data. The project is timely as it builds upon the two key recent technological advances: (a) the immense progress in visual recognition of objects, scenes and human actions achieved in the last ten years, as well as (b) the emergence of a massive amount of public image and video data now available to train visual models.

ACTIVIA addresses fundamental research issues in automated interpretation of dynamic visual scenes, but its results are expected to serve as a basis for ground-breaking technological advances in practical applications. The recognition of functional properties and intentions as explored in this project will directly support high-impact applications such as detection of abnormal events, which are likely to revolutionise today's approaches to crime protection, hazard prevention, elderly care, and many others.""","1497420","2013-01-01","2018-12-31"
"ADAPT","Theory and Algorithms for Adaptive Particle Simulation","Stephane Redon","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","""During the twentieth century, the development of macroscopic engineering has been largely stimulated by progress in digital prototyping: cars, planes, boats, etc. are nowadays designed and tested on computers. Digital prototypes have progressively replaced actual ones, and effective computer-aided engineering tools have helped cut costs and reduce production cycles of these macroscopic systems.

The twenty-first century is most likely to see a similar development at the atomic scale. Indeed, the recent years have seen tremendous progress in nanotechnology - in particular in the ability to control matter at the atomic scale. Similar to what has happened with macroscopic engineering, powerful and generic computational tools will be needed to engineer complex nanosystems, through modeling and simulation. As a result, a major challenge is to develop efficient simulation methods and algorithms.

NANO-D, the INRIA research group I started in January 2008 in Grenoble, France, aims at developing
efficient computational methods for modeling and simulating complex nanosystems, both natural and artificial. In particular, NANO-D develops SAMSON, a software application which gathers all algorithms designed by the group and its collaborators (SAMSON: Software for Adaptive Modeling and Simulation Of Nanosystems).

In this project, I propose to develop a unified theory, and associated algorithms, for adaptive particle simulation. The proposed theory will avoid problems that plague current popular multi-scale or hybrid simulation approaches by simulating a single potential throughout the system, while allowing users to finely trade precision for computational speed.

I believe the full development of the adaptive particle simulation theory will have an important impact on current modeling and simulation practices, and will enable practical design of complex nanosystems on desktop computers, which should significantly boost the emergence of generic nano-engineering.""","1476882","2012-09-01","2017-08-31"
"ADAPTIVES","Algorithmic Development and Analysis of Pioneer Techniques for Imaging with waVES","Chrysoula Tsogka","FOUNDATION FOR RESEARCH AND TECHNOLOGY HELLAS","The proposed work concerns the theoretical and numerical development of robust and adaptive methodologies for broadband imaging in clutter. The word clutter expresses our uncertainty on the wave speed of the propagation medium. Our results are expected to have a strong impact in a wide range of applications, including underwater acoustics, exploration geophysics and ultrasound non-destructive testing. Our machinery is coherent interferometry (CINT), a state-of-the-art statistically stable imaging methodology, highly suitable for the development of imaging methods in clutter. We aim to extend CINT along two complementary directions: novel types of applications, and further mathematical and numerical development so as to assess and extend its range of applicability. CINT is designed for imaging with partially coherent array data recorded in richly scattering media. It uses statistical smoothing techniques to obtain results that are independent of the clutter realization. Quantifying the amount of smoothing needed is difficult, especially when there is no a priori knowledge about the propagation medium. We intend to address this question by coupling the imaging process with the estimation of the medium's large scale features. Our algorithms rely on the residual coherence in the data. When the coherent signal is too weak, the CINT results are unsatisfactory. We propose two ways for enhancing the resolution of CINT: filter the data prior to imaging (noise reduction) and waveform design (optimize the source distribution). Finally, we propose to extend the applicability of our imaging-in-clutter methodologies by investigating the possibility of utilizing ambient noise sources to perform passive sensor imaging, as well as by studying the imaging problem in random waveguides.","690000","2010-06-01","2015-11-30"
"ADJUV-ANT VACCINES","Elucidating the Molecular Mechanisms of Synthetic Saponin Adjuvants and Development of Novel Self-Adjuvanting Vaccines","Alberto FERNANDEZ TEJADA","ASOCIACION CENTRO DE INVESTIGACION COOPERATIVA EN BIOCIENCIAS","The clinical success of anticancer and antiviral vaccines often requires the use of an adjuvant, a substance that helps stimulate the body’s immune response to the vaccine, making it work better. However, few adjuvants are sufficiently potent and non-toxic for clinical use; moreover, it is not really known how they work. Current vaccine approaches based on weak carbohydrate and glycopeptide antigens are not being particularly effective to induce the human immune system to mount an effective fight against cancer. Despite intensive research and several clinical trials, no such carbohydrate-based antitumor vaccine has yet been approved for public use. In this context, the proposed project has a double, ultimate goal based on applying chemistry to address the above clear gaps in the adjuvant-vaccine field. First, I will develop new improved adjuvants and novel chemical strategies towards more effective, self-adjuvanting synthetic vaccines. Second, I will probe deeply into the molecular mechanisms of the synthetic constructs by combining extensive immunological evaluations with molecular target identification and detailed conformational studies. Thus, the singularity of this multidisciplinary proposal stems from the integration of its main objectives and approaches connecting chemical synthesis and chemical/structural biology with cellular and molecular immunology. This ground-breaking project at the chemistry-biology frontier will allow me to establish my own independent research group and explore key unresolved mechanistic questions in the adjuvant/vaccine arena with extraordinary chemical precision. Therefore, with this transformative and timely research program I aim to (a) develop novel synthetic antitumor and antiviral vaccines with improved properties and efficacy for their prospective translation into the clinic and (b) gain new critical insights into the molecular basis and three-dimensional structure underlying the biological activity of these constructs.","1499219","2017-03-01","2022-02-28"
"ADONIS","Attosecond Dynamics On Interfaces and Solids","Reinhard Kienberger","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","New insight into ever smaller microscopic units of matter as well as in ever faster evolving chemical, physical or atomic processes pushes the frontiers in many fields in science. Pump/probe experiments turned out to be the most direct approach to time-domain investigations of fast-evolving microscopic processes. Accessing atomic and molecular inner-shell processes directly in the time-domain requires a combination of short wavelengths in the few hundred eV range and sub-femtosecond pulse duration. The concept of light-field-controlled XUV photoemission employs an XUV pulse achieved by High-order Harmonic Generation (HHG) as a pump and the light pulse as a probe or vice versa. The basic prerequisite, namely the generation and measurement of isolated sub-femtosecond XUV pulses synchronized to a strong few-cycle light pulse with attosecond precision, opens up a route to time-resolved inner-shell atomic and molecular spectroscopy with present day sources. Studies of attosecond electronic motion (1 as = 10-18 s) in solids and on surfaces and interfaces have until now remained out of reach. The unprecedented time resolution of the aforementioned technique will enable for the first time monitoring of sub-fs dynamics of such systems in the time domain. These dynamics – of electronic excitation, relaxation, and wave packet motion – are of broad scientific interest and pertinent to the development of many modern technologies including semiconductor and molecular electronics, optoelectronics, information processing, photovoltaics, and optical nano-structuring. The purpose of this project is to investigate phenomena like the temporal evolution of direct photoemission, interference effects in resonant photoemission, fast adsorbate-substrate charge transfer, and electronic dynamics in supramolecular assemblies, in a series of experiments in order to overcome the temporal limits of measurements in solid state physics and to better understand processes in microcosm.","1296000","2008-10-01","2013-09-30"
"ADULT","Analysis of the Dark Universe through Lensing Tomography","Hendrik Hoekstra","UNIVERSITEIT LEIDEN","The discoveries that the expansion of the universe is accelerating due to an unknown “dark energy”
and that most of the matter is invisible, highlight our lack of understanding of the major constituents
of the universe. These surprising findings set the stage for research in cosmology at the start of the
21st century. The objective of this proposal is to advance observational constraints to a level where we can distinguish between physical mechanisms that aim to explain the properties of dark energy and  the observed distribution of dark matter throughout the universe. We use a relatively new technique called weak gravitational lensing: the accurate measurement of correlations in the orientations of distant galaxies enables us to map the dark matter distribution directly and to extract the cosmological information that is encoded by the large-scale structure.
To study the dark universe we will analyse data from a new state-of-the-art imaging survey: the Kilo-
Degree Survey (KiDS) will cover 1500 square degrees in 9 filters. The combination of its large survey
area and the availability of exquisite photometric redshifts for the sources makes KiDS the first
project that can place interesting constraints on the dark energy equation-of-state using lensing data
alone. Combined with complementary results from Planck, our measurements will provide one of the
best views of the dark side of the universe before much larger space-based projects commence.
To reach the desired accuracy we need to carefully measure the shapes of distant background galaxies. We also need to account for any intrinsic alignments that arise due to tidal interactions, rather than through lensing. Reducing these observational and physical biases to negligible levels is a necessarystep to ensure the success of KiDS and an important part of our preparation for more challenging projects such as the European-led space mission Euclid.","1316880","2012-01-01","2016-12-31"
"AEROBIC","Assessing the Effects of Rising O2 on Biogeochemical Cycles: Integrated Laboratory Experiments and Numerical Simulations","Itay Halevy","WEIZMANN INSTITUTE OF SCIENCE LTD","The rise of atmospheric O2 ~2,500 million years ago is one of the most profound transitions in Earth's history. Yet, despite its central role in shaping Earth's surface environment, the cause for the rise of O2 remains poorly understood. Tight coupling between the O2 cycle and the biogeochemical cycles of redox-active elements, such as C, Fe and S, implies radical changes in these cycles before, during and after the rise of O2. These changes, too, are incompletely understood, but have left valuable information encoded in the geological record. This information has been qualitatively interpreted, leaving many aspects of the rise of O2, including its causes and constraints on ocean chemistry before and after it, topics of ongoing research and debate. Here, I outline a research program to address this fundamental question in geochemical Earth systems evolution. The inherently interdisciplinary program uniquely integrates laboratory experiments, numerical models, geological observations, and geochemical analyses. Laboratory experiments and geological observations will constrain unknown parameters of the early biogeochemical cycles, and, in combination with field studies, will validate and refine the use of paleoenvironmental proxies. The insight gained will be used to develop detailed models of the coupled biogeochemical cycles, which will themselves be used to quantitatively understand the events surrounding the rise of O2, and to illuminate the dynamics of elemental cycles in the early oceans.
This program is expected to yield novel, quantitative insight into these important events in Earth history and to have a major impact on our understanding of early ocean chemistry and the rise of O2. An ERC Starting Grant will enable me to use the excellent experimental and computational facilities at my disposal, to access the outstanding human resource at the Weizmann Institute of Science, and to address one of the major open questions in modern geochemistry.","1472690","2013-09-01","2018-08-31"
"AEROFLEX","AEROelastic instabilities and control of FLEXible Structures","Olivier Pierre MARQUET","OFFICE NATIONAL D'ETUDES ET DE RECHERCHES AEROSPATIALES","Aeroelastic instabilities are at the origin of large deformations of structures and are limiting the capacities of products in various industrial branches such as aeronautics, marine industry, or wind electricity production. If suppressing aeroelastic instabilities is an ultimate goal, a paradigm shift in the technological development is to take advantage of these instabilities to achieve others objectives, as reducing the drag of these flexible structures. The ground-breaking challenges addressed in this project are to design fundamentally new theoretical methodologies for (i) describing mathematically aeroelastic instabilities, (ii) suppressing them and (iii) using them to reduce mean drag of structures at a low energetic cost. To that aim, two types of aeroelastic phenomena will be specifically studied: the flutter, which arises as a result of an unstable coupling instability between two stable dynamics, that of the structures and that the flow, and vortex-induced vibrations which appear when the fluid dynamics is unstable. An aeroelastic global stability analysis will be first developed and applied to problems of increasing complexity, starting from two-dimensional free-vibrating rigid structures and progressing towards three-dimensional free-deforming elastic structures. The control of these aeroelastic instabilities will be then addressed with two different objectives: their suppression or their use for flow control. A theoretical passive control methodology will be established for suppressing linear aeroelastic instabilities, and extended to high Reynolds number flows and experimental configurations. New perturbation methods for solving strongly nonlinear problems and adjoint-based control algorithm will allow to use these aeroelastic instabilities for drag reduction. This project will allow innovative control solutions to emerge, not only in flutter or vortex-induced vibrations problems, but also in a much broader class of fluid-structure problems.","1377290","2015-07-01","2020-06-30"
"AEROSPACEPHYS","Multiphysics models and simulations for reacting and plasma flows applied to the space exploration program","Thierry Edouard Bertrand Magin","INSTITUT VON KARMAN DE DYNAMIQUE DES FLUIDES","Space exploration is one of boldest and most exciting endeavors that humanity has undertaken, and it holds enormous promise for the future. Our next challenges for the spatial conquest include bringing back samples to Earth by means of robotic missions and continuing the manned exploration program, which aims at sending human beings to Mars and bring them home safely. Inaccurate prediction of the heat-flux to the surface of the spacecraft heat shield can be fatal for the crew or the success of a robotic mission. This quantity is estimated during the design phase. An accurate prediction is a particularly complex task, regarding modelling of the following phenomena that are potential “mission killers:” 1) Radiation of the plasma in the shock layer, 2) Complex surface chemistry on the thermal protection material, 3) Flow transition from laminar to turbulent. Our poor understanding of the coupled mechanisms of radiation, ablation, and transition leads to the difficulties in flux prediction. To avoid failure and ensure safety of the astronauts and payload, engineers resort to “safety factors” to determine the thickness of the heat shield, at the expense of the mass of embarked payload. Thinking out of the box and basic research are thus necessary for advancements of the models that will better define the environment and requirements for the design and safe operation of tomorrow’s space vehicles and planetary probes for the manned space exploration. The three basic ingredients for predictive science are: 1) Physico-chemical models, 2) Computational methods, 3) Experimental data. We propose to follow a complementary approach for prediction. The proposed research aims at: “Integrating new advanced physico-chemical models and computational methods, based on a multidisciplinary approach developed together with physicists, chemists, and applied mathematicians, to create a top-notch multiphysics and multiscale numerical platform for simulations of planetary atmosphere entries, crucial to the new challenges of the manned space exploration program. Experimental data will also be used for validation, following state-of-the-art uncertainty quantification methods.”","1494892","2010-09-01","2015-08-31"
"AF and MSOGR","Automorphic Forms and Moduli Spaces of Galois Representations","Toby Gee","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","I propose to establish a research group to develop completely new tools in order to solve three important problems on the relationships between automorphic forms and Galois representations, which lie at the heart of the Langlands program. The first is to prove Serre’s conjecture for real quadratic fields. I will use automorphic induction to transfer the problem to U(4) over the rational numbers, where I will use automorphy lifting theorems and results on the weight part of Serre's conjecture that I established in my earlier work to reduce the problem to proving results in small weight and level. I will prove these base cases via integral p-adic Hodge theory and discriminant bounds.

The second is to develop a geometric theory of moduli spaces of mod p and p-adic Galois representations, and to use it to establish the Breuil–Mézard conjecture in arbitrary dimension, by reinterpreting the conjecture in geometric terms. This will transform the subject by building the first connections between the p-adic Langlands program and the geometric Langlands program, providing an entirely new world of techniques for number theorists. As a consequence of the Breuil-Mézard conjecture, I will be able to deduce far stronger automorphy lifting theorems (in arbitrary dimension) than those currently available.

The third is to completely determine the reduction mod p of certain two-dimensional crystalline representations, and as an application prove a strengthened version of the Gouvêa–Mazur conjecture. I will do this by means of explicit computations with the p-adic local Langlands correspondence for GL_2(Q_p), as well as by improving existing arguments which prove multiplicity one theorems via automorphy lifting theorems. This work will show that the existence of counterexamples to the Gouvêa-Mazur conjecture is due to a purely local phenomenon, and that when this local obstruction vanishes, far stronger conjectures of Buzzard on the slopes of the U_p operator hold.","1131339","2012-10-01","2017-09-30"
"AFFINITY","Actuation of Ferromagnetic Fibre Networks to improve Implant Longevity","Athina Markaki","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","This proposal is for an exploratory study into a radical new approach to the problem of orthopaedic implant loosening. Such loosening commonly occurs because the joint between the implant and the surrounding bone is insufficiently strong and durable. It is a serious problem both for implants cemented to the bone and for those dependent on bone in-growth into a rough/porous implant surface. In the latter case, the main problem is commonly that bone in-growth is insufficiently rapid or deep for a strong bond to be established. The idea proposed in this work is that the implant should have a highly porous surface layer, made by bonding ferromagnetic fibres together, into which bone tissue growth would occur. During the post-operative period, application of a magnetic field will cause the fibre network to deform elastically, as individual fibres tend to align with the field. This will impose strains on the bone tissue as it grows into the fibre network. Such mechanical deformation is known to be highly beneficial in promoting bone growth, providing the associated strain lies in a certain range (~0.1%). Preliminary work, involving both model development and experimental studies on the effect of magnetic fields on fibre networks, has suggested that beneficial therapeutic effects can be induced using field strengths no greater than those already employed for diagnostic purposes. A comprehensive 5-year, highly inter-disciplinary programme is planned, encompassing processing, network architecture characterisation, magneto-mechanical response investigations, various modelling activities and systematic in vitro experimentation to establish whether magneto-mechanical Actuation of Ferromagnetic Fibre Networks shows promise as a new therapeutic approach to improve implant longevity.","1442756","2010-01-01","2015-11-30"
"AFFIRM","Analysis of Biofilm Mediated Fouling of Nanofiltration Membranes","Eoin Casey","UNIVERSITY COLLEGE DUBLIN, NATIONAL UNIVERSITY OF IRELAND, DUBLIN","1.2 billion people worldwide lack access to safe drinking water. Drinking water quality is threatened by newly emerging organic micro-pollutants (pesticides, pharmaceuticals, industrial chemicals) in source waters. Nanofiltration is a technology that is expected to play a key role in future water treatment processes due to its effectiveness in removal of micropollutants. However, the loss of membrane flux due to fouling is one of the main impediments in the development of membrane processes for use in drinking water treatment. Currently there is a wholly inadequate mechanistic understanding of the role of biofilm on the fouling of nanofiltration membranes.
Applying techniques including confocal microscopy, force spectroscopy, and infrared spectroscopy using an experimental programme informed by a technique known as scale-down together with mathematical modelling, it is confidently expected that significant advances will be gained in the mechanistic understanding of nanofiltration biofouling.
The specific objectives are 1. How is the rate of formation and extent of such biofilms influenced by the biological response to the local microenvironment? 2 Elucidate the effect of extracellular polysaccharide substances on physical properties, composition and structure of these biofilms. 3: Investigate mechanisms to enhance biofilm removal by a physical detachment process complemented by techniques that alter biofilm material properties.
A more fundamental insight into the mechanisms of nanofiltration operation will help in further development of this treatment method in future water treatment processes.","1468987","2011-10-01","2016-09-30"
"AFRIVAL","African river basins: catchment-scale carbon fluxes and transformations","Steven Bouillon","KATHOLIEKE UNIVERSITEIT LEUVEN","This proposal wishes to fundamentally improve our understanding of the role of tropical freshwater ecosystems in carbon (C) cycling on the catchment scale. It uses an unprecedented combination of state-of-the-art proxies such as stable isotope, 14C and biomarker signatures to characterize organic matter, radiogenic isotope signatures to determine particle residence times, as well as field measurements of relevant biogeochemical processes. We focus on tropical systems since there is a striking lack of data on such systems, even though riverine C transport is thought to be disproportionately high in tropical areas. Furthermore, the presence of landscape-scale contrasts in vegetation (in particular, C3 vs. C4 plants) are an important asset in the use of stable isotopes as natural tracers of C cycling processes on this scale. Freshwater ecosystems are an important component in the global C cycle, and the primary link between terrestrial and marine ecosystems. Recent estimates indicate that ~2 Pg C y-1 (Pg=Petagram) enter freshwater systems, i.e., about twice the estimated global terrestrial C sink. More than half of this is thought to be remineralized before it reaches the coastal zone, and for the Amazon basin this has even been suggested to be ~90% of the lateral C inputs. The question how general these patterns are is a matter of debate, and assessing the mechanisms determining the degree of processing versus transport of organic carbon in lakes and river systems is critical to further constrain their role in the global C cycle. This proposal provides an interdisciplinary approach to describe and quantify catchment-scale C transport and cycling in tropical river basins. Besides conceptual and methodological advances, and a significant expansion of our dataset on C processes in such systems, new data gathered in this project are likely to provide exciting and novel hypotheses on the functioning of freshwater systems and their linkage to the terrestrial C budget.","1745262","2009-10-01","2014-09-30"
"AFRODITE","Advanced Fluid Research On Drag reduction In Turbulence Experiments","Jens Henrik Mikael Fransson","KUNGLIGA TEKNISKA HOEGSKOLAN","A hot topic in today's debate on global warming is drag reduction in aeronautics. The most beneficial concept for drag reduction is to maintain the major portion of the airfoil laminar. Estimations show that the potential drag reduction can be as much as 15%, which would give a significant reduction of NOx and CO emissions in the atmosphere considering that the number of aircraft take offs, only in the EU, is over 19 million per year. An important element for successful flow control, which can lead to a reduced aerodynamic drag, is enhanced physical understanding of the transition to turbulence process.

In previous wind tunnel measurements we have shown that roughness elements can be used to sensibly delay transition to turbulence. The result is revolutionary, since the common belief has been that surface roughness causes earlier transition and in turn increases the drag, and is a proof of concept of the passive control method per se. The beauty with a passive control technique is that no external energy has to be added to the flow system in order to perform the control, instead one uses the existing energy in the flow.

In this project proposal, AFRODITE, we will take this passive control method to the next level by making it twofold, more persistent and more robust. Transition prevention is the goal rather than transition delay and the method will be extended to simultaneously control separation, which is another unwanted flow phenomenon especially during airplane take offs. AFRODITE will be a catalyst for innovative research, which will lead to a cleaner sky.","1418399","2010-11-01","2015-10-31"
"AGALT","Asymptotic Geometric Analysis and Learning Theory","Shahar Mendelson","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","In a typical learning problem one tries to approximate an unknown function by a function from a given class using random data, sampled according to an unknown measure. In this project we will be interested in parameters that govern the complexity of a learning problem. It turns out that this complexity is determined by the geometry of certain sets in high dimension that are connected to the given class (random coordinate projections of the class). Thus, one has to understand the structure of these sets as a function of the dimension - which is given by the cardinality of the random sample. The resulting analysis leads to many theoretical questions in Asymptotic Geometric Analysis, Probability (most notably, Empirical Processes Theory) and Combinatorics, which are of independent interest beyond the application to Learning Theory. Our main goal is to describe the role of various complexity parameters involved in a learning problem, to analyze the connections between them and to investigate the way they determine the geometry of the relevant high dimensional sets. Some of the questions we intend to tackle are well known open problems and making progress towards their solution will have a significant theoretical impact. Moreover, this project should lead to a more complete theory of learning and is likely to have some practical impact, for example, in the design of more efficient learning algorithms.","750000","2009-03-01","2014-02-28"
"AGEnTh","Atomic Gauge and Entanglement Theories","Marcello DALMONTE","SCUOLA INTERNAZIONALE SUPERIORE DI STUDI AVANZATI DI TRIESTE","AGEnTh is an interdisciplinary proposal which aims at theoretically investigating atomic many-body systems (cold atoms and trapped ions) in close connection to concepts from quantum information, condensed matter, and high energy physics. The main goals of this programme are to:

I) Find to scalable schemes for the measurements of entanglement properties, and in particular entanglement spectra, by proposing a shifting paradigm to access entanglement focused on entanglement Hamiltonians and field theories instead of probing density matrices;

II) Show how atomic gauge theories (including dynamical gauge fields) are ideal candidates for the realization of long-sought, highly-entangled states of matter, in particular topological superconductors supporting parafermion edge modes, and novel classes of quantum spin liquids emerging from clustering;

III) Develop new implementation strategies for the realization of gauge symmetries of paramount importance, such as discrete and SU(N)xSU(2)xU(1) groups, and establish a theoretical framework for the understanding of atomic physics experiments within the light-from-chaos scenario pioneered in particle physics.

These objectives are at the cutting-edge of fundamental science, and represent a coherent effort aimed at underpinning unprecedented regimes of strongly interacting quantum matter by addressing the basic aspects of probing, many-body physics, and implementations. The results are expected to (i) build up and establish qualitatively new synergies between the aforementioned communities, and (ii) stimulate an intense theoretical and experimental activity focused on both entanglement and atomic gauge theories.

In order to achieve those, AGEnTh builds: (1) on my background working at the interface between atomic physics and quantum optics from one side, and many-body theory on the other, and (2) on exploratory studies which I carried out to mitigate the conceptual risks associated with its high-risk/high-gain goals.","1055317","2018-05-01","2023-04-30"
"AGGLONANOCOAT","The interplay between agglomeration and coating of nanoparticles in the gas phase","Jan Rudolf Van Ommen","TECHNISCHE UNIVERSITEIT DELFT","This proposal aims to develop a generic synthesis approach for core-shell nanoparticles by unravelling the relevant mechanisms. Core-shell nanoparticles have high potential in heterogeneous catalysis, energy storage, and medical applications. However, on a fundamental level there is currently a poor understanding of how to produce such nanostructured particles in a controllable and scalable manner.

The main barriers to achieving this goal are understanding how nanoparticles agglomerate to loose dynamic clusters and controlling the agglomeration process in gas flows during coating, such that uniform coatings can be made. This is very challenging because of the two-way coupling between agglomeration and coating. During the coating we change the particle surfaces and thus the way the particles stick together. Correspondingly, the stickiness of particles determines how easy reactants can reach the surface.

Innovatively the project will be the first systematic study into this multi-scale phenomenon with investigations at all relevant length scales. Current synthesis approaches – mostly carried out in the liquid phase – are typically developed case by case. I will coat nanoparticles in the gas phase with atomic layer deposition (ALD): a technique from the semi-conductor industry that can deposit a wide range of materials. ALD applied to flat substrates offers excellent control over layer thickness. I will investigate the modification of single particle surfaces, particle-particle interaction, the structure of agglomerates, and the flow behaviour of large number of agglomerates. To this end, I will apply a multidisciplinary approach, combining disciplines as physical chemistry, fluid dynamics, and reaction engineering.","1409952","2011-12-01","2016-11-30"
"AIDA","An Illumination of the Dark Ages: modeling reionization and interpreting observations","Andrei Albert Mesinger","SCUOLA NORMALE SUPERIORE","""Understanding the dawn of the first galaxies and how their light permeated the early Universe is at the very frontier of modern astrophysical cosmology.  Generous resources, including ambitions observational programs, are being devoted to studying these epochs of Cosmic Dawn (CD) and Reionization (EoR).  In order to interpret these observations, we propose to build on our widely-used, semi-numeric simulation tool, 21cmFAST, and apply it to observations.  Using sub-grid, semi-analytic models, we will incorporate additional physical processes governing the evolution of sources and sinks of ionizing photons. The resulting state-of-the-art simulations will be well poised to interpret topical observations of quasar spectra and the cosmic 21cm signal.  They would be both physically-motivated and fast, allowing us to rapidly explore astrophysical parameter space.  We will statistically quantify the resulting degeneracies and constraints, providing a robust answer to the question, """"What can we learn from EoR/CD observations?"""" As an end goal, these investigations will help us understand when the first generations of galaxies formed, how they drove the EoR, and what are the associated large-scale observational signatures.""","1468750","2015-05-01","2021-01-31"
"AISENS","New generation of high sensitive atom interferometers","Marco Fattori","CONSIGLIO NAZIONALE DELLE RICERCHE","Interferometers are fundamental tools for the study of nature laws and for the precise measurement and control of the physical world. In the last century, the scientific and technological progress has proceeded in parallel with a constant improvement of interferometric performances. For this reason, the challenge of conceiving and realizing new generations of interferometers with broader ranges of operation and with higher sensitivities is always open and actual.
Despite the introduction of laser devices has deeply improved the way of developing and performing interferometric measurements with light, the atomic matter wave analogous, i.e. the Bose-Einstein condensate (BEC), has not yet triggered any revolution in precision interferometry. However, thanks to recent improvements on the control of the quantum properties of ultra-cold atomic gases, and new original ideas on the creation and manipulation of quantum entangled particles, the field of atom interferometry is now mature to experience a big step forward.
The system I want to realize is a Mach-Zehnder spatial interferometer operating with trapped BECs. Undesired decoherence sources will be suppressed by implementing BECs with tunable interactions in ultra-stable optical potentials. Entangled states will be used to improve the sensitivity of the sensor beyond the standard quantum limit to ideally reach the ultimate, Heisenberg, limit set by quantum mechanics. The resulting apparatus will show unprecedented spatial resolution and will overcome state-of-the-art interferometers with cold (non condensed) atomic gases.
A successful completion of this project will lead to a new generation of interferometers for the immediate application to local inertial measurements with unprecedented resolution. In addition, we expect to develop experimental capabilities which might find application well beyond quantum interferometry and crucially contribute to the broader emerging field of quantum-enhanced technologies.","1068000","2011-01-01","2015-12-31"
"AlCat","Bond activation and catalysis with low-valent aluminium","Michael James COWLEY","THE UNIVERSITY OF EDINBURGH","This project will develop the principles required to enable bond-modifying redox catalysis based on aluminium by preparing and studying new Al(I) compounds capable of reversible oxidative addition.

Catalytic processes are involved in the synthesis of 75 % of all industrially produced chemicals, but most catalysts involved are based on precious metals such as rhodium, palladium or platinum. These metals are expensive and their supply limited and unstable; there is a significant need to develop the chemistry of non-precious metals as alternatives. On toxicity and abundance alone, aluminium is an attractive candidate. Furthermore, recent work, including in our group, has demonstrated that Al(I) compounds can perform a key step in catalytic cycles - the oxidative addition of E-H bonds.

In order to realise the significant potential of Al(I) for transition-metal style catalysis we urgently need to:
- establish the principles governing oxidative addition and reductive elimination reactivity in aluminium systems.
- know how the reactivity of Al(I) compounds can be controlled by varying properties of ligand frameworks.
- understand the onward reactivity of oxidative addition products of Al(I) to enable applications in catalysis.
 In this project we will:

- Study mechanisms of oxidative addition and reductive elimination of a range of synthetically relevant bonds at Al(I) centres, establishing the principles governing this fundamental reactivity.
- Develop new ligand frameworks to support of Al(I) centres and evaluate the effect of the ligand on oxidative addition/reductive elimination at Al centres.
- Investigate methods for Al-mediated functionalisation of organic compounds by exploring the reactivity of E-H oxidative addition products with unsaturated organic compounds.","1493679","2017-03-01","2022-02-28"
"ALGILE","Foundations of Algebraic and Dynamic Data Management Systems","Christoph Koch","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","""Contemporary database query languages are ultimately founded on logic and feature an additive operation – usually a form of (multi)set union or disjunction – that is asymmetric in that additions or updates do not always have an inverse. This asymmetry puts a greater part of the machinery of abstract algebra for equation solving outside the reach of databases. However, such equation solving would be a key functionality that problems such as query equivalence testing and data integration could be reduced to: In the current scenario of the presence of an asymmetric additive operation they are undecidable. Moreover, query languages with a symmetric additive operation (i.e., which has an inverse and is thus based on ring theory) would open up databases for a large range of new scientific and mathematical applications.
The goal of the proposed project is to reinvent database management systems with a foundation in abstract algebra and specifically in ring theory. The presence of an additive inverse allows to cleanly define differences between queries. This gives rise to a database analog of differential calculus that leads to radically new incremental and adaptive query evaluation algorithms that substantially outperform the state of the art techniques. These algorithms enable a new class of systems which I call Dynamic Data Management Systems. Such systems can maintain continuously fresh query views at extremely high update rates and have important applications in interactive Large-scale Data Analysis. There is a natural connection between differences and updates, motivating the group theoretic study of updates that will lead to better ways of creating out-of-core data processing algorithms for new storage devices. Basing queries on ring theory leads to a new class of systems, Algebraic Data Management Systems, which herald a convergence of database systems and computer algebra systems.""","1480548","2012-01-01","2016-12-31"
"ALGOCom","Novel Algorithmic Techniques through the Lens of Combinatorics","Parinya Chalermsook","AALTO KORKEAKOULUSAATIO SR","Real-world optimization problems pose major challenges to algorithmic research. For instance, (i) many important  problems are believed to be intractable (i.e. NP-hard) and (ii) with the growth of data size, modern applications often require a decision making under {\em incomplete and  dynamically changing input data}. After several decades of research, central  problems in these domains have remained  poorly understood (e.g. Is there an asymptotically most efficient binary search trees?)  Existing algorithmic techniques either  reach their limitation or are inherently tailored to   special cases.   

This project attempts to untangle this gap in the state of the art and seeks new interplay across  multiple areas of algorithms, such as approximation algorithms, online algorithms, fixed-parameter tractable (FPT) algorithms,  exponential time algorithms, and data structures. We propose new directions from the {\em structural perspectives} that connect the aforementioned algorithmic problems to basic questions in combinatorics.
Our approaches fall into one of the three broad schemes: (i) new structural theory, (ii) intermediate problems, and (iii) transfer of techniques. These directions  partially build on the PI's successes in resolving more than ten classical  problems in this context. 

Resolving the proposed problems will likely revolutionize our understanding about algorithms and data structures and potentially unify techniques in  multiple algorithmic regimes. Any progress is, in fact, already a significant contribution to the algorithms community. We suggest concrete intermediate goals that are of independent interest and have lower risks, so they are suitable for Ph.D students.","1411258","2018-02-01","2023-01-31"
"AlgTateGro","Constructing line bundles on algebraic varieties --around conjectures of Tate and Grothendieck","François CHARLES","UNIVERSITE PARIS-SUD","The goal of this project is to investigate two conjectures in arithmetic geometry pertaining to the geometry of projective varieties over finite and number fields. These two conjectures, formulated by Tate and Grothendieck in the 1960s, predict which cohomology classes are chern classes of line bundles. They both form an arithmetic counterpart of a theorem of Lefschetz, proved in the 1940s, which itself is the only known case of the Hodge conjecture. These two long-standing conjectures are one of the aspects of a more general web of questions regarding the topology of algebraic varieties which have been emphasized by Grothendieck and have since had a central role in modern arithmetic geometry. Special cases of these conjectures, appearing for instance in the work of Tate, Deligne, Faltings, Schneider-Lang, Masser-Wüstholz, have all had important consequences.

My goal is to investigate different lines of attack towards these conjectures, building on recent work on myself and Jean-Benoît Bost on related problems. The two main directions of the proposal are as follows. Over finite fields, the Tate conjecture is related to finiteness results for certain cohomological objects. I want to understand how to relate these to hidden boundedness properties of algebraic varieties that have appeared in my recent geometric proof of the Tate conjecture for K3 surfaces. The existence and relevance of a theory of Donaldson invariants for moduli spaces of twisted sheaves over finite fields seems to be a promising and novel direction. Over number fields, I want to combine the geometric insight above with algebraization techniques developed by Bost. In a joint project, we want to investigate how these can be used to first understand geometrically major results in transcendence theory and then attack the Grothendieck period conjecture for divisors via a number-theoretic and complex-analytic understanding of universal vector extensions of abelian schemes over curves.","1222329","2016-12-01","2021-11-30"
"ALIGN","Ab-initio computational modelling of photovoltaic interfaces","Feliciano Giustino","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The aim of the ALIGN project is to understand, predict, and optimize the photovoltaic energy conversion in third-generation solar cells, starting from an atomic-scale quantum-mechanical modelling of the photovoltaic interface. The quest for photovoltaic materials suitable for low-cost synthesis, large-area production, and functional architecture has driven substantial research efforts towards third-generation photovoltaic devices such as plastic solar cells, organic-inorganic cells, and photo-electrochemical cells. The physical and chemical processes involved in the harvesting of sunlight, the transport of electrical charge, and the build-up of the photo-voltage in these devices are fundamentally different from those encountered in traditional semiconductor heterojunction solar cells. A detailed atomic-scale quantum-mechanical description of such processes will lay down the basis for a rational approach to the modelling, optimization, and design of new photovoltaic materials. The short name of the proposal hints at one of the key materials parameters in the area of photovoltaic interfaces: the alignment of the quantum energy levels between the light-absorbing material and the electron acceptor. The level alignment drives the separation of the electron-hole pairs formed upon absorption of sunlight, and determines the open circuit voltage of the solar cell. The energy level alignment not only represents a key parameter for the design of photovoltaic devices, but also constitutes one of the grand challenges of modern computational materials science. Within this project we will develop and apply new ground-breaking computational methods to understand, predict, and optimize the energy level alignment and other design parameters of third-generation photovoltaic devices.","1000000","2010-03-01","2016-02-29"
"ALKENoNE","Algal Lipids: the Key to Earth Now and aNcient Earth","Jaime Lynn Toney","UNIVERSITY OF GLASGOW","Alkenones are algal lipids that have been used for decades to reconstruct quantitative past sea surface temperature. Although alkenones are being discovered in an increasing number of lake sites worldwide, only two terrestrial temperature records have been reconstructed so far. The development of this research field is limited by the lack of interdisciplinary research that combines modern biological and ecological algal research with the organic geochemical techniques needed to develop a quantitative biomarker (or molecular fossil) for past lake temperatures. More research is needed for alkenones to become a widely used tool for reconstructing past terrestrial temperature change. The early career Principal Investigator has discovered a new lake alkenone-producing species of haptophyte algae that produces alkenones in high abundances both in the environment and in laboratory cultures. This makes the new species an ideal organism for developing a culture-based temperature calibration and exploring other potential environmental controls. In this project, alkenone production will be manipulated, and monitored using state-of-the-art photobioreactors with real-time detectors for cell density, light, and temperature. The latest algal culture and isolation techniques that are used in microalgal biofuel development will be applied to developing the lake temperature proxy. The objectives will be achieved through the analysis of 90 new Canadian lakes to develop a core-top temperature calibration across a large latitudinal and temperature gradient (Δ latitude = 5°, Δ spring surface temperature = 9°C). The results will be used to assess how regional palaeo-temperature (Uk37), palaeo-moisture (δDwax) and palaeo-evaporation (δDalgal) respond during times of past global warmth (e.g., Medieval Warm Period, 900-1200 AD) to find an accurate analogue for assessing future drought risk in the interior of Canada.","940883","2015-04-01","2020-03-31"
"ALLQUANTUM","All-solid-state quantum electrodynamics in photonic crystals","Peter Lodahl","KOBENHAVNS UNIVERSITET","In quantum electrodynamics a range of fundamental processes are driven by omnipresent vacuum fluctuations. Photonic crystals can control vacuum fluctuations and thereby the fundamental interaction between light and matter. We will conduct experiments on quantum dots in photonic crystals and observe novel quantum electrodynamics effects including fractional decay and the modified Lamb shift. Furthermore, photonic crystals will be explored for shielding sensitive quantum-superposition states against decoherence.

Defects in photonic crystals allow novel functionalities enabling nanocavities and waveguides. We will use the tight confinement of light in a nanocavity to entangle a quantum dot and a photon, and explore the scalability. Controlled ways of generating scalable and robust quantum entanglement is the essential  missing link  limiting quantum communication and quantum computing. A single quantum dot coupled to a slowly propagating mode in a photonic crystal waveguide will be used to induce large nonlinearities at the few-photon level.

Finally we will explore a novel route to enhanced light-matter interaction employing controlled disorder in photonic crystals. In disordered media multiple scattering of light takes place and can lead to the formation of Anderson-localized modes. We will explore cavity quantum electrodynamics in Anderson-localized random cavities considering disorder a resource and not a nuisance, which is the traditional view.

The main focus of the project will be on optical experiments, but fabrication of photonic crystals and detailed theory will be carried out as well. Several of the proposed experiments will constitute milestones in quantum optics and may pave the way for all-solid-state quantum communication with quantum dots in photonic crystals.","1199648","2010-12-01","2015-11-30"
"ALOGLADIS","From Anderson localization to Bose, Fermi and spin glasses in disordered ultracold gases","Laurent Sanchez-Palencia","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The field of disordered quantum gases is developing rapidly. Dramatic progress has been achieved recently and first experimental observation of one-dimensional Anderson localization (AL) of matterwaves has been reported using Bose-Einstein condensates in controlled disorder (in our group at Institut d'Optique and at LENS; Nature, 2008). This dramatic success results from joint theoretical and experimental efforts, we have contributed to. Most importantly, it opens unprecedented routes to pursue several outstanding challenges in the multidisciplinary field of disordered systems, which, after fifty years of Anderson localization, is more active than ever.
This theoretical project aims at further developing the emerging field of disordered quantum gases towards novel challenges. Our aim is twofold. First, we will propose and analyze schemes where experiments on ultracold atoms can address unsolved issues: AL in dimensions higher than one, effects of inter-atomic interactions on AL, strongly-correlated disordered gases and quantum simulators for spin systems (spin glasses). Second, by taking into account specific features of ultracold atoms, beyond standard toy-models, we will raise and study new questions which have not been addressed before (eg long-range correlations of speckle potentials, finite-size effects, controlled interactions). Both aspects would open new frontiers to disordered quantum gases and offer new possibilities to shed new light on highly debated issues.
Our main concerns are thus to (i) study situations relevant to experiments, (ii) develop new approaches, applicable to ultracold atoms, (iii) identify key observables, and (iv) propose new challenging experiments. In this project, we will benefit from the original situation of our theory team: It is independent but forms part of a larger group (lead by A. Aspect), which is a world-leader in experiments on disordered quantum gases, we have already developed close collaborative relationship with.","985200","2011-01-01","2015-12-31"
"ALORS","Advanced Lagrangian Optimization, Receptivity and Sensitivity analysis applied to industrial situations","Matthew Pudan Juniper","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","In the last ten years there has been a surge of interest in non-modal analysis applied to canonical problems in fundamental fluid mechanics. Even in simple flows, the stability behaviour predicted by non-modal analysis can be completely different from   and far more accurate than   that predicted by conventional eigenvalue analysis.

As well as being more accurate, the tools of non-modal analysis, such as Lagrangian optimization, are very versatile. Furthermore, the outputs, such as receptivity and sensitivity maps of a flow, provide powerful insight for engineers. They describe where a flow is most receptive to forcing or where the flow is most sensitive to modification.

The application of non-modal analysis to canonical problems has set the scene for step changes in engineering practice in fluid mechanics and thermoacoustics. The technical objectives of this proposal are to apply non-modal analysis to high Reynolds number flows, reacting flows and thermoacoustic systems, to compare theoretical predictions with experimental measurements and to embed these techniques within an industrial design tool that has already been developed by the group.

This research group s vision is that future generations of engineering CFD tools will contain modules that can perform non-modal analysis. The generalized approach proposed here, combined with challenging scientific and engineering examples that are backed up by experimental evidence, will make this possible and demonstrate it to a wider engineering community.","1301196","2010-12-01","2016-06-30"
"AlterMateria","Designer Quantum Materials Out of Equilibrium","Andrea Caviglia","TECHNISCHE UNIVERSITEIT DELFT","Recently, ‘designer’ quantum materials, synthesised layer by layer, have been realised, sparking ground-breaking new scientific insights. These artificial materials, such as oxide heterostructures, are interesting building blocks for a new generation of technologies, provided that one is able to access, study and ultimately control their quantum phases in practical conditions such as at room temperature and high speeds.
On the other hand, an independent research area is emerging that uses ultra-short bursts of light to stimulate changes in the macroscopic electronic properties of solids at unprecedented speeds.
 
Here I propose to bridge the gap between material design and ultrafast control of solids. This new synergy will allow us to explore fundamental research questions on the non-equilibrium dynamics of quantum materials with competing ground states. Specifically, I will utilize intense THz and mid-infrared electromagnetic fields to manipulate the electronic properties of artificial quantum materials on pico- to femto-second time scales. Beyond the development of novel techniques to generate THz electric fields of unprecedented intensity, I will investigate metal-insulator and magnetic transitions in oxide heterostructures as they unfold in time. This research programme takes oxide electronics in a new direction and establishes a new methodology for the control of quantum phases at high temperature and high speed.","1499982","2016-06-01","2021-05-31"
"ALUFIX","Friction stir processing based local damage mitigation and healing in aluminium alloys","Aude SIMAR","UNIVERSITE CATHOLIQUE DE LOUVAIN","ALUFIX proposes an original strategy for the development of aluminium-based materials involving damage mitigation and extrinsic self-healing concepts exploiting the new opportunities of the solid-state friction stir process. Friction stir processing locally extrudes and drags material from the front to the back and around the tool pin. It involves short duration at moderate temperatures (typically 80% of the melting temperature), fast cooling rates and large plastic deformations leading to far out-of-equilibrium microstructures. The idea is that commercial aluminium alloys can be locally improved and healed in regions of stress concentration where damage is likely to occur. Self-healing in metal-based materials is still in its infancy and existing strategies can hardly be extended to applications. Friction stir processing can enhance the damage and fatigue resistance of aluminium alloys by microstructure homogenisation and refinement. In parallel, friction stir processing can be used to integrate secondary phases in an aluminium matrix. In the ALUFIX project, healing phases will thus be integrated in aluminium in addition to refining and homogenising the microstructure. The “local stress management strategy” favours crack closure and crack deviation at the sub-millimetre scale thanks to a controlled residual stress field. The “transient liquid healing agent” strategy involves the in-situ generation of an out-of-equilibrium compositionally graded microstructure at the aluminium/healing agent interface capable of liquid-phase healing after a thermal treatment. Along the road, a variety of new scientific questions concerning the damage mechanisms will have to be addressed.","1497447","2017-01-01","2021-12-31"
"aLzINK","Alzheimer's disease and Zinc: the missing link ?","Christelle Sandrine Florence HUREAU-SABATER","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Alzheimer's disease (AD) is one of the most serious diseases mankind is now facing as its social and economical impacts are increasing fastly. AD is very complex and the amyloid-β (Aβ) peptide as well as metallic ions (mainly copper and zinc) have been linked to its aetiology. While the deleterious impact of Cu is widely acknowledged, intervention of Zn is certain but still needs to be figured out.      
  
The main objective of the present proposal, which is strongly anchored in the bio-inorganic chemistry field at interface with spectroscopy and biochemistry, is to design, synthesize and study new drug candidates (ligands L) capable of (i) targeting Cu(II) bound to Aβ within the synaptic cleft, where Zn is co-localized and ultimately to develop Zn-driven Cu(II) removal from Aβ and (ii) disrupting the aberrant Cu(II)-Aβ interactions involved in ROS production and Aβ aggregation, two deleterious events in AD. The drug candidates will thus have high Cu(II) over Zn selectively to preserve the crucial physiological role of Zn in the neurotransmission process. Zn is always underestimated (if not completely neglected) in current therapeutic approaches targeting Cu(II) despite the known interference of Zn with Cu(II) binding. 

To reach this objective, it is absolutely necessary to first understand the metal ions trafficking issues in presence of Aβ alone at a molecular level (i.e. without the drug candidates).This includes: (i) determination of Zn binding site to Aβ, impact on Aβ aggregation and cell toxicity, (ii) determination of the mutual influence of Zn and Cu to their coordination to Aβ, impact on Aβ aggregation, ROS production and cell toxicity.

Methods used will span from organic synthesis to studies of neuronal model cells, with a major contribution of a wide panel of spectroscopic techniques including NMR, EPR, mass spectrometry, fluorescence, UV-Vis, circular-dichroism, X-ray absorption spectroscopy...","1499948","2015-03-01","2020-02-29"
"AMD","Algorithmic Mechanism Design: Beyond Truthful Mechanisms","Michal Feldman","TEL AVIV UNIVERSITY","""The first decade of Algorithmic Mechanism Design (AMD) concentrated, very successfully, on the design of truthful mechanisms for the allocation of resources among agents with private preferences.
Truthful mechanisms are ones that incentivize rational users to report their preferences truthfully.
Truthfulness, however, for all its theoretical appeal, suffers from several inherent limitations, mainly its high communication and computation complexities.
It is not surprising, therefore, that practical applications forego truthfulness and use simpler mechanisms instead.
Simplicity in itself, however, is not sufficient, as any meaningful mechanism should also have some notion of fairness; otherwise agents will stop using it over time.

In this project I plan to develop an innovative AMD theoretical framework that will go beyond truthfulness and focus instead on the natural themes of simplicity and fairness, in addition to computational tractability.
One of my primary goals will be the design of simple and fair poly-time mechanisms that perform at near optimal levels with respect to important economic objectives such as social welfare and revenue.
To this end, I will work toward providing precise definitions of simplicity and fairness and quantifying the effects of these restrictions on the performance levels that can be obtained.
A major challenge in the evaluation of non-truthful mechanisms is defining a reasonable behavior model that will enable their evaluation.

The success of this project could have a broad impact on Europe and beyond, as it would guide the design of natural mechanisms for markets of tens of billions of dollars in revenue, such as online advertising, or sales of wireless frequencies.
The timing of this project is ideal, as the AMD field is now sufficiently mature to lead to a breakthrough and at the same time young enough to be receptive to new approaches and themes.""","1394600","2013-11-01","2018-10-31"
"AMOPROX","Quantifying Aerobic Methane Oxidation in the Ocean: Calibration and palaeo application of a novel proxy","Helen Marie Talbot","UNIVERSITY OF NEWCASTLE UPON TYNE","Methane, a key greenhouse gas, is cycled by microorganisms via two pathways, aerobically and anaerobically. Research on the
marine methane cycle has mainly concentrated on anaerobic processes. Recent biomarker work has provided compelling
evidence that aerobic methane oxidation (AMO) can play a more significant role in cycling methane emitted from sediments than
previously considered. AMO, however, is not well studied requiring novel proxies that can be applied to the sedimentary record. A
group of complex lipids biosynthesised by aerobic methanotrophs known as aminobacteriohopanepolyols represent an ideal target
for developing such poxies. Recently BHPs have been identified in a wide range of modern and recent environments including a
continuous record from the Congo deep sea fan spanning the last 1.2 million years.
In this integrated study, the regulation and expression of BHP will be investigated and calibrated against environmental variables
including temperature, pH, salinity and, most importantly, methane concentrations. The work program has three complementary
strands. (1) Pure culture and sedimentary microcosm experiments providing an approximation to natural conditions. (2) Calibration
of BHP signatures in natural marine settings (e.g. cold seeps, mud volcanoes, pockmarks) against measured methane gradients.
(3) Application of this novel approach to the marine sedimentary record to approximate methane fluxes in the past, explore the age
and bathymetric limits of this novel molecular proxy, and identify and potentially 14C date palaeo-pockmarks structures. Crucial to
the success is also the refinement of the analytical protocols to improve both accuracy and sensitivity, using a more sensitive
analytical instrument (triple-quadrupole mass spectrometer).","1496392","2010-11-01","2016-04-30"
"AMPCAT","Self-Amplifying Stereodynamic Catalysts in Enantioselective Catalysis","Oliver Trapp","RUPRECHT-KARLS-UNIVERSITAET HEIDELBERG","Think about an enantioselective catalyst, which can switch its enantioselectivity and which can be imprinted and provides self-amplification by its own chiral reaction product. Think about a catalyst, which can be fine-tuned for efficient stereoselective synthesis of drugs and other materials, e.g. polymers.
Highly promising reactions such as enantioselective autocatalysis (Soai reaction) and chiral catalysts undergoing dynamic interconversions, e.g. BIPHEP ligands, are still not understood. Their application is very limited to a few compounds, which opens the field for novel investigations.
I propose the development of a smart or switchable chiral ligand undergoing dynamic interconversions. These catalysts will be tuned by their reaction product, and this leads to self-amplification of one of the stereoisomers. I propose a novel fundamental mechanism which has the potential to overcome the limitations of the Soai reaction, exploiting the full potential of enantioselective catalysis.
As representatives of enantioselective self-amplifying stereodynamic catalysts a novel class of diazirine based ligands will be developed, their interconversion barrier is tuneable between 80 and 130 kJ/mol. Specifically, following areas will be explored:
1. Investigation of the kinetics and thermodynamics of the Soai reaction as a model reaction by analysis of large sets of kinetic data.
2. Ligands with diaziridine moieties with flexible structure will be designed and investigated, to control the enantioselectivity.
3. Design of a ligand receptor group for product interaction to switch the chirality. Study of self-amplification in enantioselective processes.
4. Enantioselective hydrogenations, Diels-Alder reactions, epoxidations and reactions generating multiple stereocenters will be targeted.","1452000","2010-12-01","2016-05-31"
"AMPLITUDES","Manifesting the Simplicity of Scattering Amplitudes","Jacob BOURJAILY","KOBENHAVNS UNIVERSITET","I propose a program of research that may forever change the way that we understand and use quantum field theory to make predictions for experiment. This will be achieved through the advancement of new, constructive frameworks to determine and represent scattering amplitudes in perturbation theory in terms that depend only on observable quantities, make manifest (all) the symmetries of the theory, and which can be efficiently evaluated while minimally spoiling the underlying simplicity of predictions. My research has already led to the discovery and development of several approaches of this kind. 

This proposal describes the specific steps required to extend these ideas to more general theories and to higher orders of perturbation theory. Specifically, the plan of research I propose consists of three concrete goals: to fully characterize the discontinuities of loop amplitudes (`on-shell functions') for a broad class of theories; to develop powerful new representations of loop amplitude {\it integrands}, making manifest as much simplicity as possible; and to develop new techniques for loop amplitude {integration} that are compatible with and preserve the symmetries of observable quantities.

Progress toward any one of these objectives would have important theoretical implications and valuable practical applications. In combination, this proposal has the potential to significantly advance the state of the art for both our theoretical understanding and our computational reach for making predictions for experiment.

To achieve these goals, I will pursue a data-driven, `phenomenological' approach—involving the construction of new computational tools, developed in pursuit of concrete computational targets. For this work, my suitability and expertise is amply demonstrated by my research. I have not only played a key role in many of the most important theoretical developments in the past decade, but I have personally built the most powerful computational tools for their","1499695","2018-02-01","2023-01-31"
"AMPRO","Advanced Electronic Materials and Devices through  Novel Processing Paradigms","Thomas Anthopoulos","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","""I propose a structured multidisciplinary research programme that seeks to combine advanced materials, such as metal oxides and organics, with novel fabrication methods to develop devices for application in: (1) large area electronics, (2) integrated nanoelectronics and (3) sensors. At the heart of this programme lies the development of novel oxide semiconductors. These will be synthesised from solution using precursors. Chemical doping via physical blending will be explored for the tuning of the electronic properties of these compounds. This simple approach will enable the rapid development of a library of materials far beyond those accessible by traditional methods. Oxides will then be combined with inorganic/organic dielectrics to demonstrate low power transistors. Ultimate target for application area (1) is the development of transistors with hole/electron mobilities exceeding 20/200 cm^2/Vs respectively. For application area (2) I will combine the precursor formulations with advanced scanning thermochemical nanolithography. A heated atomic force microscope tip will be used for the local chemical conversion of the precursor to oxide with sub-50 nm resolution. This will enable patterning of nanostructures with desirable shape and size. Sequential patterning of semi/conductive layers combined with SAM dielectrics would enable fabrication of nano-sized devices and circuits. For application area (3), research effort will focus on novel hybrid phototransistors. Use of different light absorbing organic dyes functionalised onto the oxide channel will be explored as a mean for developing high sensitivity phototransistors and full colour sensing arrays. Organic dyes will also be combined with nano-sized transistors to demonstrate integrated nano-scale optoelectronics. The unique combination of bottom-up and top-down strategies adopted in this project will lead to the development of novel high performance devices with a host of existing and new applications.""","1497798","2012-01-01","2016-12-31"
"ANADEL","Analysis of Geometrical Effects on Dispersive Equations","Danela Oana IVANOVICI","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","We are concerned with localization properties of solutions to hyperbolic PDEs, especially problems with a geometric component: how do boundaries and heterogeneous media influence spreading and concentration of solutions. While our first focus is on wave and Schrödinger equations on manifolds with boundary, strong connections exist with phase space localization for (clusters of) eigenfunctions, which are of independent interest. Motivations come from nonlinear dispersive models (in physically relevant settings), properties of eigenfunctions in quantum chaos (related to both physics of optic fiber design as well as number theoretic questions), or harmonic analysis on manifolds.

Waves propagation in real life physics occur in media which are neither homogeneous or spatially infinity. The birth of radar/sonar technologies (and the raise of computed tomography) greatly motivated numerous developments in microlocal analysis and the linear theory. Only recently toy nonlinear models have been studied on a curved background, sometimes compact or rough. Understanding how to extend such tools, dealing with wave dispersion or focusing, will allow us to significantly progress in our mathematical understanding of physically relevant models. There, boundaries appear naturally and most earlier developments related to propagation of singularities in this context have limited scope with respect to crucial dispersive effects. Despite great progress over the last decade, driven by the study of quasilinear equations, our knowledge is still very limited. Going beyond this recent activity requires new tools whose development is at the heart of this proposal, including good approximate solutions (parametrices) going over arbitrarily large numbers of caustics, sharp pointwise bounds on Green functions, development of efficient wave packets methods, quantitative refinements of propagation of singularities (with direct applications in control theory), only to name a few important ones.","1293763","2018-02-01","2023-01-31"
"ANALYTIC","ANALYTIC PROPERTIES OF INFINITE GROUPS:
limits, curvature, and randomness","Gulnara Arzhantseva","UNIVERSITAT WIEN","The overall goal of this project is to develop new concepts and techniques in geometric and asymptotic group theory for a systematic study of the analytic properties of discrete groups. These are properties depending on the unitary representation theory of the group. The fundamental  examples are amenability, discovered by von Neumann in 1929, and property (T), introduced by Kazhdan in 1967.

My main objective is to establish the precise relations between groups recently appeared in K-theory and topology such as C*-exact groups and groups coarsely embeddable into a Hilbert space, versus those discovered in ergodic theory and operator algebra, for example, sofic and hyperlinear groups. This is a first ever attempt to confront the analytic behavior of so different nature. I plan to work on crucial open questions: Is every coarsely embeddable group C*-exact? Is every group sofic? Is every hyperlinear group sofic?

My motivation is two-fold:
- Many outstanding conjectures were recently solved for these groups, e.g. the Novikov conjecture (1965) for coarsely embeddable groups by Yu in 2000 and the Gottschalk surjunctivity conjecture (1973) for sofic groups by Gromov in 1999. However, their group-theoretical structure remains mysterious.
- In recent years, geometric group theory has undergone significant changes, mainly due to the growing impact of this theory on other branches of mathematics. However, the interplay between geometric, asymptotic, and analytic group properties has not yet been fully understood.

The main innovative contribution of this proposal lies in the interaction between 3 axes: (i) limits of groups, in the space of marked groups or metric ultralimits; (ii) analytic properties of groups with curvature, of lacunary or relatively hyperbolic groups; (iii) random groups, in a topological or statistical meaning. As a result, I will describe the above apparently unrelated classes of groups in a unified way and will detail their algebraic behavior.","1065500","2011-04-01","2016-03-31"
"ANAMORPHISM","Asymptotic and Numerical Analysis of MOdels of Resonant Physics Involving Structured Materials","Sebastien Roger Louis Guenneau","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","One already available method to expand the range of material properties is to adjust the composition of materials at the molecular level using chemistry. We would like to develop the alternative approach of homogenization which broadens the definition of a material to include artificially structured media (fluids and solids) in which the effective electromagnetic, hydrodynamic or elastic responses result from a macroscopic patterning or arrangement of two or more distinct materials. This project will explore the latter avenue in order to markedly enhance control of surface water waves and elastodynamic waves propagating within artificially structured fluids and solid materials, thereafter called acoustic metamaterials.

Pendry's perfect lens, the paradigm of electromagnetic metamaterials, is a slab of negative refractive index material that takes rays of light and causes them to converge with unprecedented resolution. This flat lens is a combination of periodically arranged resonant electric and magnetic elements. We will draw systematic analogies with resonant mechanical systems in order to achieve similar control of hydrodynamic and elastic waves. This will allow us to extend the design of metamaterials to acoustics to go beyond the scope of Snell-Descartes' laws of optics and Newton's laws of mechanics.

Acoustic metamaterials allow the construction of invisibility cloaks for non-linear surface water waves (e.g. tsunamis) propagating in structured fluids, as well as seismic waves propagating in thin structured elastic plates.
Maritime and civil engineering applications are in the protection of harbours, off-shore platforms and anti-earthquake passive systems. Acoustic cloaks for an enhanced control of pressure waves in fluids will be also designed for underwater camouflaging.

Light and sound interplay will be finally analysed in order to design controllable metamaterials with a special emphasis on undetectable microstructured fibres (acoustic wormholes).","1280391","2011-10-01","2016-09-30"
"ANaPSyS","Artificial Natural Products System Synthesis","Tanja Gaich","UNIVERSITAT KONSTANZ","""Traditionally, natural products are classified into """"natural product families"""". Within a family all congeners display specific structure elements, owing to their common biosynthetic pathway. This suggests a bio-inspired or """"collective synthesis"""", as has been devised by D: W. MacMillan. However, a biosynthetic pathway is confined to these structure elements, thus limiting synthesis with regard to structure diversification. In this research proposal the applicant exemplarily devises a strategic concept to overcome these limitations, by replacing the dogma of """"retrosynthetic analysis"""" with """"structure pattern recognition"""". This concept is termed """"Artificial Natural Product Systems Synthesis — ANaPSyS"""", and aims to supersede the current """"logic of chemical synthesis"""" as a standard practice in this field.
ANaPSyS exclusively categorizes natural products based on structural relationships — regardless of biogenetic origin. The structure pattern analysis groups natural products according to their shared core structure, and thereof creates a common precursor called """"privileged intermediate (PI)"""". This intermediate is resembled in each of these natural products and is architecturally less complex. As a result every member of this natural product group can originate from a different natural product family and is obtained via this """"privileged intermediate"""", which serves as basis for the artificial synthetic network.
With ANaPSyS a synthetic route is not restricted to a single target structure anymore (as in conventional synthesis). In comparison with bio-inspired synthesis, which is limited to a single natural product family, ANaPSyS enables the synthesis of a whole set of natural product families. With every synthesis accomplished, the network is upgraded — hence diversification leads to a rise in revenue. As a consequence, synthetic efficiency is drastically enhanced, therefore profoundly boosting and facilitating lead structure development.
""","1497000","2016-04-01","2021-03-31"
"ANGLE","Accelerated design and discovery of novel molecular materials via global lattice energy minimisation","Graeme Matthew Day","UNIVERSITY OF SOUTHAMPTON","The goal of crystal engineering is the design of functional crystalline materials in which the arrangement of basic structural building blocks imparts desired properties. The engineering of organic molecular crystals has, to date, relied largely on empirical rules governing the intermolecular association of functional groups in the solid state. However, many materials properties depend intricately on the complete crystal structure, i.e. the unit cell, space group and atomic positions, which cannot be predicted solely using such rules. Therefore, the development of computational methods for crystal structure prediction (CSP) from first principles has been a goal of computational chemistry that could significantly accelerate the design of new materials. It is only recently that the necessary advances in the modelling of intermolecular interactions and developments in algorithms for identifying all relevant crystal structures have come together to provide predictive methods that are becoming reliable and affordable on a timescale that could usefully complement an experimental research programme. The principle aim of the proposed work is to establish the use of state-of-the-art crystal structure prediction methods as a means of guiding the discovery and design of novel molecular materials.
This research proposal both continues the development of the computational methods for CSP and, by developing a computational framework for screening of potential molecules, develops the application of these methods for materials design. The areas on which we will focus are organic molecular semiconductors with high charge carrier mobilities and, building on our recently published results in Nature [1], the development of porous organic molecular materials. The project will both deliver novel materials, as well as improvements in the reliability of computational methods that will find widespread applications in materials chemistry.
[1] Nature 2011, 474, 367-371.","1499906","2012-10-01","2017-09-30"
"ANGULON","Angulon: physics and applications of a new quasiparticle","Mikhail Lemeshko","INSTITUTE OF SCIENCE AND TECHNOLOGYAUSTRIA","This project aims to develop a universal approach to angular momentum in quantum many-body systems based on the angulon quasiparticle recently discovered by the PI. We will establish a general theory of angulons in and out of equilibrium, and apply it to a variety of experimentally studied problems, ranging from chemical dynamics in solvents to solid-state systems (e.g. angular momentum transfer in the Einstein-de Haas effect and ultrafast magnetism).

The concept of angular momentum is ubiquitous across physics, whether one deals with nuclear collisions, chemical reactions, or formation of galaxies. In the microscopic world, quantum rotations are described by non-commuting operators. This makes the angular momentum theory extremely involved, even for systems consisting of only a few interacting particles, such as gas-phase atoms or molecules.

Furthermore, in most experiments the behavior of quantum particles is inevitably altered by a many-body environment of some kind. For example, molecular rotation – and therefore reactivity – depends on the presence of a solvent, electronic angular momentum in solids is coupled to lattice phonons, highly excited atomic levels can be perturbed by a surrounding ultracold gas. If approached in a brute-force fashion, understanding angular momentum in such systems is an impossible task, since a macroscopic number of particles is involved.

Recently, the PI and his team have shown that this challenge can be met by introducing a new quasiparticle – the angulon. In 2017, the PI has demonstrated the existence of angulons by comparing his theory with 20 years of measurements on molecules rotating in superfluids. Most importantly, the angulon concept allows one to gain analytical insights inaccessible to the state-of-the-art techniques of condensed matter and chemical physics. The angulon approach holds the promise of opening up a new interdisciplinary research area with applications reaching far beyond what is proposed here.","1499588","2019-02-01","2024-01-31"
"ANIMETRICS","Measurement-Based Modeling and Animation of Complex Mechanical Phenomena","Miguel Angel Otaduy Tristan","UNIVERSIDAD REY JUAN CARLOS","Computer animation has traditionally been associated with applications in virtual-reality-based training, video games or feature films. However, interactive animation is gaining relevance in a more general scope, as a tool for early-stage analysis, design and planning in many applications in science and engineering. The user can get quick and visual feedback of the results, and then proceed by refining the experiments or designs. Potential applications include nanodesign, e-commerce or tactile telecommunication, but they also reach as far as, e.g., the analysis of ecological, climate, biological or physiological processes.

The application of computer animation is extremely limited in comparison to its potential outreach due to a trade-off between accuracy and computational efficiency. Such trade-off is induced by inherent complexity sources such as nonlinear or anisotropic behaviors, heterogeneous properties, or high dynamic ranges of effects.

The Animetrics project proposes a modeling and animation methodology, which consists of a multi-scale decomposition of complex processes, the description of the process at each scale through combination of simple local models, and fitting the parameters of those local models using large amounts of data from example effects. The modeling and animation methodology will be explored on specific problems arising in complex mechanical phenomena, including viscoelasticity of solids and thin shells, multi-body contact, granular and liquid flow, and fracture of solids.","1277969","2012-01-01","2016-12-31"
"ANISOGEL","Injectable anisotropic microgel-in-hydrogel matrices for spinal cord repair","Laura De Laporte","DWI LEIBNIZ-INSTITUT FUR INTERAKTIVE MATERIALIEN EV","This project will engineer an injectable biomaterial that forms an anisotropic microheterogeneous structure in vivo. Injectable hydrogels enable a minimal invasive in situ generation of matrices for the regeneration of tissues and organs, but currently lack structural organization and unidirectional orientation. The anisotropic, injectable hydrogels to be developed will mimic local extracellular matrix architectures that cells encounter in complex tissues (e.g. nerves, muscles). This project aims for the development of a biomimetic scaffold for spinal cord regeneration.
To realize such a major breakthrough, my group will focus on three research objectives. i) Poly(ethylene glycol) microgel-in-hydrogel matrices will be fabricated with the ability to create macroscopic order due to microgel shape anisotropy and magnetic alignment. Barrel-like microgels will be prepared using an in-mold polymerization technique. Their ability to self-assemble will be investigated in function of their dimensions, aspect ratio, crosslinking density, and volume fraction. Superparamagnetic nanoparticles will be included into the microgels to enable unidirectional orientation by means of a magnetic field. Subsequently, the oriented microgels will be interlocked within a master hydrogel. ii) The microgel-in-hydrogel matrices will be equipped with (bio)functional properties for spinal cord regeneration, i.e., to control and optimize mechanical anisotropy and biological signaling by in vitro cell growth experiments. iii) Selected hydrogel composites will be injected after rat spinal cord injury and directional tissue growth and animal functional behavior will be analyzed. 
Succesful fabrication of the proposed microgel-in-hydrogel matrix will provide a new type of biomaterial, which enables investigating the effect of an anisotropic structure on physiological and pathological processes in vivo. This is a decisive step towards creating a clinical healing matrix for anisotropic tissue repair.","1435396","2015-03-01","2020-02-29"
"ANISOTROPIC UNIVERSE","The anisotropic universe -- a reality or fluke?","Hans Kristian Kamfjord Eriksen","UNIVERSITETET I OSLO","""During the last decade, a strikingly successful cosmological concordance model has been established. With only six free parameters, nearly all observables, comprising millions of data points, may be fitted with outstanding precision. However, in this beautiful picture a few """"blemishes"""" have turned up, apparently not consistent with the standard model: While the model predicts that the universe is isotropic (i.e., looks the same in all directions) and homogeneous (i.e., the statistical properties are the same everywhere), subtle hints of the contrary are now seen. For instance, peculiar preferred directions and correlations are observed in the cosmic microwave background; some studies considering nearby galaxies suggest the existence of anomalous large-scale cosmic flows; a study of distant quasars hints towards unexpected large-scale correlations. All of these reports are individually highly intriguing, and together they hint toward a more complicated and interesting universe than previously imagined -- but none of the reports can be considered decisive. One major obstacle in many cases has been the relatively poor data quality.

This is currently about to change, as the next generation of new and far more powerful experiments are coming online. Of special interest to me are Planck, an ESA-funded CMB satellite currently taking data; QUIET, a ground-based CMB polarization experiment located in Chile; and various large-scale structure (LSS) data sets, such as the SDSS and 2dF surveys, and in the future Euclid, a proposed galaxy survey satellite also funded by ESA. By combining the world s best data from both CMB and LSS measurements, I will in the proposed project attempt to settle this question: Is our universe really anisotropic? Or are these recent claims only the results of systematic errors or statistical flukes? If the claims turn out to hold against this tide of new and high-quality data, then cosmology as a whole may need to be re-written.""","1500000","2011-01-01","2015-12-31"
"ANOPTSETCON","Analysis of optimal sets and optimal constants: old questions and new results","Aldo Pratelli","FRIEDRICH-ALEXANDER-UNIVERSITAET ERLANGEN NUERNBERG","The analysis of geometric and functional inequalities naturally leads to consider the extremal cases, thus
looking for optimal sets, or optimal functions, or optimal constants. The most classical examples are the (different versions of the) isoperimetric inequality and the Sobolev-like inequalities. Much is known about equality cases and best constants, but there are still many questions which seem quite natural but yet have no answer. For instance, it is not known, even in the 2-dimensional space, the answer of a question by Brezis: which set,
among those with a given volume, has the biggest Sobolev-Poincaré constant for p=1? This is a very natural problem, and it appears reasonable that the optimal set should be the ball, but this has never been proved. The interest in problems like this relies not only in the extreme simplicity of the questions and in their classical flavour, but also in the new ideas and techniques which are needed to provide the answers.
The main techniques that we aim to use are fine arguments of symmetrization, geometric constructions and tools from mass transportation (which is well known to be deeply connected with functional inequalities). These are the basic tools that we already used to reach, in last years, many results in a specific direction, namely the search of sharp quantitative inequalities. Our first result, together with Fusco and Maggi, showed what follows. Everybody knows that the set which minimizes the perimeter with given volume is the ball.
But is it true that a set which almost minimizes the perimeter must be close to a ball? The question had been posed in the 1920's and many partial result appeared in the years. In our paper (Ann. of Math., 2007) we proved the sharp result. Many other results of this kind were obtained in last two years.","540000","2010-08-01","2015-07-31"
"ANPROB","Analytic-probabilistic methods for borderline singular integrals","Tuomas Pentinpoika Hytönen","HELSINGIN YLIOPISTO","The proposal consists of an extensive research program to advance the understanding of singular integral operators of Harmonic Analysis in various situations on the borderline of the existing theory. This is to be achieved by a creative combination of techniques from Analysis and Probability. On top of the standard arsenal of modern Harmonic Analysis, the main probabilistic tools are the martingale transform inequalities of Burkholder, and random geometric constructions in the spirit of the random dyadic cubes introduced to Nonhomogeneous Analysis by Nazarov, Treil and Volberg.

The problems to be addressed fall under the following subtitles, with many interconnections and overlap: (i) sharp weighted inequalities; (ii) nonhomogeneous singular integrals on metric spaces; (iii) local Tb theorems with borderline assumptions; (iv) functional calculus of rough differential operators; and (v) vector-valued singular integrals.

Topic (i) is a part of Classical Analysis, where new methods have led to substantial recent progress, culminating in my solution in July 2010 of a celebrated problem on the linear dependence of the weighted operator norm on the Muckenhoupt norm of the weight. The proof should be extendible to several related questions, and the aim is to also address some outstanding open problems in the area.

Topics (ii) and (v) deal with extensions of the theory of singular integrals to functions with more general domain and range spaces, allowing them to be abstract metric and Banach spaces, respectively. In case (ii), I have recently been able to relax the requirements on the space compared to the established theories, opening a new research direction here. Topics (iii) and (iv) are concerned with weakening the assumptions on singular integrals in the usual Euclidean space, to allow certain applications in the theory of Partial Differential Equations. The goal is to maintain a close contact and exchange of ideas between such abstract and concrete questions.","1100000","2011-11-01","2016-10-31"
"ANSR","Ab initio approach to nuclear structure and reactions (++)","Christian Erik Forssén","CHALMERS TEKNISKA HOEGSKOLA AB","Today, much interest in several fields of physics is devoted to the study of small, open quantum systems, whose properties are profoundly affected by the environment; i.e., the continuum of decay channels. In nuclear physics, these problems were originally studied in the context of nuclear reactions but their importance has been reestablished with the advent of radioactive-beam physics and the resulting interest in exotic nuclei. In particular, strong theory initiatives in this area of research will be instrumental for the success of the experimental program at the Facility for Antiproton and Ion Research (FAIR) in Germany. In addition, many of the aspects of open quantum systems are also being explored in the rapidly evolving research on ultracold atomic gases, quantum dots, and other nanodevices. A first-principles description of open quantum systems presents a substantial theoretical and computational challenge. However, the current availability of enormous computing power has allowed theorists to make spectacular progress on problems that were previously thought intractable. The importance of computational methods to study quantum many-body systems is stressed in this proposal. Our approach is based on the ab initio no-core shell model (NCSM), which is a well-established theoretical framework aimed originally at an exact description of nuclear structure starting from realistic inter-nucleon forces. A successful completion of this project requires extensions of the NCSM mathematical framework and the development of highly advanced computer codes. The &apos;++&apos; in the project title indicates the interdisciplinary aspects of the present research proposal and the ambition to make a significant impact on connected fields of many-body physics.","1304800","2009-12-01","2014-11-30"
"ANTHOS","Analytic Number Theory: Higher Order Structures","Valentin Blomer","GEORG-AUGUST-UNIVERSITAT GOTTINGENSTIFTUNG OFFENTLICHEN RECHTS","This is a proposal for research at the interface of analytic number theory, automorphic forms and algebraic geometry. Motivated by fundamental conjectures in number theory, classical problems will be investigated in higher order situations: general number fields, automorphic forms on higher rank groups, the arithmetic of algebraic varieties of higher degree. In particular, I want to focus on
- computation of moments of L-function of degree 3 and higher with applications to subconvexity and/or non-vanishing, as well as subconvexity for multiple L-functions;
- bounds for sup-norms of cusp forms on various spaces and equidistribution of Hecke correspondences;
- automorphic forms on higher rank groups and general number fields, in particular new bounds towards the Ramanujan conjecture;
- a proof of Manin's conjecture for a certain class of singular algebraic varieties.
The underlying methods are closely related; for example, rational points on algebraic varieties
will be counted by a multiple L-series technique.","1004000","2010-10-01","2015-09-30"
"ANTI-ATOM","Many-body theory of antimatter interactions with atoms, molecules and condensed matter","Dermot GREEN","THE QUEEN'S UNIVERSITY OF BELFAST","The ability of positrons to annihilate with electrons, producing characteristic gamma rays, gives them important use in medicine via positron-emission tomography (PET), diagnostics of industrially-important materials, and in elucidating astrophysical phenomena. Moreover, the fundamental interactions of positrons and positronium (Ps) with atoms, molecules and condensed matter are currently under intensive study in numerous international laboratories, to illuminate collision phenomena and perform precision tests of fundamental laws. 

Proper interpretation and development of these costly and difficult experiments requires accurate calculations of low-energy positron and Ps interactions with normal matter. These systems, however, involve strong correlations, e.g., polarisation of the atom and virtual-Ps formation (where an atomic electron tunnels to the positron): they significantly effect positron- and Ps-atom/molecule interactions, e.g., enhancing annihilation rates by many orders of magnitude, and making the accurate description of these systems a challenging many-body problem. Current theoretical capability lags severely behind that of experiment. Major theoretical and computational developments are required to bridge the gap.

One powerful method, which accounts for the correlations in a natural, transparent and systematic way, is many-body theory (MBT). Building on my expertise in the field, I propose to develop new MBT to deliver unique and unrivalled capability in theory and computation of low-energy positron and Ps interactions with atoms, molecules, and condensed matter. The ambitious programme will provide the basic understanding required to interpret and develop the fundamental experiments, antimatter-based materials science techniques, and wider technologies, e.g., (PET), and more broadly, potentially revolutionary and generally applicable computational methodologies that promise to define a new level of high-precision in atomic-MBT calculations.","1318419","2019-02-01","2024-01-31"
"ANTIBACTERIALS","Natural products and their cellular targets: A multidisciplinary strategy for antibacterial drug discovery","Stephan Axel Sieber","TECHNISCHE UNIVERSITAET MUENCHEN","After decades of successful treatment of bacterial infections with antibiotics, formerly treatable bacteria have developed drug resistance and consequently pose a major threat to public health. To address the urgent need for effective antibacterial drugs we will develop a streamlined chemical-biology platform that facilitates the consolidated identification and structural elucidation of natural products together with their dedicated cellular targets. This innovative concept overcomes several limitations of classical drug discovery processes by a chemical strategy that focuses on a directed isolation, enrichment and identification procedure for certain privileged natural product subclasses. This proposal consists of four specific aims: 1) synthesizing enzyme active site mimetics that capture protein reactive natural products out of complex natural sources, 2) designing natural product based probes to identify their cellular targets by a method called  activity based protein profiling , 3) developing a traceless photocrosslinking strategy for the target identification of selected non-reactive natural products, and 4) application of all probes to identify novel enzyme activities linked to viability, resistance and pathogenesis. Moreover, the compounds will be used to monitor the infection process during invasion into eukaryotic cells and will reveal host specific targets that promote and support bacterial pathogenesis. Inhibition of these targets is a novel and so far neglected approach in the treatment of infectious diseases. We anticipate that these studies will provide a powerful pharmacological platform for the development of potent natural product derived antibacterial agents directed toward novel therapeutic targets.","1500000","2010-11-01","2015-10-31"
"ANTICIPATE","Anticipatory Human-Computer Interaction","Andreas BULLING","UNIVERSITAET STUTTGART","Even after three decades of research on human-computer interaction (HCI), current general-purpose user interfaces (UI) still lack the ability to attribute mental states to their users, i.e. they fail to understand users' intentions and needs and to anticipate their actions. This drastically restricts their interactive capabilities.

ANTICIPATE aims to establish the scientific foundations for a new generation of user interfaces that pro-actively adapt to users' future input actions by monitoring their attention and predicting their interaction intentions - thereby significantly improving the naturalness, efficiency, and user experience of the interactions. Realising this vision of anticipatory human-computer interaction requires groundbreaking advances in everyday sensing of user attention from eye and brain activity. We will further pioneer methods to predict entangled user intentions and forecast interactive behaviour with fine temporal granularity during interactions in everyday stationary and mobile settings. Finally, we will develop fundamental interaction paradigms that enable anticipatory UIs to pro-actively adapt to users' attention and intentions in a mindful way. The new capabilities will be demonstrated in four challenging cases: 1) mobile information retrieval, 2) intelligent notification management, 3) Autism diagnosis and monitoring, and 4) computer-based training.

Anticipatory human-computer interaction offers a strong complement to existing UI paradigms that only react to user input post-hoc. If successful, ANTICIPATE will deliver the first important building blocks for implementing Theory of Mind in general-purpose UIs. As such, the project has the potential to drastically improve the billions of interactions we perform with computers every day, to trigger a wide range of follow-up research in HCI as well as adjacent areas within and outside computer science, and to act as a key technical enabler for new applications, e.g. in healthcare and education.","1499625","2019-02-01","2024-01-31"
"ANTICS","Algorithmic Number Theory in Computer Science","Andreas Enge","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","""During the past twenty years, we have witnessed profound technological changes, summarised under the terms of digital revolution or entering the information age. It is evident that these technological changes will have a deep societal impact, and questions of privacy and security are primordial to ensure the survival of a free and open society.

Cryptology is a main building block of any security solution, and at the heart of projects such as electronic identity and health cards, access control, digital content distribution or electronic voting, to mention only a few important applications. During the past decades, public-key cryptology has established itself as a research topic in computer science; tools of theoretical computer science are employed to “prove” the security of cryptographic primitives such as encryption or digital signatures and of more complex protocols. It is often forgotten, however, that all practically relevant public-key cryptosystems are rooted in pure mathematics, in particular, number theory and arithmetic geometry. In fact, the socalled security “proofs” are all conditional to the algorithmic untractability of certain number theoretic problems, such as factorisation of large integers or discrete logarithms in algebraic curves. Unfortunately, there is a large cultural gap between computer scientists using a black-box security reduction to a supposedly hard problem in algorithmic number theory and number theorists, who are often interested in solving small and easy instances of the same problem. The theoretical grounds on which current algorithmic number theory operates are actually rather shaky, and cryptologists are generally unaware of this fact.
The central goal of ANTICS is to rebuild algorithmic number theory on the firm grounds of theoretical computer science.""","1453507","2012-01-01","2016-12-31"
"ANTINEUTRINONOVA","Probing Fundamental Physics with Antineutrinos at the NOvA Experiment","Jeffrey Hartnell","THE UNIVERSITY OF SUSSEX","""This proposal addresses major questions in particle physics that are at the forefront of experimental and theoretical physics research today. The results offered would have far-reaching implications in other fields such as cosmology and could help answer some of the big questions such as why the universe contains so much more matter than antimatter. The research objectives of this proposal are to (i) make world-leading tests of CPT symmetry and (ii) discover the neutrino mass hierarchy and search for indications of leptonic CP violation.
The NOvA long-baseline neutrino oscillation experiment will use a novel """"totally active scintillator design"""" for the detector technology and will be exposed to the world's highest power neutrino beam. Building on the first direct observation of muon antineutrino disappearance (that was made by a group founded and led by the PI at the MINOS experiment), tests of CPT symmetry will be performed by looking for differences in the mass squared splittings and mixing angles between neutrinos and antineutrinos. The potential to discover the mass hierarchy is unique to NOvA on the timescale of this proposal due to the long 810 km baseline and the well measured beam of neutrinos and antineutrinos.
This proposal addresses several key challenges in a long-baseline neutrino oscillation experiment with the following tasks: (i) development of a new approach to event energy reconstruction that is expected to have widespread applicability for future neutrino experiments; (ii) undertaking a comprehensive calibration project, exploiting a novel technique developed by the PI, that will be essential to achieving the physics goals; (iii) development of a sophisticated statistical analyses.
The results promised in this proposal surpass the sensitivity to antineutrino oscillation parameters of current 1st generation experiments by at least an order of magnitude, offering wide scope for profound discoveries with implications across disciplines.""","1415848","2012-10-01","2018-09-30"
"ANYON","Engineering and exploring anyonic quantum gases","Christof WEITENBERG","UNIVERSITAET HAMBURG","This project enters the experimental investigation of anyonic quantum gases. We will study anyons – conjectured particles with a statistical exchange phase anywhere between 0 and π – in different many-body systems. This progress will be enabled by a unique approach of bringing together artificial gauge fields and quantum gas microscopes for ultracold atoms.

Specifically, we will implement the 1D anyon Hubbard model via a lattice shaking protocol that imprints density-dependent Peierls phases. By engineering the statistical exchange phase, we can continuously tune between bosons and fermions and explore a statistically-induced quantum phase transition. We will monitor the continuous fermionization via the build-up of Friedel oscillations. Using state-of-the-art cold atom technology, we will thus open the physics of anyons to experimental research and address open questions related to their fractional exclusion statistics.

Secondly, we will create fractional quantum Hall systems in rapidly rotating microtraps. Using the quantum gas microscope, we will i) control the optical potentials at a level which allows approaching the centrifugal limit and ii) use small atom numbers equal to the inserted angular momentum quantum number. The strongly-correlated ground states such as the Laughlin state can be identified via their characteristic density correlations. Of particular interest are the quasihole excitations, whose predicted anyonic exchange statistics have not been directly observed to date. We will probe and test their statistics via the characteristic counting sequence in the excitation spectrum. Furthermore, we will test ideas to transfer anyonic properties of the excitations to a second tracer species. This approach will enable us to both probe the fractional exclusion statistics of the excitations and to create a 2D anyonic quantum gas. 

In the long run, these techniques open a path to also study non-Abelian anyons with ultracold atoms.","1497500","2019-01-01","2023-12-31"
"APACHE","Atmospheric Pressure plAsma meets biomaterials for bone Cancer HEaling","Cristina CANAL BARNILS","UNIVERSITAT POLITECNICA DE CATALUNYA","Cold atmospheric pressure plasmas (APP) have been reported to selectively kill cancer cells without damaging the surrounding tissues. Studies have been conducted on a variety of cancer types but to the best of our knowledge not on any kind of bone cancer. Treatment options for bone cancer include surgery, chemotherapy, etc. and may involve the use of bone grafting biomaterials to replace the surgically removed bone.
APACHE brings a totally different and ground-breaking approach in the design of a novel therapy for bone cancer by taking advantage of the active species generated by APP in combination with biomaterials to deliver the active species locally in the diseased site. The feasibility of this approach is rooted in the evidence that the cellular effects of APP appear to strongly involve the suite of reactive species created by plasmas, which can be derived from a) direct treatment of the malignant cells by APP or b) indirect treatment of the liquid media by APP which is then put in contact with the cancer cells.
In APACHE we aim to investigate the fundamentals involved in the lethal effects of cold plasmas on bone cancer cells, and to develop improved bone cancer therapies.  To achieve this we will take advantage of the highly reactive species generated by APP in the liquid media, which we will use in an incremental strategy: i) to investigate the effects of APP treated liquid on bone cancer cells, ii) to evaluate the potential of combining APP treated liquid in a hydrogel vehicle with/wo CaP biomaterials and iii) to ascertain the potential three directional interactions between APP reactive species in liquid medium with biomaterials and with chemotherapeutic drugs.
The methodological approach will involve an interdisciplinary team, dealing with plasma diagnostics in gas and liquid media; with cell biology and the effects of APP treated with bone tumor cells and its combination with biomaterials and/or with anticancer drugs.","1499887","2017-04-01","2022-03-31"
"APES","Accuracy and precision for molecular solids","Jiri KLIMES","UNIVERZITA KARLOVA","The description of high pressure phases or polymorphism of molecular solids represents a significant scientific challenge both for experiment and theory. Theoretical methods that are currently used struggle to describe the tiny energy differences between different phases. It is the aim of this project to develop a scheme that would allow accurate and reliable predictions of the binding energies of molecular solids and of the energy differences between different phases.
To reach the required accuracy, we will combine the coupled cluster approach, widely used for reference quality calculations for molecules, with the random phase approximation (RPA) within periodic boundary conditions. As I have recently shown, RPA-based approaches are already some of the most accurate and practically usable methods for the description of extended systems. However, reliability is not only a question of accuracy. Reliable data need to be precise, that is, converged with the numerical parameters so that they are reproducible by other researchers.
Reproducibility is already a growing concern in the field. It is likely to become a considerable issue for highly accurate methods as the calculated energies have a stronger dependence on the simulation parameters such as the basis set size. Two main approaches will be explored to assure precision. First, we will develop the so-called asymptotic correction scheme to speed-up the convergence of the correlation energies with the basis set size. Second, we will directly compare the lattice energies from periodic and finite cluster based calculations. Both should yield identical answers, but if and how the agreement can be reached for general system is currently far from being understood for methods such as coupled cluster. Reliable data will allow us to answer some of the open questions regarding the stability of polymorphs and high pressure phases, such as the possibility of existence of high pressure ionic phases of water and ammonia.","924375","2018-01-01","2022-12-31"
"APGRAPH","Asymptotic Graph Properties","Deryk Osthus","THE UNIVERSITY OF BIRMINGHAM","Many parts of Graph Theory have witnessed a huge growth over the last years, partly because of their relation to Theoretical Computer Science and Statistical Physics. These connections arise because graphs can be used to model many diverse structures.

The focus of this proposal is on asymptotic results, i.e. the graphs under consideration are large. This often unveils patterns and connections which remain obscure when considering only small graphs.

It also allows for the use of powerful techniques such as probabilistic arguments, which have led to spectacular new developments. In particular, my aim is to make decisive progress on central problems in the following 4 areas:

(1) Factorizations: Factorizations of graphs can be viewed as partitions of the edges of a graph into simple regular structures. They have a rich history and arise in many different settings, such as edge-colouring problems, decomposition problems and in information theory. They also have applications to finding good tours for the famous Travelling salesman problem.

(2) Hamilton cycles:  A Hamilton cycle is a cycle which contains all the vertices of the graph. One of the most fundamental problems in Graph Theory/Theoretical Computer Science is to find conditions which guarantee the existence of a Hamilton cycle in a graph.

(3) Embeddings of graphs: This is a natural (but difficult) continuation of the previous question where the aim is to embed more general structures than Hamilton cycles - there has been exciting progress here in recent years which has opened up new avenues.

(4)  Resilience of graphs:  In many cases, it is important to know whether a graph `strongly’ possesses some property, i.e. one cannot destroy the property by changing a few edges. The systematic study of this notion is a new and rapidly growing area.

I have developed new methods for deep and long-standing problems in these areas which will certainly lead to further applications elsewhere.","818414","2012-12-01","2018-11-30"
"APROCS","Automated Linear Parameter-Varying Modeling and Control Synthesis for Nonlinear Complex Systems","Roland TOTH","TECHNISCHE UNIVERSITEIT EINDHOVEN","Linear Parameter-Varying (LPV) systems are flexible mathematical models capable of representing Nonlinear (NL)/Time-Varying (TV) dynamical behaviors of complex physical systems (e.g., wafer scanners, car engines, chemical reactors), often encountered in engineering, via a linear structure. The LPV framework provides computationally efficient and robust approaches to synthesize digital controllers that can ensure desired operation of such systems - making it attractive to (i) high-tech mechatronic, (ii) automotive and (iii) chemical-process applications. Such a framework is important to meet with the increasing operational demands of systems in these industrial sectors and to realize future technological targets. However, recent studies have shown that, to fully exploit the potential of the LPV framework, a number of limiting factors of the underlying theory ask a for serious innovation, as currently it is not understood how to (1) automate exact and low-complexity LPV modeling of real-world applications and how to refine uncertain aspects of these models efficiently by the help of measured data, (2) incorporate control objectives directly into modeling and to develop model reduction approaches for control, and (3) how to see modeling & control synthesis as a unified, closed-loop system synthesis approach directly oriented for the underlying NL/TV system. Furthermore, due to the increasingly cyber-physical nature of applications, (4) control synthesis is needed in a plug & play fashion, where if sub-systems are modified or exchanged, then the control design and the model of the whole system are only incrementally updated. This project aims to surmount Challenges (1)-(4) by establishing an innovative revolution of the LPV framework supported by a software suite and extensive empirical studies on real-world industrial applications; with a potential to ensure a leading role of technological innovation of the EU in the high-impact industrial sectors (i)-(iii).","1493561","2017-09-01","2022-08-31"
"AQSER","Automorphic q-series and their application","Kathrin Bringmann","UNIVERSITAET ZU KOELN","This proposal aims to unravel mysteries at the frontier of number theory and other areas of mathematics and physics. The main focus will be to understand and exploit “modularity” of q-hypergeometric series. “Modular forms are functions on the complex plane that are inordinately symmetric.” (Mazur) The motivation comes from the wide-reaching applications of modularity in combinatorics, percolation, Lie theory, and physics (black holes).

The interplay between automorphic forms, q-series, and other areas of mathematics and physics is often two-sided. On the one hand, the other areas provide interesting examples of automorphic objects and predict their behavior. Sometimes these even motivate new classes of automorphic objects which have not been previously studied. On the other hand, knowing that certain generating functions are modular gives one access to deep theoretical tools to prove results in other areas. “Mathematics is a language, and we need that language to understand the physics of our universe.”(Ooguri) Understanding this interplay has attracted attention of researchers from a variety of areas. However, proofs of modularity of q-hypergeometric series currently fall far short of a comprehensive theory to describe the interplay between them and automorphic forms. A recent conjecture of W. Nahm relates the modularity of such series to K-theory. In this proposal I aim to fill this gap and provide a better understanding of this interplay by building a general structural framework enveloping these q-series. For this I will employ new kinds of automorphic objects and embed the functions of interest into bigger families

A successful outcome of the proposed research will open further horizons and also answer open questions, even those in other areas which were not addressed in this proposal; for example the new theory could be applied to better understand Donaldson invariants.","1240500","2014-01-01","2019-04-30"
"AQSuS","Analog Quantum Simulation using Superconducting Qubits","Gerhard KIRCHMAIR","UNIVERSITAET INNSBRUCK","AQSuS aims at experimentally implementing analogue quantum simulation of interacting spin models in two-dimensional geometries. The proposed experimental approach paves the way to investigate a broad range of currently inaccessible quantum phenomena, for which existing analytical and numerical methods reach their limitations. Developing precisely controlled interacting quantum systems in 2D is an important current goal well beyond the field of quantum simulation and has applications in e.g. solid state physics, computing and metrology.
To access these models, I propose to develop a novel circuit quantum-electrodynamics (cQED) platform based on the 3D transmon qubit architecture. This platform utilizes the highly engineerable properties and long coherence times of these qubits. A central novel idea behind AQSuS is to exploit the spatial dependence of the naturally occurring dipolar interactions between the qubits to engineer the desired spin-spin interactions. This approach avoids the complicated wiring, typical for other cQED experiments and reduces the complexity of the experimental setup. The scheme is therefore directly scalable to larger systems. The experimental goals are:

1) Demonstrate analogue quantum simulation of an interacting spin system in 1D & 2D.
2) Establish methods to precisely initialize the state of the system, control the interactions and readout single qubit states and multi-qubit correlations.
3) Investigate unobserved quantum phenomena on 2D geometries e.g. kagome and triangular lattices.
4) Study open system dynamics with interacting spin systems. 

AQSuS builds on my backgrounds in both superconducting qubits and quantum simulation with trapped-ions. With theory collaborators my young research group and I have recently published an article in PRB [9] describing and analysing the proposed platform. The ERC starting grant would allow me to open a big new research direction and capitalize on the foundations established over the last two years.","1498515","2017-04-01","2022-03-31"
"AQUARAMAN","Pipet Based Scanning Probe Microscopy Tip-Enhanced Raman Spectroscopy: A Novel Approach for TERS in Liquids","Aleix Garcia Guell","ECOLE POLYTECHNIQUE","Tip-enhanced Raman spectroscopy (TERS) is often described as the most powerful tool for optical characterization of surfaces and their proximities. It combines the intrinsic spatial resolution of scanning probe techniques (AFM or STM) with the chemical information content of vibrational Raman spectroscopy. Capable to reveal surface heterogeneity at the nanoscale, TERS is currently playing a fundamental role in the understanding of interfacial physicochemical processes in key areas of science and technology such as chemistry, biology and material science. 
Unfortunately, the undeniable potential of TERS as a label-free tool for nanoscale chemical and structural characterization is, nowadays, limited to air and vacuum environments, with it failing to operate in a reliable and systematic manner in liquid. The reasons are more technical than fundamental, as what is hindering the application of TERS in water is, among other issues, the low stability of the probes and their consistency. Fields of science and technology where the presence of water/electrolyte is unavoidable, such as biology and electrochemistry, remain unexplored with this powerful technique.
We propose a revolutionary approach for TERS in liquids founded on the employment of pipet-based scanning probe microscopy techniques (pb-SPM) as an alternative to AFM and STM. The use of recent but well established pb-SPM brings the opportunity to develop unprecedented pipet-based TERS probes (beyond the classic and limited metallized solid probes from AFM and STM), together with the implementation of ingenious and innovative measures to enhance tip stability, sensitivity and reliability, unattainable with the current techniques.
We will be in possession of a unique nano-spectroscopy platform capable of experiments in liquids, to follow dynamic processes in-situ, addressing fundamental questions and bringing insight into interfacial phenomena spanning from materials science, physics, chemistry and biology.","1528442","2017-07-01","2022-06-30"
"aQUARiUM","QUAntum nanophotonics in Rolled-Up Metamaterials","Humeyra CAGLAYAN","TAMPEREEN KORKEAKOULUSAATIO SR","Novel sophisticated technologies that exploit the laws of quantum physics form a cornerstone for the future well-being, economic growth and security of Europe. Here photonic devices have gained a prominent position because the absorption, emission, propagation or storage of a photon is a process that can be harnessed at a fundamental level and render more practical ways to use light for such applications. However, the interaction of light with single quantum systems under ambient conditions is typically very weak and difficult to control. Furthermore, there are quantum phenomena occurring in matter at nanometer length scales that are currently not well understood. These deficiencies have a direct and severe impact on creating a bridge between quantum physics and photonic device technologies. aQUARiUM, precisely address the issue of controlling and enhancing the interaction between few photons and rolled-up nanostructures with ability to be deployed in practical applications. 
With aQUARiUM, we will take epsilon (permittivity)-near-zero (ENZ) metamaterials into quantum nanophotonics. To this end, we will integrate quantum emitters with rolled-up waveguides, that act as ENZ metamaterial, to expand and redefine the range of light-matter interactions. We will explore the electromagnetic design freedom enabled by the extended modes of ENZ medium, which “stretches” the effective wavelength inside the structure. Specifically, aQUARiUM is built around the following two objectives: (i) Enhancing light-matter interactions with single emitters (Enhance)  independent of emitter position. (ii) Enabling collective excitations in dense emitter ensembles (Collect) coherently connect emitters on nanophotonic devices to obtain coherent emission.
aQUARiUM aims to create novel light-sources and long-term entanglement generation and beyond. The envisioned outcome of aQUARiUM is a wholly new photonic platform applicable across a diverse range of areas.","1499431","2019-01-01","2023-12-31"
"AQUMET","Atomic Quantum Metrology","Morgan Wilfred Mitchell","FUNDACIO INSTITUT DE CIENCIES FOTONIQUES","This project aims to detect magnetic fields with high spatial and temporal resolution and unprecedented sensitivity using ultra-cold atoms as interferometric sensors. The project will, on the one hand, test and demonstrate the most advanced concepts in the dynamic field of quantum metrology, and on the other hand, develop measurement techniques with the potential to transform existing fields and open new ones to study.
Quantum metrology is in an exciting phase: on the one hand, a long-held goal of improving gravita- tional wave detection appears near at hand. At the same time, atomic instruments including atomic clocks, atomic gravimeters and atomic magnetometers are setting records in detection of time, ac- celeration, and fields, with revolutionary potential in several areas. This has stimulated new theory, including remarkable proposals suggesting that long-established “ultimate” limits can in fact be sur- passed.
This project will study quantum metrology applied to atomic sensors by developing a versatile and highly sensitive cold atom magnetometer. We set an ambitious goal: to demonstrate record sensi- tivity, and then to improve on that sensitivity using quantum entanglement. This ground-breaking accomplishment will show the way to super-precise measurements in many fields.
Fundamental topics in quantum metrology will be explored using the advanced magnetometry sys- tem. Nonlinear quantum metrology proposes to surpass the Heisenberg limit using inter-particle interactions. Compressed sensing aims to surpass the Nyquist limit, obtaining more information than normally allowed.","1387000","2012-01-01","2016-12-31"
"ARCA","Analysis and Representation of Complex Activities in Videos","Juergen Gall","RHEINISCHE FRIEDRICH-WILHELMS-UNIVERSITAT BONN","The goal of the project is to automatically analyse human activities observed in videos. Any solution to this problem will allow the development of novel applications. It could be used to create short videos that summarize daily activities to support patients suffering from Alzheimer's disease. It could also be used for education, e.g., by providing a video analysis for a trainee in the hospital that shows if the tasks have been correctly executed.

The analysis of complex activities in videos, however, is very challenging since activities vary in temporal duration between minutes and hours, involve interactions with several objects that change their appearance and shape, e.g., food during cooking, and are composed of many sub-activities, which can happen at the same time or in various orders.

While the majority of recent works in action recognition focuses on developing better feature encoding techniques for classifying sub-activities in short video clips of a few seconds, this project moves forward and aims to develop a  higher level representation of complex activities to overcome the limitations of current approaches. This includes the handling of large time variations and the ability to recognize and locate complex activities in videos. To this end, we aim to develop a unified model that provides detailed information about the activities and sub-activities in terms of time and spatial location, as well as involved pose motion, objects and their transformations.

Another aspect of the project is to learn a representation from videos that is not tied to a specific source of videos or limited to a specific application. Instead we aim to learn a representation that is invariant to a perspective change, e.g., from a third-person perspective to an egocentric perspective, and can be applied to various modalities like videos or depth data without the need of collecting massive training data for all modalities. In other words, we aim to learn the essence of activities.","1499875","2016-06-01","2021-05-31"
"ARCHEIS","Understanding the onset and impact of Aquatic Resource Consumption in Human Evolution using novel Isotopic tracerS","Klervia JAOUEN","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The onset of the systematic consumption of marine resources is thought to mark a turning point for the hominin lineage. To date, this onset cannot be traced, since classic isotope markers are not preserved beyond 50 - 100 ky. Aquatic food products are essential in human nutrition as the main source of polyunsaturated fatty acids in hunter-gatherer diets. The exploitation of marine resources is also thought to have reduced human mobility and enhanced social and technological complexification. Systematic aquatic food consumption could well have been a distinctive feature of Homo sapiens species among his fellow hominins, and has been linked to the astonishing leap in human intelligence and conscience. Yet, this hypothesis is challenged by the existence of mollusk and marine mammal bone remains at Neanderthal archeological sites. Recent work demonstrated the sensitivity of Zn isotope composition in bioapatite, the mineral part of bones and teeth, to dietary Zn. By combining classic (C and C/N isotope analyses) and innovative techniques (compound specific C/N and bulk Zn isotope analyses), I will develop a suite of sensitive tracers for shellfish, fish and marine mammal consumption. Shellfish consumption will be investigated by comparing various South American and European prehistoric populations from the Atlantic coast associated to shell-midden and fish-mounds. Marine mammal consumption will be traced using an Inuit population of Arctic Canada and the Wairau Bar population of New Zealand. C/N/Zn isotope compositions of various aquatic products will also be assessed, as well as isotope fractionation during intestinal absorption. I will then use the fully calibrated isotope tools to detect and characterize the onset of marine food exploitation in human history, which will answer the question of its specificity to our species. Neanderthal, early modern humans and possibly other hominin remains from coastal and inland sites will be compared in that purpose.","1361991","2019-03-01","2024-02-29"
"ARENA","Arrays of entangled atoms","Antoine Browaeys","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The goal of this project is to prepare in a deterministic way, and then to characterize, various entangled states of up to 25 individual atoms held in an array of optical tweezers. Such a system provides a new arena to explore quantum entangled states of a large number of particles. Entanglement is the existence of quantum correlations between different parts of a system, and it is recognized as an essential property that distinguishes the quantum and the classical worlds. It is also a resource in various areas of physics, such as quantum information processing, quantum metrology, correlated quantum systems and quantum simulation. In the proposed design, each site is individually addressable, which enables single atom manipulation and detection. This will provide the largest entangled state ever produced and fully characterized at the individual particle level. The experiment will be implemented by combining two crucial novel features, that I was able to demonstrate very recently: first, the manipulation of quantum bits written on long-lived hyperfine ground states of single ultra-cold atoms trapped in microscopic optical tweezers; second, the generation of entanglement by using the strong long-range interactions between Rydberg states. These interactions lead to the so-called dipole blockade , and enable the preparation of various classes of entangled states, such as states carrying only one excitation (W states), and states analogous to Schrödinger s cats (GHZ states). Finally, I will also explore strategies to protect these states against decoherence, developed in the framework of fault-tolerant and topological quantum computing. This project therefore combines an experimental challenge and the exploration of entanglement in a mesoscopic system.","1449600","2009-12-01","2014-11-30"
"ARIADNE","ARgon ImAging DetectioN chambEr","Konstantinos Mavrokoridis","THE UNIVERSITY OF LIVERPOOL","This proposal outlines a plan to combine Charge Couple Device (CCD) camera technologies with two-phase Liquid Argon Time Projection Chambers (LAr TPCs) utilising THick Gas Electron Multipliers (THGEMs) to evolve a next generation neutrino detector. This will be an entirely new readout option, and will open the prospect of revolutionary discoveries in fundamental particle physics. Furthermore, the Compton imaging power of this technology will be developed, which will have diverse applications in novel medical imaging techniques and detection of concealed nuclear materials.
Colossal LAr TPCs are the future for long-baseline-neutrino-oscillation physics around which the international neutrino community is rallying, with the common goal of discovering new physics beyond the Standard Model, which holds the key to our understanding of phenomena such as dark matter and the matter-antimatter asymmetry.
I have successfully provided a first demonstration of photographic capturing of muon tracks and single gammas interacting in the Liverpool 40 l LAr TPC using a CCD camera and THGEM. I propose an ambitious project of extensive research to mature this innovative LAr optical readout technology. I will construct a 650 l LAr TPC with integrated CCD/THGEM readout, capable of containing sufficient tracking information for full development and characterisation of this novel detector, with the goal of realising this game-changing technology in the planned future giant LAr TPCs. Camera readout can replace the current charge readout technology and associated scalability complications, and the excellent energy thresholds will enhance detector performance as well as extend research avenues to lower energy fundamental physics. 
Also, I will explore the Compton imaging capability of LAr CCD/THGEM technology; the superiority of the energy threshold and spatial resolution of this system can offer significant advancement to medical imaging and the detection of concealed nuclear materials.","1837911","2016-03-01","2021-02-28"
"ARTIST","Artificial cell-cell interactions for light switchable cell organization and signaling","Seraphine Valeska Wegner","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","The bottom-up assembly of tissue from cellular building blocks constitutes a promising, yet highly challenging approach to engineer complex tissues. The challenge lies in controlling cell-cell interactions, which determine how cells organize with respect to each other, how they work together and consequently whether such a multicellular architecture will be functional. The limited spatial and temporal control over cell-cell interactions current biological and chemical approaches provide severely restricts bottom-up tissue engineering. Here, I propose a new way to control cell-cell interactions. I aim to regulate cell-cell interactions with visible light using proteins that reversibly homo- or heterodimerize under blue or red light. These photoswitchable cell-cell interactions provide sustainable, non-invasive, dynamic and reversible control over cell-cell interactions with unprecedented spatial and temporal resolution. First of all, we will focus on various light dependent protein interactions to mediate cell-cell contacts. The detailed characterization (strength, dynamics, interaction modes and orthogonality) of these new photoswitchable cell-cell interactions will provide the framework for the bottom-up construction of tissue-like structures. Secondly, we will use these photoswitchable cell-cell interactions to assemble cells into multicellular architectures with predictable and programmable organization. The dynamic and reversible nature of the photoswitchable contacts will allow us to locally alter interactions at any point in time, to rearrange and obtain asymmetric multicellular structures, which are typical of tissues. Finally, we will also explore how the photoswitchable cell-cell interactions alter cell behavior and signaling. Ultimately, this will pave the way for the bottom-up assembly of multicellular architectures, enabling us to control precisely and dynamically their organization in space and time as well as regulate how cells work together.","1937000","2018-07-01","2023-06-30"
"aSCEND","Secure Computation on Encrypted Data","Hoe Teck Wee","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Recent trends in computing have prompted users and organizations to store an increasingly large amount of sensitive data at third party locations in the cloud outside of their direct control. Storing data remotely poses an acute security threat as these data are outside our control and could potentially be accessed by untrusted parties. Indeed, the reality of these threats have been borne out by the Snowden leaks and hundreds of data breaches each year. In order to protect our data, we will need to encrypt it.

Functional encryption is a novel paradigm for public-key encryption that enables both fine-grained access control and selective computation on encrypted data, as is necessary to protect big, complex data in the cloud. Functional encryption also enables searches on encrypted travel records and surveillance video as well as medical studies on encrypted medical records in a privacy-preserving manner; we can give out restricted secret keys that reveal only the outcome of specific searches and tests. These mechanisms allow us to maintain public safety without compromising on civil liberties, and to facilitate medical break-throughs without compromising on individual privacy.

The goals of the aSCEND project are (i) to design pairing and lattice-based functional encryption that are more efficient and ultimately viable in practice; and (ii) to obtain a richer understanding of expressive functional encryption schemes and to push the boundaries from encrypting data to encrypting software. My long-term vision is the ubiquitous use of functional encryption to secure our data and our computation, just as public-key encryption is widely used today to secure our communication. Realizing this vision requires new advances in the foundations of functional encryption, which is the target of this project.","1253893","2015-06-01","2020-05-31"
"ASCENT","Advanced materials and devices for hybrid spin coherent technologies","John Julian Larrarte Morton","UNIVERSITY COLLEGE LONDON","The property of spin has been harnessed in an array of revolutionary technologies, from nuclear spins in magnetic resonance imaging to spintronics in magnetic recording media. Nature at its deepest level is quantum mechanical and spins are capable of demonstrating superposition and entanglement, yet such coherent properties have not yet been fully exploited. The exquisite control over materials fabrication and spin control techniques has reached a maturity where spintronics can go beyond purely classical effects and begin to fully exploit these quantum properties. Potential applications range from quantum information processors, including the transmission of quantum information via itinerant electron spins, single microwave photon storage within spin ensembles, and a new generation of sensors exploiting entanglement to yield fundamentally enhanced precision.

The aim of ASCENT is to develop materials and devices in which electron and nuclear spins exhibit long-lived coherent quantum behaviour and interactions which can be harnessed for technological purposes. Specifically, ASCENT will exploit in range of condensed matter systems from molecular materials to silicon-based structures, the possibility of transiently generating and removing electron spins in the vicinity of nuclear spins. The project represents a new and promising direction for the development of coherent interactions between spins in materials, and one which builds upon foundations I have established in my earlier work, often supported by preliminary investigations. Strong interactions with theory throughout this project will provide insights to refine and improve the experiments. In addition to direct applications in quantum technologies, the insights and methodology gained will be fed back into the wider field of spin resonance, including dynamic nuclear polarisation, structural biology and medical imaging.","1875550","2011-12-01","2017-06-30"
"ASMIDIAS","Asymmetric microenvironments by directed assembly: Control of geometry, topography, surface biochemistry and mechanical properties via a microscale modular design principle","Holger Dr. Schönherr","UNIVERSITAET SIEGEN","The interaction of cells with the extracellular matrix or neighboring cells plays a crucial role in many cellular functions, such as motility, differentiation and controlled cell death. Expanding on pioneering studies on defined 2-D model systems, the role of the currently known determinants (geometry, topography, biochemical functionality and mechanical properties) is currently addressed in more relevant 3-D matrices. However, there is a clear lack in currently available approaches to fabricate well defined microenvironments, which are asymmetric or in which these factors can be varied independently. The central objective of ASMIDIAS is the development of a novel route to asymmetric microenvironments for cell-matrix interaction studies. Inspired by molecular self-assembly on the one hand and guided macroscale assembly on the other hand, directed assembly of highly defined microfabricated building blocks will be exploited to this end. In this modular design approach different building blocks position themselves during assembly on pre-structured surfaces to afford enclosed volumes that are restricted by the walls of the blocks. The project relies on two central elements. For the guided assembly, the balance of attractive and repulsive interactions between the building blocks (and its dependence on the object dimensions) and the structured surface shall be controlled by appropriate surface chemistry and suitable guiding structures. To afford the required functionality, new approaches to (i) topographically structure, (ii) biochemically functionalize and pattern selected sides of the microscale building blocks and (iii) to control their surface elastic properties via surface-attached polymers and hydrogels, will be developed.The resulting unique asymmetric environments will facilitate novel insight into cell-matrix interactions, which possess considerable relevance in the areas of  tissue engineering, cell (de)differentiation, bacteria-surface interactions and beyond.","1484100","2011-11-01","2016-10-31"
"ASPIRE","Aqueous Supramolecular Polymers and Peptide Conjugates in Reversible Systems","Oren Alexander Scherman","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Supramolecular polymers are of major interest in the field of self assembly with a promising outlook in areas of viscosity modification, compartmentalized architectures, bio-conjugates and drug-delivery applications. They are dynamic macromolecular materials prepared by simple mixing of relatively small components bearing complementary or self-complementary recognition motifs. A major limitation in the field, however, has been access to synthetic systems capable of undergoing self assembly in an aqueous environment. This research proposal develops well-defined, self-organizing macromolecular structures that will overcome this limitation by focusing on systems that rely on several non-covalent interactions occurring in concert rather than on single interactions alone. The envisioned supramolecular polymers and bio-conjugates are designed as dynamic water-soluble smart materials, whose architectures can be controlled and exhibit reversibility upon exposure to external stimuli such as electrochemical, temperature or pH changes. Molecular recognition events occurring between functional handles on both synthetic and bio-polymers will be investigated in order to control the formation of desired functional architectures through stoichiometrically controlled complexation. Preparation of synthetic core motifs to assemble discrete peptide aggregates such as the dimeric through hexameric oligomers of amyloid-beta(40/42) will lead to structural elucidation and insight into several peptide misfolding pathologies like Alzheimer&apos;s or Parkinson&apos;s disease.","1700000","2009-11-01","2015-10-31"
"ASTROLAB","Cold Collisions and the Pathways Toward Life in Interstellar Space","Holger Kreckel","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Modern telescopes like Herschel and ALMA open up a new window into molecular astrophysics to investigate a surprisingly rich chemistry that operates even at low densities and low temperatures. Observations with these instruments have the potential of unraveling key questions of astrobiology, like the accumulation of water and pre-biotic organic molecules on (exo)planets from asteroids and comets. Hand-in-hand with the heightened observational activities comes a strong demand for a thorough understanding of the molecular formation mechanisms. The vast majority of interstellar molecules are formed in ion-neutral reactions that remain efficient even at low temperatures. Unfortunately, the unusual nature of these processes under terrestrial conditions makes their laboratory study extremely difficult.

To address these issues, I propose to build a versatile merged beams setup for laboratory studies of ion-neutral collisions at the Cryogenic Storage Ring (CSR), the most ambitious of the next-generation storage devices under development worldwide. With this experimental setup, I will make use of a low-temperature and low-density environment that is ideal to simulate the conditions prevailing in interstellar space. The cryogenic surrounding, in combination with laser-generated ground state atom beams, will allow me to perform precise energy-resolved rate coefficient measurements for reactions between cold molecular ions (like, e.g., H2+, H3+, HCO+, CH2+, CH3+, etc.) and neutral atoms (H, D, C or O) in order to shed light on long-standing problems of astrochemistry and the formation of organic molecules in space.
With the large variability of the collision energy (corresponding to 40-40000 K), I will be able to provide data that are crucial for the interpretation of molecular observations in a variety of objects, ranging from cold molecular clouds to warm layers in protoplanetary disks.","1486800","2012-09-01","2017-11-30"
"ASTROROT","Unraveling interstellar chemistry with broadband microwave spectroscopy and next-generation telescope arrays","Melanie Schnell-Küpper","STIFTUNG DEUTSCHES ELEKTRONEN-SYNCHROTRON DESY","The goal of the research program, ASTROROT, is to significantly advance the knowledge of astrochemistry by exploring its molecular complexity and by discovering new molecule classes and key chemical processes in space. So far, mostly physical reasons were investigated for the observed variations in molecular abundances. We here propose to study the influence of chemistry on the molecular composition of the universe by combining unprecedentedly high-quality laboratory spectroscopy and pioneering telescope observations. Array telescopes provide new observations of rotational molecular emission, leading to an urgent need for microwave spectroscopic data of exotic molecules. We will use newly developed, unique broadband microwave spectrometers with the cold conditions of a molecular jet and the higher temperatures of a waveguide to mimic different interstellar conditions. Their key advantages are accurate transition intensities, tremendously reduced measurement times, and unique mixture compatibility. 
Our laboratory experiments will motivate and guide astronomic observations, and enable their interpretation. The expected results are 
• the exploration of molecular complexity by discovering new classes of molecules in space, 
• the detection of isotopologues that provide information about the stage of chemical evolution, 
• the generation of abundance maps of highly excited molecules to learn about their environment,
• the identification of key intermediates in astrochemical reactions.
The results will significantly foster and likely revolutionize our understanding of astrochemistry. The proposed research will go far beyond the state-of-the-art: We will use cutting-edge techniques both in the laboratory and at the telescope to greatly improve and speed the process of identifying molecular fingerprints. These techniques now enable studies at this important frontier of physics and chemistry that previously would have been prohibitively time-consuming or even impossible.","1499904","2015-05-01","2020-04-30"
"ASYMMETRY","Measurement of CP violation in the B_s system at LHCb","Stephanie Hansmann-Menzemer","RUPRECHT-KARLS-UNIVERSITAET HEIDELBERG","The Large Hadron collider (LHC) at CERN will be a milestone for the understanding of fundamental interactions and for the future of high energy
physics. Four large experiments at the LHC are complementarily addressing the question of the origin of our Universe by searching for so-called New Physics.
The world of particles and their interactions is nowadays described by the Standard Model. Up to now there is no single measurement from laboratory experiments which contradicts this theory. However, there are still many open questions, thus physicists are convinced that there is a more fundamental theory, which incorporates New Physics.
It is expected that at the LHC either New Physics beyond the Standard Model will be discovered or excluded up to very high energies, which would revolutionize the understanding of particle physics and require completely new experimental and theoretical concepts.
The LHCb (Large Hadron Collider beauty) experiment is dedicated to precision measurements of B hadrons (B hadrons are all particles containing a beauty quark).
The analysis proposed here is the measurement of asymmetries between B_s particles and anti-B_s particles at the LHCb experiment. Any New Physics model will change the rate of observable processes via additional quantum corrections. Particle antiparticle asymmetries are extremely sensitive to these corrections thus a very powerful tool for indirect searches for New Physics contributions. In the past, most of the ground-breaking findings in particle physics, such as the existence of the
charm quark and the existence of a third quark family, have first been observed in indirect searches.
First - still statistically  limited - measurements of the asymmetry in the B_s system indicate a 2 sigma deviation from the Standard Model prediction. A precision measurement of this asymmetry is potentially the first observation for New Physics beyond the Standard Model at the LHC. If no hint for New Physics will be found, this measurement will severely restrict the range of potential New Physics models.","1059240","2011-01-01","2015-12-31"
"ATMEN","Atomic precision materials engineering","Toma SUSI","UNIVERSITAT WIEN","Despite more than fifty years of scientific progress since Richard Feynman's 1959 vision for nanotechnology, there is only one way to manipulate individual atoms in materials: scanning tunneling microscopy. Since the late 1980s, its atomically sharp tip has been used to move atoms over clean metal surfaces held at cryogenic temperatures. Scanning transmission electron microscopy, on the other hand, has been able to resolve atoms only more recently by focusing the electron beam with sub-atomic precision. This is especially useful in the two-dimensional form of hexagonally bonded carbon called graphene, which has superb electronic and mechanical properties. Several ways to further engineer those have been proposed, including by doping the structure with substitutional heteroatoms such as boron, nitrogen, phosphorus and silicon. My recent discovery that the scattering of the energetic imaging electrons can cause a silicon impurity to move through the graphene lattice has revealed a potential for atomically precise manipulation using the Ångström-sized electron probe. To develop this into a practical technique, improvements in the description of beam-induced displacements, advances in heteroatom implantation, and a concerted effort towards the automation of manipulations are required. My project tackles these in a multidisciplinary effort combining innovative computational techniques with pioneering experiments in an instrument where a low-energy ion implantation chamber is directly connected to an advanced electron microscope. To demonstrate the power of the method, I will prototype an atomic memory with an unprecedented memory density, and create heteroatom quantum corrals optimized for their plasmonic properties. The capability for atom-scale engineering of covalent materials opens a new vista for nanotechnology, pushing back the boundaries of the possible and allowing a plethora of materials science questions to be studied at the ultimate level of control.","1497202","2017-10-01","2022-09-30"
"ATMO","Atmospheres across the Universe","Pascal TREMBLIN","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","Which molecules are present in the atmosphere of exoplanets? What are their mass, radius and age? Do they have clouds, convection (atmospheric turbulence), fingering convection, or a circulation induced by irradiation? These questions are fundamental in exoplanetology in order to study issues such as planet formation and exoplanet habitability.

Yet, the impact of fingering convection and circulation induced by irradiation remain poorly understood:
 - Fingering convection (triggered by gradients of mean-molecular-weight) has already been suggested to happen in stars (accumulation of heavy elements) and in brown dwarfs and exoplanets (chemical transition e.g. CO/CH4). A large-scale efficient turbulent transport of energy through the fingering instability can reduce the temperature gradient in the atmosphere and explain many observed spectral properties of brown dwarfs and exoplanets. Nonetheless, this large-scale efficiency is not yet characterized and standard approximations (Boussinesq) cannot be used to achieve this goal. 
- The interaction between atmospheric circulation and the fingering instability is an open question in the case of irradiated exoplanets. Fingering convection can change the location and magnitude of the hot spot induced by irradiation, whereas the hot deep atmosphere induced by irradiation can change the location of the chemical transitions that trigger the fingering instability. 

This project will characterize the impact of fingering convection in the atmosphere of stars, brown dwarfs, and exoplanets and its interaction with the circulation in the case of irradiated planets. By developing innovative numerical models, we will characterize the reduction of the temperature gradient of the atmosphere induced by the instability and study the impact of the circulation. We will then predict and interpret the mass, radius, and chemical composition of exoplanets that will be observed with future missions such as the James Webb Space Telescope (JWST).","1500000","2018-02-01","2023-01-31"
"ATMOFLEX","Turbulent Transport in the Atmosphere: Fluctuations and Extreme Events","Jérémie Bec","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","A major part of the physical and chemical processes occurring in the atmosphere involves the turbulent transport of tiny particles. Current studies and models use a formulation in terms of mean fields, where the strong variations in the dynamical and statistical properties of the particles are neglected and where the underlying fluctuations of the fluid flow velocity are oversimplified. Devising an accurate understanding of the influence of air turbulence and of the extreme fluctuations that it generates in the dispersed phase remains a challenging issue. This project aims at coordinating and integrating theoretical, numerical, experimental, and observational efforts to develop a new statistical understanding of the role of fluctuations in atmospheric transport processes. The proposed work will cover individual as well as collective behaviors and will provide a systematic and unified description of targeted specific processes involving suspended drops or particles: the dispersion of pollutants from a source, the growth by condensation and coagulation of droplets and ice crystals in clouds, the scavenging, settling and re-suspension of aerosols, and the radiative and climatic effects of particles. The proposed approach is based on the use of tools borrowed from statistical physics and field theory, and from the theory of large deviations and of random dynamical systems in order to design new observables that will be simultaneously tractable analytically in simplified models and of relevance for the quantitative handling of such physical mechanisms. One of the outcomes will be to provide a new framework for improving and refining the methods used in meteorology and atmospheric sciences and to answer the long-standing question of the effects of suspended particles onto climate.","1200000","2009-11-01","2014-10-31"
"ATMOGAIN","Atmospheric Gas-Aerosol Interface: 
From Fundamental Theory to Global Effects","Ilona Anniina Riipinen","STOCKHOLMS UNIVERSITET","Atmospheric aerosol particles are a major player in the earth system: they impact the climate by scattering and absorbing solar radiation, as well as regulating the properties of clouds. On regional scales aerosol particles are among the main pollutants deteriorating air quality. Capturing the impact of aerosols is one of the main challenges in understanding the driving forces behind changing climate and air quality.

Atmospheric aerosol numbers are governed by the ultrafine (< 100 nm in diameter) particles. Most of these particles have been formed from atmospheric vapours, and their fate and impacts are governed by the mass transport processes between the gas and particulate phases. These transport processes are currently poorly understood. Correct representation of the aerosol growth/shrinkage by condensation/evaporation of atmospheric vapours is thus a prerequisite for capturing the evolution and impacts of aerosols.

I propose to start a research group that will address the major current unknowns in atmospheric ultrafine particle growth and evaporation. First, we will develop a unified theoretical framework to describe the mass accommodation processes at aerosol surfaces, aiming to resolve the current ambiguity with respect to the uptake of atmospheric vapours by aerosols. Second, we will study the condensational properties of selected organic compounds and their mixtures. Organic compounds are known to contribute significantly to atmospheric aerosol growth, but the properties that govern their condensation, such as saturation vapour pressures and activities, are largely unknown. Third, we aim to resolve the gas and particulate phase processes that govern the growth of realistic atmospheric aerosol. Fourth, we will parameterize ultrafine aerosol growth, implement the parameterizations to chemical transport models, and quantify the impact of these condensation and evaporation processes on global and regional aerosol budgets.","1498099","2011-09-01","2016-08-31"
"AtoFun","Atomic Scale Defects: Structure and Function","Felix HOFMANN","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Atomic scale defects play a key role in determining the behaviour of all crystalline materials, profoundly modifying mechanical, thermal and electrical properties. Many current technological applications make do with phenomenological descriptions of these effects; yet myriad intriguing questions about the fundamental link between defect structure and material function remain.

Transmission electron microscopy revolutionised the study of atomic scale defects by enabling their direct imaging. The novel coherent X-ray diffraction techniques developed in this project promise a similar advancement, making it possible to probe the strain fields that govern defect interactions in 3D with high spatial resolution (<10 nm). They will allow us to clarify the effect of impurities and retained gas on dislocation strain fields, shedding light on opportunities to engineer dislocation properties. The exceptional strain sensitivity of coherent diffraction will enable us to explore the fundamental mechanisms governing the behaviour of ion-implantation-induced point defects that are invisible to TEM. While we concentrate on dislocations and point defects, the new techniques will apply to all crystalline materials where defects are important. Our characterisation of defect structure will be combined with laser transient grating measurements of thermal transport changes due to specific defect populations. This unique multifaceted perspective of defect behaviour will transform our ability to devise modelling approaches linking defect structure to material function.

Our proof-of-concept results highlight the feasibility of this ambitious research project. It opens up a vast range of exciting possibilities to gain a deep, fundamental understanding of atomic scale defects and their effect on material function. This is an essential prerequisite for exploiting and engineering defects to enhance material properties.","1610231","2017-03-01","2022-02-28"
"ATOM","Advanced Holographic Tomographies for Nanoscale Materials: Revealing Electromagnetic and Deformation Fields, Chemical Composition and Quantum States at Atomic Resolution.","Axel LUBK","LEIBNIZ-INSTITUT FUER FESTKOERPER- UND WERKSTOFFFORSCHUNG DRESDEN E.V.","The ongoing miniaturization in nanotechnology and functional materials puts an ever increasing focus on the development of three-dimensional (3D) nanostructures, such as quantum dot arrays, structured nanowires, or non-trivial topological magnetic textures such as skyrmions, which permit a better performance of logical or memory devices in terms of speed and energy efficiency. To develop and advance such technologies and to improve the understanding of the underlying fundamental solid state physics effects, the nondestructive and quantitative 3D characterization of physical, e.g., electric or magnetic, fields down to atomic resolution is indispensable. Current nanoscale metrology methods only inadequately convey this information, e.g., because they probe surfaces, record projections, or lack resolution. AToM will provide a ground-breaking tomographic methodology for current nanotechnology by mapping electric and magnetic fields as well as crucial properties of the underlying atomic structure in solids, such as the chemical composition, mechanical strain or spin configuration in 3D down to atomic resolution. To achieve that goal, advanced holographic and tomographic setups in the Transmission Electron Microscope (TEM) are combined with novel computational methods, e.g., taking into account the ramifications of electron diffraction. Moreover, fundamental application limits are overcome (A) by extending the holographic principle, requiring coherent electron beams, to quantum state reconstructions applicable to electrons of any (in)coherence; and (B) by adapting a unique in-situ TEM with a very large sample chamber to facilitate holographic field sensing down to very low temperatures (6 K) under application of external, e.g., electric, stimuli. The joint development of AToM in response to current problems of nanotechnology, including the previously mentioned ones, is anticipated to immediately and sustainably advance nanotechnology in its various aspects.","1499602","2017-01-01","2021-12-31"
"ATOMICAR","ATOMic Insight Cavity Array Reactor","Peter Christian Kjærgaard VESBORG","DANMARKS TEKNISKE UNIVERSITET","The goal of ATOMICAR is to achieve the ultimate sensitivity limit in heterogeneous catalysis: 
Quantitative measurement of chemical turnover on a single catalytic nanoparticle.

Most heterogeneous catalysis occurs on metal nanoparticle in the size range of 3 nm - 10 nm. Model studies have established that there is often a strong coupling between nanoparticle size & shape - and catalytic activity. The strong structure-activity coupling renders it probable that “super-active” nanoparticles exist. However, since there is no way to measure catalytic activity of less than ca 1 million nanoparticles at a time, any super-activity will always be hidden by “ensemble smearing” since one million nanoparticles of exactly identical size and shape cannot be made. The state-of-the-art in catalysis benchmarking is microfabricated flow reactors with mass-spectrometric detection, but the sensitivity of this approach cannot be incrementally improved by six orders of magnitude. This calls for a new measurement paradigm where the activity of a single nanoparticle can be benchmarked – the ultimate limit for catalytic measurement.

A tiny batch reactor is the solution, but there are three key problems: How to seal it; how to track catalytic turnover inside it; and how to see the nanoparticle inside it? Graphene solves all three problems: A microfabricated cavity with a thin SixNy bottom window, a single catalytic nanoparticle inside, and a graphene seal forms a gas tight batch reactor since graphene has zero gas permeability. Catalysis is then tracked as an internal pressure change via the stress & deflection of the graphene seal. Crucially, the electron-transparency of graphene and SixNy enables subsequent transmission electron microscope access with atomic resolution so that active nanoparticles can be studied in full detail.

ATOMICAR will re-define the experimental limits of catalyst benchmarking and lift the field of basic catalysis research into the single-nanoparticle age.","1496000","2018-02-01","2023-01-31"
"ATOMION","Exploring hybrid quantum systems of ultracold atoms and ions","Michael Karl Koehl","RHEINISCHE FRIEDRICH-WILHELMS-UNIVERSITAT BONN","We propose to investigate hybrid quantum systems composed of ultracold atoms and ions. The mutual interaction of the cold neutral atoms and the trapped ion offers a wealth of interesting new physical problems. They span from ultracold quantum chemistry over new concepts for quantum information processing to genuine quantum many-body physics. We plan to explore aspects of quantum chemistry with ultracold atoms and ions to obtain a full understanding of the interactions in this hybrid system. We will investigate the regime of low energy collisions and search for Feshbach resonances to tune the interaction strength between atoms and ions. Moreover, we will study collective effects in chemical reactions between a Bose-Einstein condensate and a single ion. Taking advantage of the extraordinary properties of the atom-ion mixture quantum information processing with hybrid systems will be performed. In particular, we plan to realize sympathetic ground state cooling of the ion with a Bose-Einstein condensate. When the ion is immersed into the ultracold neutral atom environment the nature of the decoherence will be tailored by tuning properties of the environment: A dissipative quantum phase transition is predicted when the ion is coupled to a one-dimensional Bose gas. Moreover, we plan to realize a scalable hybrid quantum processor composed of a single ion and an array of neutral atoms in an optical lattice. The third direction we will pursue is related to impurity effects in quantum many-body physics. We plan to study transport through a single impurity or atomic quantum dot with the goal of realizing a single atom transistor. A single atom transistor transfers the quantum state of the impurity coherently to a macroscopic neutral atom current. Finally, we plan to observe Anderson s orthogonality catastrophe in which the presence of a single impurity in a quantum gas orthogonalizes the quantum many-body function of a quantum state with respect to the unperturbed one.","1405000","2009-10-01","2014-09-30"
"ATOMKI-PPROCESS","Nuclear reaction studies relevant to the astrophysical p-process nucleosynthesis","György Gyürky","Magyar Tudomanyos Akademia Atommagkutato Intezete","The astrophysical p-process, the stellar production mechanism of the heavy, proton rich isotopes (p-isotopes), is one of the least studied processes in nucleosynthesis. The astrophysical site(s) for the p-process could not yet be clearly identified. In order to reproduce the natural abundances of the p-isotopes, the p-process models must take into account a huge nuclear reaction network. A precise knowledge of the rate of the nuclear reactions in this network is essential for a reliable abundance calculation and for a clear assignment of the astrophysical site(s). For lack of experimental data the nuclear physics inputs for the reaction networks are based on statistical model calculations. These calculations are largely untested in the mass and energy range relevant to the p-process and the uncertainties in the reaction rate values result in a correspondingly uncertain prediction of the p-isotope abundances. Therefore, experiments aiming at the determination of reaction rates for the p-process are of great importance. In this project nuclear reaction cross section measurements will be carried out in the mass and energy range of p-process to check the reliability of the statistical model calculations and to put the p-process models on a more reliable base. The accelerators of the Institute of Nuclear Research in Debrecen, Hungary provide the necessary basis for such studies. The p-process model calculations are especially sensitive to the rates of reactions involving alpha particles and heavy nuclei. Because of technical difficulties, so far there are practically no experimental data available on such reactions and the uncertainty in these reaction rates is presently one of the biggest contributions to the uncertainty of p-isotope abundance calculations. With the help of the ERC grant the alpha-induced reaction cross sections can be measured on heavy isotopes for the first time, which could contribute to a better understanding of the astrophysical p-process.","750000","2008-07-01","2013-06-30"
"ATOMPHOTONLOQIP","Experimental Linear Optics Quantum Information Processing with Atoms and Photons","Jian-Wei Pan","RUPRECHT-KARLS-UNIVERSITAET HEIDELBERG","Quantum information science and atom optics are among the most active fields in modern physics. In recent years, many theoretical efforts have been made to combine these two fields. Recent experimental progresses have shown the in-principle possibility to perform scalable quantum information processing (QIP) with linear optics and atomic ensembles. The main purpose of the present project is to use atomic qubits as quantum memory and exploit photonic qubits for information transfer and processing to achieve efficient linear optics QIP. On the one hand, utilizing the interaction between laser pulses and atomic ensembles we will experimentally investigate the potentials of atomic ensembles in the gas phase to build quantum repeaters for long-distance quantum communication, that is, to develop a new technological solution for quantum repeaters making use of the effective qubit-type entanglement of two cold atomic ensembles by a projective measurement of individual photons by spontaneous Raman processes. On this basis, we will further investigate the advantages of cold atoms in an optical trap to enhance the coherence time of atomic qubits beyond the threshold for scalable realization of quantum repeaters. Moreover, building on our long experience in research on multi-photon entanglement, we also plan to perform a number of significant experiments in the field of QIP with particular emphasis on fault-tolerant quantum computation, photon-loss-tolerant quantum computation and cluster-state based quantum simulation. Finally, by combining the techniques developed in the above quantum memory and multi-photon interference experiments, we will further experimentally investigate the possibility to achieve quantum teleportation between photonic and atomic qubits, quantum teleportation between remote atomic qubits and efficient entanglement generation via classical feed-forward. The techniques that will be developed in the present project will lay the basis for future large scale","1435000","2008-07-01","2013-12-31"
"ATTOCO","Attosecond tracing of collective dynamics 
in clusters and nanoparticles","Matthias Friedrich Kling","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","Collective electron motion can unfold on attosecond time scales in nanoplasmonic systems, as defined by the inverse spectral bandwidth of the plasmonic resonant region. Similarly, in dielectrics or semiconductors, the laser-driven collective motion of electrons can occur on this characteristic time scale. Until now, such collective electron dynamics has not been directly observed on its natural, attosecond timescale. In ATTOCO, the attosecond, sub-cycle dynamics of strong-field driven collective electron dynamics in clusters and nanoparticles will be explored. Moreover, we will explore field-dependent processes induced by strong laser fields in nanometer sized matter, such as the metallization of dielectrics, which has been recently proposed theoretically.
In order to map the collective electron motion we will apply the attosecond nanoplasmonic streaking technique, which has been proposed and developed theoretically. In this approach, the temporal resolution is achieved by limiting the emission of high energetic, direct photoelectrons to a sub-cycle time window using attosecond XUV pulses phase-locked to a driving few-cycle near-infrared field. Kinetic energy spectra of the photoelectrons recorded for different delays between the excitation field and the ionizing XUV pulse will allow extracting the spatio-temporal electron dynamics. ATTOCO offers the capability to measure field-induced material changes in real-time and to gain novel insight into collective electron dynamics. In particular, we aim to learn from ATTOCO in detail, how the collective electron motion is established, how the collective motion is driven by the strong external field and over which pathways and timescale the collective motion decays.
ATTOCO provides also a major step in the development of lightwave (nano-)electronics, which may push the frontiers of electronics from multi-gigahertz to petahertz frequencies. If successfully accomplished, this development will herald the potential scalability of electron-based information technologies to lightwave frequencies, surpassing the speed of current computation and communication technology by many orders of magnitude.","1498500","2013-06-01","2018-05-31"
"ATTOELECTRONICS","Attoelectronics: Steering electrons in atoms and molecules with synthesized waveforms of light","Eleftherios Goulielmakis","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","In order for electronics to meet the ever raising demands for higher speeds of operation, the dimensions of its basic elements drop continuously. This miniaturization, that will soon meet the dimensions of a single molecule or an atom, calls for new approaches in electronics that take advantage, rather than confront the dominant at these scales quantum laws.

Electronics on the scale of atoms and molecules require fields that are able to trigger and to steer electrons at speeds comparable to their intrinsic dynamics, determined by the quantum mechanical laws. For the valence electrons of atoms and molecules, this motion is clocked in tens to thousands of attoseconds, (1 as =10-18 sec) implying the potential for executing basic electronic operations in the PHz regime and beyond. This is approximately ~1000000 times faster as compared to any contemporary technology.

To meet this challenging goal, this project will utilize conceptual and technological advances of attosecond science as its primary tools. First, pulses of light, the fields of which can be sculpted and characterized with attosecond accuracy, for triggering as well as for terminating the ultrafast electron motion in an atom or a molecule. Second, attosecond pulses in the extreme ultraviolet, which can probe and frame-freeze the created electron motion, with unprecedented resolution, and determine the direction and the magnitude of the created currents.

This project will interrogate the limits of the fastest electronic motion that light fields can trigger as well as terminate, a few hundreds of attoseconds later, in an atom or a molecule. In this way it aims to explore new routes of atomic and molecular scale electronic switching at PHz frequencies.","1262000","2010-12-01","2016-11-30"
"ATTOSCOPE","Measuring attosecond electron dynamics in molecules","Hans Jakob Wörner","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","""The goal of the present proposal is to realize measurements of electronic dynamics in polyatomic
molecules with attosecond temporal resolution (1 as = 10^-18s). We propose to study electronic
rearrangements following photoexcitation, charge migration in a molecular chain induced by
ionization and non-adiabatic multi-electron dynamics in an intense laser field. The grand question
addressed by this research is the characterization of electron correlations which control the shape, properties and function of molecules. In all three proposed projects, a time-domain approach appears to be the most suitable since it reduces complex molecular dynamics to the purely electronic dynamics by exploiting the hierarchy of motional time scales. Experimentally, we propose to realize an innovative experimental setup. A few-cycle infrared (IR) pulse will be used to generate attosecond pulses in the extreme-ultraviolet (XUV) by high-harmonic generation. The IR pulse will be separated from the XUV by means of an innovative interferometer. Additionally, it will permit the introduction of a controlled attosecond delay between the two pulses. We propose to use the attosecond pulses as a tool to look inside individual IR- or UV-field cycles to better understand light-matter interactions. Time-resolved pump-probe experiments will be carried out on polyatomic molecules by detecting the energy and angular distribution of photoelectrons in a velocity-map imaging spectrometer. These experiments are expected to provide new insights
into the dynamics of multi-electron systems along with new results for the validation and
improvement of theoretical models. Multi-electron dynamics is indeed a very complex subject
on its own and even more so in the presence of strong laser fields. The proposed experiments
directly address theses challenges and are expected to provide new insights that will be beneficial to a wide range of scientific research areas.""","1999992","2012-09-01","2017-08-31"
"AUTO-EVO","Autonomous DNA Evolution in a Molecule Trap","Dieter Braun","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","How can we create molecular life in the lab?
That is, can we drive evolvable DNA/RNA-machines under a simple nonequilibrium setting? We will trigger basic forms
of autonomous Darwinian evolution by implementing replication, mutation and selection on the molecular level in a single
micro-chamber? We will explore protein-free replication schemes to tackle the Eigen-Paradox of replication and translation
under archaic nonequilibrium settings. The conditions mimic thermal gradients in porous rock near hydrothermal vents on the
early earth. We are in a unique position to pursue these questions due to our previous inventions of convective replication,
optothermal molecule traps and light driven microfluidics. Four interconnected strategies are pursued ranging from basic
replication using tRNA-like hairpins, entropic cooling or UV degradation down to protein-based DNA evolution in a trap, all
with biotechnological applications. The approach is risky, however very interesting physics and biology on the way. We will:
(i) Replicate DNA with continuous, convective PCR in the selection of a thermal molecule trap
(ii) Replicate sequences with metastable, tRNA-like hairpins exponentially
(iii) Build DNA complexes by structure-selective trapping to replicate by entropic decay
(iv) Drive replication by Laser-based UV degradation
Both replication and trapping are exponential processes, yielding in combination a highly nonlinear dynamics. We proceed
along publishable steps and implement highly efficient modes of continuous molecular evolution. As shown in the past, we
will create biotechnological applications from basic scientific questions (see our NanoTemper Startup). The starting grant will
allow us to compete with Jack Szostak who very recently picked up our approach [JACS 131, 9628 (2009)].","1487827","2010-08-01","2015-07-31"
"AutoCPS","Automated Synthesis of Cyber-Physical Systems: A Compositional Approach","Majid ZAMANI","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","Embedded Control software plays a critical role in many safety-critical applications. For instance, modern vehicles use interacting software and hardware components to control steering and braking. Control software forms the main core of autonomous transportation, power networks, and aerospace. These applications are examples of cyber-physical systems (CPS), where distributed software systems interact tightly with spatially distributed physical systems with complex dynamics. CPS are becoming ubiquitous due to rapid advances in computation, communication, and memory. However, the development of core control software running in these systems is still ad hoc and error-prone and much of the engineering costs today go into ensuring that control software works correctly.

In order to reduce the design costs and guaranteeing its correctness, I aim to develop an innovative design process, in which the embedded control software is synthesized from high-level correctness requirements in a push-button and formal manner. Requirements for modern CPS applications go beyond conventional properties in control theory (e.g. stability) and in computer science (e.g. protocol design). Here, I propose a compositional methodology for automated synthesis of control software by combining compositional techniques from computer science (e.g. assume-guarantee rules) with those from control theory (e.g. small-gain theorems). I will leverage decomposition and abstraction as two key tools to tackle the design complexity, by either breaking the design object into semi-independent parts or by aggregating components and eliminating unnecessary details. My project is high-risk because it requires a fundamental re-thinking of design techniques till now studied in separate disciplines. It is high-gain because a successful method for automated synthesis of control software will make it finally possible to develop complex yet reliable CPS applications while considerably reducing the engineering cost.","1470800","2019-02-01","2024-01-31"
"AV-SMP","Algorithmic Verification of String Manipulating Programs","Anthony LIN","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","String is among the most fundamental and commonly used data types in virtually all modern programming languages, especially with the rapidly growing popularity of scripting languages (e.g. JavaScript and Python). Programs written in such languages tend to perform heavy string manipulations, which are complex to reason about and could easily lead to programming mistakes. In some cases, such mistakes could have serious consequences, e.g., in the case of client-side web applications, cross-site scripting (XSS) attacks that could lead to a security breach by a malicious user.

The central objective of the proposed project is to develop novel verification algorithms for analysing the correctness (esp. with respect to safety and termination properties) of programs with string variables, and transform them into robust verification tools. To meet this key objective, we will make fundamental breakthroughs on both theoretical and tool implementation challenges. On the theoretical side, we address two important problems: (1) design expressive constraint languages over strings (in combination with other data types like integers) that permit decidability with good complexity, and (2) design generic semi-algorithms for verifying string programs that have strong theoretical performance guarantee. On the implementation side, we will address the challenging problem of designing novel implementation methods that can substantially speed up the basic string analysis procedures in practice. Finally, as a proof of concept, we will apply our technologies to two key application domains: (1) automatic detection of XSS vulnerabilities in web applications, and (2) automatic grading systems for a programming course.

The project will not only make fundamental theoretical contributions — potentially solving long-standing open problems in the area — but also yield powerful methods that can be used in various applications.","1496687","2017-11-01","2022-10-31"
"AxScale","Axions and relatives across different mass scales","Babette DÖBRICH","EUROPEAN ORGANIZATION FOR NUCLEAR RESEARCH","Pseudoscalar QCD axions and axion-like Particles (ALPs) are an excellent candidate for Dark Matter or can act as a mediator particle for Dark Matter. Since the discovery of the Higgs boson, we know that fundamental scalars exist and it is timely to explore the Axion/ALP parameter space more intensively. A look at the allowed axion/ALP parameter space makes it clear that these might exist at low mass (below few eV), as (part of) Dark Matter. Alternatively they might exist at higher mass, above roughly the MeV scale, potentially as a Dark Matter mediator particle. AxScale explores parts of these different mass regions, with complementary techniques but with one research team. 
Firstly, with RADES, it develops a novel  concept for a filter-like cavity for the search of QCD axion Dark matter at a few tens of a micro-eV. Dark Matter Axions can be discovered by their resonant conversion in that cavity embedded in a strong magnetic field. The `classical axion window' has recently received much interest from cosmological model-building and I will implement a novel cavity concept that will allow to explore this Dark Matter parameter region. 
Secondly, AxScale searches for axions and ALPs using the NA62 detector at CERN's SPS. Especially the mass region above a few MeV can be efficiently searched by the use of a proton fixed-target facility. During nominal data taking NA62 investigates a Kaon beam. NA62 can also run in a mode in which its primary proton beam is fully dumped. With the resulting high interaction rate, the existence of weakly coupled particles can be efficiently probed. Thus, searches for  ALPs from Kaon decays as well as from production in dumped protons with NA62 are foreseen in AxScale. More generally, NA62 can look for a plethora of `Dark Sector' particles with recorded and future data. With the AxScale program I aim at maximizing the reach of NA62 for these new physics models.","1134375","2018-11-01","2023-10-31"
"BACCO","Bias and Clustering Calculations Optimised: Maximising discovery with galaxy surveys","Raúl Esteban ANGULO de la Fuente","FUNDACION CENTRO DE ESTUDIOS DE FISICA DEL COSMOS DE ARAGON","A new generation of galaxy surveys will soon start measuring the spatial distribution of millions of galaxies over a broad range of redshifts, offering an imminent opportunity to discover new physics. A detailed comparison of these measurements with theoretical models of galaxy clustering may reveal a new fundamental particle, a breakdown of General Relativity, or a hint on the nature of cosmic acceleration. Despite a large progress in the analytic treatment of structure formation in recent years, traditional clustering models still suffer from large uncertainties. This limits cosmological analyses to a very restricted range of scales and statistics, which will be one of the main obstacles to reach a comprehensive exploitation of future surveys.

Here I propose to develop a novel simulation--based approach to predict galaxy clustering. Combining recent advances in computational cosmology, from cosmological N--body calculations to physically-motivated galaxy formation models, I will develop a unified framework to directly predict the position and velocity of individual dark matter structures and galaxies as function of cosmological and astrophysical parameters. In this formulation, galaxy clustering will be a prediction of a set of physical assumptions in a given cosmological setting. The new theoretical framework will be flexible, accurate and fast: it will provide predictions for any clustering statistic, down to scales 100 times smaller than in state-of-the-art perturbation--theory--based models, and in less than 1 minute of CPU time. These advances will enable major improvements in future cosmological constraints, which will significantly increase the overall power of future surveys maximising our potential to discover new physics.","1484240","2017-09-01","2022-08-31"
"BACKTOBACK","Engineering Solutions for Back Pain: Simulation of Patient Variance","Ruth Wilcox","UNIVERSITY OF LEEDS","Back pain affects eight out of ten adults during their lifetime. It a huge economic burden on society, estimated to cost as much as 1-2% of gross national product in several European countries.  Treatments for back pain have lower levels of success and are not as technologically mature as those for other musculoskeletal disorders such as hip and knee replacement. This application proposes to tackle one of the major barriers to the development of better surgical treatments for back pain.
At present, new spinal devices are commonly assessed in isolation in the laboratory under standardised conditions that do not represent the variation across the patient population. Consequently many interventions have failed during clinical trials or have proved to have poor long term success rates.
Using a combination of computational and experimental models, a new testing methodology will be developed that will enable the variation between patients to be simulated for the first time. This will enable spinal implants and therapies to be more robustly evaluated across a virtual patient population prior to clinical trial. The tools developed will be used in collaboration with clinicians and basic scientists to develop and, crucially, optimise new treatments that reduce back pain whilst preserving the unique functions of the spine.
If successful, this approach could be translated to evaluate and optimise emerging minimally invasive treatments in other joints such as the hip and knee. Research in the spine could then, for the first time, lead rather than follow that undertaken in other branches of orthopaedics.","1498777","2012-12-01","2018-11-30"
"BANDWIDTH","The cost of limited communication bandwidth in distributed computing","Keren CENSOR-HILLEL","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","Distributed systems underlie many modern technologies, a prime example being the Internet. The ever-increasing abundance of distributed systems necessitates their design and usage to be backed by strong theoretical foundations.

A major challenge that distributed systems face is the lack of a central authority, which brings many aspects  of uncertainty into the environment, in the form of unknown network topology or unpredictable dynamic behavior. A practical restriction of distributed systems, which is at the heart of this proposal, is the limited bandwidth available for communication between the network components.

A central family of distributed tasks is that of local tasks, which are informally described as tasks which are possible to solve by sending information through only a relatively small number of hops. A cornerstone example is the need to break symmetry and provide a better utilization of resources, which can be obtained by the task of producing a valid coloring of the nodes given some small number of colors. Amazingly, there are still huge gaps between the known upper and lower bounds for the complexity of many local tasks. This holds even if one allows powerful assumptions of unlimited bandwidth. While some known algorithms indeed use small messages, the complexity gaps are even larger compared to the unlimited bandwidth case. This is not a mere coincidence, and in fact the existing theoretical infrastructure is provably incapable of
giving stronger lower bounds for many local tasks under limited bandwidth.

This proposal zooms in on this crucial blind spot in the current literature on the theory of distributed computing, namely, the study of local tasks under limited bandwidth. The goal of this research is to produce fast algorithms for fundamental distributed local tasks under restricted bandwidth, as well as understand their limitations by providing lower bounds.","1486480","2018-06-01","2023-05-31"
"BASTION","Leveraging Binary Analysis to Secure the Internet of Things","Thorsten Holz","RUHR-UNIVERSITAET BOCHUM","We are in the midst of the shift towards the Internet of Things (IoT), where more and more (legacy) devices are connected to the Internet and communicate with each other.  This paradigm shift brings new security challenges and unfortunately many current security solutions are not applicable anymore, e.g., because of a lack of clear network boundaries or resource-constrained devices. However, security plays a central role: In addition to its classical function in protecting against manipulation and fraud, it also enables novel applications and innovative business models. 

We propose a research program that leverages binary analysis techniques to improve the security within the IoT. We concentrate on the software level since this enables us to both analyze a given device for potential security vulnerabilities and add security features to harden the device against future attacks. More specifically, we concentrate on the firmware (i.e., the combination of persistent memory together with program code and data that powers such devices) and develop novel mechanism for binary analysis of such software. We design an intermediate language to abstract away from the concrete assembly level and this enables an analysis of many different platforms within a unified analysis framework. We transfer and extend program analysis techniques such as control-/data-flow analysis or symbolic execution and apply them to our IL. Given this novel toolset, we can analyze security properties of a given firmware image (e.g., uncovering undocumented functionality and detecting memory corruption or logical vulnerabilities,). We also explore how to harden a firmware by retrofitting security mechanisms (e.g., adding control-flow integrity or automatically eliminating unnecessary functionality). This research will deepen our fundamental understanding of binary analysis methods and apply it to a novel area as it lays the foundations of performing this analysis on the level of intermediate languages.","1472269","2015-03-01","2020-02-29"
"BATMAN","Development of Quantitative Metrologies to Guide Lithium Ion Battery Manufacturing","Vanessa Wood","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Lithium ion batteries offer tremendous potential as an enabling technology for sustainable transportation and development. However, their widespread usage as the energy storage solution for electric mobility and grid-level integration of renewables is impeded by the fact that current state-of-the-art lithium ion batteries have energy densities that are too small, charge- and discharge rates that are too low, and costs that are too high. Highly publicized instances of catastrophic failure of lithium ion batteries raise questions of safety.  Understanding the limitations to battery performance and origins of the degradation and failure is highly complex due to the difficulties in studying interrelated processes that take place at different length and time scales in a corrosive environment. In the project, we will (1) develop and implement quantitative methods to study the complex interrelations between structure and electrochemistry occurring at the nano-, micron-, and milli-scales in lithium ion battery active materials and electrodes, (2) conduct systematic experimental studies with our new techniques to understand the origins of performance limitations and to develop design guidelines for achieving high performance and safe batteries, and (3) investigate economically viable engineering solutions based on these guidelines to achieve high performance and safe lithium ion batteries.","1500000","2016-05-01","2021-04-30"
"BCOOL","Barocaloric materials for energy-efficient solid-state cooling","Javier Eduardo Moya Raposo","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Cooling is essential for food and drinks, medicine, electronics and thermal comfort. Thermal changes due to pressure-driven phase transitions in fluids have long been used in vapour compression systems to achieve continuous refrigeration and air conditioning, but their energy efficiency is relatively low, and the working fluids that are employed harm the environment when released to the atmosphere. More recently, the discovery of large thermal changes due to pressure-driven phase transitions in magnetic solids has led to suggestions for environmentally friendly solid-state cooling applications. However, for this new cooling technology to succeed, it is still necessary to find suitable barocaloric (BC) materials that satisfy the demanding requirements set by applications, namely very large thermal changes in inexpensive materials that occur near room temperature in response to small applied pressures.

I aim to develop new BC materials by exploiting phase transitions in non-magnetic solids whose structural and thermal properties are strongly coupled, namely ferroelectric salts, molecular crystals and hybrid materials. These materials are normally made from cheap abundant elements, and display very large latent heats and volume changes at structural phase transitions, which make them ideal candidates to exhibit extremely large BC effects that outperform those observed in state-of-the-art BC magnetic materials, and that match applications.

My unique approach combines: i) materials science to identify materials with outstanding BC performance, ii) advanced experimental techniques to explore and exploit these novel materials, iii) materials engineering to create new composite materials with enhanced BC properties, and iv) fabrication of BC devices, using insight gained from modelling of materials and device parameters. If successful, my ambitious strategy will culminate in revolutionary solid-state cooling devices that are environmentally friendly and energy efficient.","1467521","2016-04-01","2021-03-31"
"Beacon","Beacons in the Dark","Paulo César Carvalho Freire","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","BEACON aims at performing an ambitious multi-disciplinary (optical, radio astronomy and theoretical physics) study to enable a fundamentally improved understanding of gravitation and space-time. For almost a century Einstein's general relativity has been the last word on gravity. However, superstring theory predicts new gravitational phenomena beyond relativity. In this proposal I will attempt to detect these new phenomena, with a sensitivity 20 times better than state-of-the-art attempts. A successful detection would take physics beyond its current understanding of the Universe.

These new gravitational phenomena are emission of dipolar gravitational waves and the violation of the strong equivalence principle (SEP). I plan to look for them by timing newly discovered binary pulsars. I will improve upon the best current limits on dipolar gravitational wave emission by a factor of 20 within the time of this proposal. I also plan to develop a test of the Strong Equivalence Principle using a new pulsar/main-sequence star binary. The precision of this test is likely to surpass the current best limits within the time frame of this proposal and then keep improving indefinitely with time. This happens because this is the cleanest gravitational experiment ever carried out.

In order to further these goals, I plan to build the ultimate pulsar observing system. By taking advantage of recent technological advances in microwave engineering (particularly sensitive ultra-wide band receivers) digital electronics (fast analogue-to-digital converters and digital spectrometers) and computing, my team and me will be able to greatly improve the sensitivity and precision for pulsar timing experiments and exploit the capabilities of modern radio telescopes to their limits.

Pulsars are the beacons that will guide me in these new, uncharted seas.","1892376","2011-09-01","2016-08-31"
"BEACON","Hybrid Digital-Analog Networking under Extreme Energy and Latency Constraints","Deniz Gunduz","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","The objective of the BEACON project is to (re-)introduce analog communications into the design of modern wireless networks. We argue that the extreme energy and latency constraints imposed by the emerging Internet of Everything (IoE) paradigm can only be met within a hybrid digital-analog communications framework. Current network architectures separate source and channel coding, orthogonalize users, and employ long block-length digital source and channel codes, which are either suboptimal or not applicable under the aforementioned constraints. BEACON questions these well-established design principles, and proposes to replace them with a hybrid digital-analog communications framework, which will meet the required energy and latency constraints while simplifying the encoding and decoding processes. BEACON pushes the performance of the IoE to its theoretical limits by i) exploiting signal correlations that are abundant in IoE applications, given the foreseen density of deployed sensing devices, ii) taking into account the limited and stochastic nature of energy availability due to, for example, energy harvesting capabilities, iii) using feedback resources to improve the end-to-end signal distortion, and iv) deriving novel converse results to identify fundamental performance benchmarks.

The results of BEACON will not only shed light on the fundamental limits on the performance any coding scheme can achieve, but will also lead to the development of unconventional codes and communication protocols that can approach these limits, combining digital and analog communication techniques. The ultimate challenge for this project is to exploit the developed hybrid digital-analog networking theory for a complete overhaul of the physical layer design for emerging IoE applications, such as smart grids, tele-robotics and smart homes. For this purpose, a proof-of-concept implementation test-bed will also be built using software defined radios and sensor nodes.","1496350","2016-10-01","2021-09-30"
"BeadsOnString","Beads on String Genomics: Experimental Toolbox for Unmasking Genetic / Epigenetic Variation in Genomic DNA and Chromatin","Yuval Ebenstein","TEL AVIV UNIVERSITY","Next generation sequencing (NGS) is revolutionizing all fields of biological research but it fails to extract the full range of information associated with genetic material and is lacking in its ability to resolve variations between genomes. The high degree of genome variation exhibited both on the population level as well as between genetically “identical” cells (even in the same organ) makes genetic and epigenetic analysis on the single cell and single genome level a necessity.
Chromosomes may be conceptually represented as a linear one-dimensional barcode. However, in contrast to a traditional binary barcode approach that considers only two possible bits of information (1 & 0), I will use colour and molecular structure to expand the variety of information represented in the barcode. Like colourful beads threaded on a string, where each bead represents a distinct type of observable, I will label each type of genomic information with a different chemical moiety thus expanding the repertoire of information that can be simultaneously measured. A major effort in this proposal is invested in the development of unique chemistries to enable this labelling.
I specifically address three types of genomic variation: Variations in genomic layout (including DNA repeats, structural and copy number variations), variations in the patterns of chemical DNA modifications (such as methylation of cytosine bases) and variations in the chromatin composition (including nucleosome and transcription factor distributions). I will use physical extension of long DNA molecules on surfaces and in nanofluidic channels to reveal this information visually in the form of a linear, fluorescent “barcode” that is read-out by advanced imaging techniques. Similarly, DNA molecules will be threaded through a nanopore where the sequential position of “bulky” molecular groups attached to the DNA may be inferred from temporal modulation of an ionic current measured across the pore.","1627600","2013-10-01","2018-09-30"
"BEAM-EDM","Unique Method for a Neutron Electric Dipole Moment Search using a Pulsed Beam","Florian Michael PIEGSA","UNIVERSITAET BERN","My research encompasses the application of novel methods and strategies in the field of low energy particle physics. The goal of the presented program is to lead an independent and highly competitive experiment to search for a CP violating neutron electric dipole moment (nEDM), as well as for new exotic interactions using highly sensitive neutron and proton spin resonance techniques.
The measurement of the nEDM is considered to be one of the most important fundamental physics experiments at low energy. It represents a promising route for finding new physics beyond the standard model (SM) and describes an important search for new sources of CP violation in order to understand the observed large baryon asymmetry in our universe. The main project will follow a novel concept based on my original idea, which plans to employ a pulsed neutron beam at high intensity instead of the established use of storable ultracold neutrons. This complementary and potentially ground-breaking method provides the possibility to distinguish between the signal due to a nEDM and previously limiting systematic effects, and should lead to an improved result compared to the present best nEDM beam experiment. The findings of these investigations will be of paramount importance and will form the cornerstone for the success of the full-scale experiment intended for the European Spallation Source. A second scientific question will be addressed by performing spin precession experiments searching for exotic short-range interactions and associated light bosons. This is a vivid field of research motivated by various extensions to the SM. The goal of these measurements, using neutrons and protons, is to search for additional interactions such new bosons mediate between ordinary particles.
Both topics describe ambitious and unique efforts. They use related techniques, address important questions in fundamental physics, and have the potential of substantial scientific implications and high-impact results.","1404062","2017-04-01","2022-03-31"
"BEBOP","Bacterial biofilms in porous structures: from biomechanics to control","Yohan, Jean-Michel, Louis DAVIT","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The key ideas motivating this project are that: 1) precise control of the properties of porous systems can be obtained by exploiting bacteria and their fantastic abilities; 2) conversely, porous media (large surface to volume ratios, complex structures) could be a major part of bacterial synthetic biology, as a scaffold for growing large quantities of microorganisms in controlled bioreactors.

The main scientific obstacle to precise control of such processes is the lack of understanding of biophysical mechanisms in complex porous structures, even in the case of single-strain biofilms. The central hypothesis of this project is that a better fundamental understanding of biofilm biomechanics and physical ecology will yield a novel theoretical basis for engineering and control.

The first scientific objective is thus to gain insight into how fluid flow, transport phenomena and biofilms interact within connected multiscale heterogeneous structures - a major scientific challenge with wide-ranging implications. To this end, we will combine microfluidic and 3D printed micro-bioreactor experiments; fluorescence and X-ray imaging; high performance computing blending CFD, individual-based models and pore network approaches.

The second scientific objective is to create the primary building blocks toward a control theory of bacteria in porous media and innovative designs of microbial bioreactors. Building upon the previous objective, we first aim to extract from the complexity of biological responses the most universal engineering principles applying to such systems. We will then design a novel porous micro-bioreactor to demonstrate how the permeability and solute residence times can be controlled in a dynamic, reversible and stable way - an initial step toward controlling reaction rates.

We envision that this will unlock a new generation of biotechnologies and novel bioreactor designs enabling translation from proof-of-concept synthetic microbiology to industrial processes.","1649861","2019-01-01","2023-12-31"
"BEBOP","Binaries Escorted By Orbiting Planets","Amaury TRIAUD","THE UNIVERSITY OF BIRMINGHAM","Planets orbiting both stars of a binary system -circumbinary planets- are challenging our understanding about how planets assemble, and how their orbits subsequently evolve. Long confined to science-fiction, circumbinary planets were confirmed by the Kepler spacecraft, in one of its most spectacular, and impactful result. Despite Kepler’s insights, a lot remains unknown about these planets. Kepler also suffered from intractable biases that the BEBOP project will solve.

BEBOP will revolutionise how we detect and study circumbinary planets. Conducting a Doppler survey, we will vastly improve the efficiency of circumbinary planet detection, and remove Kepler’s biases. BEBOP will construct a clearer picture of the circumbinary planet population, and free us from the inherent vagaries, and important costs of space-funding. Thanks to the Doppler method we will study dynamical effects unique to circumbinary planets, estimate their multiplicity, and compute their true occurrence rate.

Circumbinary planets are essential objects. Binaries disturbe planet formation. Any similarity, and any difference between the population of circumbinary planets and planets orbiting single stars, will bring novel information about how planets are produced. In addition, circumbinary planets have unique orbital properties that boost their probability to experience transits. BEBOP’s detections will open the door to atmospheric studies of colder worlds than presently available.

Based on already discovered systems, and on two successful proofs-of-concept, the BEBOP team will detect 15 circumbinary gas-giants, three times more than Kepler. BEBOP will provide an unambiguous measure of the efficiency of gas-giant formation in circumbinary environments. In addition the BEBOP project comes with an ambitious programme to combine three detection methods (Doppler, transits, and astrometry) in a holistic approach that will bolster investigations into circumbinary planets, and create a lasting legacy.","1186313","2018-11-01","2023-10-31"
"BEFINE","mechanical BEhavior of Fluid-INduced Earthquakes","Marie, Estelle, Solange VIOLAY","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Fluids play an important role in fault zone and in earthquakes generation. Fluid pressure reduces the normal effective stress, lowering the frictional strength of the fault, potentially triggering earthquake ruptures. Fluid injection induced earthquakes (FIE) are direct evidence of the effect of fluid pressure on the fault strength. In addition, natural earthquake sequences are often associated with high fluid pressures at seismogenic depths. Although simple in theory, the mechanisms that govern the nucleation, propagation and recurrence of FIEs are poorly constrained, and our ability to assess the seismic hazard that is associated with natural and induced events remains limited. This project aims to enhance our knowledge of FIE mechanisms over entire seismic cycles through multidisciplinary approaches, including the following: 
- Set-up and installation of a new and unique rock friction apparatus that is dedicated to the study of FIEs.
- Low strain rate friction experiments (coupled with electrical conductivity measurements) to investigate the influence of fluids on fault creep and earthquake recurrence. 
- Intermediate strain rate friction experiments to investigate the effect of fluids on fault stability during earthquake nucleation. 
- High strain rate friction experiments to investigate the effect of fluids on fault weakening during earthquake propagation.
- Post-mortem experimental fault analyses with state-of-art microstructural techniques.
- The theoretical friction law will be calibrated with friction experiments and faulted rock microstructural observations. 

These steps will produce fundamental discoveries regarding natural earthquakes and tectonic processes and help scientists understand and eventually manage the occurrence of induced seismicity, an increasingly hot topic in geo-engineering. The sustainable exploitation of geo-resources is a key research and technology challenge at the European scale, with a substantial economical and societal impact.","1982925","2018-03-01","2023-02-28"
"BEGMAT","Layered functional materials - beyond 'graphene'","Michael Janus Bojdys","HUMBOLDT-UNIVERSITAET ZU BERLIN","There is an apparent lack of non-metallic 2D-matrials for the construction of electronic devices, as only five materials of the “graphene family” are known: graphene, hBN, BCN, fluorographene, and graphene oxide – none of them with a narrow bandgap close to commercially used silicon. This ERC-StG proposal, BEGMAT, outlines a strategy for design, synthesis, and application of layered, functional materials that will go beyond this exclusive club. These materials “beyond graphene” (BEG) will have to meet – like graphene – the following criteria: 
(1) The BEG-materials will feature a transfer of crystalline order from the molecular (pm-range) to the macroscopic level (cm-range), 
(2) individual, free-standing layers of BEG-materials can be addressed by mechanical or chemical exfoliation, and 
(3) assemblies of different BEG-materials will be stacked as van der Waals heterostructures with unique properties. 
In contrast to the existing “graphene family”, 
(4) BEG-materials will be constructed in a controlled way by covalent organic chemistry in a bottom-up approach from abundant precursors free of metals and critical raw materials (CRMs). 
Moreover – and unlike – many covalent organic frameworks (COFs), 
(5) BEG-materials will be fully aromatic, donor-acceptor systems to ensure that electronic properties can be addressed on macroscopic scale. 
The potential to make 2D materials “beyond graphene” is a great challenge to chemical bond formation and material design. In 2014 the applicant has demonstrated the feasibility of the concept to expand the “graphene family” with triazine-based graphitic carbon, a compound highlighted as an “emerging competitor for the miracle material” graphene. Now, the PI has the opportunity to build a full-scale research program on layered functional materials that offers unique insights into controlled, covalent linking-chemistry, and that addresses practicalities in device manufacture, and structure-properties relationships.","1362538","2016-08-01","2021-07-31"
"BetaDropNMR","Ultra-sensitive NMR in liquids","Magdalena Kowalska-Wyrowska","EUROPEAN ORGANIZATION FOR NUCLEAR RESEARCH","""The nuclear magnetic resonance spectroscopy (NMR) is a versatile and powerful tool, especially in chemistry and in biology. However, its limited sensitivity and small amount of suitable probe nuclei pose severe constraints on the systems that may be explored.
This project aims at overcoming the above limitations by giving NMR an ultra-high sensitivity and by enlarging the NMR """"toolbox"""" to dozens of nuclei across the periodic table. This will be achieved by applying the β-NMR method to the soft matter samples. The method relies on anisotropic emission of β particles in the decay of highly spin-polarized nuclei. This feature results in 10 orders of magnitude more sensitivity compared to conventional NMR and makes it applicable to elements which are otherwise difficult to investigate spectroscopically. β-NMR has been successfully applied in nuclear physics and material science in solid samples and high-vacuum environments, but never before to liquid samples placed in atmospheric pressure. With this novel approach I want to create a new universal and extremely sensitive tool to study various problems in biochemistry.
The first questions which I envisage addressing with this ground-breaking and versatile method concern the interaction of essential metal ions, which are spectroscopically silent in most techniques, Mg2+, Cu+, and Zn2+, with proteins and nucleic acids. The importance of these studies is well motivated by the fact that half of the proteins in our human body contain metal ions, but their interaction mechanism and factors influencing it are still not fully understood. In this respect NMR spectroscopy is of great help: it provides information on the structure, dynamics, and chemical properties of the metal complexes, by revealing the coordination number, oxidation state, bonding situation and electronic configuration of the interacting metal.
My long-term aim is to establish a firm basis for β-NMR in soft matter studies in biology, chemistry and physics.""","1500000","2015-10-01","2020-09-30"
"BETTERSENSE","Nanodevice Engineering for a Better Chemical Gas Sensing Technology","Juan Daniel Prades Garcia","UNIVERSITAT DE BARCELONA","BetterSense aims to solve the two main problems in current gas sensor technologies: the high power consumption and the poor selectivity. For the former, we propose a radically new approach: to integrate the sensing components and the energy sources intimately, at the nanoscale, in order to achieve a new kind of sensor concept featuring zero power consumption. For the latter, we will mimic the biological receptors designing a kit of gas-specific molecular organic functionalizations to reach ultra-high gas selectivity figures, comparable to those of biological processes. Both cutting-edge concepts will be developed in parallel an integrated together to render a totally new gas sensing technology that surpasses the state-of-the-art.
As a matter of fact, the project will enable, for the first time, the integration of gas detectors in energetically autonomous sensors networks. Additionally, BetterSense will provide an integral solution to the gas sensing challenge by producing a full set of gas-specific sensors over the same platform to ease their integration in multi-analyte systems. Moreover, the project approach will certainly open opportunities in adjacent fields in which power consumption, specificity and nano/micro integration are a concern, such as liquid chemical and biological sensing.
In spite of the promising evidences that demonstrate the feasibility of this proposal, there are still many scientific and technological issues to solve, most of them in the edge of what is known and what is possible today in nano-fabrication and nano/micro integration. For this reason, BetterSense also aims to contribute to the global challenge of making nanodevices compatible with scalable, cost-effective, microelectronic technologies.
For all this, addressing this challenging proposal in full requires a funding scheme compatible with a high-risk/high-gain vision to finance the full dedication of a highly motivated research team with multidisciplinary skill","1498452","2014-02-01","2019-01-31"
"BeyondA1","Set theory beyond the first uncountable cardinal","Assaf Shmuel Rinot","BAR ILAN UNIVERSITY","We propose to establish a research group that will unveil the combinatorial nature of the second uncountable cardinal. This includes its Ramsey-theoretic, order-theoretic, graph-theoretic and topological features. Among others, we will be directly addressing fundamental problems due to  Erdos, Rado, Galvin, and Shelah. 
While some of these problems are old and well-known, an unexpected series of breakthroughs from the last three years suggest that now is a promising point in time to carry out such a project. Indeed, through a short period, four previously unattainable problems concerning the second uncountable cardinal were successfully tackled: Aspero on a club-guessing problem of Shelah, Krueger on the club-isomorphism problem for Aronszajn trees, Neeman on the isomorphism problem for dense sets of reals, and the PI on the Souslin problem. Each of these results was obtained through the development of a completely new technical framework, and these frameworks could now pave the way for the solution of some major open questions.
A goal of the highest risk in this project is the discovery of a consistent (possibly, parameterized) forcing axiom that will (preferably, simultaneously) provide structure theorems for stationary sets, linearly ordered sets, trees, graphs, and partition relations, as well as the refutation of various forms of club-guessing principles, all at the level of the second uncountable cardinal. In comparison, at the level of the first uncountable cardinal, a forcing axiom due to Foreman, Magidor and Shelah achieves exactly that. 
To approach our goals, the proposed project is divided into four core areas: Uncountable trees, Ramsey theory on ordinals, Club-guessing principles, and Forcing Axioms. There is a rich bilateral interaction between any pair of the four different cores, but the proposed division will allow an efficient allocation of manpower, and will increase the chances of parallel success.","1362500","2018-10-01","2023-09-30"
"BeyondBlackbox","Data-Driven Methods for Modelling and Optimizing the Empirical Performance of Deep Neural Networks","Frank Roman HUTTER","ALBERT-LUDWIGS-UNIVERSITAET FREIBURG","Deep neural networks (DNNs) have led to dramatic improvements of the state-of-the-art for many important classification problems, such as object recognition from images or speech recognition from audio data. However, DNNs are also notoriously dependent on the tuning of their hyperparameters. Since their manual tuning is time-consuming and requires expert knowledge, recent years have seen the rise of Bayesian optimization methods for automating this task. While these methods have had substantial successes, their treatment of DNN performance as a black box poses fundamental limitations, allowing manual tuning to be more effective for large and computationally expensive data sets: humans can (1) exploit prior knowledge and extrapolate performance from data subsets, (2) monitor the DNN's internal weight optimization by stochastic gradient descent over time, and (3) reactively change hyperparameters at runtime. We therefore propose to model DNN performance beyond a blackbox level and to use these models to develop for the first time:

1. Next-generation Bayesian optimization methods that exploit data-driven priors to optimize performance orders of magnitude faster than currently possible;
2. Graybox Bayesian optimization methods that have access to -- and exploit -- performance and state information of algorithm runs over time; and
3. Hyperparameter control strategies that learn across different datasets to adapt hyperparameters reactively to the characteristics of any given situation.

DNNs play into our project in two ways. First, in all our methods we will use (Bayesian) DNNs to model and exploit the large amounts of performance data we will collect on various datasets. Second, our application goal is to optimize and control DNN hyperparameters far better than human experts and to obtain:

4. Computationally inexpensive auto-tuned deep neural networks, even for large datasets, enabling the widespread use of deep learning by non-experts.","1495000","2017-01-01","2021-12-31"
"BeyondWorstCase","Algorithms beyond the Worst Case","Heiko Roglin","RHEINISCHE FRIEDRICH-WILHELMS-UNIVERSITAT BONN","For many optimization problems that arise in logistics, information retrieval, and other contexts the classical theory of algorithms has lost its grip on reality because it is based on a pessimistic worst-case perspective, in which the performance of an algorithm is solely measured by its behavior on the worst possible input. This does not take into consideration that worst-case inputs are often rather contrived and occur only rarely in practical applications. It led to the situation that for many problems the classical theory is not able to differentiate meaningfully between different algorithms. Even worse, for some important problems it recommends algorithms that perform badly in practice over algorithms that work well in practice only because the artificial worst-case performance of the latter ones is bad.

We will study classic optimization problems (traveling salesperson problem, linear programming, etc.) as well as problems coming from machine learning and information retrieval. All these problems have in common that the practically most successful algorithms have a devastating worst-case performance even though they clearly outperform the theoretically best algorithms.

Only in recent years a paradigm shift towards a more realistic and robust algorithmic theory has been initiated. This project will play a major role in this paradigm shift by developing and exploring novel theoretical approaches (e.g. smoothed analysis) to reconcile theory and practice. A more realistic theory will have a profound impact on the design and analysis of algorithms in the future, and the insights gained in this project will lead to algorithmic tools for large-scale optimization problems that improve on existing ad hoc methods. We will not only work theoretically but also test the applicability of our theoretical considerations in experimental studies.","1235820","2012-10-01","2017-09-30"
"BIAF","Bird Inspired Autonomous Flight","Shane Paul Windsor","UNIVERSITY OF BRISTOL","The agile and efficient flight of birds shows what flight performance is physically possible, and in theory could be achieved by unmanned air vehicles (UAVs) of the same size. The overall aim of this project is to enhance the performance of small scale UAVs by developing novel technologies inspired by understanding how birds are adapted to interact with airflows. Small UAVs have the potential to dramatically change current practices in many areas such as, search and rescue, surveillance, and environmental monitoring. Currently the utility of these systems is limited by their operational endurance and their inability to operate in strong turbulent winds, especially those that often occur in urban environments. Birds are adapted to be able to fly in these conditions and actually use them to their advantage to minimise their energy output.

This project is composed of three tracks which contain elements of technology development, as well as scientific investigation looking at bird flight behaviour and aerodynamics. The first track looks at developing path planning algorithms for UAVs in urban environments based on how birds fly in these areas, by using GPS tracking and computational fluid dynamics alongside trajectory optimization. The second track aims to develop artificial wings with improved gust tolerance inspired by the features of feathered wings. Here, high speed video measurements of birds flying through gusts will be used alongside wind tunnel testing of artificial wings to discover what features of a bird’s wing help to alleviate gusts. The third track develops novel force and flow sensor arrays for autonomous flight control based on the sensor arrays found in flying animals. These arrays will be used to make UAVs with increased agility and robustness. This unique bird inspired approach uses biology to show what is possible, and engineering to find the features that enable this performance and develop them into functional technologies.","1998546","2016-04-01","2021-03-31"
"BIDECASEOX","Bio-inspired Design of Catalysts for Selective Oxidations of C-H and C=C Bonds","Miguel Costas Salgueiro","UNIVERSITAT DE GIRONA","The selective functionalization of C-H and C=C bonds remains a formidable unsolved problem, owing to their inert nature. Novel alkane and alkene oxidation reactions exhibiting good and/or unprecedented selectivities will have a big impact on bulk and fine chemistry by opening novel methodologies that will allow removal of protection-deprotection sequences, thus streamlining synthetic strategies. These goals are targeted in this project via design of iron and manganese catalysts inspired by structural elements of the active site of non-heme enzymes of the Rieske Dioxygenase family. Selectivity is pursued via rational design of catalysts that will exploit substrate recognition-exclusion phenomena, and control over proton and electron affinity of the active species. Moreover, these catalysts will employ H2O2 as oxidant, and will operate under mild conditions (pressure and temperature). The fundamental mechanistic aspects of the catalytic reactions, and the species implicated in C-H and C=C oxidation events will also be studied with the aim of building on the necessary knowledge to design future generations of catalysts, and provide models to understand the chemistry taking place in non-heme iron and manganese-dependent oxygenases.","1299998","2009-11-01","2015-10-31"
"Big Splash","Big Splash: Efficient Simulation of Natural Phenomena at Extremely Large Scales","Christopher John Wojtan","Institute of Science and Technology Austria","Computational simulations of natural phenomena are essential in science, engineering, product design, architecture, and computer graphics applications. However, despite progress in numerical algorithms and computational power, it is still unfeasible to compute detailed simulations at large scales. To make matters worse, important phenomena like turbulent splashing liquids and fracturing solids rely on delicate coupling between small-scale details and large-scale behavior. Brute-force computation of such phenomena is intractable, and current adaptive techniques are too fragile, too costly, or too crude to capture subtle instabilities at small scales. Increases in computational power and parallel algorithms will improve the situation, but progress will only be incremental until we address the problem at its source.

I propose two main approaches to this problem of efficiently simulating large-scale liquid and solid dynamics. My first avenue of research combines numerics and shape: I will investigate a careful de-coupling of dynamics from geometry, allowing essential shape details to be preserved and retrieved without wasting computation. I will also develop methods for merging small-scale analytical solutions with large-scale numerical algorithms. (These ideas show particular promise for phenomena like splashing liquids and fracturing solids, whose small-scale behaviors are poorly captured by standard finite element methods.) My second main research direction is the manipulation of large-scale simulation data: Given the redundant and parallel nature of physics computation, we will drastically speed up computation with novel dimension reduction and data compression approaches. We can also minimize unnecessary computation by re-using existing simulation data. The novel approaches resulting from this work will undoubtedly synergize to enable the simulation and understanding of complicated natural and biological processes that are presently unfeasible to compute.","1500000","2015-03-01","2020-02-29"
"BIGCODE","Learning from Big Code: Probabilistic Models, Analysis and Synthesis","Martin Vechev","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","The goal of this proposal is to fundamentally change the way we build and reason about software. We aim to develop new kinds of statistical programming systems that provide probabilistically likely solutions to tasks that are difficult or impossible to solve with traditional approaches.

These statistical programming systems will be based on probabilistic models of massive codebases (also known as ``Big Code'') built via a combination of advanced programming languages and powerful machine learning and natural language processing techniques. To solve a particular challenge, a statistical programming system will query a probabilistic model, compute the most likely predictions, and present those to the developer.

Based on probabilistic models of ``Big Code'', we propose to investigate new statistical techniques in the context of three fundamental research directions: i) statistical program synthesis where we develop techniques that automatically synthesize and predict new programs, ii) statistical prediction of program properties where we develop new techniques that can predict important facts (e.g., types) about programs, and iii) statistical translation of programs where we investigate new techniques for statistical translation of programs (e.g., from one programming language to another, or to a natural language).

We believe the research direction outlined in this interdisciplinary proposal opens a new and exciting area of computer science. This area will combine sophisticated statistical learning and advanced programming language techniques for building the next-generation statistical programming systems. 

We expect the results of this proposal to have an immediate impact upon millions of developers worldwide, triggering a paradigm shift in the way tomorrow's software is built, as well as a long-lasting impact on scientific fields such as machine learning, natural language processing, programming languages and software engineering.","1500000","2016-04-01","2021-03-31"
"BigEarth","Accurate and Scalable Processing of Big Data in Earth Observation","Begüm Demir","TECHNISCHE UNIVERSITAT BERLIN","During the last decade, a huge number of earth observation (EO) satellites with optical and Synthetic Aperture Radar sensors onboard have been launched and advances in satellite systems have increased the amount, variety and spatial/spectral resolution of EO data. This has led to massive EO data archives with huge amount of remote sensing (RS) images, from which mining and retrieving useful information are challenging. In view of that, content based image retrieval (CBIR) has attracted great attention in the RS community. However, existing RS CBIR systems have limitations on: i) characterization of high-level semantic content and spectral information present in RS images, and ii) large-scale RS CBIR problems since their search mechanism is time-demanding and not scalable in operational applications. The BigEarth project aims to develop highly innovative feature extraction and content based retrieval methods and tools for RS images, which can significantly improve the state-of-the-art both in the theory and in the tools currently available. To this end, very important scientific and practical problems will be addressed by focusing on the main challenges of Big EO data on RS image characterization, indexing and search from massive archives. In particular, novel methods and tools will be developed, aiming to: 1) characterize and exploit high level semantic content and spectral information present in RS images; 2) extract features directly from the compressed RS images; 3) achieve accurate and scalable RS image indexing and retrieval; and 4) integrate feature representations of different RS image sources into a unified form of feature representation. Moreover, a benchmark archive with high amount of multi-source RS images will be constructed. From an application point of view, the developed methodologies and tools will have a significant impact on many EO data applications, such as accurate and scalable retrieval of: specific man-made structures and burned forest areas.","1491479","2018-04-01","2023-03-31"
"BIHSNAM","Bio-inspired Hierarchical Super Nanomaterials","Nicola Pugno","UNIVERSITA DEGLI STUDI DI TRENTO","""Nanomaterials such as carbon nanotubes or graphene sheets represent the future of material science, due to their potentially exceptional mechanical properties. One great drawback of all artificial materials, however, is the decrease of strength with increasing toughness, and viceversa. This problem is not encountered in many biological nanomaterials (e.g. spider silk, bone, nacre). Other biological materials display exceptional adhesion or damping properties, and can be self-cleaning or self-healing. The “secret” of biomaterials seems to lie in “hierarchy”: several levels can often be identified (2 in nacre, up to 7 in bone and dentine), from nano- to micro-scale.
The idea of this project is to combine Nature and Nanotechnology to design hierarchical composites with tailor made characteristics, optimized with respect to both strength and toughness, as well as materials with strong adhesion/easy detachment, smart damping, self-healing/-cleaning properties or controlled energy dissipation. For example, one possible objective is to design the “world’s toughest composite material”. The potential impact and importance of these goals on materials science, the high-tech industry and ultimately the quality of human life could be considerable.
In order to tackle such a challenging design process, the PI proposes to adopt ultimate nanomechanics theoretical tools corroborated by continuum or atomistic simulations, multi-scale numerical parametric simulations and Finite Element optimization procedures, starting from characterization experiments on biological- or nano-materials, from the macroscale to the nanoscale. Results from theoretical, numerical and experimental work packages will be applied to a specific case study in an engineering field of particular interest to demonstrate importance and feasibility, e.g. an airplane wing with a considerably enhanced fatigue resistance and reduced ice-layer adhesion, leading to a 10 fold reduction in wasted fuel.""","1004400","2012-01-01","2016-12-31"
"BILUM","Novel applications based on organic biluminescence","Sebastian Reineke","TECHNISCHE UNIVERSITAET DRESDEN","Organic semiconducting molecules often make for very good luminescent materials. Fundamental excitations are localized on single molecules, which is in stark contrast to inorganic semiconductors, such that exchange interactions lead to energetically distinct singlet and triplet states. The singlet-excited state is the origin of conventional fluorescence. However, once an excitation is in the molecular triplet state, emission of photons is very unlikely, because spin conservation needs to be broken. Here, non-radiative recombination outcompetes the radiative.
Recent research efforts led to the discovery of highly efficient biluminescence. Here, in addition to the fluorescence from the singlet state, the phosphorescence (triplet state emission) is unlocked by suppression of non-radiative channels at room temperature. The dynamics of both states is vastly different with nanosecond fluorescence and millisecond phosphorescence. If both channels are highly luminescent, then there is no room for loss channels.
Within BILUM, the virtually unexplored phenomenon of biluminescence will be the central point: On the basic science side, efforts will be focussed on the detailed understanding of structure-property relationships that are key for efficient dual state emission. At the same time, with a curiosity driven engineering approach, known bilumophores will be carefully tested in different scenarios to set the ground for future applications. Biluminescence has the potential to access non-radiative triplet states that are in many cases system limiting, to serve as ultra-broadband emitters, to introduce persistent (ultra long-lived) emission, to store photonic energy, and to allow optical sensing with internal reference emission – all on the molecular level. New bilumophores will be identified through systematic screening that will employ quantum chemical calculations and developed through organic synthesis.","1462500","2016-04-01","2021-03-31"
"BIMOC","Biomimetic Organocatalysis – Development of Novel Synthetic Catalytic Methodology and Technology","Magnus Rueping","RHEINISCH-WESTFAELISCHE TECHNISCHE HOCHSCHULE AACHEN","Biomimetic Organocatalysis – Development of Novel Synthetic Catalytic Methodology and Technology The objective of the proposed research is the design and development of unprecedented preassembled, modular, molecular factories. Inspiration comes from nature’s non-ribosomal peptide synthetases (NRPSs) and polyketide synthetases (PKSs). These large multifunctional enzymes possess catalytic modules with the capacity for recognition, activation and modification required for sequential biosynthesis of complex peptides and polyketides. Using nature as a role model we intend to design and prepare such catalyst “factories” synthetically and apply them in novel cascade reaction sequences. The single catalytic modules employed will be based on organocatalytic procedures, including enamine-, iminium-, as well as hydrogen bonding activation processes, but the potential scope is limitless. Organocatalysts have so far never been applied in a combined fashion utilizing their different activation mechanisms in multiple reaction cascades. Therefore, it is our intention to firstly demonstrate that such a production line approach is feasible and that these new catalyst systems can be applied in the synthesis of valuable enantiopure, biologically active, building blocks and natural products. Additionally, the extensive possibilities to vary organocatalyst modules in sequence will lead to science mimicking nature in its diversity.","999960","2008-09-01","2012-08-31"
"BinCosmos","The Impact of Massive Binaries Through Cosmic Time","Selma DE MINK","UNIVERSITEIT VAN AMSTERDAM","Massive stars play many key roles in Astrophysics. As COSMIC ENGINES they transformed the pristine Universe left after the Big Bang into our modern Universe.  We use massive stars, their explosions and products as COSMIC PROBES to study the conditions in the distant Universe and the extreme physics inaccessible at earth.  Models of massive stars are thus widely applied. A central common assumption is that massive stars are non-rotating single objects, in stark contrast with new data. Recent studies show that majority (70% according to our data) will experience severe interaction with a companion (Sana, de Mink et al. Science 2012). 

I propose to conduct the most ambitious and extensive exploration to date of the effects of binarity and rotation on the lives and fates of massive stars to (I) transform our understanding of the complex physical processes and how they operate in the vast parameter space and (II) explore the cosmological implications after calibrating and verifying the models. To achieve this ambitious objective I will use an innovative computational approach that combines the strength of two highly complementary codes and seek direct confrontation with observations to overcome the computational challenges that inhibited previous work.  

This timely project will provide the urgent theory framework needed for interpretation and guiding of observing programs with the new facilities (JWST, LSST, aLIGO/VIRGO).  Public release of the model grids and code will ensure wide impact of this project. I am in the unique position to successfully lead this project because of my (i) extensive experience modeling the complex physical processes, (ii) leading role in introducing large statistical simulations in the massive star community and (iii) direct involvement in surveys that will be used in this project.","1926634","2017-09-01","2022-08-31"
"BinGraSp","Modeling the Gravitational Spectrum of Neutron Star Binaries","Sebastiano Bernuzzi","FRIEDRICH-SCHILLER-UNIVERSITAT JENA","The most energetic electromagnetic phenomena in the Universe are believed to be powered by the collision of two neutron stars, the smallest and densest stars on which surface gravity is about 2 billion times stronger than gravity on Earth. However, a definitive identification of neutron star mergers as central engines for short-gamma-ray bursts and kilonovae transients is possible only by direct gravitational-wave observations. The latter provide us with unique information on neutron stars' masses, radii, and spins, including the possibility to set the strongest observational constraints on the unknown equation-of-state of matter at supranuclear densities.  

Neutron stars binary mergers are among the main targets for ground-based gravitational-wave interferometers like Advanced LIGO and Virgo, which start operations this year. The astrophysical data analysis of the signals emitted by these sources requires the availability of accurate waveform models, which are missing to date. Hence, the theoretical understanding of the gravitational spectrum is a necessary and urgent step for the development of a gravitational-based astrophysics in the next years.  

This project aims at developing, for the first time, a precise theoretical model for the complete gravitational spectrum of neutron star binaries, including the merger and postmerger stages of the coalescence process. Building on the PI's unique expertise and track record, the proposed research exploits synergy between analytical and numerical methods in General Relativity. Results from state of the art nonlinear 3D numerical relativity simulations will be combined with the most advanced analytical framework for the relativistic two-body problem. The model developed here will be used in the first gravitational-wave observations and will dramatically impact multimessenger astrophysics.","1432301","2017-10-01","2022-09-30"
"BIO-ORIGAMI","Meta-biomaterials: 3D printing meets Origami","Amir Abbas Zadpoor","TECHNISCHE UNIVERSITEIT DELFT","Meta-materials, best known for their extraordinary properties (e.g. negative stiffness), are halfway from both materials and structures: their unusual properties are direct results of their complex 3D structures. This project introduces a new class of meta-materials called meta-biomaterials. Meta-biomaterials go beyond meta-materials by adding an extra dimension to the complex 3D structure, i.e. complex and precisely controlled surface nano-patterns. The 3D structure gives rise to unprecedented or rare combination of mechanical (e.g. stiffness), mass transport (e.g. permeability, diffusivity), and biological (e.g. tissue regeneration rate) properties. Those properties optimize the distribution of mechanical loads and the transport of nutrients and oxygen while providing geometrical shapes preferable for tissue regeneration (e.g. higher curvatures). Surface nano-patterns communicate with (stem) cells, control their differentiation behavior, and enhance tissue regeneration.
There is one important problem: meta-biomaterials cannot be manufactured with current technology. 3D printing can create complex shapes while nanolithography creates complex surface nano-patterns down to a few nanometers but only on flat surfaces. There is, however, no way of combining complex shapes with complex surface nano-patterns. The groundbreaking nature of this project is in solving that deadlock using the Origami concept (the ancient Japanese art of paper folding). In this approach, I first decorate flat 3D-printed sheets with nano-patterns. Then, I apply Origami techniques to fold the decorated flat sheet and create complex 3D shapes. The sheet knows how to self-fold to the desired structure when subjected to compression, owing to pre-designed joints, crease patterns, and thickness/material distributions that control its mechanical instability. I will demonstrate the added value of meta-biomaterials in improving bone tissue regeneration using in vitro cell culture assays and animal models","1499600","2016-02-01","2021-01-31"
"BIO2CHEM-D","Biomass to chemicals: Catalysis design from first principles for a sustainable chemical industry","Nuria Lopez","FUNDACIO PRIVADA INSTITUT CATALA D'INVESTIGACIO QUIMICA","The use of renewable feedstocks by the chemical industry is fundamental due to both the depletion of fossil
resources and the increasing pressure of environmental concerns. Biomass can act as a sustainable source of
organic industrial chemicals; however, the establishment of a renewable chemical industry that is
economically competitive with the present oil-based one requires the development of new processes to
convert biomass-derived compounds into useful industrial materials following the principles of green
chemistry. To achieve these goals, developments in several fields including heterogeneous catalysis are
needed. One of the ways to accelerate the discovery of new potentially active, selective and stable catalysts is
the massive use of computational chemistry. Recent advances have demonstrated that Density Functional
Theory coupled to ab initio thermodynamics, transition state theory and microkinetic analysis can provide a
full view of the catalytic phenomena.
The aim of the present project is thus to employ these well-tested computational techniques to the
development of a theoretical framework that can accelerate the identification of new catalysts for the
conversion of biomass derived target compounds into useful chemicals. Since compared to petroleum-based
materials-biomass derived ones are multifuncionalized, the search for new catalytic materials and processes
has a strong requirement in the selectivity of the chemical transformations. The main challenges in the
project are related to the high functionalization of the molecules, their liquid nature and the large number of
potentially competitive reaction paths. The requirements of specificity and selectivity in the chemical
transformations while keeping a reasonably flexible framework constitute a major objective. The work will
be divided in three main work packages, one devoted to the properties of small molecules or fragments
containing a single functional group; the second addresses competition in multiple functionalized molecules;
and third is dedicated to the specific transformations of two molecules that have already been identified as
potential platform generators. The goal is to identify suitable candidates that could be synthetized and tested
in the Institute facilities.","1496200","2010-10-01","2015-09-30"
"BIOCERENG","Bioceramics: Multiscale Engineering of Advanced Ceramics at the Biology Interface","Kurosch Rezwan","UNIVERSITAET BREMEN","In the last decades, Materials Sciences and Life Sciences, two highly dynamically evolving and interdisciplinary research areas, have been influencing natural and engineering sciences significantly, creating new challenges and opportunities. A prime example for an increasing synergetic overlap of Materials and Life Sciences is provided by biomedical and bioengineering applications, which are of great academic, but also of steadily increasing societal and commercial interest. Bridging the traditional borders of disciplinary thinking in these areas has become one of today’s most challenging tasks for scientists. One group of key materials that are of great importance to biomedical engineering and bioengineering are advanced oxide and non-oxide ceramics with specific functionalities towards biological environments, so-called Bioceramics. The interplay at the interface of ceramics-protein-cells/bacteria is very complex and requires multiscale and interdisciplinary approaches. This expertise, that is under continuous development in my Bioceramics group, encompasses materials processing, shaping, surface functionalisation and cells/bacteria evaluation at the same time. The comprehensive research environment and expertise provides a unique opportunity to engineer materials/surfaces with immediate subsequent biological evaluation in order to achieve an extremely short development time. A centre of focus is the contribution of electrostatic and hydrophilic/hydrophobic interactions to the overall biocompatibility and -activity. The proposed research project includes four closely interrelated subprojects, addressing the following topics: “Interaction of surface functionalised ceramic particles with proteins”, “Cytotoxicity of functionalised oxide particles”, “Fabrication and testing of functionalised porous Al2O3 as filters for water cleaning and bioengineering applications” and “Novel functional scaffold composites for bone tissue engineering”.","1536120","2009-01-01","2013-12-31"
"BioCircuit","Programmable BioMolecular Circuits: Emulating Regulatory Functions in Living Cells Using a Bottom-Up Approach","Tom Antonius Franciscus De greef","TECHNISCHE UNIVERSITEIT EINDHOVEN","Programmable biomolecular circuits have received increasing attention in recent years as the scope of chemistry expands from the synthesis of individual molecules to the construction of chemical networks that can perform sophisticated functions such as logic operations and feedback control. Rationally engineered biomolecular circuits that robustly execute higher-order spatiotemporal behaviours typically associated with intracellular regulatory functions present a unique and uncharted platform to systematically explore the molecular logic and physical design principles of the cell. The experience gained by in-vitro construction of artificial cells displaying advanced system-level functions deepens our understanding of regulatory networks in living cells and allows theoretical assumptions and models to be refined in a controlled setting. This proposal combines elements from systems chemistry, in-vitro synthetic biology and micro-engineering and explores generic strategies to investigate the molecular logic of biology’s regulatory circuits by applying a physical chemistry-driven bottom-up approach. Progress in this field requires 1) proof-of-principle systems where in-vitro biomolecular circuits are designed to emulate characteristic system-level functions of regulatory circuits in living cells and 2) novel experimental tools to operate biochemical networks under out-of-equilibrium conditions. Here, a comprehensive research program is proposed that addresses these challenges by engineering three biochemical model systems that display elementary signal transduction and information processing capabilities. In addition, an open microfluidic droplet reactor is developed that will allow, for the first time, high-throughput analysis of biomolecular circuits encapsulated in water-in-oil droplets. An integral part of the research program is to combine the computational design of in-vitro circuits with novel biochemistry and innovative micro-engineering tools.","1887180","2016-08-01","2021-07-31"
"BIOELE","Functional Biointerface Elements via Biomicrofabrication","YANYAN HUANG","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Imagine in the future, bionic devices that can merge device and biology which can perform molecular sensing, simulate the functions of grown-organs in the lab, or even replace or improve parts of the organ as smart implants? Such bionic devices is set to transform a number of emerging fields, including synthetic biotechnology, regenerative medicine, and human-machine interfaces. Merging biology and man-made devices also mean that materials of vastly different properties need to be seamlessly integrated. One of the promising strategies to manufacture these devices is through 3D printing, which can structure different materials into functional devices, and simultaneously intertwining with biological matters. However, the requirement for biocompatibility, miniaturisation, portability and high performance in bionic devices pushes the current limit for micro- nanoscale 3D printing.  

This proposal aims to develop a new multi-material, cross-length scale biofabrication platform, with specific focus in making future smart bionic devices. In particular, a new mechanism is proposed to smoothly interface diverse classes of materials, such that an active device component can be ‘shrunk’ into a single small fibre. This mechanism utilises the polymeric materials’ flow property under applied tensile forces, and their abilities to combine with other classes of materials, such as semi-conductors and metals to impart further functionalities. This smart device fibre can be custom-made to perform different tasks, such as light emission or energy harvesting, to bridge 3D bioprinting for the future creation of high performance, compact, and cell-friendly bionic and medical devices.","1486938","2018-01-01","2022-12-31"
"BIOFUNCTION","Self assembly into biofunctional molecules, translating instructions into function","Nicolas Winssinger","UNIVERSITE DE STRASBOURG","The overall objective of the proposal is to develop enabling chemical technologies to address two important problems in biology: detect in a nondestructive fashion gene expression or microRNA sequences in vivo and, secondly, study the role of multivalency and spatial organization in carbohydrate recognition. Both of these projects exploit the programmable pre-organization of peptide nucleic acid (PNA) to induce a chemical reaction in the first case or modulate a ligand-receptor interaction in the second case. For nucleic acid detection, a DNA or RNA fragment will be utilized to bring two PNA fragments bearing reactive functionalities in close proximity thereby promoting a reaction. Two types of reactions are proposed, the first one to release a fluorophore for imaging purposes and the second one to release a drug as an “intelligent” therapeutic. If affinities are programmed such that hybridization is reversible, the template can work catalytically leading to large amplifications. As a proof of concept, this method will be used to measure the transcription level of genes implicated in stem cell differentiation and detect mutations in oncogenes. For the purpose of studying multivalent carbohydrate ligand architectures, the challenge of chemical synthesis has been a limiting factor. A supramolecular approach is proposed herein where different arrangements of carbohydrates can be displayed in a well organized fashion by hybridizing PNA-tagged carbohydrates to DNA templates. This will be used not only to control the distance between multiple ligands or to create combinatorial arrangements of hetero ligands but also to access more complex architectures such as Hollyday junctions. The oligosaccharide units will be prepared using de novo organoctalytic reactions. This technology will be first applied to probe the recognition events between HIV and dendritic cells which promote HIV infection.","1249980","2008-07-01","2013-06-30"
"BIOGRAPHENE","Sequencing biological molecules with graphene","Gregory Schneider","UNIVERSITEIT LEIDEN","Graphene – a one atom thin material – has the potential to act as a sensor, primarily the surface and the edges of graphene. This proposal aims at exploring new biosensing routes by exploiting the unique surface and edge chemistry of graphene.","1499996","2014-05-01","2019-04-30"
"BioInspired_SolarH2","Engineering Bio-Inspired Systems for the Conversion of Solar Energy to Hydrogen","Elisabet ROMERO MESA","FUNDACIO PRIVADA INSTITUT CATALA D'INVESTIGACIO QUIMICA","With this proposal, I aim to achieve the efficient conversion of solar energy to hydrogen. The overall objective is to engineer bio-inspired systems able to convert solar energy into a separation of charges and to construct devices by coupling these systems to catalysts in order to drive sustainable and effective water oxidation and hydrogen production.

The global energy crisis requires an urgent solution, we must replace fossil fuels for a renewable energy source: Solar energy. However, the efficient and inexpensive conversion and storage of solar energy into fuel remains a fundamental challenge. Currently, solar-energy conversion devices suffer from energy losses mainly caused by disorder in the materials used. The solution to this problem is to learn from nature. In photosynthesis, the photosystem II reaction centre (PSII RC) is a pigment-protein complex able to overcome disorder and convert solar photons into a separation of charges with near 100% efficiency. Crucially, the generated charges have enough potential to drive water oxidation and hydrogen production. 

Previously, I have investigated the charge separation process in the PSII RC by a collection of spectroscopic techniques, which allowed me to formulate the design principles of photosynthetic charge separation, where coherence plays a crucial role. Here I will put these knowledge into action to design efficient and robust chromophore-protein assemblies for the collection and conversion of solar energy, employ organic chemistry and synthetic biology tools to construct these well defined and fully controllable assemblies, and apply a complete set of spectroscopic methods to investigate these engineered systems. 

Following the approach Understand, Engineer, Implement, I will create a new generation of bio-inspired devices based on abundant and biodegradable materials that will drive the transformation of solar energy and water into hydrogen, an energy-rich molecule that can be stored and transported.","1500000","2019-04-01","2024-03-31"
"BIOIONS","Biological ions in the gas-phase: New techniques for structural characterization of isolated biomolecular ions","Caroline Dessent","UNIVERSITY OF YORK","Recent intensive research on the laser spectroscopy of neutral gas-phase biomolecules has yielded a detailed picture of their structures and conformational preferences away from the complications of the bulk environment. In contrast, work on ionic systems has been sparse despite the fact that many important molecular groups are charged under physiological conditions. To address this probelm, we have developed a custom-built laser spectrometer, which incorporates a distincitive electrospray ionisation (ESI) cluster ion source, dedicated to producing biological anions (ATP,oligonucleotides) and their microsolvated clusters for structural characterization. Many previous laser spectrometers with ESI sources have suffered from producing &quot;hot&quot; congested spectra as the ions were produced at ambient temperatures. This is a particularly serious limitation for spectroscopic studies of biomolecules, since these systems can possess high internal energies due tothe presence of numerous low frequency modes. Our spectrometer overcomes this problem by exploiting the newly developed physics technique of &quot;buffer gas cooling&quot; to produce cold ESI molecular ions. In this proposal, we now seek to exploit the new laser-spectrometer to perform detailed spectroscopic interrogations of ESI generated biomolecular anions and clusters. In addition to traditional ion-dissociation spectroscopies, we propose to develop two new laser spectroscopy techniques (Two-color tuneable IR spectroscopy and Dipole-bound excited state spectroscopy) to give the broadest possible structural characterizations of the systems of interest. Studies will focus on ATP/GTP-anions, olignonucleotides, and sulphated and carboxylated sugars. These methodologies will provide a general approach for performing temperature-controlled spectroscopic characterizations of isolated biological ions, with measurements on the corresponding micro-solvated clusters providing details of how the molecules are perturbed by solvent.","1250000","2008-10-01","2015-06-30"
"BIOMIM","Biomimetic films and membranes as advanced materials for studies on cellular processes","Catherine Cecile Picart","INSTITUT POLYTECHNIQUE DE GRENOBLE","The main objective nowadays in the field of biomaterials is to design highly performing bioinspired materials learning from natural processes. Importantly, biochemical and physical cues are key parameters that can affect cellular processes. Controlling processes that occur at the cell/material interface is also of prime importance to guide the cell response. The main aim of the current project is to develop novel functional bio-nanomaterials for in vitro biological studies. Our strategy is based on two related projects.
The first project deals with the rational design of smart films with foreseen applications in musculoskeletal tissue engineering. We will gain knowledge of key cellular processes by designing well defined self-assembled thin coatings. These multi-functional surfaces with bioactivity (incorporation of growth factors), mechanical (film stiffness) and topographical properties (spatial control of the film s properties) will serve as tools to mimic the complexity of the natural materials in vivo and to present bioactive molecules in the solid phase. We will get a better fundamental understanding of how cellular functions, including adhesion and differentiation of muscle cells are affected by the materials s surface properties.
In the second project, we will investigate at the molecular level a crucial aspect of cell adhesion and motility, which is the intracellular linkage between the plasma membrane and the cell cytoskeleton. We aim to elucidate the role of ERM proteins, especially ezrin and moesin, in the direct linkage between the plasma membrane and actin filaments. Here again, we will use a well defined microenvironment in vitro to simplify the complexity of the interactions that occur in cellulo. To this end, lipid membranes containing a key regulator lipid from the phosphoinositides familly, PIP2, will be employed in conjunction with purified proteins to investigate actin regulation by ERM proteins in the presence of PIP2-membranes.","1499996","2011-06-01","2016-05-31"
"BioMNP","Understanding the interaction between metal nanoparticles and biological membranes","Giulia Rossi","UNIVERSITA DEGLI STUDI DI GENOVA","The BioMNP objective is the molecular-level understanding of the interactions between surface functionalized metal nanoparticles and biological membranes, by means of cutting-edge computational techniques and new molecular models.
Metal nanoparticles (NP) play more and more important roles in pharmaceutical and medical technology as diagnostic or therapeutic devices. Metal NPs can nowadays be engineered in a multitude of shapes, sizes and compositions, and they can be decorated with an almost infinite variety of functionalities. Despite such technological advances, there is still poor understanding of the molecular processes that drive the interactions of metal NPs with cells. Cell membranes are the first barrier encountered by NPs entering living organisms. The understanding and control of the interaction of nanoparticles with biological membranes is therefore of paramount importance to understand the molecular basis of the NP biological effects. 
BioMNP will go beyond the state of the art by rationalizing the complex interplay of NP size, composition, functionalization and aggregation state during the interaction with model biomembranes. Membranes, in turn, will be modelled at an increasing level of complexity in terms of lipid composition and phase. BioMNP will rely on cutting-edge simulation techniques and facilities, and develop new coarse-grained models grounded on finer-level atomistic simulations, to study the NP-membrane interactions on an extremely large range of length and time scales.
BioMNP will benefit from important and complementary experimental collaborations, will propose interpretations of the available experimental data and make predictions to guide the design of functional, non-toxic metal nanoparticles for biomedical applications. BioMNP aims at answering fundamental questions at the crossroads of physics, biology and chemistry. Its results will have an impact on nanomedicine, toxicology, nanotechnology and material sciences.","1131250","2016-04-01","2021-03-31"
"BIOMOF","Biomineral-inspired growth and processing of metal-organic frameworks","Darren Bradshaw","UNIVERSITY OF SOUTHAMPTON","This ERC-StG proposal, BIOMOF, outlines a dual strategy for the growth and processing of porous metal-organic framework (MOF) materials, inspired by the interfacial interactions that characterise highly controlled biomineralisation processes. The aim is to prepare MOF (bio)-composite materials of hierarchical structure and multi-modal functionality to address key societal challenges in healthcare, catalysis and energy. In order for MOFs to reach their full potential, a transformative approach to their growth, and in particular their processability, is required since the insoluble macroscopic micron-sized crystals resulting from conventional syntheses are unsuitable for many applications. The BIOMOF project defines chemically flexible routes to MOFs under mild conditions, where the  added value  with respect to wide-ranging experimental procedures for the growth and processing of crystalline controllably nanoscale MOF materials with tunable structure and functionality that display significant porosity for wide-ranging applications is extremely high. Theme 1 exploits protein vesicles and abundant biopolymer matrices for the confined growth of soluble nanoscale MOFs for high-end biomedical applications such as cell imaging and targeted drug delivery, whereas theme 2 focuses on the cost-effective preparation of hierarchically porous MOF composites over several length scales, of relevance to bulk industrial applications such as sustainable catalysis, separations and gas-storage. This diverse yet complementary range of applications arising simply from the way the MOF is processed, coupled with the versatile structural and physical properties of MOFs themselves indicates strongly that the BIOMOF concept is a powerful convergent new approach to applied materials chemistry.","1492970","2010-11-01","2015-10-31"
"BIOMOFS","Bioapplications of Metal Organic Frameworks","Christian Serre","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","This project will focus on the use of nanoporous metal organic frameworks (Fe, Zn, Ti) for bioapplications. These systems are exciting porous solids, built up from inorganic clusters and polycarboxylates. This results in open-framework solids with different pore shapes and dimensions, and applications such as catalysis, separation and storage of gases. I have recently initiated the synthesis of new trivalent transition metal carboxylates. Among them, the metal carboxylates MIL-100 and MIL-101 (MIL: Materials of Institut Lavoisier) are spectacular solids with giant pores (25-34 Å), accessible metal sites and huge surface areas (3100-5900 m2.g-1). Recently, it was shown that these solids could be used for drug delivery with a loading of 1.4 g of Ibuprofen per gram of MIL-101 solid and a total release in six days. This project will concentrate on the implication of MOFs for drug release and other bioapplications. Whereas research on drug delivery is currently focused either on the use of bio-compatible polymers or mesoporous materials, our method will combine advantages of both routes including a high loading and a slow release of therapeutic molecules. A second application will use solids with accessible metal sites to coordinate NO for its controlled delivery. This would provide exogenous NO for prophylactic and therapeutic processes, anti-thrombogenic medical devices, improved dressings for wounds and ulcers, and the treatment of fungal and bacterial infections. Finally, other applications will be envisaged such as the purification of physiological fluids. The project, which will consist of a systematic study of the relation between these properties and both the composition and structure of the hybrid solids, will be assisted by a strong modelling effort including top of the art computational methods (QSAR and QSPKR). This highly impact project will be realised by assembling experienced researchers in multidisplinary areas including materials science, biology and modelling. It will involve P. Horcajada (Institut Lavoisier), whose background in pharmaceutical science will fit with my experience in inorganic chemistry and G. Maurin (Institut Gerhardt, Montpellier) expert in computational chemistry.","1250000","2008-06-01","2013-05-31"
"BIOMORPHIC","Brain-Inspired Organic Modular Lab-on-a-Chip for Cell Classification","Yoeri Bertin VAN DE BURGT","TECHNISCHE UNIVERSITEIT EINDHOVEN","Brain-inspired (neuromorphic) computing has recently demonstrated advancements in pattern and image recognition as well as classification of unstructured (big) data. However, the volatility and energy required for neuromorphic devices presented to date significantly complicate the path to achieve the interconnectivity and efficiency of the brain. In previous work, recently published in Nature Materials, the PI has demonstrated a low-cost solution to these drawbacks: an organic artificial synapse as a building-block for organic neuromorphics. The conductance of this single synapse can be accurately tuned by controlled ion injection in the conductive polymer, which could trigger unprecedented low-energy analogue computing. 
Hence, the major challenge in the largely unexplored field of organic neuromorphics, is to create an interconnected network of these synapses to obtain a true neuromorphic array which will not only be exceptionally pioneering in materials research for neuromorphics and machine-learning, but can also be adopted in a multitude of vital medical research devices. BIOMORPHIC will develop a unique brain-inspired organic lab-on-a-chip in which microfluidics integrated with sensors, collecting characteristics of biological cells, will serve as input to the neuromorphic array. BIOMORPHIC will combine modular microfluidics and machine-learning to develop a novel platform for low-cost lab-on-a-chip devices capable of on-chip cell classification.
In particular, BIOMORPHIC will focus on the detection of circulating tumour cells (CTC). Current methods for the detection of cancer are generally invasive, whereas analysing CTCs in blood offers a highly desired alternative. However, accurately detecting and isolating these cells remains a challenge due to their low prevalence and large variability. The strength of neuromorphics precisely lies in finding patterns in such variable data, which will result in a ground-breaking CTC classification lab-on-a-chip.","1498726","2019-01-01","2023-12-31"
"BIONET","Network Topology Complements Genome as a Source of Biological Information","Natasa Przulj","UNIVERSITY COLLEGE LONDON","Genetic sequences have had an enormous impact on our understanding of biology.  The expectation is that biological network data will have a similar impact.  However, progress is hindered by a lack of sophisticated graph theoretic tools that will mine these large networked datasets.
In recent breakthrough work at the boundary of computer science and biology supported by my USA NSF CAREER award, I developed sensitive network analysis, comparison and embedding tools which demonstrated that protein-protein interaction networks of eukaryotes are best modeled by geometric graphs.  Also, they established phenotypically validated, unprecedented link between network topology and biological function and disease.  Now I propose to substantially extend these preliminary results and design sensitive and robust network alignment methods that will lead to uncovering unknown biology and evolutionary relationships.  The potential ground-breaking impact of such network alignment tools could be parallel to the impact the BLAST family of sequence alignment tools that have revolutionized our understanding of biological systems and therapeutics.   Furthermore, I propose to develop additional sophisticated graph theoretic techniques to mine network data and hence complement biological information that can be extracted from sequence.   I propose to exploit these new techniques for biological applications in collaboration with experimentalists at Imperial College London: 1. aligning biological networks of species whose genomes are closely related, but that have very different phenotypes, in order to uncover systems-level factors that contribute to pronounced differences; 2.  compare and contrast stress response pathways and metabolic pathways in bacteria in a unified systems-level framework and exploit the findings for: (a) bioengineering of micro-organisms for industrial applications (production of bio-fuels, bioremediation, production of biopolymers); (b) biomedical applications.","1638175","2012-01-01","2017-12-31"
"BioNet","Dynamical Redesign of Biomolecular Networks","Edina ROSTA","KING'S COLLEGE LONDON","Enzymes created by Nature are still more selective and can be orders of magnitude more efficient than man-made catalysts, in spite of recent advances in the design of de novo catalysts and in enzyme redesign. The optimal engineering of either small molecular or of complex biological catalysts requires both (i) accurate quantitative computational methods capable of a priori assessing catalytic efficiency, and (ii) molecular design principles and corresponding algorithms to achieve, understand and control biomolecular catalytic function and mechanisms. Presently, the computational design of biocatalysts is challenging due to the need for accurate yet computationally-intensive quantum mechanical calculations of bond formation and cleavage, as well as to the requirement for proper statistical sampling over very many degrees of freedom. Pioneering enhanced sampling and analysis methods have been developed to address crucial challenges bridging the gap between the available simulation length and the biologically relevant timescales. However, biased simulations do not generally permit the direct calculation of kinetic information. Recently, I and others pioneered simulation tools that can enable not only accurate calculations of free energies, but also of the intrinsic molecular kinetics and the underlying reaction mechanisms as well. I propose to develop more robust, automatic, and system-tailored sampling algorithms that are optimal in each case. I will use our kinetics-based methods to develop a novel theoretical framework to address catalytic efficiency and to establish molecular design principles to key design problems for new bio-inspired nanocatalysts, and to identify and characterize small molecule modulators of enzyme activity. This is a highly interdisciplinary project that will enable fundamental advances in molecular simulations and will unveil the physical principles that will lead to design and control of catalysis with Nature-like efficiency.","1499999","2018-02-01","2023-01-31"
"BIONICS","Bio-Inspired Routes for Controlling the Structure and Properties of Materials: Reusing proven tricks on new materials","Boaz Pokroy","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","""In the course of biomineralization, organisms produce a large variety of functional biogenic crystals that exhibit fascinating mechanical, optical, magnetic and other characteristics. More specifically, when living organisms grow crystals they can effectively control polymorph selection as well as the crystal morphology, shape, and even atomic structure. Materials existing in nature have extraordinary and specific functions, yet the materials employed in nature are quite different from those engineers would select.
I propose to emulate specific strategies used by organisms in forming structural biogenic crystals, and to apply these strategies biomimetically so as to form new structural materials with new properties and characteristics. This bio-inspired approach will involve the adoption of three specific biological strategies. We believe that this procedure will open up new ways to control the structure and properties of smart materials.
The three bio-inspired strategies that we will utilize are:
(i) to control the short-range order of amorphous materials, making it possible to predetermine the polymorph obtained when they transform from the amorphous to the succeeding crystalline phase;
(ii) to control the morphology of single crystals of various functional materials so that they can have intricate and curved surfaces and yet maintain their single-crystal nature;
(iii) to entrap organic molecules into single crystals of functional materials so as to tailor and manipulate their electronic structure.
The proposed research has significant potential for opening up new routes for the formation of novel functional materials. Specifically, it will make it possible for us
(1) to produce single, intricately shaped crystals without the need to etch, drill or polish;
(2) to control the short-range order of amorphous materials and hence the polymorph of the successive crystalline phase;
(3) to tune the band gap of semiconductors via incorporation of tailored bio-molecules.""","1500000","2013-09-01","2018-08-31"
"bioPCET","Functional Proton-Electron Transfer Elements in Biological Energy Conversion","Ville KAILA","TECHNISCHE UNIVERSITAET MUENCHEN","Primary energy conversion in nature is powered by highly efficient enzymes that capture chemical or light energy and transduce it into other energy forms. These processes are catalyzed by coupled transfers of protons and electrons (PCET), but their fundamental mechanistic principles are not well understood. In order to obtain a molecular-level understanding of the functional elements powering biological energy conversion processes, we will study the catalytic machinery of one of the largest and most intricate enzymes in mitochondria and bacteria, the respiratory complex I. This gigantic redox-driven proton-pump functions as the entry point for electrons into aerobic respiratory chains, and it employs the energy released from a chemical reduction process to transport protons up to 200 Å away from its active site. Its molecular structure from bacteria and eukaryotes was recently resolved, but the origin of this remarkable action-at-a-distance effect still remains unclear. We employ and develop multi-scale quantum and classical molecular simulation techniques in combination with de novo-protein design methodology to identify and isolate the functional elements that catalyze the long-range PCET reactions in complex I. To fully understand the natural PCET-elements, we will further engineer central parts of this machinery into artificial protein frameworks, with the goal of designing modules for redox-driven proton pumps from first principles. The project aims to establish a fundamental understanding of nature's toolbox of catalytic elements, to elucidate how the complex biochemical environment contributes to the catalytic effects, and to provide blueprints that can guide the design of man-made enzymes for sustainable energy technology.","1494368","2017-02-01","2022-01-31"
"BIOSELFORGANIZATION","Biophysical aspects of self-organization in actin-based cell motility","Kinneret Magda Keren","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","Cell motility is a fascinating dynamic process crucial for a wide variety of biological phenomena including defense against injury or infection, embryogenesis and cancer metastasis. A spatially extended, self-organized, mechanochemical machine consisting of numerous actin polymers, accessory proteins and molecular motors drives this process. This impressive assembly self-organizes over several orders of magnitude in both the temporal and spatial domains bridging from the fast dynamics of individual molecular-sized building blocks to the persistent motion of whole cells over minutes and hours. The molecular players involved in the process and the basic biochemical mechanisms are largely known. However, the principles governing the assembly of the motility apparatus, which involve an intricate interplay between biophysical processes and biochemical reactions, are still poorly understood. The proposed research is focused on investigating the biophysical aspects of the self-organization processes underlying cell motility and trying to adapt these processes to instill motility in artificial cells. Important biophysical characteristics of moving cells such as the intracellular fluid flow and membrane tension will be measured and their effect on the motility process will be examined, using fish epithelial keratocytes as a model system. The dynamics of the system will be further investigated by quantitatively analyzing the morphological and kinematic variation displayed by a population of cells and by an individual cell through time. Such measurements will feed into and direct the development of quantitative theoretical models. In parallel, I will work toward the development of a synthetic physical model system for cell motility by encapsulating the actin machinery in a cell-sized compartment. This synthetic system will allow cell motility to be studied in a simplified and controlled environment, detached from the complexity of the living cell.","900000","2008-08-01","2013-07-31"
"BIOSMA","Mathematics for Shape Memory Technologies in Biomechanics","Ulisse Stefanelli","CONSIGLIO NAZIONALE DELLE RICERCHE","Shape Memory Alloys (SMAs) are nowadays widely exploited for the realization of innovative devices and have a great impact on the development of a variety of biomedical applications ranging from orthodontic archwires to vascular stents. The design, realization, and optimization of such devices are quite demanding tasks. Mathematics is involved in this process as a major tool in order to let the modeling more accurate, the numerical simulations more reliable, and the design more effective. Many material properties of SMAs such as martensitic reorientation, training, and ferromagnetic behavior, are still to be properly and efficiently addressed. Therefore, new modeling ideas, along with original analytical and numerical techniques, are required. This project is aimed at addressing novel mathematical issues in order to move from experimental materials results toward the solution of real-scale biomechanical Engineering problems. The research focus will be multidisciplinary and include modeling, analytic, numerical, and computational issues. A progress in the macroscopic description of SMAs, the computational simulation of real-scale SMA devices, and the optimization of the production processes will contribute to advance in the direction of innovative applications.","700000","2008-09-01","2013-08-31"
"BIOSTRUCT","Multiscale mathematical modelling of dynamics of structure formation in cell systems","Anna Marciniak-Czochra","RUPRECHT-KARLS-UNIVERSITAET HEIDELBERG","The aim of this transdisciplinary project is to develop and analyse multiscale mathematical models of pattern formation in multicellular systems controlled by the dynamics of intracellular signalling pathways and cell-to-cell communication and to develop new mathematical methods for the modelling of such complex processes. This aim will be achieved through a close collaboration with experimental groups and comprehensive analytical investigations of the mathematical problems arising in the modelling of these biological processes. The mathematical methods and techniques to be employed will be the analysis of systems of partial differential equations, asymptotic analysis, as well as methods of dynamical systems. These techniques will be used to formulate the models and to study the spatio-temporal behaviour of solutions, especially stability and dependence on characteristic scales, geometry, initial data and key parameters. Advanced numerical methods will be applied to simulate the models. This comprehensive methodology goes beyond the state-of-the-art, since usually the analyses are limited to a single aspect of model behaviour. Groundbreaking impacts envisioned are threefold: (i) The project will contribute to the understanding of mechanisms of structure formation in the developmental process, in the context of recently discovered signalling pathways. In addition, some of the factors and mechanisms playing a role in developmental processes, such as Wnt signalling, are implicated in carcinogenesis, for instance colon and lung cancer. (ii) Accurate quantitative and predictive mathematical models of cell proliferation and differentiation are important for the control of tumour growth and tissue egeneration; (iii) Qualitative analysis of multiscale mathematical models of biological phenomena generates challenging mathematical problems and, therefore, the project will lead to the development of new mathematical theories and tools.","750000","2008-09-01","2013-08-31"
"BIOTORQUE","Probing the angular dynamics of biological systems with the optical torque wrench","Francesco Pedaci","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""The ability to apply forces to single molecules and bio-polymers has fundamentally changed the way we can interact with and understand biological systems. Yet, for many cellular mechanisms, it is rather the torque that is the relevant physical parameter. Excitingly, novel single-molecule techniques that utilize this parameter are now poised to contribute to novel discoveries.  Here, I will study the angular dynamical behavior and response to external torque of biological systems at the molecular and cellular levels using the new optical torque wrench that I recently developed.

In a first research line, I will unravel the angular dynamics of the e.coli flagellar motor, a complex and powerful rotary nano-motor that rotates the flagellum in order to propel the bacterium forwards. I will quantitatively study different aspects of torque generation of the motor, aiming to connect evolutionary, dynamical, and structural principles. In a second research line, I will develop an in-vivo manipulation technique based on the transfer of optical torque and force onto novel nano-fabricated particles. This new scanning method will allow me to map physical properties such as the local viscosity  inside living cells and the spatial organization and topography of internal membranes, thereby expanding the capabilities of existing techniques towards in-vivo and ultra-low force scanning imaging.

This project is founded on a multidisciplinary approach in which fundamental optics, novel nanoparticle fabrication, and molecular and cellular biology are integrated. It has the potential to answer biophysical questions that have challenged the field for over two decades and to impact fields ranging from single-molecule biophysics to scanning-probe microscopy and nanorheology, provided ERC funding is granted.""","1500000","2013-01-01","2017-12-31"
"BioWater","Development of new chemical imaging techniques to understand the function of water in biocompatibility, biodegradation and biofouling","Aoife Ann Gowen","UNIVERSITY COLLEGE DUBLIN, NATIONAL UNIVERSITY OF IRELAND, DUBLIN","Water is the first molecule to come into contact with biomaterials in biological systems and thus essential to the processes of biodegradation, biocompatibility and biofouling. Despite this fact, little is currently known about how biomaterials interact with water. This knowledge is crucial for the development and optimisation of novel functional biomaterials for human health (e.g. biosensing devices, erodible biomaterials, drug release carriers, wound dressings). BioWater will develop near and mid infrared chemical imaging (NIR-MIR-CI) techniques to investigate the fundamental interaction between biomaterials and water in order to understand the key processes of biodegradation, biocompatibility and biofouling. This ambitious yet achievable project will focus on two major categories of biomaterials relevant to human health: extracellular collagens and synthetic biopolymers. Initially, interactions between these biomaterials and water will be investigated; subsequently interactions with more complicated matrices (e.g. protein solutions and cellular systems) will be studied. CI data will be correlated with standard surface characterization, biocompatibility and biodegradation measurements. Molecular dynamic simulations will complement this work to identify the most probable molecular structures of water at different biomaterial interfaces.
Advanced understanding of the role of water in biocompatibility, biofouling and biodegradation processes will facilitate the optimization of biomaterials tailored to specific cellular environments with a broad range of therapeutic applications (e.g. drug eluting stents, tissue engineering, wound healing). The new NIR-MIR-CI/chemometric methodologies developed in BioWater will allow for the rapid characterization and monitoring of novel biomaterials at pre-clinical stages, improving process control by overcoming the laborious and time consuming large-scale sampling methods currently required in biomaterials development.","1487682","2014-02-01","2019-01-31"
"BISMUTH","Breaking Inversion Symmetry in Magnets: Understand via THeory","Silvia Picozzi","CONSIGLIO NAZIONALE DELLE RICERCHE","Multiferroics (i.e. materials where ferroelectricity and magnetism coexist) are presently drawing enormous interests, due to their technologically-relevant multifunctional character and to the astoundingly rich playground for fundamental condensed-matter physics they constitute. Here, we put forward several concepts on how to break inversion symmetry and achieve sizable ferroelectricity in collinear magnets; our approach is corroborated via first-principles calculations as tools to quantitatively estimate relevant ferroelectric and magnetic properties as well as to reveal ab-initio the main mechanisms behind the dipolar and magnetic orders. In closer detail, we focus on the interplay between ferroelectricity and electronic degrees of freedom in magnets, i.e. on those cases where spin- or orbital- or charge-ordering can be the driving force for a spontaneous polarization to develop. Antiferromagnetism will be considered as a primary mechanism for lifting inversion symmetry; however, the effects of charge disproportionation and orbital ordering will also be studied by examining a wide class of materials, including ortho-manganites with E-type spin-arrangement, non-E-type antiferromagnets, nickelates, etc. Finally, as an example of materials-design accessible to our ab-initio approach, we use “chemistry” to break inversion symmetry by artificially constructing an oxide superlattice and propose a way to switch, via an electric field, from antiferromagnetism to ferrimagnetism. To our knowledge, the link between electronic degrees of freedom and ferroelectricity in collinear magnets is an almost totally unexplored field by ab-initio methods; indeed, its clear understanding and optimization would lead to a scientific breakthrough in the multiferroics area. Technologically, it would pave the way to materials design of magnetic ferroelectrics with properties persisting above room temperature and, therefore, to a novel generation of electrically-controlled spintronic devices","684000","2008-05-01","2012-04-30"
"BIVAQUM","Bivariational Approximations in Quantum Mechanics and Applications to Quantum Chemistry","Simen Kvaal","UNIVERSITETET I OSLO","The standard variational principles (VPs) are cornerstones of quantum mechanics, and one can hardly overestimate their usefulness as tools for generating approximations to the time-independent and
time-dependent Schröodinger equations. The aim of the proposal is to study and apply a generalization of these, the bivariational principles (BIVPs), which arise naturally when one does not assume a priori that the system Hamiltonian is Hermitian.  This  unconventional approach may have transformative impact on development of ab initio methodology, both for electronic structure and dynamics.

The first objective is to establish the mathematical foundation for the BIVPs. This opens up a whole new axis of method development for ab initio approaches.  For instance, it is a largely ignored fact that the popular traditional coupled cluster (TCC) method can be neatly formulated with the BIVPs, and TCC is both polynomially scaling with the number of electrons and size-consistent. No “variational” method enjoys these properties simultaneously, indeed this seems to be incompatible with the standard VPs.

Armed with the BIVPs, the project aims to develop new and understand existing ab initio methods. The second objective is thus a systematic multireference coupled cluster theory (MRCC) based on the BIVPs. This
is in itself a novel approach that carries large potential benefits and impact. The third and last objective is an implementation of a new coupled-cluster type method where the orbitals are bivariational
  parameters. This gives a size-consistent hierarchy of approximations to multiconfiguration
Hartree--Fock.

The PI's broad contact with and background in scientific disciplines such as applied mathematics and nuclear physics in addition to quantum chemistry increases the feasibility of the project.","1499572","2015-04-01","2020-03-31"
"blackQD","Optoelectronic of narrow band gap nanocrystals","Emmanuel LHUILLIER","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Over the past decades, silicon became the most used material for electronic, however its indirect band gap limits its use for optics and optoelectronics. As a result alternatives semiconductor such as III-V and II-VI materials are used to address a broad range of complementary application such as LED, laser diode and photodiode. However in the infrared (IR), the material challenge becomes far more complex.

New IR applications, such as flame detection or night car driving assistance are emerging and request low cost detectors. Current technologies, based on epitaxially grown semiconductors are unlikely to bring a cost disruption and organic electronics, often viewed as the alternative to silicon based materials is ineffective in the mid-IR. The blackQD project aims at transforming colloidal quantum dots (CQD) into the next generation of active material for IR detection. CQD are attracting a high interest because of their size tunable optical features and next challenges is their integration in optoelectronic devices and in particular for IR features. 

The project requires a combination of material knowledge, with clean room nanofabrication and IR photoconduction which is unique in Europe. I organize blackQD in three mains parts. The first part relates to the growth of mercury chalcogenides nanocrystals with unique tunable properties in the mid and far-IR. To design devices with enhanced properties, more needs to be known on the electronic structure of these nanomaterials. In part II, I propose to develop original methods to probe static and dynamic aspects of the electronic structure. Finally the main task of the project relates to the design of a new generation of transistors and IR detectors. I propose several geometries of demonstrator which for the first time integrate from the beginning the colloidal nature of the CQD and constrain of IR photodetection. The project more generally aims to develop a tool box for the design of the next generation of low cost IR.","1499903","2018-02-01","2023-01-31"
"BLAST","Eclipsing binary stars as cutting edge laboratories for astrophysics of stellar
structure, stellar evolution and planet formation","Maciej Konacki","CENTRUM ASTRONOMICZNE IM. MIKOLAJAKOPERNIKA POLSKIEJ AKADEMII NAUK","Spectroscopic binary stars (SB2s) and in particular spectroscopic eclipsing binaries are one of the most useful objects in astrophysics. Their photometric and spectroscopic observations allow one to determine basic parameters of stars and carry out a wide range of tests of stellar structure, evolution and dynamics. Perhaps somewhat surprisingly, they can also contribute to our understanding of the formation and evolution of (extrasolar) planets.  We will study eclipsing binary stars by combining the classic - stellar astronomy - and the modern - extrasolar planets - subjects into a cutting edge project.

We propose to search for and subsequently characterize circumbinary planets around ~350 eclipsing SB2s using our own novel cutting edge radial velocity technique for binary stars and a modern version of the photometry based eclipse timing of eclipsing binary stars employing 0.5-m robotic telescopes. We will also derive basic parameters of up to ~700 stars (~350 binaries) with an unprecedented precision. In particular for about 50% of our sample we expect to deliver masses of the components with an accuracy   ~10-100 times better than the current state of the art.

Our project will provide unique constraints for the theories of planet formation and evolution and an unprecedented in quality set of the basic parameters of stars to test the theories of the stellar structure and evolution.","1500000","2010-12-01","2016-11-30"
"BLOC","Mathematical study of Boundary Layers in Oceanic Motions","Anne-Laure Perrine Dalibard","SORBONNE UNIVERSITE","Boundary layer theory is a large component of fluid dynamics. It is ubiquitous in Oceanography, where boundary layer currents, such as the Gulf Stream, play an important role in the global circulation. Comprehending the underlying mechanisms in the formation of boundary layers is therefore crucial for applications. However, the  treatment of boundary layers in ocean dynamics  remains poorly understood at a theoretical level, due to the variety and complexity of the forces at stake. 

The goal of this project is to develop several  tools to bridge the gap between the mathematical state of the art and the physical reality of oceanic motion. There are four points on which we will mainly focus: degeneracy issues, including the treatment Stewartson boundary layers near the equator; rough boundaries (meaning boundaries with small amplitude and high frequency variations); the inclusion of the advection term in the construction of stationary boundary layers; and the linear and nonlinear stability of the boundary layers. We will address separately  Ekman layers and western boundary layers, since they are ruled by equations whose mathematical behaviour is very different. 

This project will allow us to have a better understanding of small scale phenomena in fluid mechanics, and in particular of the inviscid limit of incompressible fluids. 

The team will be composed of the PI, two PhD students and three two-year postdocs over the whole period. We will also rely on the historical expertise of the host institution on fluid mechanics and asymptotic methods.","1267500","2015-09-01","2020-08-31"
"BONDS","Bilayered ON-Demand Scaffolds: On-Demand Delivery from induced Pluripotent Stem Cell Derived Scaffolds for Diabetic Foot Ulcers","Cathal KEARNEY","ROYAL COLLEGE OF SURGEONS IN IRELAND","This program’s goal is to develop a scaffold using a new biomaterial source that is functionalised with on-demand delivery of genes for coordinated healing of diabetic foot ulcers (DFUs). DFUs are chronic wounds that are often recalcitrant to treatment, which devastatingly results in lower leg amputation. This project builds on the PI’s experience growing matrix from induced-pluripotent stem cell derived (iPS)-fibroblasts and in developing on-demand drug delivery technologies. The aim of this project is to first develop a SiPS: a scaffold from iPS-fibroblast grown matrix, which has never been tested as a source material for scaffolds. iPS-fibroblasts grow a more pro-repair and angiogenic matrix than (non-iPS) adult fibroblasts. The SiPS structure will be bilayered to mimic native skin: dermis made mostly by fibroblasts and epidermis made by keratinocytes. The dermal layer will consist of a porous scaffold with optimised pore size and mechanical properties and the epidermal layer will be film-like, optimised for keratinisation.
Second, the SiPS will be functionalised with delivery of plasmid-DNA (platelet derived growth factor gene, pPDGF) to direct angiogenesis on-demand. As DFUs undergo uncoordinated healing, timed pPDGF delivery will guide them through angiogenesis and healing. To achieve this, alginate microparticles, designed to respond to ultrasound by releasing pPDGF, will be interspersed throughout the SiPS. This BONDS will be tested in an in vivo pre-clinical DFU model to confirm its ability to heal wounds by providing cells with the appropriate biomimetic scaffold environment and timed directions for healing. With >100 million current diabetics expected to get a DFU, the BONDS would have a powerful clinical impact.
This research program combines a disruptive technology, the SiPS, with a new platform for on-demand delivery of pDNA to heal DFUs. The PI will build his lab around these innovative platforms, adapting them for treatment of diverse complex wounds.","1372135","2017-10-01","2022-09-30"
"BONEMECHBIO","Frontier research in bone mechanobiology during normal physiology, disease and for tissue regeneration","Laoise Maria Cunningham","NATIONAL UNIVERSITY OF IRELAND GALWAY","While previous studies have investigated cell-signalling pathways that facilitate mechanotransduction and have provided a wealth of data, to date, in vivo mechanobiology is not fully understood.  In the research study proposed the applicant will embark upon frontier research to delineate these specific aspects of bone mechanotransduction during normal physiology, disease and for tissue regeneration purposes. If these quantities were better understood the proposed research program will deliver significant advances in the understanding of the mechanical regulation of bone remodelling during normal physiology and osteoporosis, and will enhance approaches for regeneration of bone tissue for treatment of bone pathologies. The primary objective is to delineate the normal mechanosensory and signalling mechanisms of bone cells. The secondary objective is to determine whether the regulatory role of bone cells is inhibited or impaired during bone diseases such as osteoporosis. The final objective of this project is to develop an in vitro mechanical loading device that can enhance bone tissue regeneration and thereby advance current treatment approaches for bone pathologies. To address these objectives, five hypotheses have been defined, each of which will underpin the research of five work packages. A combination of experimental studies, using animal models and in vitro cell culture, and computational modelling will be taken to test each of these hypotheses. Answering these hypotheses will bring us closer to an understanding of the origins of bone mechanobiology and diseases such as osteoporosis. Furthermore, the results of these studies will facilitate development of novel approaches to enhance bone regeneration in vitro.","1499911","2011-02-01","2016-01-31"
"BOTTOM-UP_SYSCHEM","Systems Chemistry from Bottom Up: Switching, Gating and Oscillations in Non Enzymatic Peptide Networks","Gonen Ashkenasy","BEN-GURION UNIVERSITY OF THE NEGEV","The study of synthetic molecular networks is of fundamental importance for understanding the organizational principles of biological systems and may well be the key to unraveling the origins of life. In addition, such systems may be useful for parallel synthesis of molecules, implementation of catalysis via multi-step pathways, and as media for various applications in nano-medicine and nano-electronics. We have been involved recently in developing peptide-based replicating networks and revealed their dynamic characteristics. We argue here that the structural information embedded in the polypeptide chains is sufficiently rich to allow the construction of peptide 'Systems Chemistry', namely, to facilitate the use of replicating networks as cell-mimetics, featuring complex dynamic behavior. To bring this novel idea to reality, we plan to take a unique holistic approach by studying such networks both experimentally and via simulations, for elucidating basic-principles and towards applications in adjacent fields, such as molecular electronics. Towards realizing these aims, we will study three separate but inter-related objectives: (i) design and characterization of networks that react and rewire in response to external triggers, such as light, (ii) design of networks that operate via new dynamic rules of product formation that lead to oscillations, and (iii) exploitation of the molecular information gathered from the networks as means to control switching and gating in molecular electronic devices. We believe that achieving the project's objectives will be highly significant for the development of the arising field of Systems Chemistry, and in addition will provide valuable tools for studying related scientific fields, such as systems biology and molecular electronics.","1500000","2010-10-01","2015-09-30"
"BRAIN MICRO SNOOPER","A mimetic implant for low perturbation, stable stimulation and recording of neural units inside the brain.","Gaelle Offranc piret","INSTITUT NATIONAL DE LA SANTE ET DE LA RECHERCHE MEDICALE","Developing brain implants is crucial to better decipher the neuronal information and intervene in a very thin way on neural networks using microstimulations. This project aims to address two major challenges: to achieve the realization of a highly mechanically stable implant, allowing long term connection between neurons and microelectrodes and to provide neural implants with a high temporal and spatial resolution. To do so, the present project will develop implants with structural and mechanical properties that resemble those of the natural brain environment. According to the literature, using electrodes and electric leads with a size of a few microns allows for a better neural tissue reconstruction around the implant. Also, the mechanical mismatch between the usually stiff implant material and the soft brain tissue affects the adhesion between tissue cells and electrodes. With the objective to implant a highly flexible free-floating microelectrode array in the brain tissue, we will develop a new method using micro-nanotechnology steps as well as a combination of polymers. Moreover, the literature and preliminary studies indicate that some surface chemistries and nanotopographies can promote neurite outgrowth while limiting glial cell proliferation. Implants will be nanostructured so as to help the neural tissue growth and to be provided with a highly adhesive property, which will ensure its stable contact with the brain neural tissue over time. Implants with different microelectrode configurations and number will be tested in vitro and in vivo for their biocompatibility and their ability to record and stimulate neurons with high stability. This project will produce high-performance generic implants that can be used for various fundamental studies and applications, including neural prostheses and brain machine interfaces.","1499850","2015-08-01","2021-07-31"
"BrainConquest","Boosting Brain-Computer Communication with high Quality User Training","Fabien LOTTE","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","Brain-Computer Interfaces (BCIs) are communication systems that enable users to send commands to computers through brain signals only, by measuring and processing these signals. Making computer control possible without any physical activity, BCIs have promised to revolutionize many application areas, notably assistive technologies, e.g., for wheelchair control, and human-machine interaction. Despite this promising potential, BCIs are still barely used outside laboratories, due to their current poor reliability. For instance, BCIs only using two imagined hand movements as mental commands decode, on average, less than 80% of these commands correctly, while 10 to 30% of users cannot control a BCI at all.
A BCI should be considered a co-adaptive communication system: its users learn to encode commands in their brain signals (with mental imagery) that the machine learns to decode using signal processing. Most research efforts so far have been dedicated to decoding the commands. However, BCI control is a skill that users have to learn too. Unfortunately how BCI users learn to encode the commands is essential but is barely studied, i.e., fundamental knowledge about how users learn BCI control is lacking. Moreover standard training approaches are only based on heuristics, without satisfying human learning principles. Thus, poor BCI reliability is probably largely due to highly suboptimal user training.
In order to obtain a truly reliable BCI we need to completely redefine user training approaches. To do so, I propose to study and statistically model how users learn to encode BCI commands. Then, based on human learning principles and this model, I propose to create a new generation of BCIs which ensure that users learn how to successfully encode commands with high signal-to-noise ratio in their brain signals, hence making BCIs dramatically more reliable. Such a reliable BCI could positively change human-machine interaction as BCIs have promised but failed to do so far.","1498751","2017-07-01","2022-06-30"
"BRiCPT","Basic Research in Cryptographic Protocol Theory","Jesper Buus Nielsen","AARHUS UNIVERSITET","In cryptographic protocol theory, we consider a situation where a number of entities want to solve some problem over a computer network. Each entity has some secret data it does not want the other entities to learn, yet, they all want to learn something about the common set of data. In an electronic election, they want to know the number of yes-votes without revealing who voted what. For instance, in an electronic auction, they want to find the winner without leaking the bids of the losers.

A main focus of the project is to develop new techniques for solving such protocol problems. We are in particular interested in techniques which can automatically construct a protocol solving a problem given only a description of what the problem is. My focus will be theoretical basic research, but I believe that advancing the theory of secure protocol compilers will have an immense impact on the practice of developing secure protocols for practice.

When one develops complex protocols, it is important to be able to verify their correctness before they are deployed, in particular so, when the purpose of the protocols is to protect information. If and when an error is found and corrected, the sensitive data will possibly already be compromised. Therefore, cryptographic protocol theory develops models of what it means for a protocol to be secure, and techniques for analyzing whether a given protocol is secure or not.

A main focuses of the project is to develop better security models, as existing security models either suffer from the problem that it is possible to prove some protocols secure which are not secure in practice, or they suffer from the problem that it is impossible to prove security of some protocol which are believed to be secure in practice. My focus will again be on theoretical basic research, but I believe that better security models are important for advancing a practice where protocols are verified as secure before deployed.","1171019","2011-12-01","2016-11-30"
"BRIDGE","Biomimetic process design for tissue regeneration: 
from bench to bedside via in silico modelling","Liesbet Geris","UNIVERSITE DE LIEGE","""Tissue engineering (TE), the interdisciplinary field combining biomedical and engineering sciences in the search for functional man-made organ replacements, has key issues with the quantity and quality of the generated products.  Protocols followed in the lab are mainly trial and error based, requiring a huge amount of manual interventions and lacking clear early time-point quality criteria to guide the process. As a result, these processes are very hard to scale up to industrial production levels.  BRIDGE aims to fortify the engineering aspects of the TE field by adding a higher level of understanding and control to the manufacturing process (MP) through the use of in silico models. BRIDGE will focus on the bone TE field to provide proof of concept for its in silico approach.

The combination of the applicant's well-received published and ongoing work on a wide range of modelling tools in the bone field combined with the state-of-the-art experimental techniques present in the TE lab of the additional participant allows envisaging following innovation and impact:
1. proof-of-concept of the use of an in silico blue-print for the design and control of a robust modular TE MP;
2. model-derived optimised culture conditions for patient derived cell populations increasing modular robustness of in vitro chondrogenesis/endochondral ossification;
3. in silico identification of a limited set of in vitro biomarkers that is predictive of the in vivo outcome;
4. model-derived optimised culture conditions increasing quantity and quality of the in vivo outcome of the TE MP;
5. incorporation of congenital defects in the in silico MP design, constituting a further validation of BRIDGE’s in silico approach and a necessary step towards personalised medical care.

We believe that the systematic – and unprecedented – integration of (bone) TE and mathematical modelling, as proposed in BRIDGE, is required to come to a rationalized, engineering approach to design and control bone TE MPs.""","1191440","2011-12-01","2016-11-30"
"BRIDGE","Bridging the gap between Gas Emissions and geophysical observations at active volcanoes","Alessandro Aiuppa","UNIVERSITA DEGLI STUDI DI PALERMO","In spite of their significance in a variety of volcanological aspects, gas observations at volcanoes have lagged behind geophysical studies for a long time. This has primarily reflected the inherent technical limitations met by gas geochemists in capturing volcanic gas properties (chemistry and flux) at high-rate (1 Hz), and using permanent instrumental arrays. The poor temporal resolution of volcanic gas observations has, in addition, precluded the real-time analysis of fast-occurring volcanic processes, as those occurring shortly prior to eruptions, therefore generally limiting the use of gas geochemistry in volcanic hazard assessment. However, the recent progresses made by modern multi-component/high frequency measurement techniques now open the way for decisive step ahead in the current state-of-the-art to be finally attempted.
The BRIDGE research proposal has the ambitious goals to bridge the existing technological gap between geochemical and geophysical observations at volcanoes. This will be achieved by designing, setting up, and deploying in the field, innovative instruments for 1 Hz observations of volcanic SO2 and CO2 fluxes. From this, the co-acquired volcanic gas and geophysical information will be then combined within a single interpretative framework, therefore contributing to fill our current gap of knowledge on fast (timescales of seconds/minutes) degassing processes, and to deeper exploration of the role played by gas exsolution from (and migration through) silicate liquids as effective source mechanism of the physical signals (e.g., LP and VLP seismicity, and tremor) measured at volcanoes. Finally, this combined volcanic gas-geophysical approach will be used to yield improved modelling/understanding of a variety of volcanic features, including modes/rates of gas separation from magmas, mechanisms of gas flow in conduits, and trigger mechanisms of explosive volcanic eruptions.","1496222","2012-10-01","2016-09-30"
"BroadSem","Induction of Broad-Coverage Semantic Parsers","Ivan Titov","THE UNIVERSITY OF EDINBURGH","In the last one or two decades, language technology has achieved a number of important successes, for example, producing functional machine translation systems and beating humans in quiz games. The key bottleneck which prevents further progress in these and many other natural language processing (NLP) applications (e.g., text summarization, information retrieval, opinion mining, dialog and tutoring systems) is the lack of accurate methods for producing meaning representations of texts. Accurately predicting such meaning representations on an open domain with an automatic parser is a challenging and unsolved problem, primarily because of language variability and ambiguity. The reason for the unsatisfactory performance is reliance on supervised learning (learning from annotated resources), with the amounts of annotation required for accurate open-domain parsing exceeding what is practically feasible.  Moreover, representations defined in these resources typically do not provide abstractions suitable for reasoning.
 
In this project, we will induce semantic representations from large amounts of unannotated data (i.e. text which has not been labeled by humans) while guided by information contained in human-annotated data and other forms of linguistic knowledge. This will allow us to scale our approach to many domains and across languages. We will specialize meaning representations for reasoning by modeling relations (e.g., facts) appearing across sentences in texts (document-level modeling), across different texts, and across texts and knowledge bases. Learning to predict this linked data is closely related to learning to reason, including learning the notions of semantic equivalence and entailment. We will jointly induce semantic parsers (e.g., log-linear feature-rich models) and reasoning models (latent factor models) relying on this data, thus, ensuring that the semantic representations are informative for applications requiring reasoning.","1457185","2016-05-01","2021-04-30"
"BSMFLEET","Challenging the Standard Model using an extended Physics program in LHCb","Diego Martinez Santos","UNIVERSIDAD DE SANTIAGO DE COMPOSTELA","We know that the Standard Model (SM) of Particle Physics is not the ultimate theory of Nature. It misses a quantum description of gravity, it does not offer any explanation to the composition of Dark Matter, and the matter-antimatter unbalance of the Universe is predicted to be significantly smaller than what we actually see. Those are fundamental questions that still need an answer. Alternative models to SM exist, based on ideas such as SuperSymmetry or extra dimensions, and are currently being tested at the Large Hadron Collider (LHC) at CERN. But after the first run of the LHC the SM is yet unbeaten at accelerators, which imposes severe constraints in Physics beyond the SM (BSM). From this point, I see two further working directions: on one side, we must increase our precision in the previous measurements in order to access smaller BSM effects. On the other hand; we should attack the SM with a new fleet of observables sensitive to different BSM scenarios, and make sure that we are making full use of what the LHC offers to us. I propose to create a team at Universidade de Santiago de Compostela that will expand the use of LHCb beyond its original design, while also reinforcing the core LHCb analyses in which I played a leading role so far. LHCb has up to now collected world-leading samples of decays of b and c quarks. My proposal implies to use LHCb for collecting and analysing also world-leading samples of rare s quarks complementary to those of NA62. In the rare s decays the SM sources of Flavour Violation have a stronger suppression than anywhere else, and therefore those decays are excellent places to search for new Flavour Violating sources that otherwise would be hidden behind the SM contributions. It is very important to do this now, since we may not have a similar opportunity in years. In addition, the team will also exploit LHCb to search for μμ resonances predicted in models like NMSSM, and for which LHCb also offers a unique potential that must be used.","1499855","2015-04-01","2020-03-31"
"BSMWLHCB","Advanced techniques to Search for Physics Beyond the Standard Model with the LHCb Detector at CERN","Timothy John Gershon","THE UNIVERSITY OF WARWICK","I propose a programme of precision tests of the Standard Model of particle physics to be carried out using the LHCb experiment at CERN. The proposal is focussed on studies of CP violation - differences between the behaviour of particles and antiparticles that are fundamental to understanding why the Universe we see today is made up of matter, not antimatter. The innovative feature of this research is the use of Dalitz plot analyses to improve the sensitivity to interesting CP violation effects. Recently I have developed a number of new methods to search for CP violation based on this technique. These methods can be used at LHCb and will extend the physics reach of the experiment beyond what was previously considered possible. I propose to create a small research team, based at the University of Warwick, to develop these methods and to make a number of precise measurements of CP violation parameters using the LHCb experiment. By comparing the results with the Standard Model predictions for these parameters, effects due to non-standard particles can be observed or highly constrained. The results of this work have the potential to redefine the direction of this research field. They will be essential to develop theories of particle physics that go beyond the Standard Model and attempt to address great unanswered questions, such as the origin of the matter--antimatter asymmetry of the Universe.","1682800","2010-02-01","2016-01-31"
"BUCOPHSYS","Bottom-up hybrid control and planning synthesis with application to multi-robot multi-human coordination","DIMOS Dimarogonas","KUNGLIGA TEKNISKA HOEGSKOLAN","Current control applications necessitate the treatment of systems with multiple interconnected components, rather than the traditional single component paradigm that has been studied extensively. The individual subsystems may need to fulfil different and possibly conflicting specifications in a real-time manner.  At the same time, they may need to fulfill coupled constraints that are defined as relations between their states.  Towards this end, the need for methods for decentralized control at the continuous level and planning at the task level becomes apparent.  We aim here towards unification of these two complementary approaches. Existing solutions rely on a top down centralized approach. We instead consider here a decentralized, bottom-up solution to the problem. The approach relies on three layers of interaction. In the first layer, agents aim at coordinating in order to fulfil their coupled constraints with limited communication exchange of their state information and design of appropriate feedback controllers; in the second layer, agents coordinate in order to mutually satisfy their discrete tasks through exchange of the corresponding plans in the form of automata; in the third and most challenging layer, the communication exchange for coordination now includes both continuous state and discrete plan/abstraction information. The results will be demonstrated in a scenario involving multiple (possibly human) users and multiple robots.
The unification will yield a completely  decentralized system, in which the bottom up approach to define tasks, the consideration of coupled constraints and their combination towards distributed hybrid control and planning in a coordinated  fashion require for 
new ways of thinking and approaches to analysis and constitute the proposal a beyond the SoA and  groundbreaking  approach to the fields of control and computer science.","1498729","2015-03-01","2020-02-29"
"BuildNet","Smart Building Networks","Colin Jones","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The Smart Building Networks (BuildNet) program will develop optimizing controllers capable of coordinating the flow of power to and from large networks of smart buildings in order to offer critical services to the power grid. The network will make use of the thermal storage of the structures and on-site micro generation capabilities of next-generation buildings, as well as the electrical capacity of attached electric vehicles in order to intelligently control the interaction between the network of buildings and the grid. The wide range of electric utility applications, such as wind capacity firming or congestion relief, that will be possible as a result of this coordinated control will in turn allow a significant increase in the percentage of European power generated from destabilizing renewable sources.

Technologically, BuildNet will be built around optimization-based or model predictive control (MPC), a paradigm that is ideally suited to the task of incorporating the current network state and forward-looking information into an optimal decision-making process. The project team will develop novel distributed MPC controllers that utilize the flexibility in the consumption, storage and generation of a distributed network of buildings by exploiting the extensive experience of the PI in optimization-based control and MPC for energy efficient buildings.

Because of its theoretically grounded optimization-based control approach, holistic view of building systems and connected networks, as well as a future-looking technological scope, BuildNet's outputs will deliver impact and be relevant to researchers and practitioners alike.","1460232","2012-12-01","2017-11-30"
"BUNGEE-TOOLS","Building Next-Generation Computational Tools for High Resolution Neuroimaging Studies","Juan Eugenio Iglesias","UNIVERSITY COLLEGE LONDON","Recent advances in magnetic resonance (MR) acquisition technology are providing us with images of the human brain of increasing detail and resolution. While these images hold promise to greatly increase our understanding of such a complex organ, the neuroimaging community relies on tools (e.g. SPM, FSL, FreeSurfer) which, being over a decade old, were designed to work at much lower resolutions. These tools do not consider brain substructures that are visible in present-day scans, and this inability to capitalize on the vast improvement of MR is hampering progress in the neuroimaging field.

In this ambitious project, which lies at the nexus of medical histology, neuroscience, biomedical imaging, computer vision and statistics, we propose to build a set of next-generation computational tools that will enable neuroimaging studies to take full advantage of the increased resolution of modern MR technology. The core of the tools will be an ultra-high resolution probabilistic atlas of the human brain, built upon multimodal data combining from histology and ex vivo MR. The resulting atlas will be used to analyze in vivo brain MR scans, which will require the development of Bayesian segmentation methods beyond the state of the art.

The developed tools, which will be made freely available to the scientific community, will enable the analysis of MR data at a superior level of structural detail, opening completely new opportunities of research in neuroscience. Therefore, we expect the tools to have a tremendous impact on the quest to understand the human brain (in health and in disease), and ultimately on public health and the economy.","1450075","2016-09-01","2021-08-31"
"C-H ACTIVATION","New Concepts for Utilizing a Ubiquitous (Non-)Functional Group - C-H Bond Activation for Increased Efficiency in Organic Synthesis","Frank Klaus Glorius","WESTFAELISCHE WILHELMS-UNIVERSITAET MUENSTER","C-H activations and related reactions can potentially revolutionize the way organic molecules are made and allow a more efficient use of earth's natural resources. Despite the rapid progress of the last couple of years, many problems like limited scope, extreme reaction conditions (temperature, excess of reagents) or low reactivities and selectivities remain in many cases. In this comprehensive proposal containing a number of projects and work packages, we want to develope new C-H activation methods 1) for the efficient synthesis of heterocycles, 2) for the activation of unactivated C(sp3)-H bonds, 3) by employing newly designed Fe-NHC complexes and 4) demonstrating the application of C-H activation for the functionalization of metal-organic frameworks (MOFs). The realization of these goals would render organic synthesis greener and more efficient and would have an impact on the preparation of compounds in academia and industry.","1499400","2010-12-01","2015-11-30"
"C2Phase","Closure of the Cloud Phase","Corinna HOOSE","KARLSRUHER INSTITUT FUER TECHNOLOGIE","Whether and where clouds consist of liquid water, ice or both (i.e. their thermodynamic phase distribution), has major impacts on the clouds’ dynamical development, their radiative properties, their efficiency to form precipitation, and their impacts on the atmospheric environment. Cloud ice formation in the temperature range between 0 and -37°C is initiated by aerosol particles acting as heterogeneous ice nuclei and propagates through the cloud via a multitude of microphysical processes. Enormous progress has been made in recent years concerning the understanding and model parameterization of primary ice formation. In addition, high-resolution atmospheric models with complex cloud microphysics schemes can now be employed for realistic case studies of clouds. Finally, new retrieval schemes for the cloud (top) phase have recently been developed for various satellites, including passive polar orbiting and geostationary sensors, which provide a good spatial and temporal coverage and a long data record.  

We propose here to merge the bottom-up, forward modeling approach for the cloud phase distribution with the top-down view of satellites. C2Phase will conduct systematic closure studies for variables related to the cloud phase distribution such as the cloud ice area fraction, its distribution as function of temperature and its temporal evolution, with a focus on Europe. For this, we will (1) use clustering techniques to separate different cloud regimes in model and satellite data, (2) explore the parameters and processes which the simulated phase distribution is most sensitive to, (3) investigate whether closure is reached between state-of-the art cloud resolving models and satellite observations, and how this closure can be improved by consistent and physically justified changes in microphysical parameterizations, and (4) use our results to improve the representation of mixed-phase clouds in weather and climate models and to quantify the impacts of these improvements.","1499549","2017-04-01","2022-03-31"
"C3ENV","Combinatorial Computational Chemistry   A new field to tackle environmental problems","Thomas Heine","JACOBS UNIVERSITY BREMEN GGMBH","Combinatorial Computational Chemistry is developed as a standard tool to tackle complex problems in chemistry and materials science. The method employs a series of state-of-the-art methods, ranging from empirical molecular mechanics to first principles calculations, as well as of mathematical (graph theoretical and combinatorial) methods. The process is similar as in experimental combinatorial chemistry: First, a large set of candidate structures is generated which is complete in the sense that the best possible structure for a particular purpose must be found among the set. This structure is then identified using computational chemistry. We will apply methodologies at different stages in hierarchical order and successively screen the set of candidate structures. Screening criteria are based on the computer simulations and include geometry, stability and properties of the candidate structures. Detailed characteristics of the final materials will be simulated, including the X-ray diffraction pattern, the electronic structure, and the target properties. We will apply C3 to two important problems of environmental science. (i) We will optimise nanoporous materials to act as molecular sieves to separate water from ethanol, an important task for the production of biofuels. Here, materials are optimised to transport ethanol, but not water (or vice versa). The tuning parameters are the channel size of the material and its polarity. (ii) We will optimise nanoporous materials to transport protons, an important task for the design of energy-efficient fuel cells, by distributing flexible functional groups, acting as hopping sites for the protons, in the framework.","1500000","2011-02-01","2016-04-30"
"C4T","Climate change across Cenozoic cooling steps reconstructed with clumped isotope thermometry","Anna Nele Meckler","UNIVERSITETET I BERGEN","The Earth's climate system contains a highly complex interplay of numerous components, such as atmospheric greenhouse gases, ice sheets, and ocean circulation. Due to nonlinearities and feedbacks, changes to the system can result in rapid transitions to radically different climate states. In light of rising greenhouse gas levels there is an urgent need to better understand climate at such tipping points. Reconstructions of profound climate changes in the past provide crucial insight into our climate system and help to predict future changes. However, all proxies we use to reconstruct past climate depend on assumptions that are in addition increasingly uncertain back in time. A new kind of temperature proxy, the carbonate ‘clumped isotope’ thermometer, has great potential to overcome these obstacles. The proxy relies on thermodynamic principles, taking advantage of the temperature-dependence of the binding strength between different isotopes of carbon and oxygen, which makes it independent of other variables. Yet, widespread application of this technique in paleoceanography is currently prevented by the required large sample amounts, which are difficult to obtain from ocean sediments. If applied to the minute carbonate shells preserved in the sediments, this proxy would allow robust reconstructions of past temperatures in the surface and deep ocean, as well as global ice volume, far back in time. Here I propose to considerably decrease sample amount requirements of clumped isotope thermometry, building on recent successful modifications of the method and ideas for further analytical improvements. This will enable my group and me to thoroughly ground-truth the proxy for application in paleoceanography and for the first time apply it to aspects of past climate change across major climate transitions in the past, where clumped isotope thermometry can immediately contribute to solving long-standing first-order questions and allow for major progress in the field.","1877209","2015-08-01","2020-07-31"
"CA2PVM","Multi-field and multi-scale Computational Approach to design and durability of PhotoVoltaic Modules","Marco Paggi","ALTI STUDI DI LUCCA","""Photovoltaics (PV) based on Silicon (Si) semiconductors is one the most growing technology in the World for renewable, sustainable, non-polluting, widely available clean energy sources. Theoretical and applied research aims at increasing the conversion efficiency of PV modules and their lifetime. The Si crystalline microstructure has an important role on both issues. Grain boundaries introduce additional resistance and reduce the conversion efficiency. Moreover, they are prone to microcracking, thus influencing the lifetime. At present, the existing standard qualification tests are not sufficient to provide a quantitative definition of lifetime, since all the possible failure mechanisms are not accounted for. In this proposal, an innovative computational approach to design and durability assessment of PV modules is put forward. The aim is to complement real tests by virtual (numerical) simulations. To achieve a predictive stage, a challenging multi-field (multi-physics) computational approach is proposed, coupling the nonlinear elastic field, the thermal field and the electric field. To model real PV modules, an adaptive multi-scale and multi-field strategy will be proposed by introducing error indicators based on the gradients of the involved fields. This numerical approach will be applied to determine the upper bound to the probability of failure of the system. This statistical assessment will involve an optimization analysis that will be efficiently handled by a Mathematica-based hybrid symbolic-numerical framework. Standard and non-standard experimental testing on Si cells and PV modules will also be performed to complement and validate the numerical approach. The new methodology based on the challenging integration of advanced physical and mathematical modelling, innovative computational methods and non-standard experimental techniques is expected to have a significant impact on the design, qualification and lifetime assessment of complex PV systems.""","1483980","2012-12-01","2017-11-30"
"CAC","Cryptography and Complexity","Yuval Ishai","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","Modern cryptography has deeply rooted connections with computational complexity theory and other areas of computer science. This proposal suggests to explore several {\em new connections} between questions in cryptography and questions from other domains, including computational complexity, coding theory, and even the natural sciences. The project is expected to broaden the impact of ideas from cryptography on other domains, and on the other hand to benefit cryptography by applying tools from other domains towards better solutions for central problems in cryptography.","1459703","2010-12-01","2015-11-30"
"CACH","Reconstructing abrupt Changes in Chemistry and Circulation of the Equatorial Atlantic Ocean: Implications for global Climate and deep-water Habitats","Laura Frances Robinson","UNIVERSITY OF BRISTOL","""Ice-core records show that glacials had lower atmospheric pCO2 and cooler temperatures than today and that the last deglaciation was punctuated by large, abrupt millennial-scale climate events. Explaining the mechanism controlling these oscillations remains an outstanding puzzle.  The ocean is a key player, and the Atlantic is particularly dynamic as it transports heat, carbon and nutrients across the equator. This project proposes to consolidate my research through a focused study of present and past ocean chemistry in the Equatorial Atlantic and to assess the impact of ocean chemistry on fragile deep-sea ecosystems. Despite decades of research there are distinct gaps in our knowledge of the history of the deep and intermediate ocean. Major hurdles include access to suitable archives, development of geochemical proxies and analyses that are sufficiently precise to test climate hypotheses. Through a combination of ship board field work, modern calibrations and cutting-edge geochemical analyses this project will produce samples and data that address each of these gaps. A particular focus will be on using the skeletons of deep-sea corals. Research using deep-sea corals as climate archives, and indeed research into their habitats, environmental controls and potential threats to their survival are still fields in their infancy. The expense and logistics of working in the deep ocean, the complexity of the ecosystem and the biogeochemistry of the coral skeletons have all proved to be significant challenges. The potential payoffs of high-resolution, dateable archives, however, make the effort worthwhile. There have been no studies that attempt to match up co-located deep-sea coral, seawater and sediment samples in a single program, so this would be the first directed study of its type, and as such promises to provide a substantial step in quantifying the fluxes and transport of mass, heat and nutrients across the equator in the past.""","1998833","2011-10-01","2017-09-30"
"CAD4FACE","Computational modelling for personalised treatment of congenital craniofacial abnormalities","Silvia SCHIEVANO","UNIVERSITY COLLEGE LONDON","Craniosynostosis is a group of congenital craniofacial abnormalities consisting in premature fusion (ossification) of one or more cranial sutures during infancy. This results in growth restriction perpendicular to the axis of the suture and promotes growth parallel to it, causing physical deformation of the cranial and facial skeleton, as well as distortion of the underling brain, with potential detrimental effects on its function: visual loss, sleep apnoea, feeding and breathing difficulties, and neurodevelopment delay. Conventional management of craniosynostosis involves craniofacial surgery delivered by excision of the prematurely fused sutures, multiple bone cuts and remodelling of the skull deformities, with the primary goal of improving patient function, while normalising their appearance. Cranial vault remodelling surgical procedures, aided by internal and external devices, have proven functionally and aesthetically effective in correcting skull deformities, but final results remain unpredictable and often suboptimal because of an incomplete understanding of the biomechanical interaction between the device and the skull.
The overall aim of this grant is to create a validated and robust computational framework that integrates patient information and device design to deliver personalised care in paediatric craniofacial surgery in order to improve clinical outcomes. A virtual model of the infant skull with craniosynostosis, including viscoelastic properties and mechano-biology regulation, will be developed to simulate device implantation and performance over time, and will be validated using clinical data from patient populations treated with current devices. Bespoke new devices will be designed allowing for pre-programmed 3D shapes to be delivered with continuous force during the implantation period. Patient specific skull models will be used to virtually test and optimise the personalised devices, and to tailor the surgical approach for each individual case.","1498772","2018-03-01","2023-02-28"
"CAFES","Causal Analysis of Feedback Systems","Joris Marten Mooij","UNIVERSITEIT VAN AMSTERDAM","Many questions in science, policy making and everyday life are of a causal nature: how would changing A influence B? Causal inference, a branch of statistics and machine learning, studies how cause-effect relationships can be discovered from data and how these can be used for making predictions in situations where a system has been perturbed by an external intervention. The ability to reliably make such causal predictions is of great value for practical applications in a variety of disciplines. Over the last two decades, remarkable progress has been made in the field. However, even though state-of-the-art causal inference algorithms work well on simulated data when all their assumptions are met, there is still a considerable gap between theory and practice. The goal of CAFES is to bridge that gap by developing theory and algorithms that will enable large-scale applications of causal inference in various challenging domains in science, industry and decision making. 

The key challenge that will be addressed is how to deal with cyclic causal relationships (""feedback loops""). Feedback loops are very common in many domains (e.g., biology, economy and climatology), but have mostly been ignored so far in the field. Building on recently established connections between dynamical systems and causal models, CAFES will develop theory and algorithms for causal modeling, reasoning, discovery and prediction for cyclic causal systems. Extensions to stationary and non-stationary processes will be developed to advance the state-of-the-art in causal analysis of time-series data. In order to optimally use available resources, computationally efficient and statistically robust algorithms for causal inference from observational and interventional data in the context of confounders and feedback will be developed. The work will be done with a strong focus on applications in molecular biology, one of the most promising areas for automated causal inference from data.","1405652","2015-09-01","2020-08-31"
"CALCEAM","Cooperative Acceptor Ligands for Catalysis with Earth-Abundant Metals","Marc-Etienne Moret","UNIVERSITEIT UTRECHT","Homogeneous catalysis is of prime importance for the selective synthesis of high added value chemicals. Many of the currently available catalysts rely on noble metals (Ru, Os, Rh, Ir, Pd, Pt), which suffer from a high toxicity and environmental impact in addition to their high cost, calling for the development of new systems based on first-row transition metals (Mn, Fe, Co, Ni, Cu). The historical paradigm for catalyst design, i.e. one or more donor ligands giving electron density to stabilize a metal center and tune its reactivity, is currently being challenged by the development of acceptor ligands that mostly withdraw electron density from the metal center upon binding. In the last decade, such ligands – mostly based on  boron and heavier main-group elements – have evolved from a structural curiosity to a powerful tool in designing new reactive units for homogeneous catalysis. 
I will develop a novel class of ligands that use C=E (E=O, S, NR) multiple bonds anchored in close proximity to the metal by phosphine tethers. The electrophilic C=E multiple bond is designed to act as an acceptor moiety that adapts its binding mode to the electronic structure of reactive intermediates with the unique additional possibility of involving the lone pairs on heteroelement E in cooperative reactivity. Building on preliminary results showing that a C=O bond can function as a hemilabile ligand in a catalytic cycle, I will undertake a systematic, experimental and theoretical investigation of the structure and reactivity of M–C–E three membered rings formed by side-on coordination of C=E bonds to a first-row metal. Their ability to facilitate multi-electron transformations (oxidative addition, atom/group transfer reactions) will be investigated. In particular, hemilability of the C=E bond is expected to facilitate challenging C–C bond forming reactions mediated by Fe and Ni. This approach will demonstrate a new conceptual tool for the design of efficient base-metal catalysts.","1500000","2017-08-01","2022-07-31"
"Calcyan","A living carbonate factory: how do cyanobacteria make rocks? (Calcification in Cyanobacteria)","Karim Benzerara","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","This interdisciplinary proposal stems from our recent discovery of deep-branching cyanobacteria that form intracellular Ca-Mg-Sr-Ba carbonates. So far, calcification by cyanobacteria was considered as exclusively extracellular, hence dependent on external conditions. The existence of intracellularly calcifying cyanobacteria may thus deeply modify our view on the role of cyanobacteria in the formation of modern and past carbonate deposits and the degree of control they achieve on this geochemically significant process. Moreover, since these cyanobacteria concentrate selectively Sr and Ba over Ca, it suggests the existence of processes that can alter the message conveyed by proxies such as Sr/Ca ratios in carbonates, classically used for paleoenvironmental reconstruction. Finally, such a biomineralization process, if globally significant may impact our view of how an ecosystem responds to external CO2 changes in particular by affecting most likely a key parameter such as the balance between organic carbon fixed by photosynthesis and inorganic carbon fixed by CaCO3 precipitation.
Here, I aim to bring a qualitative jump in the understanding of this process. The core of this project is to provide a detailed picture of intracellular calcification by cyanobacteria. This will be achieved by studying laboratory cultures of cyanobacteria, field samples of modern calcifying biofilms and ancient microbialites. Diverse tools from molecular biology, biochemistry, mineralogy and geochemistry will be used. Altogether these techniques will help unveiling the molecular and mineralogical mechanisms involved in cyanobacterial intracellular calcification, assessing the phylogenetic diversity of these cyanobacteria and the preservability of their traces in ancient rocks. My goal is to establish a unique expertise in the study of calcification by cyanobacteria, the scope of which can be developed and broadened in the future for the study of interactions between life and minerals.","1659478","2013-02-01","2018-01-31"
"CALDER","Cryogenic wide-Area Light Detectors  with Excellent Resolution","Marco Vignati","ISTITUTO NAZIONALE DI FISICA NUCLEARE","""In the comprehension of fundamental laws of nature, particle physics is now facing two important questions:
1) What is the nature of the neutrino, is it a standard (Dirac) particle or a Majorana particle? The nature of the neutrino plays a crucial role in the global framework of particle interactions and in cosmology. The only practicable way to answer this question is to search for a nuclear process called """"neutrinoless double beta decay"""" (0nuDBD).
2) What is the so called """"dark matter"""" made of? Astrophysical observations suggest that the largest part of the mass of the Universe is composed by a form of matter other than atoms and known matter constituents. We still do not know what dark matter is made of because its rate of interaction with ordinary matter is really low, thus making the direct experimental detection extremely difficult.
Both 0nuDBD and dark matter interactions are rare processes and can be detected using the same experimental technique. Bolometers are promising devices and their combination with light detectors provides the identification of interacting particles, a powerful tool to reduce the background.  

The goal of CALDER is to realize a new type of light detectors to improve the upcoming generation of bolometric experiments. The detectors will be designed to feature unprecedented energy resolution and reliability, to ensure an almost complete particle identification. In case of success, CUORE, a 0nuDBD experiment in construction, would gain in sensitivity by up to a factor 6. LUCIFER, a 0nuDBD experiment already implementing the light detection, could be sensitive also to dark matter interactions, thus increasing its research potential. The light detectors will be based on Kinetic Inductance Detectors (KIDs), a new technology that proved its potential in astrophysical applications but that is still new in the field of particle physics and rare event searches.""","1176758","2014-03-01","2019-02-28"
"CALENDS","Clusters And LENsing of Distant Sources","Johan Pierre Richard","UNIVERSITE LYON 1 CLAUDE BERNARD","Some of the primary questions in extragalactic astronomy concern the formation and evolution of galaxies in the distant Universe. In particular, little is known about the less luminous (and therefore less massive) galaxy populations, which are currently missed from large observing surveys and could contribute significantly to the overall star formation happening at early times. One way to overcome the current observing limitations prior to the arrival of the future James Webb Space Telescope or the European Extremely Large Telescopes is to use the natural magnification of strong lensing clusters to look at distant sources with an improved sensitivity and resolution.

The aim of CALENDS is to build and study in great details a large sample of accurately-modelled,  strongly lensed galaxies at high redshift (1<z<5) selected in the fields of massive clusters, and compare them with the more luminous or lower redshift populations. We will develop novel techniques in this process, in order to improve the accuracy of strong-lensing models and precisely determine the mass content of these clusters. By performing a systematic modelling of the cluster sample we will look into the relative distribution of baryons and dark matter as well as the amount of substructure in cluster cores. Regarding the population of lensed galaxies, we will study their global properties through a multiwavelength analysis covering the optical to millimeter domains, including spectroscopic information from MUSE and KMOS on the VLT, and ALMA.
We will look for scaling relations between the stellar, gas and dust parameters, and compare them with known relations for lower redshift and more massive galaxy samples. For the most extended sources, we will be able to spatially resolve their inner properties, and compare the results of individual regions with predictions from simulations. We will look into key physical processes: star formation, gas accretion, inflows and outflows, in these distant sources.","1450992","2013-09-01","2019-08-31"
"CAMAP","CAMAP: Computer Aided Modeling for Astrophysical Plasmas","Miguel-Ángel Aloy-Torás","UNIVERSITAT DE VALENCIA","This project will be aimed at obtaining a deeper insight into the physical processes taking place in astrophysical magnetized plasmas. To study these scenarios I will employ different numerical codes as virtual tools that enable me to experiment on computers (virtual labs) with distinct initial and boundary conditions. Among the kind of sources I am interested to consider, I outline the following: Gamma-Ray Bursts (GRBs), extragalactic jets from Active Galactic Nuclei (AGN), magnetars and collapsing stellar cores. A number of important questions are still open regarding the fundamental properties of these astrophysical sources (e.g., collimation, acceleration mechanism, composition, high-energy emission, gravitational wave signature). Additionally, there are analytical issues on the formalism in relativistic dynamics not resolved yet, e.g., the covariant extension of resistive magnetohydrodynamics (MHD). All these problems are so complex that only a computational approach is feasible. I plan to study them by means of relativistic and Newtonian MHD numerical simulations. A principal focus of the project will be to assess the relevance of magnetic fields in the generation, collimation and ulterior propagation of relativistic jets from the GRB progenitors and from AGNs. More generally, I will pursue the goal of understanding the process of amplification of seed magnetic fields until they become dynamically relevant, e.g., using semi-global and local simulations of representative boxes of collapsed stellar cores. A big emphasis will be put on including all the relevant microphysics (e.g. neutrino physics), non-ideal effects (particularly, reconnection physics) and energy transport due to neutrinos and photons to account for the relevant processes in the former systems. A milestone of this project will be to end up with a numerical tool that enables us to deal with General Relativistic Radiation Magnetohydrodynamics problems in Astrophysics.","1497000","2011-03-01","2017-02-28"
"CAMBAT","Calcium and magnesium metal anode based batteries","Alexandre PONROUCH","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","Li-ion battery is ubiquitous and has emerged as the major contender to power electric vehicles, yet Li-ion is slowly but surely reaching its limits and controversial debates on lithium supply cannot be ignored. New sustainable battery chemistries must be developed and the most appealing alternatives are to use Ca or Mg metal anodes which would bring a breakthrough in terms of energy density relying on much more abundant elements. Since Mg and Ca do not appear to be plagued by dendrite formation like Li, metal anodes could thus safely be used. While standard electrolytes forming stable passivation layers at the electrode/electrolyte interfaces enabled the success of the Li-ion technology, the migration of divalent cations through a passivation layer was thought to be impossible. Thus, all research efforts to date have been devoted to the formulation of electrolytes that do not form such layer. This approach comes with complex electrolyte, highly corrosive and with narrow electrochemical stability window leading to incompatibility with high voltage cathodes thus penalizing energy density. 
The applicant demonstrated that calcium can be reversibly plated and stripped through a stable passivation layer when transport properties within the electrolyte are tuned (decreasing ion pair formation). CAMBAT aims at developing new electrolytes forming stable passivation layers and allowing the migration of Ca2+ and Mg2+. Such a dramatic shift in the methodology would allow considering a completely new family of electrolytes enabling the evaluation of high voltage cathode materials that cannot be tested in the electrolytes available nowadays. 1Ah prototype cells will be assembled as proof of concept, targets for energy density and cost being ca. 300 Wh/kg and 250 $/kWh, respectively, thus doubling the energy density while dividing by at least a factor of 2 the price when compared to state of the art Li-ion batteries and having the potential for being SAFER (absence of dendrite).","1688705","2017-01-01","2021-12-31"
"CapBed","Engineered Capillary Beds for Successful Prevascularization of Tissue Engineering Constructs","Rogério Pedro Lemos de Sousa Pirraco","UNIVERSIDADE DO MINHO","The demand for donated organs vastly outnumbers the supply, leading each year to the death of thousands of people and the suffering of millions more. Engineered tissues and organs following Tissue Engineering approaches are a possible solution to this problem. However, a prevascularization solution to irrigate complex engineered tissues and assure their survival after transplantation is currently elusive. In the human body, complex organs and tissues irrigation is achieved by a network of blood vessels termed capillary bed which suggests such a structure is needed in engineered tissues. Previous approaches to engineer capillary beds reached different levels of success but none yielded a fully functional one due to the inability in simultaneously addressing key elements such as correct angiogenic cell populations, a suitable matrix and dynamic conditions that mimic blood flow.
CapBed aims at proposing a new technology to fabricate in vitro capillary beds that include a vascular axis that can be anastomosed with a patient circulation. Such capillary beds could be used as prime tools to prevascularize in vitro engineered tissues and provide fast perfusion of those after transplantation to a patient. Cutting edge techniques will be for the first time integrated in a disruptive approach to address the requirements listed above. Angiogenic cell sheets of human Adipose-derived Stromal Vascular fraction cells will provide the cell populations that integrate the capillaries and manage its intricate formation, as well as the collagen required to build the matrix that will hold the capillary beds. Innovative fabrication technologies such as 3D printing and laser photoablation will be used for the fabrication of the micropatterned matrix that will allow fluid flow through microfluidics. The resulting functional capillary beds can be used with virtually every tissue engineering strategy rendering the proposed strategy with massive economical, scientific and medical potential","1499940","2018-11-01","2023-10-31"
"CapReal","Performance Capture of the Real World in Motion","Christian Theobalt","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Computer graphics technology for realistic rendering has improved
dramatically; however, the technology to create scene models to be rendered,
e.g., for movies, has not developed at the same pace. In practice, the state
of the art in model creation still requires months of complex manual design,
and this is a serious threat to progress. To attack this problem, computer
graphics and computer vision researchers jointly developed methods that
capture scene models from real world examples. Of particular importance is
the capturing of moving scenes. The pinnacle of dynamic scene capture
technology in research is marker-less performance capture. From multi-view
video, they capture dynamic surface and texture models of the real world.
Performance capture is hardly used in practice due to profound limitations:
recording is usually limited to indoor studios, controlled lighting, and
dense static camera arrays. Methods are often limited to single objects, and
reconstructed shape detail is very limited. Assumptions about materials,
reflectance, and lighting in a scene are simplistic, and we cannot easily
modify captured data.

In this project, we will pioneer a new generation of performance capture
techniques to overcome these limitations. Our methods will allow the
reconstruction of dynamic surface models of unprecedented shape detail. They
will succeed on general scenes outside of the lab and outdoors, scenes with
complex material and reflectance distributions, and scenes in which lighting
is general, uncontrolled, and unknown. They will capture dense and crowded
scenes with complex shape deformations. They will reconstruct conveniently
modifiable scene models. They will work with sparse and moving sets of
cameras, ultimately even with mobile phones. This far-reaching,
multi-disciplinary project will turn performance capture from a research
technology into a practical technology, provide groundbreaking scientific
insights, and open up revolutionary new applications.","1480800","2013-09-01","2018-08-31"
"CAPRI","Clouds and Precipitation Response to Anthropogenic Changes in the Natural Environment","Ilan Koren","WEIZMANN INSTITUTE OF SCIENCE LTD","Clouds and precipitation play a crucial role in the Earth's energy balance, global atmospheric circulation and the water cycle. Despite their importance, clouds still pose the largest uncertainty in climate research.
I propose a new approach for studying anthropogenic effects on cloud fields and rain, tackling the challenge from both scientific ends: reductionism and systems approach. We will develop a novel research approach using observations and models interactively that will allow us to “peel apart” detailed physical processes. In parallel we will develop a systems view of cloud fields looking for Emergent Behavior rising out of the complexity, as the end result of all of the coupled processes. Better understanding of key processes on a detailed (reductionist) manner will enable us to formulate the important basic rules that control the field and to look for emergence of the overall effects.
We will merge ideas and methods from four different disciplines: remote sensing and radiative transfer, cloud physics, pattern recognition and computer vision and ideas developed in systems approach. All of this will be done against the backdrop of natural variability of meteorological systems.
The outcomes of this work will include fundamental new understanding of the coupled surface-aerosol-cloud-precipitation system. More importantly this work will emphasize the consequences of human actions on the environment, and how we change our climate and hydrological cycle as we input pollutants and transform the Earth’s surface. This work will open new horizons in cloud research by developing novel methods and employing the bulk knowledge of pattern recognition, complexity, networking and self organization to cloud and climate studies. We are proposing a long-term, open-ended program of study that will have scientific and societal relevance as long as human-caused influences continue, evolve and change.","1428169","2012-09-01","2017-08-31"
"CAPS","Capillary suspensions: a novel route for versatile, cost efficient and environmentally friendly material design","Erin Crystal Koos","KATHOLIEKE UNIVERSITEIT LEUVEN","A wide variety of materials including coatings and adhesives, emerging materials for nanotechnology products, as well as everyday food products are processed or delivered as suspensions. The flow properties of such suspensions must be finely adjusted according to the demands of the respective processing techniques, even for the feel of cosmetics and the perception of food products is highly influenced by their rheological properties. The recently developed capillary suspensions concept has the potential to revolutionize product formulations and material design. When a small amount (less than 1%) of a second immiscible liquid is added to the continuous phase of a suspension, the rheological properties of the mixture are dramatically altered from a fluid-like to a gel-like state or from a weak to a strong gel and the strength can be tuned in a wide range covering orders of magnitude. Capillary suspensions can be used to create smart, tunable fluids, stabilize mixtures that would otherwise phase separate, significantly reduce the amount organic or polymeric additives, and the strong particle network can be used as a precursor for the manufacturing of cost-efficient porous ceramics and foams with unprecedented properties.
This project will investigate the influence of factors determining capillary suspension formation, the strength of these admixtures as a function of these aspects, and how capillary suspensions depend on external forces. Only such a fundamental understanding of the network formation in capillary suspensions on both the micro- and macroscopic scale will allow for the design of sophisticated new materials. The main objectives of this proposal are to quantify and predict the strength of these admixtures and then use this information to design a variety of new materials in very different application areas including, e.g., porous materials, water-based coatings, ultra low fat foods, and conductive films.","1489618","2013-08-01","2018-07-31"
"CARBENZYMES","Probing the relevance of carbene binding motifs in enzyme reactivity","Martin Albrecht","UNIVERSITY COLLEGE DUBLIN, NATIONAL UNIVERSITY OF IRELAND, DUBLIN","Histidine (His) is an ubiquitous ligand in the active site of metalloenzymes that is assumed by default to bind the metal center through one of its nitrogen atoms. However, protonation of His, which is likely to occur in locally slightly acidic environment, gives imidazolium sites that can bind a metal in a carbene-type structure as found in N-heterocyclic carbene complexes. Such carbene bonding has a dramatic effect on the properties of the metal center and may provide a rational for the mode of action of metalloenzymes that are still lacking a solid understanding. Up to now, the possibility of carbene bonding has been completely overlooked. Hence, any evidence for such His coordination via carbon will induce a shift of paradigm in classical peptide chemistry and will be directly included in basic textbooks. Moreover, this unprecedented bonding mode will provide access to unique and hitherto unknown reactivity patterns for artificial enzyme mimics. Undoubtedly, such a break-through will set a new stage in modern metalloenzyme research. A multicentered approach is proposed to identify for the first time carbene bonding in enzymes. This approach unconventionally combines the current frontiers of organometallic and biochemical knowledge and hence crosses traditional boarders. Specifically, we aim at probing carbene bonding of His by identifying reactivity patterns that are selective for metal-carbenes but not for metal-imine complexes. This will allow for efficient screening of large classes of metalloenzymes. In parallel, active site models will be constructed in which the His ligand is substituted by a heterocyclic carbene as a rigidly C-bonding His analog. For this purpose chemical synthesis will be considered as well as enzyme mutagenesis and subsequent carbene coordination. While such new bioorganometallic entities will be highly attractive to probe the influence of C-bound His on the metal site, they also provide conceputally new types of versatile catalysts.","1249808","2008-07-01","2013-06-30"
"CARBONFIX","Towards a Self-Amplifying Carbon-Fixing Anabolic Cycle","Joseph Moran","CENTRE INTERNATIONAL DE RECHERCHE AUX FRONTIERES DE LA CHIMIE FONDATION","How can simple molecules self-organize into a growing synthetic reaction network like biochemical metabolism? This proposal takes a novel synthesis-driven approach to the question by mimicking a central self-amplifying CO2-fixing biochemical reaction cycle known as the reductive tricarboxylic acid cycle. The intermediates of this cycle are the synthetic precursors to all major classes of biomolecules and are built from CO2, an anhydride and electrons from simple reducing agents. Based on the nature of the reactions in the cycle and the specific structural features of the intermediates that comprise it, we propose that the entire cycle may be enabled in a single reaction vessel with a surprisingly small number of simple, mutually compatible catalysts from the recent synthetic organic literature. However, since one of the required reactions does not yet have an efficient synthetic equivalent in the literature and since those that do have not yet been carried out sequentially in a single reaction vessel, we will first independently develop the new reaction and sequences before attempting to combine them into the entire cycle. The new reaction and sequences will be useful green synthetic methods in their own right. Most significantly, this endeavour could provide the first experimental evidence of an exciting new alternative model for early biochemical evolution that finally illuminates the origins and necessity of biochemistry’s core reactions.","1500000","2015-09-01","2020-08-31"
"CARBONLIGHT","Tunable light tightly bound to a single sheet of carbon atoms:
graphene as a novel platform for nano-optoelectronics","Frank Henricus Louis Koppens","FUNDACIO INSTITUT DE CIENCIES FOTONIQUES","Graphene, a one-atom-thick layer of carbon, has attracted enormous attention in diverse areas of applied and fundamental physics. Due to its unique crystal structure, charge carriers have an effective mass of zero and a very high mobility, even at room temperature. While graphene-based devices have an enormous potential for high-speed electronics, graphene has recently been recognized as a photonic material for novel optoelectronic applications.
Interestingly, graphene is also a promising host material for light that is confined to nanoscale dimensions, more than 100 times below the diffraction limit. Due to its ultra-small thickness and extremely high purity, graphene can support strongly confined propagating light fields coupled to the charge carriers in the material: surface plasmons.  The properties of these plasmons are controllable by electrostatic gates, holding promise for in-situ tunability of light-matter interactions at a length scale far below the wavelength.
This project will experimentally investigate the new and virtually unexplored field of graphene surface plasmonics, and combine this with other appealing properties of graphene to demonstrate the unique potential of carbon-based nano-optoelectronics. The aim is to explore the limits of unprecedented light concentration, manipulation and detection at the nanoscale, to dramatically intensify nonlinear interactions between photons towards the quantum regime, and to reveal the subtle effects of cavity quantum electrodynamics on graphene-emitter systems. This research will reveal the far-reaching potential of a single sheet of carbon atoms as a host for light and electrons at the nanoscale, with prospects for novel nanoscale optical circuits and detectors, nano-optomechanical systems and tunable artificial quantum emitters.","1466000","2012-11-01","2017-10-31"
"CARBONNEMS","NanoElectroMechanical Systems based on Carbon Nanotube and Graphene","Adrian Bachtold","FUNDACIO INSTITUT DE CIENCIES FOTONIQUES","Carbon nanotubes and graphene form a class of nanoscale objects with exceptional electrical, mechanical and structural properties. I propose to exploit these unique properties to fabricate and study various nanoelectromechanical systems (NEMS) based on graphene and nanotubes. Specifically, I will address two directions with major scientific interests:

1- I propose to study electromechanical resonators based on an individual nanotube or on a single layer of graphene. My group has a leading position in this recent research field and the idea is to take advantage of our expertise for two sets of experiments, one on inertial mass sensing and one on the exploration of quantum motion. These two topics are generating at present an intense activity in the NEMS community. Experiments are usually carried out using microfabricated silicon resonators but the ultra low mass of nanotubes and graphene has here an enormous asset. It drastically improves the sensitivity of mass sensing and it dramatically enhances the amplitude of the motion in the quantum regime.

2- My team will fabricate and exploit nanomotors based on nanotube and graphene. Only few man-made nanomotors have been demonstrated so far. Reasons are multiple. For instance, the fabrication of nanomotors is technically challenging. In addition, friction forces are often so strong that they hinder motion. Because of their unique properties, nanotubes and graphene represent a material of choice for the development of new nanomotors. We will construct nanomotors with different layouts and address how electrical, thermal or chemical energy can be transformed into mechanical energy in order to drive motion at the nanoscale.","1996789","2012-01-01","2016-12-31"
"CARBONSINK","Life beneath the ocean floor: The subsurface sink of carbon in the marine environment","Alexandra Turchyn","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","""One prominent idea for mitigating global climate change is to remove CO2 from the atmosphere by storing it in fluids in the natural environment; for example dissolved within sediments below the ocean floor or in oceanic crust.  This carbon sequestration is popular because it would allow us to place carbon into semi-permanent (on human timescales) storage, ‘buying time’ to wean us from our dependence on carbon-based energy sources.  Application of such a mitigation technique presumes knowledge of what will happen to carbon when it is dissolved in various environments.  Studies of naturally produced excess dissolved CO2 are, however, equivocal; this lack of knowledge represents a huge deficit in our comprehension of the global carbon cycle and specifically the processes removing carbon from the surface of the planet over geological timescales.

This proposal will resolve the sink for CO2 within marine sediments and oceanic crust.  Beneath much of the ocean floor exists the ‘deep biosphere’, microbial populations living largely in the absence of oxygen, consuming organic carbon that has fallen to the sea floor, producing a large excess of dissolved inorganic carbon.  This dissolved inorganic carbon can diffuse back to the ocean or can precipitate in situ as carbonate minerals.  Previous attempts to quantify the flux of carbon through the deep biosphere focused mostly on studies of sulfur and carbon, and these studies cannot reveal the fate of the produced inorganic carbon. I propose a novel approach to constrain the fate of carbon through the study of the subsurface calcium cycle.  Calcium is the element involved in precipitating carbon as in situ carbonate minerals and thus will directly provide the required mass balance to determine the fate of CO2 in the marine subsurface.  This mass balance will be achieved through experiments, measurements, and numerical modeling, to achieve the primary objective of constraining the fate of carbon in submarine environments.""","1945695","2012-12-01","2017-11-30"
"CartographY","Mapping Stellar Helium","Guy DAVIES","THE UNIVERSITY OF BIRMINGHAM","In the epoch of Gaia, fundamental stellar properties will be made widely available for large numbers of stars.  These properties are expected to unleash a new wave of discovery in the field of astrophysics.  But while many properties of stars are measurable, meaningful Helium abundances (Y) remain elusive and as a result fundamental properties are not accurate.

Helium enrichment laws, which underpin most stellar properties, link initial Y to initial metallicity, but these relations are very uncertain with gradients (dY/dZ) spanning the range 1 to 3.  This uncertainty is the initial Y problem and this is a bottleneck that must be overcome to unleash the true potential of Gaia.

Without measurements of initial Y for all stars we need to find alternative observables that trace out the evolution of initial Y.  We will search for better tracers using the power of asteroseismology as a calibrator.
 
Asteroseismic measures of Helium will be used to construct a map from observable properties (fundamental, chemical or even dynamical) back to initial Helium.  This is a challenge that can only be solved through the use of the latest asteroseismic techniques coupled to a rigorous yet flexible statistical scheme.  I am uniquely qualified in the cutting edge methods of asteroseismology and the application of advanced multi-level statistical models.  The intersection of these two skill sets will allow me to solve the initial Helium problem.

The motivation for a timely solution to this problem could not be stronger.  We have just entered an age of large asteroseismic datasets, vast spectroscopic surveys, and the billion star program of Gaia. The next wave of scientific breakthroughs in stellar physics, exoplanetary science, and Galactic archeology will be held back unless accurate fundamental stellar properties are available. We can only produce these accurate properties with a reliable map of stellar Helium.","1496203","2019-04-01","2024-03-31"
"CASAA","Catalytic asymmetric synthesis of amines and amides","Jeffrey William Bode","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","""Amines and their acylated derivatives – amides – are among the most common chemical functional groups found in modern pharmaceuticals. Despite this there are few methods for their efficient, environmentally sustainable production in enantiomerically pure form. This proposal seeks to provide new catalytic chemical methods including 1) the catalytic, enantioselective synthesis of peptides and 2) catalytic methods for the preparation of enantiopure nitrogen-containing heterocycles. The proposed work features innovative chemistry including novel reaction mechanism and catalysts. These methods have far reaching applications for the sustainable production of valuable compounds as well as fundamental science.""","1500000","2012-12-01","2017-11-30"
"CASCAde","Confidentiality-preserving Security Assurance","Thomas GROSS","UNIVERSITY OF NEWCASTLE UPON TYNE","""This proposal aims to create a new generation of security assurance. It investigates whether one can certify an inter-connected dynamically changing system in such a way that one can prove its security properties without disclosing sensitive information about the system's blueprint. 

This has several compelling advantages. First, the security of large-scale dynamically changing systems will be significantly improved. Second, we can prove properties of topologies, hosts and users who participate in transactions in one go, while keeping sensitive information confidential. Third, we can prove the integrity of graph data structures to others, while maintaining their their confidentiality. This will benefit EU governments and citizens through the increased security of critical systems.

The proposal pursues the main research hypothesis that usable confidentiality-preserving security assurance will trigger a paradigm shift in security and dependability. It will pursue this objective by the creation of new cryptographic techniques to certify and prove properties of graph data structures. A preliminary investigation in 2015 showed that graph signature schemes are indeed feasible. The essence of this solution can be traced back to my earlier research on highly efficient attribute encodings for anonymous credential schemes in 2008.

However, the invention of graph signature schemes only clears one obstacle in a long journey to create a new generation of security assurance systems. There are still many complex obstacles, first and foremost, assuring """"soundness"""" in the sense that integrity proofs a verifier accepts translate to the state of the system at that time. The work program involves six WPs: 1) to develop graph signatures and new cryptographic primitives; 2) to establish cross-system soundness; 3) to handle scale and change; 4) to establish human trust and usability; 5) to create new architectures; and 6) to test prototypes in practice.""","1485643","2017-11-01","2022-10-31"
"CASe","Combinatorics with an analytic structure","Karim ADIPRASITO","THE HEBREW UNIVERSITY OF JERUSALEM","""Combinatorics, and its interplay with geometry, has fascinated our ancestors as shown by early stone carvings in the Neolithic period. Modern combinatorics is motivated by the ubiquity of its structures in both pure and applied mathematics.
The work of Hochster and Stanley, who realized the relation of enumerative questions to commutative algebra and toric geometry made a vital contribution to the development of this subject. Their work was a central contribution to the classification of face numbers of simple polytopes, and the initial success lead to a wealth of research in which combinatorial problems were translated to algebra and geometry and then solved using deep results such as Saito's hard Lefschetz theorem. As a caveat, this also made branches of combinatorics reliant on algebra and geometry to provide new ideas. 

In this proposal, I want to reverse this approach  and extend our understanding of geometry and algebra guided by combinatorial methods. In this spirit I propose new combinatorial approaches to the interplay of curvature and topology, to isoperimetry,  geometric analysis, and  intersection theory, to name a few.  In addition, while these subjects are interesting by themselves, they are also designed to  advance classical topics, for example, the diameter of polyhedra (as in the Hirsch conjecture), arrangement theory (and the study of arrangement complements), Hodge theory (as in Grothendieck's standard conjectures), and realization problems of discrete objects (as in Connes embedding problem for type II factors).

This proposal is supported by the review of some already developed tools, such as relative Stanley--Reisner theory (which is equipped to deal with combinatorial isoperimetries), combinatorial Hodge theory (which extends the ``K\""""ahler package'' to purely combinatorial settings), and discrete PDEs (which were used to construct counterexamples to old problems in discrete geometry).""","1337200","2016-12-01","2021-11-30"
"CASTLES","Charge And Spin in TopologicaL Edge States","ERWANN YANN EMILE BOCQUILLON","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Topology provides mathematical tools to sort objects according to global properties regardless of local details, and manifests itself in various fields of physics. In solid-state physics, specific topological properties of the band structure, such as a band inversion, can for example robustly enforce the appearance of spin-polarized conducting states at the boundaries of the material, while its bulk remains insulating. The boundary states of these ‘topological insulators’ in fact provide a support system to encode information non-locally in ‘topological quantum bits’ robust to local perturbations. The emerging ‘topological quantum computation’ is as such an envisioned solution to decoherence problems in the realization of quantum computers. Despite immense theoretical and experimental efforts, the rise of these new materials has however been hampered by strong difficulties to observe robust and clear signatures of their predicted properties such as spin-polarization or perfect conductance.

These challenges strongly motivate my proposal to study two-dimensional topological insulators, and in particular explore the unknown dynamics of their topological edge states in normal and superconducting regimes. First it is possible to capture information both on charge and spin dynamics, and more clearly highlight the basic properties of topological edge states. Second, the dynamics reveals the effects of Coulomb interactions, an unexplored aspect that may explain the fragility of topological edge states. Finally, it enables the manipulation and characterization of quantum states on short time scales, relevant to quantum information processing. This project relies on the powerful toolbox offered by radiofrequency and current-correlations techniques and promises to open a new field of dynamical explorations of topological materials.","1499940","2018-02-01","2023-01-31"
"CAstRA","Comet and Asteroid Re-Shaping through Activity","Jessica AGARWAL","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","The proposed project will significantly improve the insight in the processes that have changed a comet nucleus or asteroid since their formation. These processes typically go along with activity, the observable release of gas and/or dust. Understanding the evolutionary processes of comets and asteroids will allow us to answer the crucial question which aspects of these present-day bodies still provide essential clues to their formation in the protoplanetary disc of the early solar system. 

Ground-breaking progress in understanding these fundamental questions can now be made thanks to the huge and unprecedented data set returned between 2014 and 2016 by the European Space Agency’s Rosetta mission to comet 67P/Churyumov-Gerasimenko, and by recent major advances in the observational study of active asteroids facilitated by the increased availability of sky surveys and follow-on observations with world-class telescopes.

The key aims of this proposal are to
- Obtain a unified quantitative picture of the different erosion processes active in comets and asteroids,
- Investigate how ice is stored in comets and asteroids,
- Characterize the ejected dust (size distribution, optical and thermal properties) and relate it to dust around other stars,
- Understand in which respects comet 67P can be considered as representative of a wider sample of comets or even asteroids.

We will follow a highly multi-disciplinary approach analyzing data from many Rosetta instruments, ground- and space-based telescopes, and connect these through numerical models of the dust dynamics and thermal properties.","1484688","2018-03-01","2023-02-28"
"CAT","Climbing the Asian Water Tower","Wouter Willem Immerzeel","UNIVERSITEIT UTRECHT","The water cycle in the Himalaya is poorly understood because of its extreme topography that results in complex interactions between climate and water stored in snow and glaciers. Hydrological extremes in the greater Himalayas regularly cause great damage, e.g. the Pakistan floods in 2010, while the Himalayas also supply water to over 25% of the global population. So, the stakes are high and an accurate understanding of the Himalayan water cycle is imperative. The discovery of the monumental error on the future of the Himalayan glaciers in the fourth assessment report of the IPCC is exemplary for the scientific misconceptions which are associated to the Himalayan glaciers and its water supplying function. The underlying reason is the huge scale gap that exists between studies for individual glaciers that are not representative of the entire region and hydrological modelling studies that represent the variability in Himalayan climates. In CAT, I will bridge this knowledge gap and explain spatial differences in Himalayan glacio-hydrology at an unprecedented level of detail by combining high-altitude observations, the latest remote sensing technology and state-of-the-art atmospheric and hydrological models. I will generate a high-altitude meteorological observations and will employ drones to monitor glacier dynamics. The data will be used to parameterize key processes in hydro-meteorological models such as cloud resolving mechanisms, glacier dynamics and the ice and snow energy balance. The results will be integrated into atmospheric and glacio-hyrological models for two representative, but contrasting catchments using in combination with the systematic inclusion of the newly developed algorithms. CAT will unambiguously reveal spatial differences in Himalayan glacio-hydrology necessary to project future changes in water availability and extreme events. As such, CAT may provide the scientific base for climate change adaptation policies in this vulnerable region.","1499631","2016-02-01","2021-01-31"
"Cat-In-hAT","Catastrophic Interactions of Binary Stars and the Associated Transients","Ondrej PEJCHA","UNIVERZITA KARLOVA","""One of the crucial formation channels of compact object binaries, including sources of gravitational waves, critically depends on catastrophic binary interactions accompanied by the loss of mass, angular momentum, and energy (""""common envelope"""" evolution - CEE). Despite its importance, CEE is perhaps the least understood major phase of binary star evolution and progress in this area is urgently needed to interpret observations from the new facilities (gravitational wave detectors, time-domain surveys).

Recently, the dynamical phase of the CEE has been associated with a class of transient brightenings exhibiting slow expansion velocities and copious formation of dust and molecules  (red transients - RT). A number of RT features, especially the long timescale of mass loss, challenge the existing CEE paradigm.

Motivated by RT, I will use a new variant of magnetohydrodynamics to comprehensively examine the 3D evolution of CEE from the moment when the mass loss commences to the remnant phase. I expect to resolve the long timescales observed in RT, characterize binary stability in 3D with detailed microphysics, illuminate the fundamental problem of how is orbital energy used to unbind the common envelope in a regime that was inaccessible before, and break new ground on the amplification of magnetic fields during CEE.

I will establish RT as an entirely new probe of the CEE physics by comparing my detailed theoretical predictions of light curves from different viewing angles, spectra, line profiles, and polarimetric signatures with observations of RT.  I will accomplish this  by coupling multi-dimensional moving mesh hydrodynamics with radiation, dust formation, and chemical reactions. Finally, I will examine the physical processes in RT remnants on timescales of years to centuries after the outburst to connect RT with the proposed merger products and to identify them in time-domain surveys.
""","1243219","2019-01-01","2023-12-31"
"CAT4ENSUS","Molecular Catalysts Made of Earth-Abundant Elements for Energy and Sustainability","Xile Hu","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Energy and sustainability are among the biggest challenges humanity faces this century. Catalysis is an indispensable component for many potential solutions, and fundamental research in catalysis is as urgent as ever. Here, we propose to build up an interdisciplinary research program in molecular catalysis to address the challenges of energy and sustainability. There are two specific aims: (I) bio-inspired sulfur-rich metal complexes as efficient and practical electrocatalysts for hydrogen production and CO2 reduction; (II) well-defined Fe complexes of chelating pincer ligands for chemo- and stereoselective organic synthesis. An important feature of the proposed catalysts is that they are made of earth-abundant and readily available elements such as Fe, Co, Ni, S, N, etc.
Design and synthesis of catalysts are the starting point and a key aspect of this project. A major inspiration comes from nature, where metallo-enzymes use readily available metals for fuel production and challenging reactions. Our accumulated knowledge and experience in spectroscopy, electrochemistry, reaction chemistry, mechanism, and catalysis will enable us to thoroughly study the synthetic catalysts and their applications towards the research targets. Furthermore, we will explore research territories such as electrode modification and fabrication, catalyst immobilization and attachment, and asymmetric catalysis.
The proposed research should not only result in new insights and knowledge in catalysis that are relevant to energy and sustainability, but also produce functional, scalable, and economically feasible catalysts for fuel production and organic synthesis. The program can contribute to excellence in European research.","1475712","2011-01-01","2015-12-31"
"CATACOAT","Nanostructured catalyst overcoats for renewable chemical production from biomass","Jeremy Scott LUTERBACHER","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","In the CATACOAT project, we will develop layer-by-layer solution-processed catalyst overcoating methods, which will result in catalysts that have both targeted and broad impacts. We will produce highly active, stable and selective catalysts for the upgrading of lignin – the largest natural source of aromatic chemicals – into commodity chemicals, which will have an important targeted impact. The broader impact of our work will lie in the production of catalytic materials with unprecedented control over the active site architecture.
There is an urgent need to provide these cheap, stable, selective, and highly active catalysts for renewable molecule production. Thanks to its availability and relatively low cost, lignocellulosic biomass is an attractive source of renewable carbon. However, unlike petroleum, biomass-derived molecules are highly oxygenated, and often produced in dilute-aqueous streams. Heterogeneous catalysts – the workhorses of the petrochemical industry – are sensitive to water and contain many metals that easily sinter and leach in liquid-phase conditions. The production of renewable chemicals from biomass, especially valuable aromatics, often requires expensive platinum group metals and suffers from low selectivity.
Catalyst overcoating presents a potential solution to this problem. Recent breakthroughs using catalyst overcoating with atomic layer deposition (ALD) showed that base metal catalysts can be stabilized against sintering and leaching in liquid phase conditions. However, ALD creates dramatic drops in activity due to excessive coverage, and forms an overcoat that cannot be tuned.
Our materials will feature the controlled placement of metal sites (including single atoms), several oxide sites, and even molecular imprints with sub-nanometer precision within highly accessible nanocavities. We anticipate that such materials will create unprecedented opportunities for reducing cost and increasing sustainability in the chemical industry and beyond.","1785195","2017-12-01","2022-11-30"
"CATALIGHT","Exploiting Energy Flow in Plasmonic-Catalytic Colloids","Emiliano CORTÉS","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","The aim of CATALIGHT is to use sunlight as a source of energy in order to trigger chemical reactions by harvesting photons with plasmonic nanoparticles and channelling the energy into catalytic materials. Plasmonic-catalytic devices would allow efficient harvest, transport, and injection of solar energy into molecules. To achieve this, imaging the energy flow at the nanoscale will be crucial for establishing the true potential of plasmonics, both in the context of yielding fundamental knowledge about the light-into-chemical energy conversion processes, and for moving from active towards efficient reactive devices within nanoscale environments.

CATALIGHT has roots in three underlying components, making this project an interwoven effort to break new grounds in a crucial field for the further development of nanoscale energy manipulation: A) Super-resolution imaging of the energy-flow at the nanoscale – with a view to unravel the most efficient mechanisms to guide solar energy into catalytic materials using plasmonic structures as photon harvesters. B) Scaling-up this process through the fabrication of hierarchical photocatalytic colloids – using image-learning for the design of colloidal sources for energy manipulation. C) Light-into-chemical energy conversion – boosting efficiencies in environmental and industrial catalytic processes using tailored photocatalysts. 

The outcomes of this project will not only yield a substantial amount of fundamental knowledge in these crucial areas for the further development of the field, but also provide directly exploitable results for the applied sciences, particularly photocatalysis and fuel cells.","1500000","2019-01-01","2023-12-31"
"CatASus","Cleave and couple: Fully sustainable catalytic conversion of renewable resources to amines","Katalin Barta Weissert","RIJKSUNIVERSITEIT GRONINGEN","Amines are crucially important classes of chemicals, widely present in pharmaceuticals, agrochemicals and surfactants. Yet, surprisingly, a systematic approach to obtaining this essential class of compounds from renewables has not been realized to date. 
The aim of this proposal is to enable chemical pathways for the production of amines through alcohols from renewable resources, preferably lignocellulose waste. Two key scientific challenges will be addressed: The development of efficient cleavage reactions of complex renewable resources by novel heterogeneous catalysts; and finding new homogeneous catalyst based on earth-abundant metals for the atom-economic coupling of the derived alcohol building blocks directly with ammonia as well as possible further functionalization reactions. The program is divided into 3 interrelated but not mutually dependent work packages, each research addressing a key challenge in their respective fields, these are:  
WP1: Lignin conversion to aromatics; WP2: Cellulose-derived platform chemicals to aromatic and aliphatic diols and solvents. WP3: New iron-based homogeneous catalysts for the direct, atom-economic C-O to C-N transformations.
 
The approach taken will embrace the inherent complexity present in the renewable feedstock. A unique balance between cleavage and coupling pathways will allow to access chemical diversity in products that is necessary to achieve economic competitiveness with current fossil fuel-based pathways and will permit rapid conversion to higher value products such as functionalized amines that can enter the chemical supply chain at a much later stage than bulk chemicals derived from petroleum. The proposed high risk-high gain research will push the frontiers of sustainable and green chemistry and reach well beyond state of the art in this area. This universal, flexible and iterative approach is anticipated to give rise to a variety of similar systems targeting diverse product outcomes starting from renewables.","1500000","2016-05-01","2021-04-30"
"CatCHFun","Sustainable Catalytic C-H Bond Functionalization","Lutz Ackermann","GEORG-AUGUST-UNIVERSITAT GOTTINGENSTIFTUNG OFFENTLICHEN RECHTS","The impressive progress in synthetic organic chemistry during the past century has propelled this discipline to its current central place as the key enabling technology in the physical and life sciences. Despite these remarkable advances, our ability to construct molecules of even moderate structural complexity remains unsatisfactory, since these syntheses continue to be inefficient, rely on a high number of reaction steps, and generate undesired, often toxic waste. These features led to the general need for greener transformations that will stimulate the development of more sustainable chemical industries.
Conventional approaches in synthetic organic chemistry make use of starting materials displaying specific functional groups, the installation of which results in costly reaction and purification steps. Therefore, an environmentally-sound and economically-attractive alternative is represented by the direct functionalization of ubiquitous carbon-hydrogen (C–H) bonds. These transition-metal-catalyzed processes avoid prefunctionalization strategies, prevent the formation of undesired waste, and thus enable an overall streamlining of organic synthesis.
While considerable recent progress has been accomplished in C–H bond functionalizations, available methodologies continue to be limited in scope, and key challenges are still to be overcome. Establishing a full set of sustainable C–H bond functionalization protocols will undeniably have a tremendous impact on various applied areas, such as drug discovery, chemical industries or material sciences.","1499338","2012-10-01","2017-09-30"
"CATCIR","Catalytic Carbene Insertion Reactions; Creating Diversity in (Material) Synthesis","De Bruin","UNIVERSITEIT VAN AMSTERDAM","With this proposal the PI capitalises on his recent breakthroughs in transition metal catalysed carbene (migratory) insertion reactions to build up a new research line for controlled catalytic preparation of a variety of new functionalised (co)polymers with expected special material properties. Metallo-carbenes are well-known intermediates in olefin cyclopropanation and olefin metathesis, but the PI recently discovered that their chemistry is far richer. He demonstrated for the first time that metallo-carbenoids can be used in transition metal catalysed insertion polymerisation to arrive at completely new types of stereoregular carbon-chain polymers functionalised at each carbon of the polymer backbone. Rhodium mediated polymerisation of carbenes provides the means to prepare new materials with yet unknown properties. It also provides a valuable alternative to prepare practically identical polymers as in the desirable (but still unachievable) highly stereo-selective (co)polymerisation of functionalised olefins, representing the ‘holey-grail’ in world-wide TM polymerisation catalysis research. The mechanism and scope of this remarkable new discovery will be investigated and new, improved catalysts will be developed for the preparation of novel materials based on homo- and copolymerisation of a variety of carbene precursors. Copolymerisation of carbenes and other reactive monomers will also be investigated and the properties of all new materials will be investigated. In addition the team will try to uncover new reactions in which carbene insertion reactions play a central role. DFT calculations suggest that the transition state (TS) of the new carbene polymerisation reaction is very similar to the TS’s of a variety of carbonyl insertion reactions. Based on this analogy, the team will investigate several new carbene insertion reactions, potentially leading to new, useful polymeric materials and new synthetic routes to prepare small functional organic molecules.","1250000","2008-08-01","2013-07-31"
"CatDT","Categorified Donaldson-Thomas Theory","DAVISON","THE UNIVERSITY OF EDINBURGH","According to string theory, coherent sheaves on three-dimensional Calabi-Yau spaces encode fundamental properties of the universe. On the other hand, they have a purely mathematical definition. We will develop and use the new field of categorified Donaldson-Thomas (DT) theory, which counts these objects. Via the powerful perspective of noncommutative algebraic geometry, this theory has found application in recent years in a wide variety of contexts, far from classical algebraic geometry.

Categorification has proved tremendously powerful across mathematics, for example the entire subject of algebraic topology was started by the categorification of Betti numbers.  The categorification of DT theory leads to the replacement of the numbers of DT theory by vector spaces, of which these numbers are the dimensions. In the area of categorified DT theory we have been able to prove fundamental conjectures upgrading the famous wall crossing formula and integrality conjecture in noncommutative algebraic geometry. The first three projects involve applications of the resulting new subject:

1. Complete the categorification of quantum cluster algebras, proving the strong positivity conjecture.

2. Use cohomological DT theory to prove the outstanding conjectures in the nonabelian Hodge theory of Riemann surfaces, and the subject of Higgs bundles.

3. Prove the comparison conjecture, realising the study of Yangian quantum groups and the geometric representation theory around them as a special case of DT theory.

The final objective involves coming full circle, and applying our recent advances in noncommutative DT theory to the original theory that united string theory with algebraic geometry: 

4. Develop a generalised theory of categorified DT theory extending our results in noncommutative DT theory, proving the integrality conjecture for categories of coherent sheaves on Calabi-Yau 3-folds.","1239435","2017-11-01","2022-10-31"
"CatHet","New Catalytic Asymmetric Strategies for N-Heterocycle Synthesis","John Forwood Bower","UNIVERSITY OF BRISTOL","Medicinal chemistry requires more efficient and diverse methods for the asymmetric synthesis of chiral scaffolds. Over 60% of the world’s top selling small molecule drug compounds are chiral and, of these, approximately 80% are marketed as single enantiomers. There is a compelling correlation between drug candidate “chiral complexity” and the likelihood of progression to the marketplace. Surprisingly, and despite the tremendous advances made in catalysis over the past several decades, the “chiral complexity” of drug discovery libraries has actually decreased, while, at the same time, for the reasons mentioned above, the “chiral complexity” of marketed drugs has increased. Since the mid-1990s, there has been a notable acceleration of this “complexity divergence”. Consequently, there is now an urgent need to provide efficient processes that directly access privileged chiral scaffolds. It is our philosophy that catalysis holds the key here and new processes should be based upon platforms that can exert control over both absolute and relative stereochemistry.  In this proposal we outline the development of a range of N-heteroannulation processes based upon the catalytic generation and trapping of unique or unusual classes of organometallic intermediate derived from transition metal insertion into C-C and C-N sigma-bonds. We will provide a variety of enabling methodologies and demonstrate applicability in flexible total syntheses of important natural product scaffolds. The processes proposed are synthetically flexible, operationally simple and amenable to asymmetric catalysis. Likely starting points, based upon preliminary results, will set the stage for the realisation of aspirational and transformative goals. Through the study of the organometallic intermediates involved here, there is potential to generalise these new catalytic manifolds, such that this research will transcend N heterocyclic chemistry to provide enabling methods for organic chemistry as a whole.","1548738","2015-04-01","2020-03-31"
"CC-MEM","Coordination and Composability: The Keys to Efficient Memory System Design","David BLACK-SCHAFFER","UPPSALA UNIVERSITET","Computer systems today are power limited. As a result, efficiency gains can be translated into performance. Over the past decade we have been so effective at making computation more efficient that we are now at the point where we spend as much energy moving data (from memory to cache to processor) as we do computing the results. And this trend is only becoming worse as we demand more bandwidth for more powerful processors. To improve performance we need to revisit the way we design memory systems from an energy-first perspective, both at the hardware level and by coordinating data movement between hardware and software.

CC-MEM will address memory system efficiency by redesigning low-level hardware and high-level hardware/software integration for energy efficiency. The key novelty is in developing a framework for creating efficient memory systems. This framework will enable researchers and designers to compose solutions to different memory system problems (through a shared exchange of metadata) and coordinate them towards high-level system efficiency goals (through a shared policy framework). Central to this framework is a bilateral exchange of metadata and policy between hardware and software components. This novel communication will open new challenges and opportunities for fine-grained optimizations, system-level efficiency metrics, and more effective divisions of responsibility between hardware and software components. 

CC-MEM will change how researchers and designers approach memory system design from today’s ad hoc development of local solutions to one wherein disparate components can be integrated (composed) and driven (coordinated) by system-level metrics. As a result, we will be able to more intelligently manage data, leading to dramatically lower memory system energy and increased performance, and open new possibilities for hardware and software optimizations.","1610000","2017-03-01","2022-02-28"
"CC4SOL","Towards chemical accuracy in computational materials science","Andreas GRÜNEIS","TECHNISCHE UNIVERSITAET WIEN","This project aims at the development of a novel toolbox of ab-initio methods that approximate the true many-electron wavefunction using systematically improvable perturbation and coupled-cluster theories. The demand and prospects for these methods are excellent given that the highly-accurate coupled-cluster theories can predict atomization- and reaction energies in a wide range of solids and molecules with chemical accuracy (≈43 meV). However, the computational cost involved inhibits their widespread use in the field of materials science so far. A multitude of suggested developments in the present proposal hold the promise to reduce the computational cost beyond what is currently considered possible by the community. These include explicit correlation methods that augment the conventional wavefunction expansion with terms that depend on the electron pair correlation factors. In contrast to the widely-used homogeneous correlation factors, this proposal aims at the investigation of inhomogeneous correlation factors that can also capture van der Waals interactions. Furthermore this proposal seeks to employ a recently developed combination of atom-centered basis functions and plane wave basis sets, maximizing the compactness in the wavefunction expansion. The combination of these ideas bears the potential to reduce the computational cost of coupled-cluster calculations in solids by three orders of magnitude, leading to a breakthrough in the field of highly-accurate ab-initio simulations. As such the study of challenging solid state physics and chemistry problems forms an important part of this proposal. We seek to investigate molecular adsorption and reactions in zeolites and on surfaces, pressure-driven solid-solid phase transitions of two dimensional layered materials and defects in solids. These problems are paradigmatic for van der Waals interactions and strong correlation, and methods that describe their electronic structure accurately are highly sought after.","1460826","2017-07-01","2022-06-30"
"CCCAN","Characterizing and Controlling Carbon Nanomaterials","Janina Maultzsch","TECHNISCHE UNIVERSITAT BERLIN","The aim of this project is to understand and control the fundamental physical properties of novel carbon nanomaterials:
carbon nanotubes and graphene. By a combination of complementary methods, i.e. vibrational spectroscopy, scanning probe microscopy, and theoretical modelling, a comprehensive understanding of the electronic, vibrational, optical properties, and their connection with the material’s structure will be obtained. A diagnostics “toolbox” will be established on the materials in
their most unperturbed, ideal states. Taking the results as reference, the materials will be studied under conditions relevant when incorporated into devices. These include imperfections of the materials and interaction with different environments, with other carbon nanotubes/graphene, and with extrinsic materials introduced during device processing. The gained insight and understanding on a fundamental level will also advance technological routes for scaling up carbon-nanomaterial electronic device fabrication, which is still lacking sufficient control over selectivity towards the desired physical properties. Control over the electronic and optical properties will be sought through deliberately induced interactions and chemical functionalization
of the materials. The project benefits from close collaborations between experimental and theoretical physics, chemistry, and materials science.","1468960","2010-12-01","2015-11-30"
"CCMP","Physics Of Magma Propagation and Emplacement: a multi-methodological Investigation","Eleonora Rivalta","HELMHOLTZ ZENTRUM POTSDAM DEUTSCHESGEOFORSCHUNGSZENTRUM GFZ","Dikes and sills are large sheet-like intrusions transporting and storing magma in the Earth’s crust.
When propagating, they generate seismicity and deformation and may lead to volcanic eruption. The physics of magma-filled structures is similar to that of any fluid-filled reservoir, such as oil fields and CO2 reservoirs created by sequestration. This project aims to address old and new unresolved challenging questions related to dike propagation, sill emplacement and in general to the dynamics of fluid and gas-filled reservoirs. I propose to focus on crustal deformation, induced seismicity and external stress fields to study the signals dikes
and sills produce, how they grow and why they reactivate after years of non-detected activity. I will combine experimental, numerical and analytical techniques, in close cooperation with volcano observatories providing us with the data necessary to validate our models. In the lab, I will simulate magma propagation injecting fluid into solidified gelatin. I will also contribute to a project, currently under evaluation, on the monitoring of a CO2
sequestration site. At the same time, I will address theoretical aspects, extending static models to dynamic cases and eventually developing a comprehensive picture of the multi faceted interaction between external stress field,
magma and rock properties, crustal deformation and seismicity. I also plan, besides presenting my team’s work in the major national and international geophysical conferences, to produce, with technical support from the media services of DKRZ (Deutsches Klimarechenzentrum), an audiovisual teaching DVD illustrating scientific advances and unresolved issues in magma dynamics, in the prediction of eruptive activity and in the physics of reservoirs.","1507679","2010-07-01","2015-06-30"
"CCOSA","Classes of combinatorial objects: from structure to algorithms","Daniel Kral","THE UNIVERSITY OF WARWICK","The proposed project aims at analyzing fundamental problems from combinatorics using the most current methods available and at providing new structural and algorithmic insights to such problems. The problems considered will be treated on a general level of classes of combinatorial objects of the same kind and the developed general methods will also be applied to specific open problems. Classes of dense and sparse objects will be treated using different techniques. Dense combinatorial objects appear in extremal combinatorics and tools developed to handle them found their applications in different
areas of mathematics and computer science. The project will focus on extending known methods to new classes of combinatorial objects, in particular those from algebra, and applying the most current techniques including Razborov flag algebras to problems from extremal combinatorics. Applications of the obtained results in property testing will also be considered. On the other hand, algorithmic applications often include manipulating with sparse objects. Examples of sparse objects are graphs embeddable in a fixed surface and more general minor-closed classes of graphs. The project objectives include providing new structural results and algorithmic metatheorems for classes of sparse objects using both classical tools based on the theory of graph minors as well as new tools based on the framework of classes of nowhere-dense structures.","849000","2010-12-01","2015-11-30"
"CDMAN","Control of Spatially Distributed Complex Multi-Agent Networks","Ming Cao","RIJKSUNIVERSITEIT GRONINGEN","""Spatially distributed multi-agent networks have been used successfully to model a wide range of natural, social and engineered complex systems, such as animal groups, online communities and electric power grids. In various contexts, it is crucial to introduce control actions into such networks to either achieve desired collective dynamics or test the understanding of the systems’ behavior. However, controlling such systems is extremely challenging due to agents’ complicated sensing, communication and control interactions that are distributed in space. Systematic methodologies to attack this challenge are in urgent need, especially when vast efforts are being made in multiple disciplines to apply the model of complex multi-agent networks.
The goal of the project is twofold. First, understand whether a complex multi-agent network can be controlled effectively when the agents can only sense and communicate locally. Second, provide methodologies to implement distributed control in typical spatially distributed complex multi-agent networks. The project requires integrated skills since both rigorous theoretical analysis and novel empirical explorations are necessary.
The research methods that I plan to adopt have two distinguishing features. First, I use tools from algebraic graph theory and complex network theory to investigate the impact of network topologies on the systems’ controller performances characterized by mathematical control theory. Second, I utilize a homemade robotic-fish testbed to implement various multi-agent control algorithms. The unique combination of theoretical and empirical studies is expected to lead to breakthroughs in developing an integrated set of principles and techniques to control effectively spatially distributed multi-agent networks. The expected results will make original contributions to control engineering and robotics, and inspire innovative research methods in theoretical biology and theoretical sociology.""","1495444","2013-01-01","2017-12-31"
"CDSIF","Contour dynamics and singularities in incompressible flows","Diego Cordoba","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","The search of singularities in incompressible flows has become a major challenge in the area of non-linear partial differential equations and is relevant in applied mathematics, physics and engineering. The existence of such singularities would have important consequences for the understanding of turbulence. One way to make progress in this direction, is to study plausible scenarios for the singularities supported by experiments or numerical analysis. With the more sophisticated numerical tools now available, the subject has recently gained considerable momentum. The main goal of this project is to study analytically several incompressible fluid models. In particular solutions that involve the possible formation of singularities or quasi-singular structures.","650000","2008-09-01","2013-08-31"
"CEESC","Control of entangled electron spins on a chip","Lieven Mark Koenraad Vandersypen","TECHNISCHE UNIVERSITEIT DELFT","The promise of nanoscience stems from the fundamentally new behavior that emerges at the nanoscale. Here, we propose to explore, control and exploit one of the most dramatic aspects of this unusual behavior: quantum entanglement of spins. Our nanoscale system of choice is an array of semiconductor quantum dots that each contain one single electron. Thanks to a string of recent breakthroughs, it is now possible to initialize, coherently manipulate and read out the spin state of one such electron, and to couple it coherently to a spin in a neighboring dot. Today, we are at the brink of a new era in this field, in which entanglement will play the central part. The primary goal of this proposal, therefore, is to experimentally demonstrate that electron spins in quantum dots can really be entangled, and to control this entanglement in time. We will then use this capability to implement various quantum information protocols such as quantum algorithms and teleportation, which intrinsically rely on entanglement to realize tasks that are classically impossible. In order to push the level of coherent control to its limits, we will suppress fluctuations in the normally uncontrolled spin environment, and pursue novel quantum dot technologies which offer an intrinsically ‘quiet’ environment. Our long-term dream is to demonstrate that the accuracy threshold for fault-tolerant quantum computation can be reached in this system, which would permit quantum coherence and entanglement to be preserved indefinitely. This research is presently very much at the stage of exploratory research and is bound to produce surprising and unexpected outcomes. Furthermore, we are convinced that pushing the frontier of quantum control in nanoscale devices has a real potential to lead to future quantum technologies.","1296000","2008-07-01","2013-06-30"
"CELL HYBRIDGE","3D Scaffolds as a Stem Cell Delivery System for Musculoskeletal Regenerative Medicine","Lorenzo Moroni","UNIVERSITEIT MAASTRICHT","Aging worldwide population demands new solutions to permanently restore damaged tissues, thus reducing healthcare costs. Regenerative medicine offers alternative therapies for tissue repair. Although first clinical trials revealed excellent initial response after implantation of these engineered tissues, long-term follow-ups demonstrated that degeneration and lack of integration with the surrounding tissues occur. Causes are related to insufficient cell-material interactions and loss of cell potency when cultured in two-dimensional substrates, among others.
Stem cells are a promising alternative due to their differentiation potential into multiple lineages. Yet, better control over cell-material interactions is necessary to maintain tissue engineered constructs in time. It is crucial to control stem cell quiescence, proliferation and differentiation in three-dimensional scaffolds while maintaining cells viable in situ. Stem cell activity is controlled by a complex cascade of signals called “niche”, where the extra-cellular matrix (ECM) surrounding the cells play a major role. Designing scaffolds inspired by this cellular niche and its ECM may lead to engineered tissues with instructive properties characterized by enhanced homeostasis, stability and integration with the surrounding milieu.
This research proposal aims at engineering constructs where scaffolds work as stem cell delivery systems actively controlling cell quiescence, proliferation, and differentiation. This challenge will be approached through a biomimetic design inspired by the mesenchymal stem cell niche. Three different scaffolds will be combined to achieve this purpose: (i) a scaffold designed to maintain cell quiescence; (ii) a scaffold designed to promote cell proliferation; and (iii) a scaffold designed to control cell differentiation. To prove the design criteria the evaluation of stem cell quiescence, proliferation, and differentiation will be assessed for musculoskeletal regenerative therapies.","1500000","2015-05-01","2020-04-30"
"CellInspired","Mechanotransduction mediating cell adhesion - towards cell-inspired adaptive materials","Christine Johanna Maria Selhuber-Unkel","CHRISTIAN-ALBRECHTS-UNIVERSITAET  ZU KIEL","Adhesion is a key event for eukaryotic cells to establish contact with the extracellular matrix and other cells. It allows cells to quickly adapt to mechanical changes in their environment by either adhesion reinforcement or release. Understanding and mimicking the interplay between adhesion reinforcement and release could result in novel cell-inspired adaptive materials. In order to ultimately be able to transfer functional principles of cell adhesion to a next generation of biomimetic materials, we will elucidate the biophysics of cell adhesion in response to external force. We have already obtained important results that have provided new insights into cell adhesion. For example, we have found that the nanoscale spacing of adhesion sites controls cell adhesion reinforcement. With the project proposed here I want to advance our understanding of cell adhesion by generating a comprehensive model of mechanotransduction-mediated cell adhesion. Therefore, my group will develop new force measurement methods based on atomic force microscopy and 2D force sensor arrays that allow for a systematic investigation of key parameters in the cell adhesion system, including the concept of cellular mechanosensing. My hypothesis is that there is a transition between adhesion reinforcement and release as a function of external mechanical stress, stress history, and the biofunctionalization of the adhesive surface. Transferring our biophysical knowledge into materials science promises new materials with a dynamic adaptive mechanical and adhesion response. This transfer of biological concepts into cell-inspired materials will follow the construction principles of cells: the proposed material will be based on polymer fibers that are reversibly cross-linked and reinforce adhesion upon mechanical stress. The ultimate goal of the proposed project is to develop an intelligent polymer material with an adaptive adhesive and mechanical response similar to that found in living cells.","1467483","2013-09-01","2018-08-31"
"CEMOS","Crystal Engineering for Molecular Organic Semiconductors","Kevin Sivula","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","""The urgent need to develop inexpensive and ubiquitous solar energy conversion cannot be overstated. Solution processed organic semiconductors can enable this goal as they support drastically less expensive fabrication techniques compared to traditional semiconductors. Molecular organic semiconductors (MOSs) offer many advantages to their more-common pi-conjugated polymer counterparts, however a clear and fundamental challenge to enable the goal of high performance solution-processable molecular organic semiconductor devices is to develop the ability to control the crystal packing, crystalline domain size, and mixing ability (for multicomponent blends) in the thin-film device geometry. The CEMOS project will accomplish this by pioneering innovative methods of “bottom-up” crystal engineering for organic semiconductors. We will employ specifically tailored molecules designed to leverage both thermodynamic and kinetic aspects of molecular organic semiconductor systems to direct and control crystalline packing, promote crystallite nucleation, compatibilize disparate phases, and plasticize inelastic materials. We will demonstrate that our new classes of materials can enable the tuning of the charge carrier transport and morphology in MOS thin films, and we will evaluate their performance in actual thin-film transistor (TFT) and organic photovoltaic (OPV) devices. Our highly interdisciplinary approach, combining material synthesis and device fabrication/evaluation, will not only lead to improvements in the performance and stability of OPVs and TFTs but will also give deep insights into how the crystalline packing—independent from the molecular structure—affects the optoelectronic properties. The success of CEMOS will rapidly advance the performance of MOS devices by enabling reproducible and tuneable performance comparable to traditional semiconductors—but at radically lower processing costs.""","1477472","2014-01-01","2018-12-31"
"CENNS","Probing new physics with Coherent Elastic Neutrino-Nucleus Scattering and a tabletop experiment","Julien Billard","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Ever since the Higgs boson was discovered at the LHC in 2012, we had the confirmation that the Standard Model (SM) of particle physics has to be extended. In parallel, the long lasting Dark Matter (DM) problem, supported by a wealth of evidence ranging from precision cosmology to local astrophysical observations, has been suggesting that new particles should exist. Unfortunately, neither the LHC nor the DM dedicated experiments have significantly detected any exotic signals pointing toward a particular new physics extension of the SM so far.

With this proposal, I want to take a new path in the quest of new physics searches by providing the first high-precision measurement of the neutral current Coherent Elastic Neutrino-Nucleus Scattering (CENNS). By focusing on the sub-100 eV CENNS induced nuclear recoils, my goal is to reach unprecedented sensitivities to various exotic physics scenarios with major implications from cosmology to particle physics, beyond the reach of existing particle physics experiments. These include for instance the existence of sterile neutrinos and of new mediators, that could be related to the DM problem, and the possibility of Non Standard Interactions that would have tremendous implications on the global neutrino physics program. 

To this end, I propose to build a kg-scale cryogenic tabletop neutrino experiment with outstanding sensitivity to low-energy nuclear recoils, called CryoCube, that will be deployed at an optimal nuclear reactor site. The key feature of this proposed detector technology is to combine two target materials: Ge-semiconductor and Zn-superconducting metal. I want to push these two detector techniques beyond the state-of-the-art performance to reach sub-100 eV energy thresholds with unparalleled background rejection capabilities. 

As my proposed CryoCube detector will reach a 5-sigma level CENNS detection significance in a single day, it will be uniquely positioned to probe new physics extensions beyond the SM.","1495000","2019-02-01","2024-01-31"
"CFT-MAP","Charting the space of Conformal Field Theories: a combined nuMerical and Analytical aPproach","Alessandro VICHI","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Conformal Field Theory (CFT) was originally conceived in four and three dimensions, with applications to particle physics and critical phenomena in mind. However, it is in two dimensions that the most spectacular results  have been obtained. In higher dimensions, there used to be a general feeling that the constraining power of conformal symmetry by itself is insufficient to tell nontrivial things about the dynamics. Hence the interest in various additional assumptions. This is not fully satisfactory, since there are likely many CFTs that do not fulfill any of them.
The main focus of this proposal is to take a fresh look at the idea that the mathematical structure of CFTs is instead such a strong constraint that it can allow for a complete solution of the theory. This program, known as conformal bootstrap, has provided a new element in the quantum field theory toolbox to describe genuine non-perturbative cases.
This project aims to explore new directions and push forward the frontiers of conformal filed theories, with the ultimate objective of a detailed classification and understanding of scale invariant systems and their properties. 
CFT-MAP will develop more efficient numerical techniques and complementary analytical tools making use of two main methods: by studying correlation functions of operators present in any quantum field theory, such as global symmetry conserved currents and the energy momentum tensor; by inspecting the analytical structure of correlation functions.
The project will scan the landscape of CFTs, identifying where and how they exist. By significantly improving over the methods at disposal, this proposal will be able to study theories currently are out of reach.
Besides the innovative methodologies, a fundamental outcome of CFT-MAP will be a word record determination of critical exponents in second phase transition,  together with additional information that allows an approximate reconstruction of the QFT in the neighborhood of fixed points.","1500000","2018-03-01","2023-02-28"
"CGinsideNP","Complexity Inside NP - A Computational Geometry Perspective","Wolfgang MULZER","FREIE UNIVERSITAET BERLIN","Traditional complexity theory focuses on the dichotomy between P and NP-hard 
problems. Lately, it has become increasingly clear that this misses a major part 
of the picture. Results by the PI and others offer glimpses on a fascinating structure 
hiding inside NP: new computational problems that seem to lie between polynomial 
and NP-hard have been identified; new conditional lower bounds for problems with 
large polynomial running times have been found; long-held beliefs on the difficulty 
of problems in P have been overturned. Computational geometry plays a major role 
in these developments, providing some of the main questions and concepts.

We propose to explore this fascinating landscape inside NP from the perspective 
of computational geometry, guided by three complementary questions:

(A) What can we say about the complexity of search problems derived from
existence theorems in discrete geometry? These problems offer a new 
perspective on complexity classes previously studied in algorithmic game 
theory (PPAD, PLS, CLS). Preliminary work indicates that they have the 
potential to answer long-standing open questions on these classes.

(B) Can we provide meaningful conditional lower bounds on geometric 
problems for which we have only algorithms with large polynomial running 
time? Prompted by a question raised by the PI and collaborators, such lower 
bounds were developed for the Frechet distance. Are similar results possible 
for problems not related to distance measures? If so, this could dramatically
extend the traditional theory based on 3SUM-hardness to a much more 
diverse and nuanced picture.

(C) Can we find subquadratic decision trees and faster algorithms for 
3SUM-hard problems? After recent results by Pettie and Gronlund on 
3SUM and by the PI and collaborators on the Frechet distance, we 
have the potential to gain new insights on this large class of well-studied 
problems and to improve long-standing complexity bounds for them.","1486800","2018-02-01","2023-01-31"
"CGR2011TPS","Challenging General Relativity","Thomas Sotiriou","THE UNIVERSITY OF NOTTINGHAM","General relativity, Einstein's celebrated theory, has been very successful as a theory of the gravitational interaction. However, within the course of the last decades several issues have been pointed out as indicating its limitations: the inevitable existence of spacetime singularities and  the fact that it is not a renormalizable theory manifest as shortcomings at very small scales. The inability of the theory to explain the late time accelerated expansion of the universe or the rotational curves of galaxies without the need of unobserved, mysterious forms of matter/energy can be interpreted as shortcomings at large scales. These riddles make gravity by far the most enigmatic of interactions nowadays. Therefore, the understanding of gravity beyond general relativity seems to be more pertinent than ever.

We propose to address this difficult issue by considering a synthetic approach towards the understand of the limitations of general relativity and the study of phenomenology which is usually considered to be outsides its realm. The proposed directions include, but are not limited to: the study of quantum gravity candidates and their phenomenology; extensions or modifications of general relativity which may address renormalizability issues or cosmological observations; explorations of fundamental principles of general relativity and the possible violation of such principles; the study of the implications of deviations from Einstein's theory for astrophysics and cosmology and the possible ways to constrain such deviations; and the study of effects within the framework of general relativity which lie at the limit of its validity as a gravity theory. The deeper understanding of each of these issues will provide an important piece to the puzzle. The synthesis of this pieces is most likely to significantly aid our understanding of gravity, and this is our ultimate goal.","1375226","2012-08-01","2018-01-31"
"CHAOS-PIQUANT","Universality and chaos in PT-symmetric quantum systems","Eva-Maria GRAEFE","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","The world of our daily experiences, described by classical physics, is built out of fundamental particles, governed by the laws of quantum mechanics. The striking difference between quantum and classical behaviour becomes most apparent in the realm of chaos, an extreme sensitivity to initial conditions, which is common in classical systems but impossible under quantum laws. The investigation of characteristic features of quantum systems whose classical counterparts are chaotic has illuminated foundational problems and led to a variety of technological applications. Traditional quantum theory focuses on the description of closed systems without losses. Every realistic system, however, contains unwanted losses and dissipation, but the idea to engineer them to generate desirable effects has recently come into the focus of scientific attention. The surprising properties of quantum systems with balanced gain and loss (PT-symmetric systems) have sparked much interest. The first experiments on PT-symmetry in optics have been identified as one of the top ten physics discoveries of the past decade in Nature Physics. New experimental areas are rapidly emerging. Our understanding of PT-symmetric quantum systems, however, is still limited. One major shortcoming is that the emergence of chaos and universality in these systems is hitherto nearly unexplored. I propose to investigate PT-symmetric quantum chaos to establish this new research area and overturn some common perceptions in the existing fields of PT-symmetry and quantum chaos. Ultimately this will lead to new experimental applications and quantum technologies. Building on recent conceptual breakthroughs I have made, I will a) identify spectral and dynamical features of chaos in PT-symmetric quantum systems, b) establish new universality classes, c) provide powerful semiclassical tools for the simulation of generic quantum systems, and d) facilitate experimental applications in microwave cavities and cold atoms.","1293023","2018-02-01","2023-01-31"
"CHAOSNETS","""Building Scalable, Secure, and Reliable """"Chaotic"""" Wireless Networks""","Kyle Andrew Stuart Jamieson","UNIVERSITY COLLEGE LONDON","As a result of their unplanned, license-free nature, WiFi networks have grown quickly in recent years, giving users unprecedented improvements in wireless access to the Internet. But being “chaotic,” i.e. unplanned, they have grown to be victims of their own success: when eager users set up too many wireless access points in a densely-populated area, the resulting noise and interference hurt everyones throughput and connectivity. Cellular mobile telephone networks are planned carefully, but in order to expand coverage indoors, providers are turning to customer-deployed femtocells, thus incuring the drawbacks of chaotic WiFi networks. We propose a ground-up redesign of chaotic wireless networks, with new architectural contributions focusing on what information the physical layer should pass up to higher layers. We propose a new physical layer interface called SoftAoA that passes angle-of-arrival (AoA) information from the physical layer up to higher layers. Using this expanded physical layer interface, we will first investigate fountain coding and receiver-based rate adaptation methods to improve wireless capacity in the vagaries of the “grey zone” of marginal coverage. Second, we will investigate improvements to security and localization that can be made based on the profiling of incoming packets’ AoA at an access point. Finally, we will investigate how a chaotically-deployed network can mitigate the interference it experiences from networks not under the same administrative control, and manage the interference it causes to those networks. The result will be more scalable, secure, and reliable chaotic wireless networks that play an even more prominent role in our lives.","1457675","2011-11-01","2016-10-31"
"CHAPARDYN","Chaos in Parabolic Dynamics: Mixing, Rigidity, Spectra","Corinna Ulcigrai","UNIVERSITY OF BRISTOL","""The theme of the proposal is the mathematical investigation of chaos (in particular ergodic and spectral properties) in parabolic dynamics, via analytic, geometric and probabilistic techniques. Parabolic dynamical systems are mathematical models of the many phenomena which display a """"slow"""" form of chaotic evolution, in the sense that nearby trajectories diverge polynomially in time.  In contrast with the hyperbolic case and with the elliptic case, there is no general theory which describes parabolic dynamical systems. Only few classical examples are well understood.

The research plan aims at bridging this gap, by studying new classes of parabolic systems and unexplored properties of classical ones. More precisely, I propose to study parabolic flows beyond the algebraic set-up and infinite measure-preserving parabolic systems, both of which are very virgin fields of research, and to attack open conjectures and questions on fine chaotic properties, such as spectra and rigidity, for area-preserving flows.  Moreover, connections between parabolic dynamics and respectively number theory, mathematical physics and probability will be explored. g New techniques, stemming from some recent breakthroughs in Teichmueller dynamics, spectral theory and infinite ergodic theory, will be developed.

The proposed research will bring our knowledge significantly beyond the current state-of-the art, both in breadth and depth and will identify common features and mechanisms for chaos in parabolic systems. Understanding similar features and common geometric mechanisms responsible for mixing, rigidity and spectral properties of parabolic systems will provide important insight towards an universal theory of parabolic dynamics.""","1193534","2014-01-01","2019-08-31"
"CHASM","Convective Heat Transport and Stellar Magnetism","Matthew Keith Morris Browning","THE UNIVERSITY OF EXETER","""Magnetism plays a profound role in stars and planets. In the Sun, magnetic fields are ultimately responsible for solar flares and coronal mass ejections that can impact our technological society. Earth's own magnetic field partly shields us from these events, but solar storms can still interrupt satellite communications, disrupt power grids, and pose a danger to astronauts on spacewalks.  More generally, magnetic fields partly control the rotational evolution of stars, likely impact the habitability of extrasolar planets, and may modify the sizes and internal structures of
low-mass stars and gaseous planets.  In all cases, the magnetism is generally thought to arise from a convective dynamo -- but a detailed theoretical understanding of this process, and its influence on the overall evolution of stars and planets, has remained elusive.  Particularly fascinating observational puzzles have recently come from the study of low-mass M-dwarf stars: the most numerous type of stars in our galaxy and perhaps the most likely to host habitable planets.

We therefore propose to study how stars and sub-stellar objects build magnetic fields using 3-D magnetohydrodynamic simulations, and to quantify the effects of those fields on stellar structure and evolution.  Using the Anelastic Spherical Harmonic (ASH) and Compressible Spherical Segment (CSS) codes, we will examine (a) how global magnetic field generation in these stars depends upon parameters like stellar mass, rotation rate, and the presence of a stable core, and (b) how the deep convection and magnetism imprints through (and is shaped by) the near-surface layers of these objects.  We will (c) determine the impact of the resulting fields on the convective transport of heat and angular momentum, incorporate our results into state of the art 1-D evolutionary models of stars, and explore the consequences for stellar evolution. Separately, we will (d) develop and maintain a public database of 3-D convective dynamo models.""","1469070","2013-12-01","2018-11-30"
"chem-fs-MOF","Chemical Engineering of Functional Stable Metal-Organic Frameworks: Porous Crystals and Thin Film Devices","Carlos MARTI-GASTALDO","UNIVERSITAT DE VALENCIA","Metal-Organic-Frameworks (MOFs) offer appealing advantages over classical solids from combination of high surface areas with the crystallinity of inorganic materials and the synthetic versatility (unlimited combination of metals and linkers for fine tuning of properties) and processability of organic materials. Provided chemical stability, I expect combination of porosity with manipulable electrical and optical properties to open a new world of possibilities, with MOFs playing an emerging role in fields of key environmental value like photovoltaics, photocatalysis or electrocatalysis. The conventional insulating character of MOFs and their poor chemical stability (only a minimum fraction are hydrolytically stable) are arguably the two key limitations hindering further development in this context.
With chem-fs-MOF I expect to deliver: 
1. New synthetic routes specifically designed for producing new, hydrolytically stable Fe(III) and Ti(IV)-MOFs (new synthetic platforms for new materials).
2. More advanced crystalline materials to feature tunable function by chemical manipulation of MOF’s optical/electrical properties and pore activity (function-led chemical engineering).
3. High-quality ultrathin films, reliant on the transfer of single-layers, alongside establishing the techniques required for evaluating their electric properties (key to device integration). Recent works on graphene and layered dichalcogenides anticipate the benefits of nanostructuration for more efficient optoelectronic devices. Notwithstanding great potential, this possibility remains still unexplored for MOFs.
Overall, I seek to exploit MOFs’ unparalleled chemical/structural flexibility to produce advanced crystalline materials that combine hydrolytical stability and tunable performance to be used in environmentally relevant applications like visible light photocatalysis. This is an emerging research front that holds great potential for influencing future R&D in Chemistry and Materials Science.","1527351","2017-01-01","2021-12-31"
"CHEMBIOLPBINT","Chemical biology of natural products in plant-bacteria interactions","Markus Kaiser","UNIVERSITAET DUISBURG-ESSEN","This project deals with the elucidation of the biological role of natural products in plant-bacteria interactions. Plant-associated bacteria synthesize a vast number of biologically active natural products that modulate the physiology and functioning of their host plants. For example, plant pathogens often cause devastating crop losses by secreting low molecular weight
phytotoxins, while some symbiotic bacteria biosynthesize plant-protecting compounds that assist in lowering biotic and abiotic plant stresses. It is therefore surprising that although natural products seem to play key roles in the complex interaction network between bacteria and plants, most of their biological functions and molecular targets are still unknown.
To date, almost all studies on plant-bacteria interactions have been performed with  biological  approaches. Here, we propose to investigate the biological role of plant-associated natural products with the aid of a chemistry-driven approach, relying on the power of chemical synthesis to i) generate these natural products and/or suitable natural product derivatives, ii) to elucidate their targets in plants, and iii) to apply them in plant-bacteria studies. Although natural products have long been in the focus of chemical research, such a systematic chemistry-driven approach has, to our knowledge, never been performed before in plant-bacteria interactions. Our project will therefore not only serve to i) decipher basic research questions and ii) identify potential lead structures for agricultural and medicinal applications, but will also contribute to iii) the refinement of chemical syntheses strategies, iv) the advancement of target finding approaches and v) the establishment of chemical biology approaches in plant biology.","1490900","2011-03-01","2016-02-29"
"CHEMBIOMECH","Exploring mechanism in chemical biology by high-throughput approaches","Florian Hollfelder","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","In the biomedical sciences, where endless combinatorial diversity of genes, proteins and synthetic molecules is involved, miniaturisation has not simply allowed an increase in the speed at which experiment can be performed: it has given birth to new areas such as combinatorial chemistry and biology, proteomics, genomics, and more recently, systems and synthetic biology. In all these areas, the synthesis, assay and analysis of large molecular ensembles has become the essence of experimental progress. However, it is the systematic analysis of the enormous amounts of data generated that will ultimately lead to an understanding of fundamental chemical and biological problems. This proposal deals with approaches in which libraries of molecules are employed to give such mechanistic insight – into how enzyme catalysis is brought about in proteins and polymeric enzyme models and into the molecular recognition and cell biology of drug delivery reagents. In each case considerable technical challenges are involved in the way diversity is brought about and probed: ranging from either using the tools of synthetic chemistry to using gene repertoires in emulsion microdroplet reactors with femtolitre volumes, handled in microfluidic devices.","563848","2008-09-01","2013-08-31"
"CHEMCOMP","Building-up Chemical Complexity 
into Multifunctional Molecule-based Hybrid Materials","Jose Ramon Galan Mascaros","FUNDACIO PRIVADA INSTITUT CATALA D'INVESTIGACIO QUIMICA","Molecular sciences offer unparalleled opportunities for the development of tailor-made materials. By chemical design, molecules with the desired features can be prepared and incorporated into hybrid systems to yield molecule-based materials with novel chemical and/or physical properties. The CHEMCOMP project aims to develop new hybrid materials targeting the study of new physical phenomena that have already been theoretically predicted or experimentally hinted. The main goals will be:
i) Molecules with memory: Memory effect at the molecular scale is of great interest because it represents the size limit in the miniaturization of information storage media. My goal will be to develop spin crossover molecules with bulk-like hysteretic behavior where the switching between the low spin ground state and the high spin metastable state can be controlled through external stimuli.
ii) Bistable organic conductors: Bistable molecules could also be embedded into hybrid organic conductors to induce structural phase transitions. This strategy will allow for the transport properties to be controlled through external stimuli in unprecedented switchable conducting media.
iii) Hybrid conducting magnets: Combination of magnetism and electrical conductivity has given rise to new phenomena in the past, such as spin glass behavior or giant magnetoresistance. We propose to incorporate Single Molecule Magnets (molecules with magnet-like behavior) into organic (super)conductors to understand and optimize the synergy between these two physical properties.
iv) Chiral magnets and conductors: New phenomena is expected to appear in optically active media. Experimental evidence for the so-called MagnetoChiral Dichroism has already been found. Electrical Magnetochiral Anisotropy has been predicted. I will develop systematic strategies for the preparation of hybrid chiral materials to understand and optimize the synergy between chirality and bulk physical properties.","1940396","2012-01-01","2016-12-31"
"ChemEpigen","The chemical understanding of biomolecular recognition in epigenetics","Jasmin MECINOVIC","SYDDANSK UNIVERSITET","The ultimate aim of this ERC project is to provide a comprehensive and complete understanding, at the atomic-level of sophistication, of genuinely important biomolecular recognition processes in epigenetics that play key roles in human health and disease. At the biochemical level, epigenetics refers to mechanisms, such as enzymatic modifications of DNA and posttranslational modifications of the associated histone proteins, that regulate the activity of human genes. The proposed work aims to address epigenetics using the physical-organic chemistry approach that enables the elucidation of the elemental processes with unprecedented molecular/atomic detail. The project will experimentally and computationally examine non-covalent interactions between three essential constituents of the epigenetic biomolecular system, namely epigenetic proteins, histones and water, at the level of short histone peptides, intact histone proteins, the nucleosome assembly and nucleosome arrays. Our programme, built on synergistic thermodynamic, structural and computational studies, aims to unravel i) the underlying chemical origin of methyllysine-containing histones in epigenetics, ii) the chemical basis for the recognition of methylarginine-containing histones in epigenetic processes, and iii) the role of unstructured histone tails in biomolecular recognition, which together form the three main structural elements found in the epigenetic framework. Results from this work will be important from both a fundamental molecular perspective as well as from the biomedical perspective, because proteins involved in epigenetic regulation processes are currently regarded as important targets for numerous therapeutic interventions, most notably for cancer treatment.","1500000","2017-04-01","2022-03-31"
"CHEMHEAT","Chemical Control of Heating and Cooling in Molecular Junctions: Optimizing Function and Stability","Gemma Solomon","KOBENHAVNS UNIVERSITET","Nanoscale systems binding single molecules, or small numbers of molecules, in conducting junctions show considerable promise for a range of technological applications, from photovoltaics to rectifiers to sensors. These environments differ significantly from the traditional domain of chemical studies involving molecules in solution and the gas phase, necessitating renewed efforts to understand the physical properties of these systems. The objective of this proposal concerns one particular class of physical processes: understanding and controlling local heating in molecular junctions in terms of excitation, dissipation and transfer.

Local heating and dissipation in molecular junctions has long been a concern due to the possibly detrimental impact on device stability and function. More recently there has been increased interest, as these processes underlie both spectroscopic techniques and potential technological applications. Together these issues make an investigation of ways to chemically control local heating in molecular junctions timely and important.

The proposal objective will be addressed through the investigation of three challenges:
- Developing chemical control of local heating in molecular junctions.
- Developing chemical control of heat dissipation in molecular junctions.
- Design of optimal thermoelectric materials.

These three challenges constitute distinct, yet complementary, avenues for investigation with progress in each area supporting the other two. All three challenges build on existing theoretical methods, with the important shift of focus to methods to achieve chemical control. The combination of state-of-the-art computational methods with careful chemical studies promises significant new developments for the area.","1499999","2010-12-01","2015-11-30"
"ChemLife","Artificial micro-vehicles with life-like behaviour","Larisa FLOREA","THE PROVOST, FELLOWS, FOUNDATION SCHOLARS & THE OTHER MEMBERS OF BOARD OF THE COLLEGE OF THE HOLY & UNDIVIDED TRINITY OF QUEEN ELIZABETH NEAR DUBLIN","One of the most interesting properties of living organisms is the way in which they can sense and respond to changes by moving. Movement has been essential to the survival of all life; even units as small as cells can react to different chemicals through movement. This is a phenomenon known as chemotaxis. Bacteria use chemotaxis to find sources of food, while white blood cells use chemotaxis to follow a chemical trail left by a virus, then find it and destroy it. Throughout areas of science, from robotics to drug delivery, if we could mimic a fraction of this fascinating complexity, the possibilities would be endless. 
Imagine micro-structured vehicles, which could ‘navigate’ through complex fluidic environments, and could effectively ‘recognise’, ‘sense’, ‘diagnose’ and ‘treat’ a variety of conditions. This is exactly what this proposed project, ChemLife, will explore. I will make smart droplets which travel through complicated mazes by chemotaxis, communicate with each other, and move to find their partners or locate and neutralise a ‘droplet intruder’. Other biological systems have much more complicated means of movement, such as swimming, crawling or gliding along surfaces. In an attempt to replicate this, I will fabricate ‘swimmers’ and ‘crawlers’, from soft materials which will move independently and travel through liquids or at the bottom of fluidic channels. Not only will these micro-vehicles be able to travel inside fluids, but they will also be able to detect molecules, signal to other vehicles, and repair problems which they encounter. They underpin a key ambition of ChemLife: the realisation of a Biomimetic Toolbox, a library of adaptable vehicles, which can be demonstrated in a wide range of scenarios. The assembly of these micro-vehicles in to ‘smart’ societies which can perform complicated tasks would be a really exciting achievement, with the potential to become a disruptive foundational breakthrough for movement and transport at the micro-scale.","1499887","2018-10-01","2023-09-30"
"CHEMO-RISK","Chemometers for in situ risk assessment of mixtures of pollutants","Annika Jahnke Berger","HELMHOLTZ-ZENTRUM FUR UMWELTFORSCHUNG GMBH - UFZ","CHEMO-RISK aims for a novel scientifically sound chemical risk assessment paradigm that integrates exposure and effect assessment of a broad range of chemicals into a single procedure and provides information relevant to ecosystem and human health. The key innovation is polymer “chemometers” that will be equilibrated with their surroundings and deliver information on the pollutant’s chemical activity in the environment, biota, and humans. A chemometer functions analogously to a thermometer, but instead of the temperature, it yields a measure of chemical activity. Chemical activity in turn indicates the thermodynamic potential for, e.g., partitioning, biouptake and toxicity. CHEMO-RISK aims at breaking the current paradigm in environmental risk assessment of single chemicals that disregards bioavailability, ignores mixture effects, lacks site-specificity and is difficult to extrapolate to human health.
The chemometer extracts will be investigated using top-notch (a) GC and LC/Orbitrap chemical analysis to characterise the pollutant mixtures and (b) cell-based reporter gene bioassays to determine mixture effects covering baseline toxicity, specific (e.g., endocrine disruption) and reactive (e.g., genotoxicity) modes of toxic action and adaptive stress responses. Within CHEMO-RISK, the following important research questions will be tackled: (A) Which processes drive the enrichment of pollutants in aquatic biota on a thermodynamic basis? (B) How do pollutants distribute within an organism, and which effects do they elicit at the key target sites? (C) Can we apply everyday-life items such as eyeglass-nose pads to replace invasive sampling in human health risk assessment? (D) To which degree can non-target analysis of chemometer extracts explain the observed toxicity profiles across media? By combining all these research efforts, CHEMO-RISK will provide a unified risk assessment paradigm with risk-based trigger values distinguishing acceptable from unacceptable effects.","1496030","2017-05-01","2022-04-30"
"Chi2-Nano-Oxides","Second-Order Nano-Oxides for Enhanced Nonlinear Photonics","Rachel GRANGE RODUIT","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Nonlinear optics is present in our daily life with applications, e.g. light sources for microsurgery or green laser pointer. All of them use bulk materials such as glass fibers or crystals. Generating nonlinear effects from materials at the nanoscale would expand the applications to biology as imaging markers or optoelectronic integrated devices. However, nonlinear signals scale with the volume of a material. Therefore finding materials with high nonlinearities to avoid using high power and large interaction length is challenging. Many studies focus on third order nonlinearities (described by a χ(3) tensor) present in every material (silicon, graphene…) or on metals for enhancing nonlinearities with plasmonics. My approach is to explore second-order χ(2) nanomaterials, since they show higher nonlinearities than χ(3) ones, additional properties such as birefringence, wide band gap for transparency, high refractive index (n>2), and no ohmic losses. Typical χ(2) materials are oxides (BaTiO3, LiNbO3…) with a non-centrosymmetric crystal used for wavelength conversion like in second-harmonic generation (SHG).
The key idea is to demonstrate original strategies to enhance SHG of χ(2)  nano-oxides with the material itself and without involving any hybrid effects from other materials such as plasmonic resonances of metals. First, I propose to use multiple Mie resonances from BaTiO3 nanoparticles to boost SHG in the UV to NIR range. Up to now, Mie effects at the nanoscale have been measured in materials with no χ(2) nonlinearities (silicon spheres). Second, since χ(2)  oxides are difficult to etch, I will overcome this fabrication issue by demonstrating solution processed imprint lithography to form high-quality photonic crystal cavities from nanoparticles. Third, I will use facet processing of single LiNbO3 nanowire to obtain directionality effects for spectroscopy on-a-chip. This work fosters applications and commercial devices offering a sustainable future to this field.","1500000","2017-02-01","2022-01-31"
"CHIRALMICROBOTS","Chiral Nanostructured Surfaces and Colloidal Microbots","Peer Fischer","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","""From scientific publications to the popular media, there have been numerous speculations about wirelessly controlled microrobots (microbots) navigating the human body. Microbots have the potential to revolutionize analytics, targeted drug delivery, and microsurgery, but until now there has not been any untethered microscopic system that could be properly moved let alone controlled in fluidic environments. Using glancing angle (physical vapor deposition) we will grow billions of micron-sized colloidal screw-propellers on a wafer. These chiral mesoscopic screws can be magnetized and moved through solution under computer control. The screw-propellers resemble artificial flagella and are the only ‘microbots’ to date that can be fully controlled in solution at micron length scales. The proposed work will advance the fabrication so that active microbots can be applied in rheological measurements and analytics. We will use these novel probes in bio-microrheology with the potential to probe the viscoelastic properties of membranes and tissues, and to explore questions of micro-hydrodynamics. At the same time we will develop these structures as """"colloidal molecules"""" and grow asymmetric mesoscopic particles with tailored shapes and properties. We propose experiments that allow the observation of fundamental effects, such as chiral Brownian motion, something that exist at the molecular scale, but has never been observed to date. Similarly, we will be able to demonstrate for the first time chiral separations based purely on physical fields. The proposed technical advances of the growth of nanostructured surfaces will at the same time permit wafer-scale 3-D nano-structuring for photonic and plasmonic applications, which we plan to demonstrate. We will develop a system for targeted drug delivery, study the interaction of swarms of microbots and devise techniques to control and image these swarms.""","1479760","2012-02-01","2018-01-31"
"CHMIFLUORS","Carbohydrate Mimesis using Fluorinated Sugars for Chemical Biology: From Reaction Design to Applications in Molecular Imaging","Ryan Gilmour","WESTFAELISCHE WILHELMS-UNIVERSITAET MUENSTER","The principle objective of this proposal is to validate fluorinated glyco-structures as effective carbohydrate mimics for the next frontier in pharmaceutical research. Herein we propose to capitalise on the major advances in statistical data analysis which are unravelling the complexity of mammalian and bacterial “glycospace”. Molecular mimicry is a powerful drug design approach. It is therefore envisaged to develop a focussed programme of research to validate fluorinated glycostructures, and in particular 2-fluoro sugars, as carbohydrate mimics for chemical biology, exploiting the ubiquitous role of carbohydrates in molecular recognition. Salient features of the 2-fluoro substituent include (i) enhanced hydrolytic stability to enzymatic degradation, (ii) the presence of a NMR active reporter nucleus (19F) for facile analysis, and (iii) the possibility for molecular imaging application when using 18F labelled glycostructures. Phase one of this project will aim to develop synthetic routes to the target fluoro-glycostructures. This will involve a substantial component of physical organic chemistry including conformational analysis, advanced 19F NMR spectroscopy and the possible isolation of oxo-carbenium analogues by exploiting advances in the development of large, weakly co-ordinating anions. From first principle reaction design and development, through a basic understanding of conformation and reactivity, phase 2 will focus on the application of these materials for chemical biology applications. Phase 2 will then heavily focus on the application of complex oligosaccharides containing the PET active 18F moiety. It is envisaged that by exploiting the ubiquitous role of carbohydrates in molecular recognition that this would conceivably lead to the development of selective imaging agents, thus bypassing the current problem of relying on the metabolically controlled distribution of the commonly used PET tracer 2-fluorodeoxy glucose (18F-FDG).","1253880","2013-11-01","2018-10-31"
"CHOBOTIX","Chemical Processing by Swarm Robotics","Frantisek Stepanek","VYSOKA SKOLA CHEMICKO-TECHNOLOGICKA V PRAZE","The aim of the project is to develop chemical processing systems based on the principle of swarm robotics. The inspiration for swarm robotics comes from the behaviour of collective organisms – such as bees or ants – that can perform complex tasks by the combined actions of a large number of relatively simple, identical agents. The main scientific challenge of the project will be the design and synthesis of chemical swarm robots (“chobots”), which we envisage as internally structured particulate entities in the 10-100 µm size range that can move in their environment, selectively exchange molecules with their surrounding in response to a local change in temperature or concentration, chemically process those molecules and either accumulate or release the product. Such chemically active autonomous entities can be viewed as very simple pre-biotic life forms, although without the ability to self-replicate or evolve. In the course of the project, the following topics will be explored in detail: (i) the synthesis of suitable shells for chemically active swarm robots, both soft (with a flexible membrane) and hard (porous solid shells); (ii) the mechanisms of molecular transport into and out of such shells and means of its active control; (iii) chemical reaction kinetics in spatially complex compartmental structures within the shells; (iv) collective behaviour of chemical swarm robots and their response to external stimuli. The project will be carried out by a multi-disciplinary team of enthusiastic young researchers and the concepts and technologies developed in course of the project, as well as the advancements in the fundamental understanding of the behaviour of “chemical robots” and their functional sub-systems, will open up new opportunities in diverse areas including next-generation distributed chemical processing, synthesis and delivery of personalised medicines, recovery of valuable chemicals from dilute resources, environmental clean-up, and others.","1644000","2008-06-01","2013-05-31"
"CHROMTISOL","Towards New Generation of Solid-State Photovoltaic Cell: Harvesting Nanotubular Titania and Hybrid Chromophores","Jan Macak","UNIVERZITA PARDUBICE","In photovoltaics (PVs), a significant scientific and technological attention has been given to technologies that have the potential to boost the solar-to-electricity conversion efficiency and to power recently unpowerable devices and objects. The research of various solar cell concepts for diversified applications (building integrated PVs, powering mobile devices) has recently resulted in many innovations. However, designs and concepts of solar cells fulfilling stringent criteria of efficiency, stability, low prize, flexibility, transparency, tunable cell size, esthetics, are still lacking.  
Herein, the research focus is given to a new physical concept of a solar cell that explores extremely promising materials, yet unseen and unexplored in a joint device, whose combination may solve traditional solar cells drawbacks (carrier recombination, narrow light absorption). 
It features a high surface area interface (higher than any other known PVs concept) based on ordered anodic TiO2 nanotube arrays, homogenously infilled with nanolayers of high absorption coefficient crystalline chalcogenide or organic chromophores using different techniques, yet unexplored for this purpose. After addition of supporting constituents, a solid-state solar cell with an extremely large incident area for the solar light absorption and optimized electron pathways will be created. The CHROMTISOL solar cell concept bears a large potential to outperform existing thin film photovoltaic technologies and concepts due to unique combination of materials and their complementary properties. 

The project aims towards important scientific findings in highly interdisciplinary fields. Being extremely challenging and in the same time risky, it is based on feasible ideas and steps, that will result in exciting achievements.
The principal investigator, Jan Macak, has an outstanding research profile in the field of self-organized anodic nanostructures and is an experienced researcher in the photovoltaic field","1644380","2015-03-01","2020-02-29"
"CiliaMechanoBio","Primary Cilium-Mediated Mesenchymal Stem Cell Mechanobiology in Bone","David Hoey","THE PROVOST, FELLOWS, FOUNDATION SCHOLARS & THE OTHER MEMBERS OF BOARD OF THE COLLEGE OF THE HOLY & UNDIVIDED TRINITY OF QUEEN ELIZABETH NEAR DUBLIN","Every 30 seconds a person suffers an osteoporosis-related bone fracture in the EU, resulting in significant morbidity, mortality, and health-care costs estimated at €36billion annually. Current therapeutics target bone resorbing osteoclasts, but these are associated with severe side effects. Osteoporosis arises when mesenchymal stem cells (MSC) fail to produce sufficient numbers of bone forming osteoblasts. A key regulator of MSC behaviour is physical loading, yet the mechanisms by which MSCs sense and respond to changes in their mechanical environment are virtually unknown. Primary cilia are nearly ubiquitous ‘antennae-like’ cellular organelles that have very recently emerged as extracellular mechano/chemo-sensors and thus, are strong candidates to play a role in regulating MSC responses in bone. Therefore, the objective of this research program is to determine the role of the primary cilium and associated molecular components in the osteogenic differentiation and recruitment of human MSCs in loading-induced bone adaptation. This will be achieved through ground-breaking in vitro and in vivo techniques developed by the applicant. The knowledge generated in this proposal will represent a profound advance in our understanding of stem cell mechanobiology. In particular, the identification of the cilium and associated molecules as central to stem cell behaviour will lead to the direct manipulation of MSCs via novel cilia-targeted therapeutics that mimic the regenerative influence of loading at a molecular level. These novel therapeutics would therefore target bone formation, providing an alternative path to treatment, resulting in an improved supply of bone forming cells, preventing osteoporosis. Furthermore, these novel therapeutics will be incorporated into biomaterials, generating bioactive osteoinductive scaffolds. These advances will not only improve quality of life for the patient but will significantly reduce the financial burden of bone loss diseases in the EU.","1455068","2013-11-01","2018-10-31"
"CirQys","Circuit QED with hybrid electronic states","Takis Kontos","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","We propose to develop a new scheme for detecting and manipulating exotic states formed by combinations of conductors with different dimensionalities and/or electronic orders. For that purpose, we will use tools of cavity quantum electrodynamics to study in a very controlled way the interaction of light and this exotic matter.
Our experiments will be implemented with nanowires connected to normal, ferromagnetic or superconducting electrodes embedded in high finesse on-chip superconducting photonic cavities. The experimental technique proposed here will inaugurate a novel method for investigating the spectroscopy and the dynamics of tailored nano-systems.
During the project, we will focus on three key experiments. We will demonstrate the strong coupling between a single spin and cavity photons, bringing spin quantum bits a step closer to scalability. We will probe coherence in Cooper pair splitters using lasing and sub-radiance. Finally, we will probe the non-local nature of Majorana bound states predicted to appear at the edges of  topological superconductors via their interaction with cavity photons.","1456608","2013-02-01","2018-01-31"
"CLAPO","The Coevolution of Life and Arsenic in Precambrian Oceans","Ernest Chi Fru","CARDIFF UNIVERSITY","The ubiquity of arsenic resistant genes across all of life’s variety suggests a close intimacy between arsenic biogeochemistry and evolution, over geological time scales. However, the behaviour of arsenic in past environments where life originated and its impact on our evolution is essentially unknown. Arsenic is of particular importance because of its toxic properties, prevalence in tight association with ubiquitous iron and sulfide minerals and as a major component of sulfide-rich waters, all common features of Precambrian oceans. Arsenic obstructs the synthesis of the building blocks of life, exhibiting both chronic and acute toxicity at very low concentrations. These properties make arsenic an agent capable of exerting strong selective pressure on the distribution, success and diversity of life. This is exemplified by when the release of arsenic into groundwater following rock-weathering processes results in widespread poisoning. Using the state of the art stable isotopes tools, coupled to biomass production, bacterial iron, arsenic and sulfur cycling under ancient oceanic conditions, this project will open a new discussion on the much debated relationship between ocean chemistry and evolution, by introducing a new arsenic framework. This will be achieved under three majors themes: 1) Does there exist a biogeochemical connection between arsenic and the timing and transition from the iron-rich to the hypothesized sulfide-rich oceans that are linked to the rise of atmospheric oxygen? 2) Does arsenic and sulfide show concomitant cyclicity during the Precambrian? 3) Could arsenic thus serve as a proxy for the calibration of key transitional steps in the timing of biological innovation?","1486374","2013-09-01","2018-08-31"
"CLC","Cryptography with Low Complexity","Benny Applebaum","TEL AVIV UNIVERSITY","The efficiency of cryptographic constructions is a fundamental question. Theoretically, it is important to understand how much computational resources are needed to guarantee strong notions of security. Practically, highly efficient schemes are always desirable for real-world applications. More generally, the possibility of cryptography with low complexity has wide applications for problems in computational complexity, combinatorial optimization, and computational learning theory.

In this proposal we aim to understand what are the minimal computational resources needed to perform basic cryptographic tasks. In a nutshell, we suggest to focus on three main objectives. First, we would like to get better understanding of the cryptographic hardness of random local functions. Such functions can be computed by highly-efficient circuits and their cryptographic hardness provides a strong and clean formulation for the conjectured average-case hardness of constraint satisfaction problems - a fundamental subject which lies at the core of the theory of computer science. Our second objective is to harness our insights into the hardness of local functions to improve the efficiency of basic cryptographic building blocks such as pseudorandom functions. Finally, our third objective is to expand our theoretical understanding of garbled circuits, study their limitations, and improve their efficiency.

The suggested project can bridge across different regions of computer science such as random combinatorial structures, cryptography, and circuit complexity. It is expected to impact central problems in cryptography, while enriching the general landscape of theoretical computer science.","1265750","2015-05-01","2020-04-30"
"CloudBrake","How nature's smallest clouds slow down large-scale circulations critical for climate","Aloisia NUIJENS","TECHNISCHE UNIVERSITEIT DELFT","Do even the smallest clouds simply drift with the wind? 

Vast areas of our oceans and land are covered with shallow cumulus clouds. These low-level clouds are receiving increased attention as uncertainties in their representation in global climate models lead to a spread in predictions of future climate. This attention emphasizes radiative and thermodynamic impacts of clouds, which are thought to energize the large-scale Hadley circulation. But broadly overlooked is the impact of shallow cumuli on the trade-winds that drive this circulation. Reasons for this negligence are a lack of observations of vertical wind structure and the wide range of scales involved.

My project will test the hypothesis that shallow cumuli can also slow down the Hadley circulation by vertical transport of momentum. First, observations of clouds and winds will be explicitly connected and the causality of their relationship will be exposed using ground-based and airborne measurements and high-resolution modeling. Second, new lidar techniques aboard aircraft are exploited to validate low-level winds measured by the space-borne Aeolus wind lidar and collect high-resolution wind and turbulence data. Third, different models of momentum transport by shallow convection will be developed to represent its impact on winds. Last, evidence of global relationships between winds and shallow cumulus are traced in Aeolus and additional satellite data and the impact of momentum transport on circulations in a control and warmer climate is tested in a general circulation model. 

This project exploits my expertise in observing and modeling clouds and convection focused on a hypothesis which, if true, will strongly influence our understanding of the sensitivity of circulations and the sensitivity of climate. It will increase the predictability of low-level winds and convergence patterns, which are important to many disciplines, including climate studies, numerical weather prediction and wind-energy research.","1867120","2017-01-01","2021-12-31"
"CLUSTER","organisation of CLoUdS, and implications for Tropical cyclones and for the Energetics of the tropics, in current and in a waRming climate","caroline MULLER","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Few geophysical phenomena are as spectacular as tropical cyclones, with their eye surrounded by sharp cloudy eyewalls. There are other types of spatially organised convection (convection refers to overturning of air within which clouds are embedded), in fact organised convection is ubiquitous in the tropics. But it is still poorly understood and poorly represented in convective parameterisations of global climate models, despite its strong societal and climatic impact. It is associated with extreme weather, and with dramatic changes of the large scales, including drying of the atmosphere and increased outgoing longwave radiation to space. The latter can have dramatic consequences on tropical energetics, and hence on global climate. Thus, convective organisation could be a key missing ingredient in current estimates of climate sensitivity from climate models. 

CLUSTER will lead to improved fundamental understanding of convective organisation to help guide and improve convective parameterisations. It is closely related to the World Climate Research Programme (WCRP) grand challenge: Clouds, circulation and climate sensitivity. Grand challenges identify areas of emphasis in the coming decade, targeting specific barriers preventing progress in critical areas of climate science.

Until recently, progress on this topic was hindered by high numerical cost and lack of fundamental understanding. Advances in computer power combined with new discoveries based on idealised frameworks, theory and observational findings, make this the ideal time to determine the fundamental processes governing convective organisation in nature. Using a synergy of theory, high-resolution cloud-resolving simulations, and in-situ and satellite observations, CLUSTER will specifically target two feedbacks recently identified as being essential to convective aggregation, and assess their impact on tropical cyclones, large-scale properties including precipitation extremes, and energetics of the tropics.","1078021","2019-06-01","2024-05-31"
"CLUSTERS","Galaxy formation through the eyes of globular clusters","Mark Gieles","UNIVERSITY OF SURREY","""Globular clusters (GCs) are among the first baryonic structures to form at a redshift of 10 and  they witnessed the earliest phases of galaxy formation. Despite their ubiquity and importance for our understanding of the stellar initial mass function, star formation and chemical evolution in the early Universe, their origin is shrouded in mystery. They could have formed in gas rich discs, similarly to young massive clusters (YMCs) that we see forming today in starburst environments; or they could require a more exotic environment such as the centre of dark matter ``mini-haloes"""".

The Milky Way GCs are resolved into their constituent stellar population making them the obvious place to look for clues. Their pristine properties are, however, affected by a Hubble time of dynamical evolution within an evolving Milky Way. In this proposal I present three projects to determine the initial properties of GCs, allowing them to be used as robust probes of early star formation, stellar evolution and cosmology. Specifically,  I will: (1) dynamically evolve YMCs on a star-by-star basis and achieve a complete census of the fate of the clusters and their debris (``cold"""" streams) within the framework of the hierarchical  assembly of the Milky Way; (2) I will develop an extremely fast cluster evolution algorithm to do population synthesis of (globular) star clusters which will uniquely establish their initial masses, densities and the corresponding distributions; and (3) I will break the degeneracy of a dark matter halo, tidal heating and alternative gravity laws on the kinematics of GCs and determine whether  Milky Way GCs contain dark matter, or not.

Galactic archaeology is entering a Golden Age. ALMA is operational and already putting constraints on the formation of YMCs and Gaia is due to fly next year. The three novel projects presented here will pave the way and prepare for the wealth of unprecedented data.""","1499863","2013-11-01","2018-10-31"
"ClustersXCosmo","Fundamental physics, Cosmology and Astrophysics: Galaxy Clusters at the Cross-roads","Alexandro SARO","ISTITUTO NAZIONALE DI ASTROFISICA","The ClustersXCosmo ERC Starting Grant proposal has the goal of investigating the role of Galaxy Clusters as a cosmological probe and of exploiting the strong synergies between observational cosmology, galaxy formation and fundamental physics related to the tracers of the extreme peaks in the matter density field. In the last decade, astronomical data-sets have started to be widely and quantitatively used by the scientific community to address important physical questions such as: the nature of the dark matter and dark energy components and their evolution; the physical properties of the baryonic matter; the variation of fundamental constants over cosmic time; the sum of neutrino masses; the interplay between the galaxy population and the intergalactic medium; the nature of gravity over megaparsec scales and over cosmic times; the temperature evolution of the Universe. Most of these results are based on well-established geometrical cosmological probes (e.g., galaxies, supernovae, cosmic microwave background). Galaxy clusters provide a complementary and necessary approach, as their distribution as a function of time and observables is sensitive to both the geometrical and the dynamical evolution of the Universe, driven by the growth of structures. Among different cluster surveys, Sunyaev Zel'Dovich effect (SZE) detected catalogs have registered the most dramatic improvement over the last ~5 years, yielding samples extending up to the earliest times these systems appeared. This proposal aims at using a combination of the best available SZE cluster surveys and to interpret them by means of state-of-the-art computational facilities in order to firmly establish the yet controversial role of Galaxy Clusters as a probe for cosmology, fundamental physics and astrophysics. The timely convergence of current and next generation multi-wavelength surveys (DES/SPT/Planck/eRosita/Euclid) will be important to establish the role of Galaxy Clusters as a cosmological tool.","1230403","2017-09-01","2022-08-31"
"ClusterWeb","Unravelling the physics of particle acceleration and feedback in galaxy clusters and the cosmic web","Reinout Johannes VAN WEEREN","UNIVERSITEIT LEIDEN","We will unravel the origin of cosmic magnetic fields, the physics of particle acceleration in dilute plasmas, and the nature of AGN feedback with state-of-the-art radio telescopes. With the enormous gains in sensitivity, survey speed, and resolution of these telescopes – combined with recent breakthroughs that correct for phased-arrays and the Earth’s distorting ionosphere – we can now take the next big step in this field.

Cosmic web filaments and galaxy clusters are the Universe’s largest structures. Clusters grow by a sequence of mergers, generating shock waves and turbulence which heat the cluster plasma. In merging clusters, cosmic rays are accelerated to extreme energies, producing Mpc-size diffuse synchrotron emitting sources. However, these acceleration processes are still poorly understood. Clusters are also heated by AGN feedback from radio galaxies, but the total energy input by feedback and its evolution over cosmic time are unknown. We will construct the largest low-frequency sample of galaxy clusters to (1) establish how particles are accelerated in cluster plasmas, (2) quantify how the cosmic ray content scales with cluster mass, (3) determine the importance of AGN fossil plasma in the acceleration processes, (4) characterize current and past episodes of AGN feedback, and (5) determine the evolution of feedback up to the epoch of cluster formation (z=1-2). These results will be essential to understand cluster formation and its associated energy budget.

As in clusters, cosmic web accretion shocks should also accelerate particles producing radio emission. Based on the deepest low-frequency images ever produced, we will (5) carry out the first studies of these giant accelerators, opening up a new window on the elusive warm-hot intergalactic medium, where many of the cosmic baryons reside. Even more important, (6) we aim to obtain measurements of the intergalactic magnetic field, providing key constraints on the origin of our Universe’s magnetic fields.","1487755","2019-09-01","2024-08-31"
"CM3","Controlled Mechanical Manipulation of Molecules","Christian Wagner","FORSCHUNGSZENTRUM JULICH GMBH","The idea to freely control the atomic-scale structure of matter has intrigued scientists for many decades. The low-temperature scanning probe microscope (LT SPM) has become the instrument of choice for this task since it allows the rearrangement of atoms and molecules on a surface. There is, however, no generic SPM-based method for the manipulation of molecules beyond lateral rearrangement. The goal of this project is to develop controlled mechanical manipulation of molecules (CM3) in which a LT SPM is used to handle large organic molecules in three dimensions with optimal control over position, orientation and shape. CM3 will become a game-changing technique for research on molecular properties and molecular-scale engineering, because it combines fully deterministic manipulation with broad access to molecular degrees of freedom for the first time. In CM3 the tip is attached to a single reactive atom within a molecule. Tip displacement guides the molecule into a desired conformation while the surface provides a second (weaker) fixation. The fundamental challenge addressed by this project is the identification of precise molecular conformations at any time during manipulation. The solution is a big data approach where large batches of automatically recorded SPM manipulation data are structured using machine learning and interpreted by comparison to atomistic simulations. The key idea is a comparison of entire conformation spaces at once, which is robust, even if the theory is not fully quantitative. The obtained map of the conformation space is used to determine molecular conformations during manipulation by methods of control theory. The effectiveness of this approach will be demonstrated in experiments that unambiguously reveal the structure-conductance relation for a series of molecules and that realize the engineering paradigm of piecewise assembly on the molecular scale by constructing a direct current rotor / motor from individual components.","1465944","2018-01-01","2022-12-31"
"CNTBBB","Targeting potential of carbon nanotubes at the blood brain barrier","Alexandra Elizabeth Porter","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","Targeted drug delivery across the blood brain barrier (BBB) to the central nervous system is a large challenge for the treatment of neurological disorders. This 4 year ERC program is aimed towards the evaluating the BBB penetration capacity and toxicological potential of novel carbon nanotube (CNT) carriers using an integrated multidisciplinary approach. State-of-art characterisation techniques developed by the PI will be applied and further developed to detect the interaction of carbon nanotubes with in vitro BBB model and neuronal cells. Specific aims:

1.	Identify the mechanisms of translocation of CNT across the endothelial cells which comprise the BBB, as well as uptake by neuronal cells in vitro.
2.	To investigate the effect of length, diameter and surface charge of CNTs on the BBB and neuronal cells penetration capacity in vitro.
3.	To investigate the toxicological profile of CNT on the BBB and the various neuronal cell types (immortalised and primary neuronal cultures).
4.	Develop protocols to assess whether the CNTs degrade inside the cell.

The ERC Grant will consolidate the new Research Group in nanomaterials-cell interfaces, and allow them to perform stimulating investigator-initiated frontier research in nanotoxicology and nanomedicine. To this end, a multi-disciplinary laboratory will be realized within the framework of this 4-year the ERC Programme. This will permit the group around the PI, to expand activities, push limits, create new boundaries, and develop new protocols for studying nanoparticle-cell interactions in close collaboration with ICL s Department of medicine and chemistry. Within the proposed program there is an underlying ambition both to gain a fundamental understanding for which parameters of CNTs determine their penetration capacity through the BBB and also to assess their toxicological potential at the BBB two highlighted themes by the ERC.","1229998","2011-02-01","2017-01-31"
"CNTM","Cryptography on Non-Trusted Machines","Stefan Dziembowski","UNIWERSYTET WARSZAWSKI","This project is about the design of cryptographic schemes that are secure even if implemented on not-secure devices. The motivation for this problem comes from an observation that most of the real-life attacks on cryptographic devices do not break their mathematical foundations, but exploit vulnerabilities of their implementations. This concerns both the cryptographic software executed on PCs (that can be attacked by viruses), and the implementations on hardware (that can be subject to the side-channel attacks). Traditionally fixing this problem was left to the practitioners, since it was a common belief that theory cannot be of any help here. However, new exciting results in cryptography suggest that this view was too pessimistic: there exist methods to design cryptographic protocols in such a way that they are secure even if the hardware on which they are executed cannot be fully trusted. The goal of this project is to investigate these methods further, unify them in a solid mathematical theory (many of them were developed independently), and propose new ideas in this area. The project will be mostly theoretical (although some practical experiments may be performed). Our main interest lies within the theory of private circuits, bounded-retrieval model, physically-observable cryptography, and human-assisted cryptography. We view these theories just as the departing points, since the area is very fresh and we expect to soon witness completely new ideas in this field.","872550","2008-11-01","2013-10-31"
"CO2LIFE","BIOMIMETIC FIXATION OF CO2 AS SOURCE OF SALTS AND GLUCOSE","Patricia LUIS ALCONERO","UNIVERSITE CATHOLIQUE DE LOUVAIN","The continued increase in the atmospheric concentration of CO2 due to anthropogenic emissions is leading to significant changes in climate, with the industry accounting for one-third of all the energy used globally and for almost 40% of worldwide CO2 emissions. Fast actions are required to decrease the concentration of this greenhouse gas in the atmosphere, value that has currently reaching 400 ppm. Among the technological possibilities that are on the table to reduce CO2 emissions, carbon capture and storage into geological deposits is one of the main strategies that is being applied. However, the final objective of this strategy is to remove CO2 without considering the enormous potential of this molecule as a source of carbon for the production of valuable compounds. Nature has developed an effective and equilibrated mechanism to concentrate CO2 and fixate the inorganic carbon into organic material (e.g., glucose) by means of enzymatic action. Mimicking Nature and take advantage of millions of years of evolution should be considered as a basic starting point in the development of smart and highly effective processes. In addition, the use of amino-acid salts for CO2 capture is envisaged as a potential approach to recover CO2 in the form of (bi)carbonates. 
The project CO2LIFE presents the overall objective of developing a chemical process that converts carbon dioxide into valuable molecules using membrane technology. The strategy followed in this project is two-fold: i) CO2 membrane-based absorption-crystallization process on basis of using amino-acid salts, and ii) CO2 conversion into glucose or salts by using enzymes as catalysts supported on or retained by membranes. The final product, i.e. (bi)carbonates or glucose, has a large interest in the (bio)chemical industry, thus, new CO2 emissions are avoided and the carbon cycle is closed. This project will provide a technological solution at industrial scale for the removal and reutilization of CO2.","1302710","2018-01-01","2022-12-31"
"CO2Recycling","A Diagonal Approach to CO2 Recycling to Fine Chemicals","Thibault Matthias Daniel Cantat","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","Because fossil resources are a limited feedstock and their use results in the accumulation of atmospheric CO2, the organic chemistry industry will face important challenges in the next decades to find alternative feedstocks. New methods for the recycling of CO2 are therefore needed, to use CO2 as a carbon source for the production of organic chemicals. Yet, CO2 is difficult to transform and only 3 chemical processes recycling CO2 have been industrialized to date. To tackle this problem, my idea is to design novel catalytic transformations where CO2 is reacted, in a single step, with a functionalizing reagent and a reductant that can be independently modified, to produce a large spectrum of molecules. The proof of concept for this new “diagonal approach” has been established in 2012, in my team, with a new reaction able to co-recycle CO2 and a chemical waste of the silicones industry (PMHS) to convert amines to formamides.  The goal of this proposal is to develop new diagonal reactions to enable the use of CO2 for the synthesis of amines, esters and amides, which are currently obtained from fossil materials. The novel catalytic reactions will be applied to the production of important molecules: methylamines, acrylamide and methyladipic acid. The methodology will rely on the development of molecular catalysts able to promote the reductive functionalization of CO2 in the presence of H2 or hydrosilanes. Rational design of efficient catalysts will be performed based on theoretical and experimental mechanistic investigations and utilized for the production of industrially important chemicals. Overall, this proposal will contribute to achieving sustainability in the chemical industry. The results will also increase our understanding of CO2 activation and provide invaluable insights into the basic modes of action of organocatalysts in reduction chemistry. They will serve the scientific community involved in the field of organocatalysis, green chemistry and energy storage.","1494734","2013-11-01","2018-10-31"
"CO2VOLC","CO2VOLC: Quantifying the global volcanic CO2 cycle","Michael Burton","THE UNIVERSITY OF MANCHESTER","""Global climate change induced by anthropogenic emissions of CO2 is currently a major issue facing humanity, but uncertainties in the magnitude and rate of climate change remain, and deterministic predictions are beyond our capacity. In this context, the study of how the geochemical carbon cycle established a relatively narrow band of variability in atmospheric CO2 concentrations over the last 400 ka is of great interest. However, large uncertainties in both weathering and volcanic CO2 fluxes prevent a truly quantitative assessment of this critical cycle. Measuring the global volcanic CO2 flux, GVFCO2, would allow us to better understand the likely impact large eruptions have had in Earth’s history, and constrain the natural vs. anthropogenic CO2 flux.

We propose a truly innovative project to address head on the problem of determining GVFCO2. We will create new, compact instruments, utilising cutting-edge laser technologies, which will allow us to measure volcanic CO2, H2O, SO2 and HCl fluxes from aircraft. By flying below and through the volcanic plumes created by ~50 active volcanoes (~10% of all active volcanoes) of the Banda-Sunda arc in Indonesia, the majority of which have never been measured before, we will dramatically increase our understanding of GVFCO2 and geochemical cycles for all these species.

Measuring the volcanic emissions from an entire subduction arc is an unprecedented experiment, providing insight into the slab and mantle heterogeneity and volatile mass balance. Perhaps the most important breakthrough that we will pursue will be the determination of the 37Cl/35Cl ratio from HCl emitted from each volcano. This ratio reflects the mantle/slab source proportion, and allows the input rate of volatiles to the mantle to be measured.

The application of innovative new technology we propose here will produce ground-breaking insights into volcanology, isotope and gas geochemistry, volatile cycles, subduction and climate change.""","1721000","2012-01-01","2017-12-31"
"COALA","Comprehensive molecular characterization of secondary organic aerosol formation in the atmosphere","Mikael Ehn","HELSINGIN YLIOPISTO","Key words: Atmospheric secondary organic aerosol, chemical ionization mass spectrometry

The increase in anthropogenic atmospheric aerosol since the industrial revolution has considerably mitigated the global warming caused by concurrent anthropogenic greenhouse gas emissions. However, the uncertainty in the magnitude of the aerosol climate influence is larger than that of any other man-made climate-perturbing component. 

Secondary organic aerosol (SOA) is one of the most prominent aerosol types, yet a detailed mechanistic understanding of its formation process is still lacking. We recently presented the ground-breaking discovery of a new important compound group in our publication in Nature: a prompt and abundant source of extremely low-volatility organic compounds (ELVOC), able to explain the majority of the SOA formed from important atmospheric precursors. 

Quantifying the atmospheric role of ELVOCs requires further focused studies and I will start a research group with the main task of providing a comprehensive, quantitative and mechanistic understanding of the formation and evolution of SOA. Our recent discovery of an important missing component of SOA highlights the need for comprehensive chemical characterization of both the gas and particle phase composition. 

This project will use state-of-the-art chemical ionization mass spectrometry (CIMS), which was critical also in the detection of the ELVOCs. We will extend the applicability of CIMS techniques and conduct innovative experiments in both laboratory and field settings using a novel suite of instrumentation to achieve the goals set out in this project.

We will provide unprecedented insights into the compounds and mechanisms producing SOA, helping to decrease the uncertainties in assessing the magnitude of aerosol effects on climate. Anthropogenic SOA contributes strongly to air quality deterioration as well and therefore our results will find direct applicability also in this extremely important field.","1892221","2015-03-01","2020-02-29"
"COCOON","Conformal coating of nanoporous materials","Christophe Detavernier","UNIVERSITEIT GENT","CONTEXT - Nanoporous structures are used for application in catalysis, molecular separation, fuel cells, dye sensitized solar cells etc. Given the near molecular size of the porous network, it is extremely challenging to modify the interior surface of the pores after the nanoporous material has been synthesized.

THIS PROPOSAL - Atomic Layer Deposition (ALD) is envisioned as a novel technique for creating catalytically active sites and for controlling the pore size distribution in nanoporous materials. ALD is a self-limited growth method that is characterized by alternating exposure of the growing film to precursor vapours, resulting in the sequential deposition of (sub)monolayers. It provides atomic level control of thickness and composition, and is currently used in micro-electronics to grow films into structures with aspect ratios of up to 100 / 1. We aim to make the fundamental breakthroughs necessary to enable atomic layer deposition to engineer the composition, size and shape of the interior surface of nanoporous materials with aspect ratios in excess of 10,000 / 1.

POTENTIAL IMPACT Achieving these objectives will enable atomic level engineering of the interior surface of any porous material. We plan to focus on three specific applications where our results will have both medium and long term impacts:

- Engineering the composition of pore walls using ALD, e.g. to create catalytic sites (e.g. Al for acid sites, Ti for redox sites, or Pt, Pd or Ni)

- chemical functionalization of the pore walls with atomic level control can result in breakthrough applications in the fields of catalysis and sensors.

- Atomic level control of the size of nanopores through ALD controlling the pore size distribution of molecular sieves can potentially lead to breakthrough applications in molecular separation and filtration.

- Nanocasting replication of a mesoporous template by means of ALD can result in the mass-scale production of nanotubes.","1432800","2010-01-01","2014-12-31"
"CODAMODA","Controlling Data Movement in the Digital Age","Aggelos Kiayias","ETHNIKO KAI KAPODISTRIAKO PANEPISTIMIO ATHINON","Nowadays human intellectual product is increasingly produced and disseminated solely in digital form. The capability of digital data for effortless reproduction and transfer has lead to a true revolution that impacts every aspect of human creativity. Nevertheless, as with every technological revolution, this digital media revolution comes with a dark side that, if left unaddressed, it will limit its impact and may counter its potential advantages. In particular, the way we produce and disseminate digital content today does not lend itself to controlling the way data move and change. It turns out that the power of being digital can be a double-edged sword: the ease of production, dissemination and editing also implies the ease of misappropriation, plagiarism and improper modification.

To counter the above problems, the proposed research activity will focus on the development of a new generation of enabling cryptographic technologies that have the power to facilitate the appropriate controls for data movement. Using the techniques developed in this project it will be feasible to build digital content distribution systems where content producers will have the full possible control on the dissemination of their intellectual product, while at the same time the rights of the end-users in terms of privacy and fair use can be preserved. The PI is uniquely qualified to carry out the proposed research activity as he has extensive prior experience in making innovations in the area of digital content distribution as well as in the management of research projects. As part of the project activities, the PI will establish the CODAMODA laboratory in the University of Athens and will seek opportunities for technology transfer and interdisciplinary work with the legal science community.","1212960","2011-04-01","2017-03-31"
"CODEMAP","COmplex Deep-sea Environments: Mapping habitat heterogeneity As Proxy for biodiversity","Veerle Ann Ida Huvenne","NATURAL ENVIRONMENT RESEARCH COUNCIL","Human impact on the deep ocean is rapidly increasing, with largely unknown consequences. Effective management and conservation, based on an ecosystem approach, is hampered by our poor understanding of the deep-sea environment. Measuring biodiversity, the main indicator of ecosystem status and functioning, is a major challenge in deep water: traditional sampling schemes are expensive and time-consuming, and their limited coverage makes it difficult to relate the results to regional patterns. Complex deep-sea environments are especially problematic. Ecosystem hotspots such as canyons or coral reefs contain true 3D morphology that cannot be surveyed with conventional techniques. CODEMAP will quantify habitat heterogeneity in complex deep-sea terrains, and will evaluate its potential as a proxy for benthic biodiversity at a variety of scales. Habitat heterogeneity has been suggested as a major driver for deep-sea biodiversity, but is rarely quantified in a spatial context in the marine realm.
To achieve its goal, CODEMAP will combine the fields of marine geology, ecology, remote sensing and underwater vehicle technology to establish an integrated, statistically robust and fully 3D methodology to map complex deep-sea habitats. Statistical techniques will be developed to extrapolate quantitative habitat information from fine-scale surveys to broad-scale maps. The optimal parameters to measure habitat heterogeneity will be defined, and their potential as biodiversity indicators tested through correlation with traditional approaches. The project focuses on submarine canyons, but the techniques will also be transferred to other environments. CODEMAP is expected to have a strong impact on the fundamental understanding of the deep sea and on ecosystem-based deep-sea management.","1401012","2011-04-01","2017-01-31"
"COEVOLUTION","Black holes and their host galaxies: coevolution across cosmic time","Debora Sijacki","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Galaxy formation is one of the most fascinating yet challenging fields of astrophysics. The desire to understand
galaxy formation has led to the design of ever more sophisticated telescopes which show a bewildering variety
of galaxies in the Universe. However, the degree to which an interpretation of this wealth of data can succeed
depends critically on having accurate and realistic theoretical models of galaxy formation. While cosmological
simulations of galaxy formation provide the most powerful technique for calculating the non-linear evolution of
cosmic structures, the enormous dynamic range and poorly understood baryonic physics are main uncertainties
of present simulations. This impacts on their predictive power and is the major obstacle to our understanding of
observational data. The objective of this proposal is to drastically improve upon the current state-of-the-art by i)
including more realistic physical processes, such as those occurring at the sphere of influence of a galaxy’s central
black hole and ii) greatly extending spatial dynamical range with the aid of a novel technique I have developed.
With this technique I want to address one of the major unsolved issues of galaxy formation: “How do galaxies and
their central black holes coevolve?” Specifically, I want to focus on three crucial areas of galaxy formation: a) How
and where the very first black holes form, what are their observational signatures, and when is the coevolution with
host galaxies established? b) Is black hole heating solely responsible for the morphological transformation and
quenching of massive galaxies, or are other processes important as well? c) What is the impact of supermassive
black holes on galaxy clusters and can we calibrate baryonic physics in clusters to use them as high precision
cosmological probes? The requested funding is for 50% of the PI’s time and three postdoctoral researchers to
establish an independent research group at the KICC and IoA, Cambridge.","1975062","2015-09-01","2020-08-31"
"COFLeaf","Fuel from sunlight: Covalent organic frameworks as integrated platforms for photocatalytic water splitting and CO2 reduction","Bettina Valeska Lotsch","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","The efficient conversion of solar energy into renewable chemical fuels has been identified as one of the grand challenges facing society today and one of the major driving forces of materials innovation.
Nature’s photosynthesis producing chemical fuels through the revaluation of sunlight has inspired generations of chemists to develop platforms mimicking the natural photosynthetic process, albeit at lower levels of complexity. While artificial photosynthesis remains a considerable challenge due to the intricate interplay between materials design, photochemistry and catalysis, the spotlights – light-driven water splitting into hydrogen and oxygen and carbon dioxide reduction into methane or methanol – have emerged as viable pathways into both a clean and sustainable energy future. With this proposal, we aim at introducing a new class of polymeric photocatalysts based on covalent organic frameworks, COFs, to bridge the gap between semiconductor and molecular systems and explore rational ways to design single-site heterogeneous photocatalysts offering both chemical tunability and stability. 
The development of a photocatalytic model system is proposed, which will be tailored by molecular synthetic protocols and optimized by solid-state chemical procedures and crystal engineering so as to provide insights into the architectures, reactive intermediates and mechanistic steps involved in the photocatalytic process, with complementary insights from theory. We envision the integration of various molecular subsystems including photosensitizers, redox shuttles and molecular co-catalysts in a single semiconducting COF backbone. Taking advantage of the hallmarks of COFs – molecular definition and tunability, crystallinity, porosity and rigidity – we describe the design of COF systems capable of light-induced hydrogen evolution, oxygen evolution and overall water splitting, and delineate strategies to use COFs as integrated platforms for CO2 capture, activation and conversion.","1497125","2015-09-01","2020-08-31"
"COGNIMUND","Cognitive Image Understanding: Image representations and Multimodal learning","Tinne Tuytelaars","KATHOLIEKE UNIVERSITEIT LEUVEN","One of the primary and most appealing goals of computer vision is to automatically understand the content of images on a cognitive level. Ultimately we want to have computers interpret images as we humans do, recognizing all the objects, scenes, and people as well as their relations as they appear in natural images or video. With this project, I want to advance the state of the art in this field in two directions, which I believe to be crucial to build the next generation of image understanding tools. First, novel more robust yet descriptive image representations will be designed, that incorporate the intrinsic structure of images. These should already go a long way towards removing irrelevant sources of variability while capturing the essence of the image content. I believe the importance of further research into image representations is currently underestimated within the research community, yet I claim this is a crucial step with lots of opportunities good learning cannot easily make up for bad features. Second, weakly supervised methods to learn from multimodal input (especially the combination of images and text) will be investigated, making it possible to leverage the large amount of weak annotations available via the internet. This is essential if we want to scale the methods to a larger number of object categories (several hundreds instead of a few tens). As more data can be used for training, such weakly supervised methods might in the end even come on par with or outperform supervised schemes. Here we will call upon the latest results in semi-supervised learning, datamining, and computational linguistics.","1538380","2010-02-01","2015-01-31"
"COGS","Capitalizing on Gravitational Shear","Sarah Louise Bridle","THE UNIVERSITY OF MANCHESTER","Our Universe appears to be filled with mysterious ingredients: 25 per cent appears to be dark matter, perhaps an as-yet undiscovered particle, and 70 per cent seems to be a bizarre fluid, dubbed dark energy, for which there is no satisfactory theory. Solving the dark energy problem is the most pressing question in cosmology today. It is possible that dark energy does not exist at all, and instead Einstein s theory of General Relativity is flawed. Cosmologists hope to measure the properties of the dark energy using the next generation of cosmological observations, in which I am playing a leading role. I believe the most promising technique to crack the dark energy problem is gravitational shear, in which images of distant galaxies are distorted as they pass through the intervening dark matter distribution. Analysis of the distortions allows a map of the dark matter to be reconstructed; by examining the dark matter distribution we uncover the nature of the apparent dark energy. However to capitalize on the great potential of gravitational shear we must measure incredibly small image distortions in the presence of much larger image modifications that occur in the measurement process. I am proposing a fresh look at this problem using an inter-disciplinary approach in collaboration with computer scientists. This grant would enable my team to play a central role in the key results from the upcoming Dark Energy Survey. We would further capitalize on the gravitational shear signal by moving away from the current dark energy bandwagon by instead focusing on testing General Relativity using novel approaches. Our work will produce results which will lead the next Einstein to solve the biggest puzzle in cosmology, and arguably physics.","1400000","2010-04-01","2016-03-31"
"COHEGRAPH","Electron quantum optics in Graphene","Séverin Preden Roulleau","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","Quantum computing is based on the manipulation of quantum bits (qubits) to enhance the efficiency of information processing. In solid-state systems, two approaches have been explored: 

• static qubits, coupled to quantum buses used for manipulation and information transmission,
• flying qubits which are mobile qubits propagating in quantum circuits for further manipulation.

 Flying qubits research led to the recent emergence of the field of electron quantum optics, where electrons play the role of photons in quantum optic like experiments. This has recently led to the development of electronic quantum interferometry as well as single electron sources. As of yet, such experiments have only been successfully implemented in semi-conductor heterostructures cooled at extremely low temperatures. Realizing electron quantum optics experiments in graphene, an inexpensive material showing a high degree of quantum coherence even at moderately low temperatures, would be a strong evidence that quantum computing in graphene is within reach.
One of the most elementary building blocks necessary to perform electron quantum optics experiments is the electron beam splitter, which is the electronic analog of a beam splitter for light. However, the usual scheme for electron beam splitters in semi-conductor heterostructures is not available in graphene because of its gapless band structure. I propose a breakthrough in this direction where pn junction plays the role of electron beam splitter. This will lead to the following achievements considered as important steps towards quantum computing:

• electronic Mach Zehnder interferometry used to study the quantum  coherence properties of graphene,
• two electrons Aharonov Bohm interferometry used to generate entangled states as an elementary quantum gate,
• the implementation of on-demand electronic sources in the GHz range for graphene flying qubits.","1500000","2016-05-01","2021-04-30"
"COHOMCODES","Robust Codes from Higher Dimesional Expanders","Tali Kaufman Halman","BAR ILAN UNIVERSITY","Error correcting codes play a fundamental role in computer science. Good codes are codes with rate and distance that are asymptotically optimal. Some of the most successful good codes are constructed using expander graphs. In recent years a new notion of {\em robust} error correcting codes, known as locally testable codes (LTCs), has emerged. Locally testable codes are codes in which a proximity of a vector to an error correcting code can be achieved by probing the vector in {\em constant} many locations (independent of its length). LTCs are at the heart of Probabilistically Checkable Proofs (PCPs) and their construction has been sought since the discovery of the PCP theorem in the early 1990s.

Despite 20 years of research, it is still widely open whether good locally testable codes exist. LTCs present completely new challenge to the field of error correcting codes. In the old paradigm a random code is a good code and the main focus was to construct explicit codes that imitate the random code. However, a random code is not an LTC. Thus, contrary to traditional codes, there are no natural candidates for LTCs. The known constructions of robust codes are ad hoc, and there is a lack of theory that explains their existence.

The goal of the current research plan is to harness the emerging field of higher dimensional expanders and their topological properties for a systematic study of robust error correcting codes. Higher dimensional expanders are natural candidates for obtaining robust codes since they offer a strong form of redundancy that is essential for robustness. Such form of redundancy is lacking by their one dimensional analogue (i.e., expander graphs). Hence, the known expander codes are not robust. We expect that our study will draw new connections between error correcting codes, high dimensional expanders, topology and probability that will shed new light on these fields, and in particular, will advance the constructing of good and robust codes.","1302000","2014-02-01","2020-01-31"
"COINFLIP","Coupled Organic Inorganic Nanostructures for Fast, Light-Induced Data Processing","Marcus Scheele","EBERHARD KARLS UNIVERSITAET TUEBINGEN","The main objective of this project is to design optical switches with a response time < 5 ps, a switching energy < 1 fJ/bit and compatibility with silicon technology to excel in high-speed data processing at low heat dissipation. This will be pursued by combining the chemistry of inorganic, nanocrystalline colloids and organic semiconductor molecules to fabricate thin films of organic-inorganic hybrid nanostructures. Optical switches play a pivotal role in modern data processing based on silicon photonics, where they control the interface between photonic optical fibers used for data transmission and electronic processing units for computing. Data transfer across this interface is slow compared to that in optical interconnects and high-speed silicon transistors, such that faster optical switching accelerates the overall speed of data processing of the system as a whole. By modifying the surface of the inorganic nanocrystals with conductive molecular linkers and self-assembly into macroscopic solid state materials, new electronic and photonic properties arise due to charge transfer at the organic/inorganic interface. The multiple optical resonances in these hybrid materials result in strong optoelectronic interactions with external light beams, which are exploited for converting photonic into electronic signals at unprecedented speed. A key concept here is an activated absorption mechanism, in which the nanocrystals act as sensitizers with short-lived excited states, which are activated by a first optical pump beam. Efficient charge transfer at the organic/inorganic interface temporarily creates additional resonances in the molecular linkers, which may be probed by a second optical beam for as long as the sensitizer is in its excited state. Utilizing nanocrystals with excited state lifetimes < 5ps will reward ultrafast response times to pave the way for novel optical switches and high-speed data processing rates for silicon photonics.","1497375","2019-02-01","2024-01-31"
"COLD","Climate Sensitivity of Glacial Landscape Dynamics","Dirk SCHERLER","HELMHOLTZ ZENTRUM POTSDAM DEUTSCHESGEOFORSCHUNGSZENTRUM GFZ","How do erosion rates in glacial landscapes vary with climate change and how do such changes affect the dynamics of mountain glaciers? Providing quantitative constraints towards this question is the main objective of COLD. These constraints are so important because mountain glaciers are sensitive to climate change and their deposits provide a unique history of Earths terrestrial climate that allows reconstructing leads and lags in the climate system.

The climate sensitivity of mountain glaciers is influenced by debris on their surface that impedes ice melting. Theoretical models of frost-related bedrock fracturing predict that rates of debris production are temperature-sensitive and that its supply to mountain glaciers increases during warming periods. Thus a previously unrecognized negative feedback emerges that lowers ice melt rates and potentially buffers part of the ice retreat due to warming. However, the temperature-sensitivity of debris production in glacial landscapes is poorly understood. Specifically, we lack robust erosion rate estimates for these landscapes, which are key for testing models of frost-related bedrock fracturing.

Here, I propose an innovative combination of new tools that capitalize on recent developments in cosmogenic nuclide geochemistry, landscape evolution modelling, and planetary-scale remote sensing analysis. I will use these tools to quantify headwall erosion rates in mountainous glacial landscapes and to gauge the sensitivity of mountain glaciers to variations in debris supply. Expected results will provide a basis for assessing the impacts of global warming, for improved predictions of valley glacier evolution, and for palaeoclimate interpretations of glacial landforms. COLD will focus on glacial landscapes, but the inverse modelling approach I will develop is applicable to any landscape on Earth and has the potential to fundamentally transform how we use cosmogenic nuclides to constrain Earth surface dynamics.","1499308","2018-01-01","2022-12-31"
"COLDNANO","UltraCOLD ion and electron beams for NANOscience","Daniel Comparat","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","COLDNANO (UltraCOLD ion and electron beams for NANOscience), aspires to build novel ion and electron sources with superior performance in terms of brightness, energy spread and minimum achievable spot size. Such monochromatic, spatially focused and well controlled electron and ion beams are expected to open many research possibilities in material sciences, in surface investigations (imaging, lithography) and in semiconductor diagnostics. The proposed project intends to develop sources with the best beam quality ever produced and to assess them in some advanced surface science research domains. Laterally, I will develop expertise exchange with one Small and Medium Enterprise who will exploit industrial prototypes.

The novel concept is to create ion and electron sources using advanced laser cooling techniques combined with the particular ionization properties of cold atoms. It would then be first time that “laser cooling” would lead to a real industrial development.

A cesium magneto-optical trap will first be used. The atoms will then be excited by lasers and ionized in order to provide the electron source. The specific extraction optics for the electrons will be developed. This source will be compact and portable to be used for several applications such as Low Energy Electron Microscopy, functionalization of semi-conducting surfaces or high resolution Electron Energy Loss Spectrometry by coupling to a Scanning Transmission Electron Microscope.

Based on the knowledge developed with the first experiment, a second ambitious xenon dual ion and electron beam machine will then be realized and used to study the scattering of ion and electron at low energy.

Finally, I present a very innovative scheme to control the time, position and velocity of individual particles in the beams. Such a machine providing ions or electrons on demand would open the way for the “ultimate” resolution in time and space for surface analysis, lithography, microscopy or implantation.","1944000","2012-02-01","2017-01-31"
"ColDSIM","Cold gases with long-range interactions: 
Non-equilibrium dynamics and complex simulations","Guido Pupillo","CENTRE INTERNATIONAL DE RECHERCHE AUX FRONTIERES DE LA CHIMIE FONDATION","Cold gases of electronically excited Rydberg atoms and groundstate polar molecules have generated considerable interest in cold matter physics, by introducing for the first time many-body systems with interactions which are both long-range and tunable with external fields.  The overall objective of this proposal is (i) the development of theoretical ideas and tools for the understanding and control of non-equilibrium dynamics in these diverse systems and in their mixtures, including dissipative effects leading to cooling, and (ii) to analyse emerging fundamental phenomena in the classical and quantum regimes of strong interactions, leading to innovative simulations and experiments of complex classical and quantum systems. The project is divided into three parts, with strong overlap:
1)  Rydberg atom dynamics: The study of complex open-system dynamics in gases of laser-driven Rydberg atoms, including the study of the effects and control of dissipation and decoherence from spontaneous emission in strongly interacting gases.
2) Cooling of complex molecules in atom-molecule mixtures: The theoretical investigation of novel ways to perform cooling towards quantum degeneracy of generic, comparatively complex molecules, beyond bialkali ones, in mixtures of groundstate molecules and of Rydberg-excited atoms.
3) Simulations of strongly interacting many-body systems at the quantum/classical crossover: Atomistic characterization of formation and dynamics of formation of strongly correlated phases with long-range interactions.
For each of these subjects, the objectives are at the cutting edge of fundamental atomic and molecular science and technology.","1496400","2013-02-01","2018-01-31"
"collectiveQCD","Collectivity in small, srongly interacting systems","Korinna ZAPP","LUNDS UNIVERSITET","In collisions of heavy nuclei at collider energies, for instance at the Large Hadron Collider (LHC) at CERN, the energy density is so high that an equilibrated Quark-Gluon Plasma (QGP), an exotic state of matter consisting of deconfined quarks and gluons, is formed. In proton-proton (p+p) collisions, on the other hand, the density of produced particles is low. The traditional view on such reactions is that final state particles are free and do not rescatter. This picture is challenged by recent LHC data, which found features in p+p collisions that are indicative of collective behaviour and/or the formation of a hot and dense system. These findings have been taken as signs of QGP formation in p+p reactions. Such an interpretation is complicated by the fact that jets, which are the manifestation of very energetic quarks and gluons, are quenched in heavy ion collisions, but appear to be unmodified in p+p reactions. This is puzzling because collectivity and jet quenching are caused by the same processes. So far there is no consensus about the interpretation of these results, which is also due to a lack of suitable tools.
It is the objective of this proposal to address the question whether there are collective effects in p+p collisions. To this end two models capable of describing all relevant aspects of p+p and heavy ion collisions will be developed. They will be obtained by extending a successful description of p+p to heavy ion reactions and vice versa.
The answer to these questions will either clarify the long-standing problem how collectivity emerges from fundamental interactions, or it will necessitate qualitative changes to our interpretation of collective phenomena in p+p and/or heavy ion collisions.
The PI is in a unique position to accomplish this goal, as she has spent her entire career working on different aspects of p+p and heavy ion collisions. The group in Lund is the ideal host, as it is very active in developing alternative interpretations of the data.","1500000","2019-02-01","2024-01-31"
"CollectSwim","Individual and Collective Swimming of Active Microparticles","Sebastien MICHELIN","ECOLE POLYTECHNIQUE","Bacteria are tiny; yet their collective dynamics generate large-scale flows and profoundly modify a fluid’s viscosity or diffusivity. So do autophoretic microswimmers, an example of active microscopic particles that draw their motion from physico-chemical exchanges with their environment. How do such ``active fluids'' turn individual microscopic propulsion into macroscopic fluid dynamics? What controls this self-organization process? These are fundamental questions for biologists but also for engineers, to use these suspensions for mixing or chemical sensing and, more generally, for creating active fluids whose macroscopic physical properties can be controlled precisely.

Self-propulsion of autophoretic swimmers was reported only recently. Major scientific gaps impair the quantitative understanding of their individual and collective dynamics, which is required to exploit these active fluids. Existing models scarcely account for important experimental characteristics such as complex hydrodynamics, physico-chemical processes and confinement. Thus, these models cannot yet be used as predictive tools, even at the individual level.

Further, to use phoretic suspensions as active fluids with microscopically-controlled properties, quantitatively-predictive models are needed for the collective dynamics. Instead of ad-hoc interaction rules, collective models must be built on a detailed physico-mechanical description of each swimmer’s interaction with its environment.

This project will develop these tools and validate them against experimental data. This requires overcoming several major challenges: the diversity of electro-chemical processes, the confined geometry, the large number of particles, and the plurality of interaction mechanisms and their nonlinear coupling. 

To address these issues, rigorous physical, mathematical and numerical models will be developed to obtain a complete multi-scale description of the individual and collective dynamics of active particles.","1497698","2017-09-01","2022-08-31"
"COLLREGEN","Collagen scaffolds for bone regeneration: applied biomaterials, bioreactor and stem cell technology","Fergal Joseph O'brien","ROYAL COLLEGE OF SURGEONS IN IRELAND","Regenerative medicine aims to regenerate damaged tissues by developing functional cell, tissue, and organ substitutes to repair, replace or enhance biological function in damaged tissues. The focus of this research programme is to develop bone graft substitute biomaterials and laboratory-engineered bone tissue for implantation in damaged sites. At a simplistic level, biological tissues consist of cells, signalling mechanisms and extracellular matrix. Regenerative medicine/tissue engineering technologies are based on this biological triad and involve the successful interaction between three components: the scaffold that holds the cells together to create the tissues physical form, the cells that create the tissue, and the biological signalling mechanisms (such as growth factors or bioreactors) that direct the cells to express the desired tissue phenotype. The research proposed in this project includes specific projects in all three areas. The programme will be centred on the collagen-based biomaterials developed in the applicant s laboratory and will incorporate cutting edge stem cell technologies, growth factor delivery, gene therapy and bioreactor technology which will translate to in vivo tissue repair. This translational research programme will be divided into four specific themes: (i) development of novel osteoinductive and angiogenic smart scaffolds for bone tissue regeneration, (ii) scaffold and stem cell therapies for bone tissue regeneration, (iii) bone tissue engineering using a flow perfusion bioreactor and (iv) in vivo bone repair using engineered bone and smart scaffolds.","1999530","2009-11-01","2015-09-30"
"COLORAMAP","Constrained Low-Rank Matrix Approximations: Theoretical and Algorithmic Developments for Practitioners","Nicolas Benoit P Gillis","UNIVERSITE DE MONS","Low-rank matrix approximation (LRA) techniques such as principal component analysis (PCA) are powerful tools for the representation and analysis of high dimensional data, and are used in a wide variety of areas such as machine learning, signal and image processing, data mining, and optimization. Without any constraints and using the least squares error, LRA can be solved via the singular value decomposition. However, in practice, this model is often not suitable mainly because (i) the data might be contaminated with outliers, missing data and non-Gaussian noise, and (ii) the low-rank factors of the decomposition might have to satisfy some specific constraints. Hence, in recent years, many variants of LRA have been introduced, using different constraints on the factors and using different objective functions to assess the quality of the approximation; e.g., sparse PCA, PCA with missing data, independent component analysis and nonnegative matrix factorization. Although these new constrained LRA models have become very popular and standard in some fields, there is still a significant gap between theory and practice. In this project, our goal is to reduce this gap by attacking the problem in an integrated way making connections between LRA variants, and by using four very different but complementary perspectives: (1) computational complexity issues, (2) provably correct algorithms, (3) heuristics for difficult instances, and (4) application-oriented aspects. This unified and multi-disciplinary approach will enable us to understand these problems better, to develop and analyze new and existing algorithms and to then use them for applications. Our ultimate goal is to provide practitioners with new tools and to allow them to decide which method to use in which situation and to know what to expect from it.","1291750","2016-09-01","2021-08-31"
"COLORLANDS","COLOR Ordering Templated by Hierarchized Supramolecular Porous FlatLANDS","Davide Bonifazi","CARDIFF UNIVERSITY","The idea of this research project is to take advantage of molecular self-assembly to create a new generation of periodically-organized porous organic materials that, acting as specific molecular hosts, can structurally control the positioning of multiple functional guests on surfaces, opening new horizons toward the understanding and development of rationale protocols for the patterning of unprecedented materials. Taking advantage of a supramolecular approach to engineer extended mono- and two-dimensional organic networks, the ultimate aim of COLORLANDS is to create novel hosting frameworks accommodating in a predetermined fashion organic chromophores and/or fluorophores. For instance, these can be oligophenylenes as blue emitters, cumarines/oligophenylethylenes as green emitters, or perylenebisimides conjugates as red emitters. Depending on their spatial organization, such materials will be the springboard for further technological development in the fields of electroluminescent devices or artificial leafs mimicking natural light harvesting antenna systems. The self-assembly of selected rigid molecular modules alternatively functionalized with complementary connectors (PNA strands) will yield, under equilibrium conditions, one exclusive structural pattern. This will feature controllable (in shape, size and chemical nature) periodic receptor sites, each programmed to selectively accommodate a specific molecular chromophore and/or fluorophore. Particular attention will be given to the design and fundamental understanding of specific orthogonal interactions between the self-assembled receptor sites and the functional molecular guests. This will be achieved through the lateral organic functionalization of the PNA strands with novel orthogonal H-bonding-based recognition motifs. Depending on the ratio between the different receptors, one can tailor the desired emission or absorption colour, virtually enabling unlimited surfing through the color coordinate diagram.","1295400","2012-01-01","2016-12-31"
"COLORTTH","The Higgs: A colored View from the Top at ATLAS","Reinhild Fatima Yvonne Peters","THE UNIVERSITY OF MANCHESTER","""With the ground-breaking discovery of a new, Higgs-like boson on July 4th, 2012, by the CMS and ATLAS collaborations at CERN, a new era of particle physics has begun. The discovery is the first step in answering an unsolved problem in particle physics, the question how fundamental bosons and fermions acquire their mass. One of the major goals in collider physics in the next few years will be the deeper insight into the nature of the new particle, its connection to the known fundamental particles and possible extensions beyond the standard model (SM) of particle physics.

My project aims at a particular interesting field to study, the relation of the new particle with the heaviest known elementary particle, the top quark. I aim to develop new, innovative techniques and beyond state-of-the-art methods to extract the Yukawa coupling between the top quark and the Higgs boson, which is expected to be of the order of one - much higher than that of any other quark. I will analyse the only process where the top-Higgs Yukawa coupling can be measured, in associated production of top quark pairs and a Higgs boson. The Higgs boson mainly decays into a pair of b-quarks. This is one of the most challenging channels at the LHC, as huge background processes from gluon splitting contribute.  In particular, I will develop and study color flow variables, which provide a unique, powerful technique to distinguish color singlet Higgs bosons from the main background, color octet gluons.

The ultimate goal of the project is the first measurement of the top-Higgs Yukawa coupling and its confrontation with SM and beyond SM Higgs boson models, resulting in an unprecedented insight into the fundamental laws of nature.

The LHC will soon reach a new energy frontier of 13 TeV starting in 2014. This new environment will provide never seen opportunities to study hints of new physics and precisely measure properties of the newly found particle. This sets the stage for the project.""","1163755","2014-02-01","2019-01-31"
"COLOURATOM","Colouring Atoms in 3 Dimensions","Sara Bals","UNIVERSITEIT ANTWERPEN","""Matter is a three dimensional (3D) agglomeration of atoms. The properties of materials are determined by the positions of the atoms, their chemical nature and the bonding between them. If we are able to determine these parameters in 3D, we will be able to provide the necessary input for predicting the properties and we can guide the synthesis and development of new nanomaterials.

The aim of this project is therefore to provide a complete 3D characterisation of complex hetero-nanosystems down to the atomic scale. The combination of advanced aberration corrected electron microscopy and novel 3D reconstruction algorithms is envisioned as a groundbreaking new approach to quantify the position AND the colour (chemical nature and bonding) of each individual atom in 3D for any given nanomaterial.

So far, only 3D imaging at the atomic scale was carried out for model-like systems. Measuring the position and the colour of the atoms in a complex nanomaterial can therefore be considered as an extremely challenging goal that will lead to a wealth of new information. Our objectives will enable 3D strain measurements at the atomic scale, localisation of atomic vacancies and interface characterisation in hetero-nanocrystals or hybrid soft-hard matter nanocompounds. Quantification of the oxidation states of surface atoms and of 3D surface relaxation will yield new insights concerning preferential functionalities.

Although these goals already go beyond the state-of-the-art, we plan to break fundamental limits and completely eliminate the need to tilt the sample for electron tomography. Especially for beam sensitive materials, this technique, so-called """"multi-detector stereoscopy"""", can be considered as a groundbreaking approach to obtain 3D information at the atomic scale. As an ultimate ambition, we will investigate the dynamic behaviour of ultra-small binary clusters.""","1461466","2013-12-01","2018-11-30"
"CombiCompGeom","Combinatorial Aspects of Computational Geometry","Natan Rubin","BEN-GURION UNIVERSITY OF THE NEGEV","The project focuses on the interface between computational and combinatorial geometry. 
Geometric problems emerge in a variety of computational fields that interact with the physical world.
The performance of geometric algorithms is determined by the description complexity of their underlying combinatorial structures. Hence, most theoretical challenges faced by computational geometry are of a distinctly combinatorial nature.

In the past two decades, computational geometry has been revolutionized by the powerful combination of random sampling techniques with the abstract machinery of geometric arrangements. These insights were used, in turn, to establish state-of-the-art results in combinatorial geometry. Nevertheless, a number of fundamental problems remained open and resisted numerous attempts to solve them. 

Motivated by the recent breakthrough results, in which the PI played a central role, we propose two exciting lines of study with the potential to change the landscape of this field.

The first research direction concerns the complexity of Voronoi diagrams -- arguably the most common structures in computational geometry. 

The second direction concerns combinatorial and algorithmic aspects of geometric intersection structures, including some fundamental open problems in geometric transversal theory. Many of these questions are motivated by geometric variants of general covering and packing problems, and all efficient approximation schemes for them must rely on the intrinsic properties of geometric graphs and hypergraphs.

Any progress in responding to these challenges will constitute a major breakthrough in both computational and combinatorial geometry.","1303750","2016-09-01","2021-08-31"
"COMBINEPIC","Elliptic Combinatorics: Solving famous models from combinatorics, probability and statistical mechanics, via a transversal approach of special functions","Kilian RASCHEL","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","I am willing to solve several well-known models from combinatorics, probability theory and statistical mechanics: the Ising model on isoradial graphs, dimer models, spanning forests, random walks in cones, occupation time problems. Although completely unrelated a priori, these models have the common feature of being presumed “exactly solvable” models, for which surprising and spectacular formulas should exist for quantities of interest. This is captured by the title “Elliptic Combinatorics”, the wording elliptic referring to the use of special functions, in a broad sense: algebraic/differentially finite (or holonomic)/diagonals/(hyper)elliptic/ hypergeometric/etc.

Besides the exciting nature of the models which we aim at solving, one main strength of our project lies in the variety of modern methods and fields that we cover: combinatorics, probability, algebra (representation theory), computer algebra, algebraic geometry, with a spectrum going from applied to pure mathematics.

We propose in addition two major applications, in finance (Markovian order books) and in population biology (evolution of multitype populations). We plan to work in close collaborations with researchers from these fields, to eventually apply our results (study of extinction probabilities for self-incompatible flower populations, for instance).","1242400","2018-02-01","2023-01-31"
"COMBINISO","Quantitative picture of interactions between climate, hydrological cycle and stratospheric inputs in Antarctica over the last 100 years via the combined use of all water isotopes","Amaelle Israel","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Climate change and associated water cycle modifications have a strong impact on polar ice sheets through their influence on the global sea-level. The most promising tool for reconstructing temperature and water cycle evolution in Antarctica is to use water isotopic records in ice cores. Still, interpreting these records is nowadays limited by known biases linked to a too simple description of isotopic fractionations and cloud microphysics. Another key issue in this region is the stratosphere-troposphere flux influencing both the chemistry of ozone and decadal climate change. Data are lacking for constraining such flux even on the recent period (100 years). COMBINISO aims at making use of innovative methods combining measurements of the 5 major water isotopes (H217O, H218O, HTO, HDO, H2O) and global modelling to address the following key points: 1- Provide a strongly improved physical frame for interpretation of water isotopic records in polar regions; 2- Provide a quantitative picture of Antarctica temperature changes and links with the tropospheric water cycle prior to the instrumental period; 3- Quantify recent variability of the stratosphere water vapor input.
The proposed method, based on strong experimental – modelling interaction, includes innovative tools such as (1) the intensive use of the recently developed triple isotopic composition of oxygen in water for constraining water isotopic fractionation, hydrological cycle organisation and potentially stratospheric water input, (2) the development of a laser spectroscopy instrument to accurately measure this parameter in water vapour, (3) modelling development including stratospheric tracers (e.g. HTO and 10Be) in addition to water isotopes in Atmospheric General Circulation Models equipped with a detailed description of the stratosphere, (4) a first documentation of climate, hydrological cycle and stratospheric input in Antarctica through combined measurements of isotopes in ice cores for the last 100 years.","1869950","2013-01-01","2017-12-31"
"COMBIPATTERNING","Combinatorial Patterning of Particles for High Density Peptide Arrays","Alexander Nesterov-Mueller","KARLSRUHER INSTITUT FUER TECHNOLOGIE","We want to use selective laser melting to pattern a substrate with different solid micro particles at a density of 1 million spots per cm2. First, a homogeneous particle layer is deposited on a substrate and a pattern of micro spots of melted matrix is generated by laser radiation. Then, non-melted particles are blown away. Embedded within the particles are different chemically reactive amino acid derivatives that will start coupling to very small synthesis sites upon melting the particle pattern in an oven. This is done once all of the 20 different amino acid particles have been glued by laser patterning to the surface. Washing away uncoupled material, removing Fmoc protecting group, and repeating the patterning steps according to standard Merrifield synthesis, leads to the combinatorial synthesis of very high-density peptide arrays. The main objective of this proposal is to develop this method up to the level of a semi-automated synthesis machine. In addition, we will use the manufactured very high-density peptide arrays to readout the information that is deposited in the immune system, i.e. find a peptide binder for every one of the 200-500 antibody species that patrol the serum of an individual in elevated levels. These experiments might lead to novel tools to find out the causes of hitherto enigmatic diseases because then we might be able to correlate antibody patterns with disease status without knowing in advance the disease-specific antibodies. Beyond the life sciences, we want to embed 10.000 peptides per cm2 within an insulating layer of alkane thiols, each on a different gold pad of a specially designed screening chip. Then, we could readout I/V characteristics of individual peptide species, and eventually find peptide-based diodes. These could be modified in their sequence and screened again for better performance. This evolution-inspired screening approach might lead to novel materials that could be used in fuel cells.","1494600","2011-11-01","2016-10-31"
"CombiTop","New Interactions of Combinatorics through Topological Expansions, at the crossroads of Probability, Graph theory, and Mathematical Physics","Guillaume CHAPUY","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""The purpose of this project is to use the ubiquitous nature of certain combinatorial topological objects called maps in order to unveil deep connections between several areas of mathematics. Maps, that describe the embedding of a graph into a surface, appear in probability theory, mathematical physics, enumerative geometry or graph theory, and different combinatorial viewpoints on these objects have been developed in connection with each topic. The originality of our project will be to study these approaches together and to unify them.

The outcome will be triple, as we will:
1. build a new, well structured branch of combinatorics  of which many existing results in different areas of enumerative and algebraic combinatorics are only first fruits;
2. connect and unify several aspects of the domains related to it, most importantly between probability and integrable hierarchies thus proposing new directions, new tools and new results for each of them;
3. export the tools of this unified framework to reach at new applications, especially in random graph theory and in a rising domain of algebraic combinatorics related to Tamari lattices.

The methodology to reach the unification will be the study of some strategic interactions at different places involving topological expansions, that is to say, places where enumerative problems dealing with maps appear and their genus invariant  plays a natural role, in particular:  1. the combinatorial theory of maps developped by the ""French school"" of combinatorics, and the study of random maps; 2. the combinatorics of Fermions underlying the theory of  KP and 2-Toda hierarchies; 3; the  Eynard-Orantin ""topological recursion"" coming from mathematical physics.

We present some key set of tasks in view of relating these different topics together. The pertinence of the approach is demonstrated by recent research of the principal investigator.""","1086125","2017-03-01","2022-02-28"
"COMBOS","Collective phenomena in quantum and classical many body systems","Alessandro Giuliani","UNIVERSITA DEGLI STUDI ROMA TRE","The collective behavior of quantum and classical many body systems such as ultracold atomic gases, nanowires, cuprates and micromagnets are currently subject of an intense experimental and theoretical research worldwide. Understanding the fascinating phenomena of Bose-Einstein condensation, Luttinger liquid vs non-Luttinger liquid behavior, high temperature superconductivity, and spontaneous formation of periodic patterns in magnetic systems, is an exciting challenge for theoreticians. Most of these phenomena are still far from being fully understood, even from a heuristic point of view. Unveiling the exotic properties of such systems by rigorous mathematical analysis is an important and difficult challenge for mathematical physics. In the last two decades, substantial progress has been made on various aspects of many-body theory, including Fermi liquids, Luttinger liquids, perturbed Ising models at criticality, bosonization, trapped Bose gases and spontaneous formation of periodic patterns. The techniques successfully employed in this field are diverse, and range from constructive renormalization group to functional variational estimates. In this research project we propose to investigate a number of statistical mechanics models by a combination of different mathematical methods. The objective is, on the one hand, to understand crossover phenomena, phase transitions and low-temperature states with broken symmetry, which are of interest in the theory of condensed matter and that we believe to be accessible to the currently available methods; on the other, to develop new techiques combining different and complementary methods, such as multiscale analysis and localization bounds, or reflection positivity and cluster expansion, which may be useful to further progress on important open problems, such as Bose-Einstein condensation, conformal invariance in non-integrable models, existence of magnetic or superconducting long range order.","650000","2010-01-01","2014-12-31"
"COMCOM","Communication and Computation - Two Sides of One Tapestry","Michael Christoph Gastpar","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Networks have been studied in depth for several decades, but one aspect has received little attention: Interference. Most networks use clever algorithms to avoid interference, and this strategy has proved effective for traditional supply-chain or wired communication networks. However, the emergence of wireless networks revealed that simply avoiding interference leads to significant performance loss. A wealth of cooperative communication strategies have recently been developed to address this issue. Two fundamental roadblocks are emerging: First, it is ultimately unclear how to integrate cooperative techniques into the larger fabric of networks (short of case-by-case redesigns); and second, the lack of source/channel separation in networks (i.e., more bits do not imply better end-to-end signal quality) calls for ever more specialized cooperative techniques.

This proposal advocates a new understanding of interference as computation: Interference garbles together inputs to produce an output. This can be thought of as a certain computation, perhaps subject to noise or other stochastic effects. The proposed work will develop strategies that permit to exploit this computational potential. Building on these ``computation codes,'' an enhanced physical layer is proposed: Rather than only forwarding bits, the revised physical layer can also forward functions from several transmitting nodes to a receiver, much more efficiently than the full information. Near-seamless integration into the fabric of existing network architectures is thus possible, providing a solution for the first roadblock. In response to the second roadblock, computation codes suggest new computational primitives as fundamental currencies of information.","1776473","2011-05-01","2016-04-30"
"COMEDIA","Complex Media Investigation with Adaptive Optics","Sylvain Hervé Gigan","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""Wave propagation in complex (disordered) media stretches our knowledge to the limit in many different fields of physics. It has important applications in seismology, acoustics, radar, and condensed matter. It is a problem of large fundamental interest, notably for the study of Anderson localization.
In optics, it is of great importance in photonic devices, such as photonic crystals, plasmonic structures or random lasers. It is also at the heart of many biomedical-imaging issues: scattering ultimately limits the depth and resolution of all imaging techniques.
We have recently demonstrated that wavefront shaping –i.e. adaptive optics applied to complex media- is the tool of choice to match and address the huge complexity of this problem in optics. The COMEDIA project aims at developing a novel wavefront shaping toolbox, addressing both spatial and spectral degrees of freedom of light. Thanks to this toolbox, we plan to fulfill the following objectives:
1) A full spatiotemporal control of the optical field in a complex environment,
2) Breakthrough results in imaging and nano-optics,
3) Original answers to some of the most intriguing fundamental questions in mesoscopic physics.""","1497000","2011-11-01","2016-10-31"
"COMET","foundations of COmputational similarity geoMETtry","Michael Bronstein","UNIVERSITA DELLA SVIZZERA ITALIANA","""Similarity is one of the most fundamental notions encountered in problems practically in every branch of science, and is especially crucial in image sciences such as computer vision and pattern recognition. The need to quantify similarity or dissimilarity of some data is central to broad categories of problems involving comparison, search, matching, alignment, or reconstruction. The most common way to model a similarity is using metrics (distances). Such constructions are well-studied in the field of metric geometry, and there exist numerous computational algorithms allowing, for example, to represent one metric using another by means of isometric embeddings.
However, in many applications such a model appears to be too restrictive: many types of similarity are non-metric; it is not always possible to model the similarity precisely or completely e.g. due to missing data; some objects might be mutually incomparable e.g. if they are coming from different modalities. Such deficiencies of the metric similarity model are especially pronounced in large-scale computer vision, pattern recognition, and medical imaging applications.
The ambitious goal of this project is to introduce a paradigm shift in the way we model and compute similarity. We will develop a unifying framework of computational similarity geometry that extends the theoretical metric model, and will allow developing efficient numerical and computational tools for the representation and computation of generic similarity models. The methods will be developed all the way from mathematical concepts to efficiently implemented code and will be applied to today’s most important and challenging problems in Internet-scale computer vision and pattern recognition, shape analysis, and medical imaging.""","1495020","2012-10-01","2017-09-30"
"COMFUS","Computational Methods for Fusion Technology","Santiago Ignacio Badia Rodríguez","CENTRE INTERNACIONAL DE METODES NUMERICS EN ENGINYERIA","The simulation of multidisciplinary applications use very often a combination of heterogeneous and disjoint numerical techniques that are hard to put together by the user, and whose mathematical foundation is obscure. An example of this situation is the numerical modeling of the physical processes taking place in nuclear fusion reactors. This problem, which can be modeled by a set of partial differential equations, is extremely challenging. It involves (essentially) fluid mechanics, electromagnetics, thermal radiation and neutronics. The most common numerical approaches to each of these problems separately are very different and their coupling is a hard and inefficient task.

Our main objective in this proposal is to develop and analyze a unified numerical framework based on stabilized finite element methods based on multi-scale decompositions capable to simulate all the physical processes taking place in nuclear fusion technology. The project aims at giving a substantial contribution to the numerical approximation of every physical process as well as efficient coupling techniques for the multiphysics problems.

The development of the numerical formulations we propose and their application require mastering different physics, designing numerical approximations for these different physical problems, analyzing mathematically the resulting methods, implementing them in an efficient way in parallel platforms and understanding the results and drawing conclusions, both from a physical and from an engineering perspective. Advanced research in physical modeling, numerical approximations, mathematical analysis and computer implementation are the keys to meeting these objectives.

The successful implementation of the project will provide advanced numerical techniques for the simulation of the processes taking place in a fusion reactor. A deliverable product of the project will be a unified finite element software package that will be an extremely valuable tool.","1320000","2011-01-01","2015-12-31"
"COMITAC","An integrated geoscientific study of the thermodynamics and composition of the Earth's core-mantle interface","James Wookey","UNIVERSITY OF BRISTOL","The core-mantle interface is the central cog in the Earth&apos;s titanic heat engine. As the boundary between the two major convecting parts of the Earth system (the solid silicate mantle and the liquid iron outer core) the properties of this region have a profound influence on the thermochemical and dynamic evolution of the entire planet, including tectonic phenomena at the surface. Evidence from seismology shows that D&quot; (the lowermost few hundred kilometres of the mantle) is strongly heterogeneous in temperature, chemistry, structure and dynamics; this may dominate the long term evolution of the Earth&apos;s magnetic field and the morphology of mantle convection and chemical stratification, for example. Mapping and characterising this heterogeneity requires a detailed knowledge of the properties of the constituents and dynamics of D&quot;; this is achievable by resolving its seismic anisotropy. The observation of anisotropy in the shallow lithosphere was an important piece of evidence for the theory of plate tectonics; now such a breakthrough is possible for the analogous deep boundary. We are at a critical juncture where developments in modelling strain in the mantle, petrofabrics and seismic wave propagation can be combined to produce a new generation of integrated models of D&quot;, embodying more complete information than any currently available. I propose a groundbreaking project to build such multidisciplinary models and to produce the first complete image of lowermost mantle anisotropy using the best available global, high resolution seismic dataset. The comparison of the models with these data is the key to making a fundamental improvement in our understanding of the thermodynamics and composition of the core-mantle interface, and illuminating its role in the wider Earth system.","1639615","2009-09-01","2015-08-31"
"CoMMaD","Computational Molecular Materials Discovery","Kim JELFS","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","The objective of the project is to develop a computational approach to accelerate the discovery of molecular materials. These materials will include porous molecules, small organic molecules and macromolecular polymers, which have application as a result of either their porosity or optoelectronic properties. The applications that will be targeted include in molecular separations, sensing, (photo)catalysis and photovoltaics. To achieve my aims, I will screen libraries of building blocks through a combination of techniques including evolutionary algorithms and machine learning. Through the application of cheminformatics algorithms, I will target the most promising libraries, assess synthetic diversity and accessibility and analyse structure-property relationships. I will develop software that will predict the (macro)molecular structures and properties; the molecular property screening calculations will include void characterisation, binding energies, diffusion barriers, local assembly, charge transport and energy level assessment. A consideration of synthetic accessibility at every stage will be central to my approach, which will ensure the realisation of our predicted targets. I have several synthetic collaborators who can provide pathways to synthetic realisation. Improved materials in this field have the potential to either reduce our energy needs or provide renewable energy, helping the EU meet the targets of the 2030 Energy Strategy.","1499390","2018-04-01","2023-03-31"
"COMMOTION","Communication between Functional Molecules using Photocontrolled Ions","Nathan Mcclenaghan","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The goal of COMMOTION is to establish a strategy whereby functional molecular devices (e.g. photo-/electroactive) can communicate with one another in solution and in organized, self-assembled media (biotic and abiotic). Despite intense research, no single strategy has been shown to satisfactorily connect artificial molecular components in networks. This is perhaps the greatest hurdle to overcome if implementation of artificial molecular devices and sophisticated molecule-based arrays are to become a reality. In this project, communication between distant sites / molecules will be based on the use of photoejected ions in solution and organized media (membranes, thin films, nanostructured hosts, micellar nanodomains). Ultimately this will lead to coded information transfer through ion movement, signalled by fluorescent reporter groups and induced by photomodulated receptor groups in small photoactive molecules. Integrated photonic and ionic processes operate efficiently in the biological world for the transfer of information and multiplexing distinct functional systems. Application in small artificial systems, combining “light-in, ion-out” (photoejection of an ion) and “ion-in, light-out” processes (ion-induced fluorescence), has great potential in a bottom-up approach to nanoscopic components and sensors and understanding and implementing logic operations in biological systems. Fast processes of photoejection and migration of ions will be studied in real-time (using time-resolved photophysical techniques) with high spatial resolution (using fluorescence confocal microscopy techniques) allowing evaluation of the versatility of this strategy in the treatment and transfer of information and incorporation into devices. Additionally, an understanding of the fundamental events implicated during the process of photoejection / decomplexion of coordinated ions and ion-exchange processes at membrane surfaces will be obtained.","1250000","2008-09-01","2013-08-31"
"COMNFT","Communication Using the Nonlinear Fourier Transform","Mansoor ISVAND YOUSEFI","INSTITUT MINES-TELECOM","High-speed optical fiber networks form the backbone of the information and communication technologies, including the Internet. More than 99% of the Internet data traffic is carried by a network of global optical fibers. Despite their great importance, today's optical fiber networks face a looming capacity crunch: The achievable rates of all current technologies characteristically vanish at high input powers due to distortions that arise from fiber nonlinearity. The solution of this long-standing complex problem has become the holy grail of the field of the optical communication.

The aim of this project is to develop a novel foundation for optical fiber communication based on the nonlinear Fourier transform (NFT). The NFT decorrelates signal degrees-of-freedom in optical fiber, in much the same way that the conventional Fourier transform does for linear systems. My collaborators and I have recently proposed nonlinear frequency-division multiplexing (NFDM) based on the NFT, in which the information is encoded in the generalized frequencies and their spectral amplitudes (similar to orthogonal frequency-division multiplexing). Since distortions such as inter-symbol and inter-channel interference are absent in NFDM, it achieves data rates higher than conventional methods. The objective of this proposal is to advance NFDM to the extent that it can be built in practical large-scale systems, thereby overcoming the limitation that fiber nonlinearity sets on the transmission rate of the communication networks. The proposed research relies on novel methodology and spans all aspects of the NFDM system design, including determining the fundamental information-theoretic limits, design of the NFDM transmitter and receiver, algorithms and implementations. 

The feasibility of the project is manifest in preliminary proof-of-concepts in small examples and toy models, PI's leadership and track-record in the field, as well as the ideal research environment.","1499180","2019-05-01","2024-04-30"
"COMOSYEL","Complex Molecular-scale Systems for NanoElectronics and NanoPlasmonics","Erik Dujardin","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","COMOSYEL aims at designing complex nanometric and molecular systems to process electronic or optical information from the macroscopic to the molecular scale. It proposes two specific, unconventional approaches to molecular electronics and plasmonics and the development of two multidisciplinary technical toolkits, one in bio-inspired chemistry and one in surface nanopatterning by liquid nanodispensing that will support the first two topics, and eventually become a part of the team's culture for future research developments. (1) Graphene-based nanoelectronics is an experimental implementation of mono-molecular electronics concept using graphene to bridge the macroscopic world to the molecular scale. This topic aims at encoding and processing electronic information in a single complex molecular system in order to achieve complex logic functions. (2) Self-assembled nanoplasmonics aims at developing a molecular plasmonics concept. Here, complex networks of sub-20nm crystalline metallic nanoparticle chains are produced and interfaced to convert photons to plasmons and ultimately confine, enhance and route light energy from a conventional light source to an arbitrary chromophore on a substrate. (3) Bio-inspired nanomaterials chemistry will be the main synthetic tool to produce new multifunctional nanostructured materials able to address and collect information from/to the macroscopic world to/from the single molecule level. Both morphogenesis and self-assembly will be explored to better control size and shape of nano-objects and the topology of higher-order architectures. (4) Liquid nanodispensing is a promising tool to interface nanosized/molecular sized systems with both lithographically produced host structures and individual molecular systems. A nanoscale liquid dispensing technique derived from AFM combines resolution and versatility and will be pushed to its extreme to master the deposition of nanoobjects onto a substrate or a precise modification of surfaces.","1439712","2008-08-01","2013-12-31"
"COMPASS","Control for Orbit Manoeuvring through Perturbations for Application to Space Systems","Camilla Colombo","POLITECNICO DI MILANO","Space benefits mankind through the services it provides to Earth. Future space activities progress thanks to space transfer and are safeguarded by space situation awareness. Natural orbit perturbations are responsible for the trajectory divergence from the nominal two-body problem, increasing the requirements for orbit control; whereas, in space situation awareness, they influence the orbit evolution of space debris that could cause hazard to operational spacecraft and near Earth objects that may intersect the Earth. However, this project proposes to leverage the dynamics of natural orbit perturbations to significantly reduce current extreme high mission cost and create new opportunities for space exploration and exploitation.
The COMPASS project will bridge over the disciplines of orbital dynamics, dynamical systems theory, optimisation and space mission design by developing novel techniques for orbit manoeuvring by “surfing” through orbit perturbations. The use of semi-analytical techniques and tools of dynamical systems theory will lay the foundation for a new understanding of the dynamics of orbit perturbations. We will develop an optimiser that progressively explores the phase space and, though spacecraft parameters and propulsion manoeuvres, governs the effect of perturbations to reach the desired orbit. It is the ambition of COMPASS to radically change the current space mission design philosophy: from counteracting disturbances, to exploiting natural and artificial perturbations.
COMPASS will benefit from the extensive international network of the PI, including the ESA, NASA, JAXA, CNES, and the UK space agency. Indeed, the proposed idea of optimal navigation through orbit perturbations will address various major engineering challenges in space situation awareness, for application to space debris evolution and mitigation, missions to asteroids for their detection, exploration and deflection, and in space transfers, for perturbation-enhanced trajectory design.","1499021","2016-08-01","2021-07-31"
"COMPBIOMAT","Computing Biomaterials","Wilfried Weber","ALBERT-LUDWIGS-UNIVERSITAET FREIBURG","The objective of the proposal is to establish the foundations of a new discipline at the intersections of Materials Science, Synthetic Biology and Computer Science: the development of Computing Biomaterials. Computing biomaterials will be able to perceive multiple input signals, to process these signals by complex computational operations and to produce a corresponding output signal. The design principle of computing biomaterials will be inspired by computer science, the molecular control elements will be derived from synthetic biology and the overall framework for the construction of biomaterials will rely on materials science.
The design principle will be hierarchical: at the basis, synthetic biology tools will act as sensor, processor and actuator. These tools will be integrated into biomaterials to build logic gates that perceive different input signals, process these signals by Boolean algebra and produce a corresponding readout. By the functional interconnected of several such gates, we will construct integrated biomaterial circuits that perform complex computational operations.
The fundamental and generally applicable design principles as established in this proposal, will enable the rapid and predictable synthesis of integrated biomaterial circuits that function as integrated sensor, processor and actuator with custom-tailored performance and will show a vast application potential in emerging disciplines like biomedicine or microsystems engineering.","1499040","2010-12-01","2015-11-30"
"COMPCAMERAANALYZ","Understanding Designing and Analyzing Computational Cameras","Anat Levin","WEIZMANN INSTITUTE OF SCIENCE LTD","Computational cameras go beyond 2D images and allow the extraction of more dimensions from the visual world such as depth, multiple viewpoints and multiple illumination conditions. They also allow us to overcome some of the traditional photography challenges such as defocus blur, motion blur, noise and resolution. The increasing variety of computational cameras is raising the need for a meaningful comparison across camera types. We would like to understand which cameras are better for specific tasks, which aspects of a camera make it better than others and what is the best performance we can hope to achieve.

Our 2008 paper introduced a general framework to address the design and analysis of computational cameras. A camera is modeled as a linear projection in ray space. Decoding the camera data then deals with inverting the linear projection. Since the number of sensor measurements is usually much smaller than the number of rays, the inversion must be treated as a Bayesian inference problem accounting for prior knowledge on the world.

Despite significant progress which has been made in the recent years, the space of computational cameras is still far from being understood.
Computational camera analysis raises the following research challenges: 1) What is a good way to model prior knowledge on ray space? 2) Seeking efficient inference algorithms and robust ways to decode the world from the camera measurements. 3) Evaluating the expected reconstruction accuracy of a given camera. 4) Using the expected reconstruction performance for evaluating and comparing camera types. 5) What is the best camera? Can we derive upper bounds on the optimal performance?

We propose research on all aspects of computational camera design and analysis. We propose new prior models which will significantly simplify the inference and evaluation tasks. We also propose new ways to bound and evaluate computational cameras with existing priors.","756845","2010-12-01","2015-11-30"
"COMPENZYMEEVOLUTION","Harnessing Proto-Enzymes for Novel Catalytic Functions","Shina Caroline Lynn Kamerlin","UPPSALA UNIVERSITET","Enzymes are Nature’s catalysts, reducing the timescales of the chemical reactions that drive life from millions of years to seconds. There is also great scope for enzymes as biocatalysts outside the cell, from therapeutic and synthetic applications, to bioremediation and even for the generation of novel biofuels. Recent years have seen several impressive breakthroughs in the design of artificial enzymes, particularly through experimental studies that iteratively introduce random mutations to refine existing systems until a property of interest is observed (directed evolution), as well as examples of de novo enzyme design using combined in silico / in vitro approaches. However, the tremendous catalytic proficiencies of naturally occurring enzymes are, as yet, unmatched by any man made system, in no small part due the vastness of the sequence space that needs navigating and the almost surgical precision by which enzymatic catalysis is regulated. The proposed work aims to combine state of the art computational approaches capable of consistently reproducing the catalytic activities of both wild-type and mutant enzymes with novel screening approaches for predicting mutation hotspots, in order to redesign selected showcase systems. Specifically, we aim to (1) map catalytic promiscuity in the alkaline phosphatase superfamily, using the existing multifunctionality of these enzymes as a training set for the introduction of novel functionality, and (2) computationally design enantioselective enzymes, a problem which is of particular importance to the pharmaceutical industry due to the role of chirality in drug efficacy. The resulting theoretical constructs will be subjected to rigorous testing by our collaborators, providing a feedback loop for further design effort and methodology development. In this way, we plan to push existing theoretical tools to the limit in order to bridge the gap that exists between the catalytic proficiencies of biological and man-made catalysts.","1497667","2012-10-01","2017-09-30"
"COMPLEX REASON","The Parameterized Complexity of Reasoning Problems","Stefan Szeider","TECHNISCHE UNIVERSITAET WIEN","Reasoning, to derive conclusions from facts, is a fundamental task in Artificial Intelligence, arising in a wide range of applications from Robotics to Expert Systems. The aim of this project is to devise new efficient algorithms for real-world reasoning problems and to get new insights into the question of what makes a reasoning problem hard, and what makes it easy. As key to novel and groundbreaking results we propose to study reasoning problems within the framework of Parameterized Complexity, a new and rapidly emerging field of Algorithms and Complexity. Parameterized Complexity takes structural aspects of problem instances into account which are most significant for empirically observed problem-hardness. Most of the considered reasoning problems are intractable in general, but the real-world context of their origin provides structural information that can be made accessible to algorithms in form of parameters. This makes Parameterized Complexity an ideal setting for the analysis and efficient solution of these problems. A systematic study of the Parameterized Complexity of reasoning problems that covers theoretical and empirical aspects is so far outstanding. This proposal sets out to do exactly this and has therefore a great potential for groundbreaking new results. The proposed research aims at a significant impact on the research culture by setting the grounds for a closer cooperation between theorists and practitioners.","1421130","2010-01-01","2014-12-31"
"COMPLEXDATA","Statistics for Complex Data: Understanding Randomness, Geometry and Complexity with a view Towards Biophysics","Victor Michael Panaretos","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The ComplexData project aims at advancing our understanding of the statistical treatment of varied types of complex data by generating new theory and methods, and to obtain progress in concrete current biophysical problems through the implementation of the new tools developed. Complex Data constitute data where the basic object of observation cannot be described in the standard Euclidean context of statistics, but rather needs to be thought of as an element of an abstract mathematical space with special properties. Scientific progress has, in recent years, begun to generate an increasing number of new and complex types of data that require statistical understanding and analysis. Four such types of data that are arising in the context of current scientific research and that the project will be focusing on are: random integral transforms, random unlabelled shapes, random flows of functions, and random tensor fields. In these unconventional contexts for statistics, the strategy of the project will be to carefully exploit the special aspects involved due to geometry, dimension and randomness in order to be able to either adapt and synthesize existing statistical methods, or to generate new statistical ideas altogether. However, the project will not restrict itself to merely studying the theoretical aspects of complex data, but will be truly interdisciplinary. The connecting thread among all the above data types is that their study is motivated by, and will be applied to concrete practical problems arising in the study of biological structure, dynamics, and function: biophysics. For this reason, the programme will be in interaction with local and international contacts from this field. In particular, the theoretical/methodological output of the four programme research foci will be applied to gain insights in the following corresponding four application areas: electron microscopy, protein homology, DNA molecular dynamics, brain imaging.","681146","2011-06-01","2016-05-31"
"COMPLEXLIGHT","Light and complexity","Claudio Conti","CONSIGLIO NAZIONALE DELLE RICERCHE","The project is aimed at funding a multi-disciplinary laboratory on nonlinear optics and photonics in soft-colloidal materials and on “complex lightwave systems”. A team of talented young researchers, divided among experiments, theory, parallel computation and nano-fabrication is involved. The proposed research will foster several breakthrough discoveries from soft-matter to biophysics, from nonlinear and integrated optics to the science of complexity and cryptography. The underlying vision is driven by the physics of complex systems, those displaying a large number of thermodynamically equivalent states and emergent properties. There are 4 original and high-impact activities, which explore applicative potentialities: 1) sub-wavelength light filaments in soft- and bio-matter; 2) lasers in soft-matter and bio-tissues; 3) control of soft-matter lasers by light filaments; 4) complex lightwave systems, encryption by nano-structured disordered lasers. Activity 1 will lead to ultra-thin re-addressable light beams (sub-wavelength spatial solitons) propagating in soft- and bio-matter that can be used in laser-surgery, matter manipulation and able to guide high power laser pulses; activity 2 attains novel structural diagnostic techniques in bone tissue surpassing limits of nuclear magnetic resonance imaging, and assesses the field of lasers in soft-materials; activity 3 will demonstrate the control of self-organization processes in soft-matter by light filaments probed by laser emission; activity 4 is based on specific features mutuated from spin-glass theory, and will realize a novel cryptographic technique superior to chaotic systems in terms of security. Activity 1 and 2 are propaedeutic to the others. The team is composed by the Principal Investigator (P.I.), 4 post-doctoral researchers and 3 Ph.D. students. The budget will be used for paying the P.I., two post-doctoral positions, laser sources, high performance computing facilities, and instrumentation.","1085000","2008-05-01","2013-04-30"
"complexNMR","Structural Dynamics of Protein Complexes by Solid-State NMR","Józef Romuald Lewandowski","THE UNIVERSITY OF WARWICK","Multidrug resistant bacteria that render worthless the current arsenal of antibiotics are a growing global problem. This grave challenge could be tackled by polyketide synthases (PKSs), which are gigantic modular enzymatic assembly lines for natural products. PKSs could be developed for industry to produce chemically difficult to synthesize drugs, but cannot be harnessed until we understand how they work on the molecular level. However, such understanding is missing because we cannot easily investigate large complexes with current structural biology and modeling methods. A key puzzle is how the function of these multicomponent systems emerges from atomic-scale interactions of their parts. Solving this puzzle requires a holistic approach involving measuring and modeling the relevant interacting parts together. 
Our goal is to develop a multidisciplinary approach rooted in solid and solution state NMR that will make possible studies of complexes from PKSs. The two main challenges for the NMR of PKSs are increasing sensitivity and resolution. Recent innovations from our lab allow application of solid-state to study large complexes in 2–10 nanomole quantities. Building on this approach, with a protein-antibody complex as a test case, we will develop new NMR methods that will enable a study of structure and motions of domains in complexes. We will probe, for the first time, the structural dynamics of PKSs of enacyloxin and gladiolin, which are antibiotics against life-threatening multidrug resistant hospital-acquired Acinetobacter baumannii infections and tuberculosis. These studies will guide rational engineering of the PKSs to enable synthetic biology approaches to produce new antibiotics. 
If successful, this project will go beyond the state of the art by: enabling studies of unknown proteins in large complexes and providing unique insights into novel mechanisms for controlling biosynthesis in PKSs, turning them into truly programmable synthetic biology devices.","1999044","2015-05-01","2020-04-30"
"ComplexSwimmers","Biocompatible and Interactive Artificial Micro- and Nanoswimmers and Their Applications","Giovanni Volpe","GOETEBORGS UNIVERSITET","Microswimmers, i.e., biological and artificial microscopic objects capable of self-propulsion, have been attracting a growing interest from the biological and physical communities. From the fundamental side, their study can shed light on the far-from-equilibrium physics underlying the adaptive and collective behavior of biological entities such as chemotactic bacteria and eukaryotic cells. From the more applied side, they provide tantalizing options to perform tasks not easily achievable with other available techniques, such as the targeted localization, pick-up and delivery of microscopic and nanoscopic cargoes, e.g., in drug delivery, bioremediation and chemical sensing.
However, there are still several open challenges that need to be tackled in order to achieve the full scientific and technological potential of microswimmers in real-life settings. The main challenges are: (1) to identify a biocompatible propulstion mechanism and energy supply capable of lasting for the whole particle life-cycle; (2) to understand their behavior in complex and crowded environments; (3) to learn how to engineer emergent behaviors; and (4) to scale down their dimensions towards the nanoscale. 
This project aims at tackling these challenges by developing biocompatible microswimmers capable of elaborate behaviors, by engineering their performance when interacting with other particles and with a complex environment, and by developing working nanoswimmers.
To achieve these goals, we have laid out a roadmap that will lead us to push the frontiers of the current understanding of active matter both at the mesoscopic and at the nanoscopic scale, and will permit us to develop some technologically disruptive techniques, namely, targeted delivery of cargoes within complex environments, which is of interest for drug delivery and bioremediation, and efficient sorting of chiral nanoparticles, which is of interest for biomedical and pharmaceutical applications.","1497500","2016-09-01","2021-08-31"
"COMPNET","Dynamics and Self-organisation in Complex Cytoskeletal Networks","Andreas Bausch","TECHNISCHE UNIVERSITAET MUENCHEN","The requirements on the eukaryotic cytoskeleton are not only of high complexity, but include demands that are actually contradictory in the first place: While the dynamic character of cytoskeletal structures is essential for the motility of cells, their ability for morphological reorganisations and cell division, the structural integrity of cells relies on the stability of cytoskeletal structures. From a biophysical point of view, this dynamic structure formation and stabilization stems from a self-organisation process that is tightly controlled by the simultaneous and competing function of a plethora of actin binding proteins (ABPs). To understand the self-organisation phenomena observed in the cytoskeleton it is therefore indispensable to first shed light on the functional role of ABPs and their underlying molecular mechanisms. Hereby development of reliable reconstituted model systems as has been proven by the great progress achieved in our understanding of individual crosslinking proteins that turn the cytoskeleton into a viscoelastic physical gel. The advantage of such reconstituted systems is that the biological complexity is decreased to an accessible level that the physical principles can be explored and identified.
It is the aim of the present proposal to successively increase the complexity in a well defined manner to further progress in understanding the functional units of a cell. On the way to a sound physical understanding of cellular self organizing principles, the planned major step comprises the incorporation of active processes like the active (de-)polymerisation of filaments and motor mediated active reorganisation and contraction.  We plan to develop new tools and approaches to address how the different kinds of ABPs are interacting with each other and how the structure, dynamics and function of the cytoskeleton is locally governed by the competition and interplay between them.","1495196","2011-10-01","2012-03-31"
"comporel","Large-Scale Computational Screening and Design of Highly-ordered pi-conjugated Molecular Precursors to Organic Electronic","Anne-Clemence Corminboeuf","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The field of electronics has been a veritable powerhouse of the economy, driving technological breakthroughs that affect all aspects of everyday life. Aside from silicon, there has been growing interest in developing a novel generation of electronic devices based on pi-conjugated polymers and oligomers. While their goal is not to exceed the performance of silicon technologies, they could enable far reduced fabrication costs as well as completely new functionalities (e.g. mechanical flexibility, transparency, impact resistance). The performance of these organic devices is greatly dependent on the organization and electronic structures of π-conjugated polymer chains at the molecular level. To achieve full potential, technological developments require fine-tuning of the relative orientation/position of the pi-conjugated moieties, which provide a practical means to enhance electronic properties. The discovery pace of novel materials can be accelerated considerably by the development of efficient computational schemes. This requires an integrated approach, based on which the structural, electronic, and charge transport properties of novel molecular candidates are evaluated computationally and predictions benchmarked by proof of principle experiments. This research program aims at developing a threefold computational screening strategy enabling the design of an emerging class of molecular precursors based on the insertion of π-conjugated molecules into self-assembled hydrogen bond aggregator segments (e.g. oligopeptide, nucleotide and carbohydrate motifs). These bioinspired functionalized pi-conjugated systems offer the highly desirable prospect of achieving ordered suprastructures abundant in nature with the enhanced functionalities only observed in synthetic polymers. A more holistic objective is to definitively establish the relationship between highly ordered architectures and the nature of the electronic interactions and charge transfer properties in the assemblies.","1482240","2012-12-01","2017-11-30"
"COMPUTED","Computational User Interface Design","Antti Olavi Oulasvirta","AALTO KORKEAKOULUSAATIO SR","PROBLEM: Despite extensive research on human-computer interaction (HCI), no method exists that guarantees the optimal or even a provably good user interface (UI) design. The prevailing approach relies on heuristics and iteration, which can be costly and even ineffective, because UI design often involves combinatorially hard problems with immense design spaces, multiple objectives and constraints, and complex user behavior.

OBJECTIVES: COMPUTED establishes the foundations for optimizing UI designs. A design can be automatically optimized to given objectives and constraints by using combinatorial optimization methods that deploy predictive models of user behavior as objective functions. Although previous work has shown some improvements to usability, the scope has been restricted to keyboards and widgets. COMPUTED researches methods that can vastly expand the scope of optimizable problems. First, algorithmic support is developed for acquiring objective functions that cover the main human factors in a given HCI task. Second, formal analysis of decision problems in UI design allows combating a broader range of design tasks with efficient and appropriate optimization methods. Third, a novel interactive UI optimization paradigm for UI designers promotes fast convergence to good results even in the face of uncertainty and incomplete knowledge.

IMPACT: Combinatorial UI optimization offers a strong complement to the prevailing design approaches. Because the structured search process has a high chance of finding good solutions, optimization could improve the quality of interfaces used in everyday life. Optimization can also increase cost-efficiency, because reference to optimality can eliminate fruitless iteration. Moreover, because no preknowledge of UI design is required, even novices will be able to design great UIs. Even in “messy,” less well-defined problems, it may support designers by allowing them to delegate the solving of well-known sub-problems.","1499790","2015-04-01","2020-03-31"
"COMUNEM","Computational Multiscale Neuron Mechanics","Antoine Guy Bernard Jerusalem","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","""The last few years have seen a growing interest for computational cell mechanics. This field encompasses different scales ranging from individual monomers, cytoskeleton constituents, up to the full cell. Its focus, fueled by the development of interdisciplinary collaborative efforts between engineering, computer science and biology, until recently relatively isolated, has allowed for important breakthroughs in biomedicine, bioengineering or even neurology. However, the natural “knowledge barrier” between fields often leads to the use of one numerical tool for one bioengineering application with a limited understanding of either the tool or the field of application itself. Few groups, to date, have the knowledge and expertise to properly avoid both pits. Within the computational mechanics realm, new methods aim at bridging scale and modeling techniques ranging from density functional theory up to continuum modeling on very large scale parallel supercomputers. To the best of the knowledge of the author, a thorough and comprehensive research campaign aiming at bridging scales from proteins to the cell level while including its interaction with its surrounding media/stimulus is yet to be done. Among all cells, neurons are at the heart of tremendous medical challenges (TBI, Alzheimer, etc.). In nearly all of these challenges, the intrinsic coupling between mechanical and chemical mechanisms in neuron is of drastic relevance. I thus propose here the development of a neuron model constituted of length-scale dedicated numerical techniques, adequately bridged together. As an illustration of its usability, the model will be used for two specific applications: neurite growth and electrical-chemical-mechanical coupling in neurons. This multiscale computational framework will ultimately be made available to the bio- medical community to enhance their knowledge on neuron deformation, growth, electrosignaling and thus, Alzheimer’s disease, cancer or TBI.""","1128960","2013-05-01","2018-04-30"
"CON-HUMO","Control based on Human Models","Sandra Hirche","TECHNISCHE UNIVERSITAET MUENCHEN","""CON-HUMO focuses on novel concepts for automatic control based on data-driven human models and machine learning. This enables innovative control applications that are difficult if not impossible to realize using traditional control and identification methods, in particular in the challenging area of smart human-machine interaction. In order to achieve intuitive and efficient goal-oriented interaction, anticipation is key. For control selection based on prediction a dynamic model of the human interaction behavior is required, which, however, is difficult to obtain from first principles. In order to cope with the high complexity of human behavior with unknown inputs and only sparsely available training data we propose to use machine-learning techniques for statistical modeling of the dynamics. In this new field of human interaction modeling – data-driven and machine-learned – control methods with guaranteed properties do not exist. CON-HUMO addresses this niche.
Key methodological innovation and breakthrough is the merger of probabilistic learning with model-based control concepts through model confidence and prediction uncertainty. For the sake of concreteness and evaluation the focus is on one of the most challenging problem classes, namely physical human-machine interaction: Because of the physical contact between the human and the machine not only information, but also energy is exchanged posing fundamental challenges for real-time human-adaptive and safe decision making/control and requiring provable stability and performance guarantees. The developed methods are a direct enabler for societally important applications such as machine-based physical rehabilitation, mobility and manipulation aids for elderly, and collaborative human-machine production systems. With its fundamental results CON-HUMO lays the ground for the systematic control design for smart human-machine/infrastructure interaction.""","1494640","2014-02-01","2019-01-31"
"CONC-VIA-RIEMANN","High-Dimensional Convexity, Isoperimetry and Concentration via a Riemannian Vantage Point","Emanuel Milman","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","""In recent years, the importance of superimposing the contribution of the measure to that of the metric, in determining the underlying space's (generalized Ricci) curvature, has been clarified in the works of Lott, Sturm, Villani and others, following the definition of Curvature-Dimension introduced by Bakry and Emery. We wish to systematically incorporate 
this important idea of considering the measure and metric in tandem, in the study of questions pertaining to isoperimetric and concentration properties of convex domains in high-dimensional Euclidean space, where a-priori there is only a trivial metric (Euclidean) and trivial measure (Lebesgue).  

The first step of enriching the class of uniform measures on convex domains to that of non-negatively curved (""""log-concave"""") measures in Euclidean space has been very successfully implemented in the last decades, leading to substantial progress in our understanding of volumetric properties of convex domains, mostly regarding concentration of linear functionals. However, the potential advantages of altering the Euclidean metric into a more general Riemannian one or exploiting related Riemannian structures have not been systematically explored. Our main paradigm is that in order to progress in non-linear questions pertaining to concentration in Euclidean space, it is imperative to cast and study these problems in the more general Riemannian context.

As witnessed by our own work over the last years, we expect that broadening the scope and incorporating tools from the Riemannian world will lead to significant progress in our understanding of the qualitative and quantitative structure of isoperimetric minimizers in the purely Euclidean setting. Such progress would have dramatic impact on long-standing fundamental conjectures regarding concentration of measure on high-dimensional convex domains, as well as other closely related fields such as Probability Theory, Learning Theory, Random Matrix Theory and Algorithmic Geometry.""","1194190","2015-10-01","2020-09-30"
"CONDMATH","Mathematical Problems in Superconductivity and Bose-Einstein Condensation","Soeren Fournais","AARHUS UNIVERSITET","This project in mathematical physics is concerned with the mathematical understanding of superconductivity and Bose-Einstein condensation. These physical phenomena are the subject of intense research activity both in the experimental and theoretical physics communities and in mathematics. However, despite a lot of effort, many key questions lack a mathematically rigorous answer. The ambition of the present project is to improve this situation. I plan to analyze both the effective models and the underlying microscopic description of superconductivity and Bose-Einstein condensation. The effective models are (systems of) non-linear partial differential equations, and I will apply recently developed mathematical techniques for their analysis. To mention an important specific problem in this part of the project, I am interested in the appearance of regular (Abrikosov) lattices of vortices. For superconductivity, which I will treat in the Ginzburg-Landau model, it is an experimental fact that this happens when an exterior magnetic field comes close to a critical value. For rotating Bose-Einstein condensates, in the Gross-Pitaevskii model, a similar phenomenon occurs for sufficiently large rotations. However, as yet we are unable to derive these lattices directly from the relevant equations. Even more fundamental are the questions about the microscopic models. The aim here is to prove that the desired condensation actually occurs under conditions relevant to experiment, i.e. to prove that the condensation phenomena are correctly described by our fundamental equations of Nature. The microscopic models are systems with a large number of variables and developing the mathematical techniques necessary for the analysis of such systems is an important question in current research in Mathematics.","749571","2008-09-01","2013-08-31"
"CONENE","Control of Large-scale Stochastic Hybrid Systems for Stability of Power Grid with Renewable  Energy","Maryam Kamgarpour","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","The increasing uptake of renewable energy sources and liberalization of electricity markets are significantly changing power system operations. To ensure stability of the grid, it is critical to develop provably safe feedback control algorithms that take into account uncertainties in the output of weather-based renewable generation and in participation of distributed producers and consumers in electricity markets. The focus of this proposal is to develop the theory and algorithms for control of large-scale stochastic hybrid systems in order to guarantee safe and efficient grid operations. Stochastic hybrid systems are a powerful modeling framework. They capture uncertainties in the output of weather-based renewable generation as well as complex hybrid state interactions arising from discrete-valued network topologies with continuous-valued voltages and frequencies. The problems of stability and efficiency of the grid in the face of its changes will be formulated as safety and optimal control problems for stochastic hybrid systems. Using recent advances in numerical optimization and statistics, provably safe and scalable numerical algorithms for control of this class of systems will be developed. These algorithms will be implemented and validated on realistic power grid simulation platforms  and will take advantage of recent advances in sensing, control and communication technologies for the grid. The end outcome of the project is better quantifying and controlling effects of increased uncertainties on the stability of the grid. The societal and economic implications of this study are tied with the value and price of a secure power grid. Addressing the questions formulated in this proposal will bring the EU closer to its ambitious renewable energy goals.","1346438","2016-04-01","2020-09-30"
"CONFINEDCHEM","Synthetic Confined Environments as Tools for Manipulating Chemical Reactivities and Preparing New Nanostructures","Rafal Klajn","WEIZMANN INSTITUTE OF SCIENCE LTD","""Nature has long inspired chemists with its abilities to stabilize ephemeral chemical species, to perform chemical reactions with unprecedented rates and selectivities, and to synthesize complex molecules and fascinating inorganic nanostructures. What natural systems consistently exploit - which is yet fundamentally different from how chemists perform reactions - is their aspect of nanoscale confinement. The goal of the proposed research program is to integrate the worlds of organic and inorganic colloidal chemistry by means of manipulating chemical reactivities and synthesizing novel molecules and nanostructures inside synthetic confined environments created using novel, unconventional approaches based on inorganic, nanostructured building blocks. The three types of confined spaces we propose are as follows: 1) nanopores within reversibly self-assembling colloidal crystals (""""dynamic nanoflasks""""), 2) cavities of bowl-shaped metallic nanoparticles (NPs), and 3) surfaces of spherical NPs. By taking advantage of these unique tools, we will attempt to develop, respectively, 1) a conceptually new method for catalyzing chemical reactions using light, 2) nanoscale inclusion chemistry (a field based on host-guest """"complexes"""" assembled form nanosized components) and 3) to use NPs as platforms for the development of new organic reactions. While these objectives are predominantly of a fundamental nature, they can easily evolve into a variety of practical applications. Specifically, we will pursue diverse goals such as the preparation of 1) a new family of inverse opals (with potentially fascinating optical and mechanical properties), 2) artificial chaperones (NPs assisting in protein folding), and 3) size- and shape-controlled polymeric vesicles. Overall, it is believed that this marriage of organic and colloidal chemistry has the potential to change the fundamental way we perform chemical reactions, paving the way to the discovery of new phenomena and unique structures.""","1499992","2013-10-01","2018-09-30"
"CONLAWS","Hyperbolic Systems of Conservation Laws: singular limits, properties of solutions and control problems","Stefano Bianchini","SCUOLA INTERNAZIONALE SUPERIORE DI STUDI AVANZATI DI TRIESTE","The research program concerns various theoretic aspects of hyperbolic conservation laws. In first place we plan to study the existence and uniqueness of solutions to systems of equations of mathematical physics with physic viscosity. This is one of the main open problems within the theory of conservation laws in one space dimension, which cannot be tackled relying on the techniques developed in the case where the viscosity matrix is the identity. Furthermore, this represents a first step toward the analysis of more complex relaxation and kinetic models with a finite number of velocities as for Broadwell equation, or with a continuous distribution of velocities as for Boltzmann equation. A second research topic concerns the study of conservation laws with large data. Even in this case the basic model is provided by fluidodynamic equations. We wish to extend the results of existence, uniqueness and continuous dependence of solutions to the case of large (in BV or in L^infty) data, at least for the simplest systems of mathematical physics such as the isentropic gas dynamics. A third research topic that we wish to pursue concerns the analysis of fine properties of solutions to conservation laws. Many of such properties depend on the existence of one or more entropies of the system. In particular, we have in mind to study the regularity and the concentration of the dissipativity measure for an entropic solution of a system of conservation laws. Finally, we wish to continue the study of hyperbolic equations from the control theory point of view along two directions: (i) the analysis of controllability and asymptotic stabilizability properties; (ii) the study of optimal control problems related to hyperbolic systems.","422000","2009-11-01","2013-10-31"
"CONNEXIO","Physiologically relevant microfluidic neuro-engineering","Thibault Frédéric Johan HONEGGER","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Developing minimalistic biological neural networks and observing their functional activity is crucial to decipher the information processing in the brain. This project aims to address two major challenges: to design and fabricate in vitro biological neural networks that are organized in physiological relevant ways and to provide a label-free monitoring platform capable of observing neural activity both at the neuron resolution and at large fields of view. To do so, the project will develop a unique microfluidic compartmentalized chips where populations of primary neurons will be seeded in deposition chambers with physiological relevant number and densities. Chambers will be connected by microgrooves in which neurites only can grow and whose dimensions will be tuned according to the connectivity pattern to reproduce. To observe the activity of such complex neural networks, we will develop a disruptive observation technique that will transduce the electrical activity of spiking neurons into optical differences observed on a lens-free platform, without calcium labelling and constantly in-incubo. By combining neuro-engineering patterning and the lens-free platform, we will compare individual spiking to global oscillators in basic neural networks under localized external stimulations. Such results will provide experimental insight into computational neuroscience current approaches. Finally, we will design an in vitro network that will reproduce a neural loop implied in major neurodegenerative diseases with physiological relevant neural types, densities and connectivities. This circuitry will be manipulated in order to model Huntington and Parkinson diseases on the chip and assess the impact of known drugs on the functional activity of the entire network. This project will engineer microfluidics chips with physiological relevant neural network and a lensfree activity monitoring platform to answer fundamental and clinically relevant issues in neuroscience.","1727731","2016-11-01","2021-10-31"
"CONQUEST","Controlled quantum effects and spin technology 
- from non-equilibrium physics to functional magnetics","Henrik Ronnow","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The technology of the 20th century was dominated by a single material class: The semiconductors, whose properties can be tuned between those of metals and insulators   all of which describable by single-electron effects. In contrast, quantum magnets and strongly correlated electron systems offer a full palette of quantum mechanical many-electron states. CONQUEST aim to discover, understand and demonstrate control over such quantum states. A new experimental approach, building on established powerful laboratory and neutron scattering techniques combined with dynamical control-perturbations, will be developed to study correlated quantum effects in magnetic materials. The immediate goal is to open a new field of non-equilibrium and time dependent studies in solid state physics. The long-term vision is that the approach might nurture the materials of the 21st century.","1500000","2011-04-01","2016-03-31"
"CONSTAMIS","Connecting Statistical Mechanics and Conformal Field Theory: an Ising Model Perspective","CLEMENT HONGLER","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The developments of Statistical Mechanics and Quantum Field Theory are among the major achievements of the 20th century's science. During the second half of the century, these two subjects started to converge. In two dimensions, this resulted in a most remarkable chapter of mathematical physics: Conformal Field Theory (CFT) reveals deep structures allowing for extremely precise investigations, making such theories powerful building blocks of many subjects of mathematics and physics. Unfortunately, this convergence has remained non-rigorous, leaving most of the spectacular field-theoretic applications to Statistical Mechanics conjectural. 

About 15 years ago, several mathematical breakthroughs shed new light on this picture. The development of SLE curves and discrete complex analysis has enabled one to connect various statistical mechanics models with conformally symmetric processes. Recently, major progress was made on a key statistical mechanics model, the Ising model: the connection with SLE was established, and many formulae predicted by CFT were proven.

Important advances towards connecting Statistical Mechanics and CFT now appear possible. This is the goal of this proposal, which is organized in three objectives:

(I) Build a deep correspondence between the Ising model and CFT: reveal clear links between the objects and structures arising in the Ising and CFT frameworks.

(II) Gather the insights of (I) to study new connections to CFT, particularly for minimal models, current algebras and parafermions.

(III) Combine (I) and (II) to go beyond conformal symmetry: link the Ising model with massive integrable field theories.

The aim is to build one of the first rigorous bridges between Statistical Mechanics and CFT. It will help to close the gap between physical derivations and mathematical theorems. By linking the deep structures of CFT to concrete models that are applicable in many subjects, it will be potentially useful to theoretical and applied scientists.","998005","2017-03-01","2022-02-28"
"CONT-ACT","Control of contact interactions for robots acting in the world","Ludovic Dominique Righetti","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","What are the algorithmic principles that would allow a robot to run through a rocky terrain, lift a couch while reaching for an object that rolled under it or manipulate a screwdriver while balancing on top of a ladder? Answering this seemingly naïve question resorts to understanding the fundamental principles for robot locomotion and manipulation, which is very challenging. However, it is a necessary step towards ubiquitous robots capable of helping humans in an uncountable number of tasks. The fundamental aspect of both locomotion and manipulation is that the dynamic interaction of the robot with its environment through the creation of physical contacts is at the heart of the tasks. The planning of such interactions in a general manner is an unsolved problem. Moreover, it is not clear how sensory information (e.g. tactile and force sensors) can be included to improve the robustness of robot behaviors. Most of the time, it is simply discarded. CONT-ACT has the ambition to develop a consistent theoretical framework for motion generation and control where contact interaction is at the core of the approach and an efficient use of sensory information drives the development of high performance, adaptive and robust planning and control methods. CONT-ACT develops an architecture based on real-time predictive controllers that fully exploit contact interactions. In addition, the structure of sensory information during contact interactions is experimentally analyzed to create sensor representations adapted for control. It is then possible to learn predictive models in sensor space that are used to create very reactive controllers. The robot constantly improves its performance as it learns better sensory models. It is a step towards a general theory for robot movement that can be used to control any robot with legs and arms for both manipulation and locomotion tasks and that allows robots to constantly improve their performances as they experience the world.","1495500","2015-06-01","2020-05-31"
"CONTACTMATH","Legendrian contact homology and generating families","Frédéric Bourgeois","UNIVERSITE PARIS-SUD","A contact structure on an odd dimensional manifold in a maximally non integrable hyperplane field. It is the odd dimensional counterpart of a symplectic structure. Contact and symplectic topology is a recent and very active area that studies intrinsic questions about existence, (non) uniqueness and rigidity of contact and symplectic structures. It is intimately related to many other important disciplines, such as dynamical systems, singularity theory, knot theory, Morse theory, complex analysis, ... Legendrian submanifolds are a distinguished class of submanifolds in a contact manifold, which are tangent to the contact distribution. These manifolds are of a particular interest in contact topology. Important classes of Legendrian submanifolds can be described using generating families, and this description can be used to define Legendrian invariants via Morse theory. Other the other hand, Legendrian contact homology is an invariant for Legendrian submanifolds, based on holomorphic curves. The goal of this research proposal is to study the relationship between these two approaches. More precisely, we plan to show that the generating family homology and the linearized Legendrian contact homology can be defined for the same class of Legendrian submanifolds, and are isomorphic. This correspondence should be established using a parametrized version of symplectic homology, being developed by the Principal Investigator in collaboration with Oancea. Such a result would give an entirely new type of information about holomorphic curves invariants. Moreover, it can be used to obtain more general structural results on linearized Legendrian contact homology, to extend recent results on existence of Reeb chords, and to gain a much better understanding of the geography of Legendrian submanifolds.","710000","2009-11-01","2014-10-31"
"CONTREX","Controlling Triplet Excitons in Organic Semiconductors","Hugo Bronstein","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The urgent need to reduce carbon emissions in order to mitigate climate change requires the development of clean, renewable energy sources. Solar power offers a virtually unlimited supply of energy, providing it can be harnessed efficiently. Traditional silicon solar cells demonstrate high performance (~20%) but their required method of manufacture prohibits large area production rendering them too expensive to be used on a global scale. Organic solar cells (made from conjugated polymers and fullerenes) have the potential to be fabricated by low cost printing methods allowing for large scale modules to be produced cheaply. Conventional organic solar cells function by generating charge from a singlet excited state. In order to achieve optimum performance the precise morphology of polymer and fullerene must be controlled which can be extremely challenging. These devices however, have attained good efficiencies (10%) but are hampered by severe loss mechanisms which generally involve the formation of a lower energy triplet excited state. 

We propose to develop novel materials for organic solar cells which will instead utilise this triplet excited state to generate charges. This will enable us to not only eliminate this loss mechanism but due to the unique properties of the triplet excited state will allow for numerous benefits. Firstly, the long lifetime of the triplet excited state will be exploited to allow for a simpler organic solar cell where precise morphological control is not required. Secondly, the proposed new materials will allow for the utilisation of near-IR light which is typically wasted in ALL current solar cell devices. Thirdly, exploiting a unique photophysical process we will produce materials capable of delivering efficiencies in excess of the theoretical limit available to conventional solar cells. Thus we propose that utilisation of triplet excitons is the required step-change to allow for organic solar cells to achieve their ultimate efficiencies","1499223","2016-04-01","2021-03-31"
"CONVEXVISION","Convex Optimization Methods for Computer Vision and Image Analysis","Daniel Cremers","TECHNISCHE UNIVERSITAET MUENCHEN","Optimization methods have become an established paradigm to address most Computer Vision challenges including the
reconstruction of three-dimensional objects from multiple images, or the tracking of a deformable shape over time. Yet, it has
been largely overlooked that optimization approaches are practically useless if they do not come with efficient algorithms to
compute minimizers of respective energies. Most existing formulations give rise to non-convex energies. As a consequence,
solutions highly depend on the choice of minimization scheme and implementational (initialization, time step sizes, etc.), with
little or no guarantees regarding the quality of computed solutions and their robustness to perturbations of the input data.
In the proposed research project, we plan to develop optimization methods for Computer Vision which allow to efficiently
compute globally optimal solutions. Preliminary results indicate that this will drastically leverage the power of optimization
methods and their applicability in a substantially broader context. Specifically we will focus on three lines of research: 1) We
will develop convex formulations for a variety of challenges. While convex formulations are currently being developed for
low-level problems such as image segmentation, our main effort will focus on carrying convex optimization to higher level
problems of image understanding and scene interpretation. 2) We will investigate alternative strategies of global optimization
by means of discrete graph theoretic methods. We will characterize advantages and drawbacks of continuous and discrete
methods and thereby develop novel algorithms combining the advantages of both approaches. 3) We will go beyond convex
formulations, developing relaxation schemes that compute near-optimal solutions for problems that cannot be expressed by
convex functionals.","1985400","2010-09-01","2015-08-31"
"COOPAIRENT","Cooper pairs as a source of entanglement","Szabolcs Csonka","BUDAPESTI MUSZAKI ES GAZDASAGTUDOMANYI EGYETEM","Entanglement and non-locality are spectacular fundamentals of quantum mechanics and basic resources of future quantum computation algorithms. Electronic entanglement has attracted increasing attention during the last years. The electron spin as a purely quantum mechanical two level system has been put forward as a promising candidate for storing quantum information in solid state. Recently, great progress has been achieved in manipulation and read-out of quantum dot based spin Qubits.  However, electron spin is also suitable to transfer quantum information, since mobile electrons can be coherently transmitted in a solid state device preserving the spin information. Thus, electron spin could provide a general platform for on-chip quantum computation and information processing.
Although several theoretical concepts have been worked out to address spin entangled mobile electrons, the absence of an entangler device has not allowed their realization so far. The aim of the present proposal is to overcome this experimental challenge and explore the entanglement of spatially separated electron pairs. Superconductors provide a natural source of entanglement, because their ground-state is composed of Cooper pairs in a spin-singlet state. However, the splitting of the Cooper pairs into separate electrons has to be enforced, which has been very recently realized by the applicant in two quantum dot Y-junction. This Y-junction will be used as a central building block to split Cooper pairs in a controlled fashion and the non-local nature of spin and charge correlations will be addressed in various device configurations.
Our research project will lead to a fundamental understanding of the production, manipulation and detection of spin entangled mobile electron pairs, thus it will significantly extend the frontiers of quantum coherence and opens a new horizon in the field of on-chip quantum information technologies.","1496112","2011-02-01","2016-10-31"
"CoopCat","Cooperative Catalysis: Using Interdisciplinary Chemical Systems to Develop New Cooperative Catalysts","Jesus CAMPOS MANZANO","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","Catalysis, a multidisciplinary science at the heart of many industrial processes, is crucial to deliver future growth and minimize anthropogenic environmental impact, thus being critical to our quality of life. Thus, the development and fundamental understanding of innovative new catalyst systems has clear, direct and long-term benefits to the chemical manufacturing sector and to the broader knowledge-based economy.
In this ERC project I will develop novel innovative cooperative catalysts using interdisciplinary chemical systems based on main group elements, transition metals and molecular clusters to achieve better efficiency and improve chemical scope and sustainability of key chemical transformations.
This will be achieved through 3 complementary and original strategies based on catalytic cooperation: (i) Transition-Metal Frustrated Lewis Pairs (TM-FLPs); (ii) hybrid systems combining low-valent heavier main group elements with transition metals (Hybrid TM/MGs); and (iii) intercluster compounds (ICCs) as versatile heterogeneized materials for Green Catalysis.
These systems, of high synthetic feasibility, combine fundamental concepts from independent areas, e.g. FLPs and low-valent heavier main group elements with transition metal chemistry, and homogeneous with heterogeneous catalysis. The overall approach will be pivotal in discovering novel reactions that rely on the activation of otherwise unreactive substrates. The experience and knowledge gained from (i)-(iii) will be used to inform the design of a second generation of ICC materials in which at least one of the nanoscale bricks is based on polymetallic TM-FLPs or Hybrid TM/MG systems.
Delivering ground-breaking new fundamental science, this pioneering project will lay the foundation for future broad ranging benefits to a number of EU priority areas dependant on innovations in catalysis: innovative and sustainable future energy systems, solar technologies, sustainable chemistry, manufacturing, and healthcare.","1445000","2018-02-01","2023-01-31"
"COOPNET","Cooperative Situational Awareness for Wireless Networks","Henk Wymeersch","CHALMERS TEKNISKA HOEGSKOLA AB","Devices in wireless networks are no longer used only for communicating binary information, but also for navigation and to sense their surroundings. We are currently approaching fundamental limitations in terms of communication throughput, position information availability and accuracy, and decision making based on sensory data. The goal of this proposal is to understand how the cooperative nature of future wireless networks can be leveraged to perform timekeeping, positioning, communication, and decision making, so as to obtain orders of magnitude performance improvements compared to current architectures.

Our research will have implications in many fields and will comprise fundamental theoretical contributions as well as a cooperative wireless testbed. The fundamental contributions will lead to a deep understanding of cooperative wireless networks and will enable new pervasive applications which currently cannot be supported. The testbed will be used to validate the research, and will serve as a kernel for other researchers worldwide to advance knowledge on cooperative networks. Our work will build on and consolidate knowledge currently dispersed in different scientific disciplines and communities (such as communication theory, sensor networks, distributed estimation and detection, environmental monitoring, control theory, positioning and timekeeping, distributed optimization). It will give a new thrust to research within those communities and forge relations between them.","1500000","2011-05-01","2016-04-30"
"CoPS","Coevolutionary Policy Search","Shimon Azariah Whiteson","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","I propose to develop a new class of decision-theoretic planning methods that overcome fundamental obstacles to the efficient optimization of autonomous agents.  Creating agents that are effective in diverse settings is a key goal of artificial intelligence with enormous potential implications: robotic agents would be invaluable in homes, factories, and high-risk settings; software agents could revolutionize e-commerce, information retrieval, and traffic control.
The main challenge lies in specifying an agent's policy: the behavioral strategy that determines its actions.  Since the complexity of realistic tasks makes manual policy construction hopeless, there is great demand for decision-theoretic planning methods that automatically discover good policies. Despite enormous progress, the grand challenge of efficiently discovering effective policies for complex tasks remains unmet.
A fundamental obstacle is the cost of policy evaluation: estimating a policy's quality by averaging performance over multiple trials.  This cost grows quickly with increases in task complexity (making trials more expensive) or stochasticity (necessitating more trials).  
To address this difficulty, I propose a new approach that simultaneously optimizes both policies and the manner in which those policies are evaluated.  The key insight is that, in many tasks, many trials are wasted because they do not elicit the controllable rare events critical for distinguishing between policies. Thus, I will develop methods that leverage coevolution to automatically discover the best events, instead of sampling them randomly.
If successful, this project will greatly improve the efficiency of decision-theoretic planning and, in turn, help realize the potential of autonomous agents.  In addition, by automatically identifying the most useful events, the resulting methods will help isolate critical factors in performance and thus yield new insights into what makes decision-theoretic problems hard.","1480632","2015-10-01","2020-09-30"
"CoqHoTT","Coq for Homotopy Type Theory","nicolas Tabareau","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","Every year, software bugs cost hundreds of millions of euros to companies and administrations. Hence, software quality is a prevalent notion and interactive theorem provers based on type theory have shown their efficiency to prove correctness of important pieces of software like the C compiler of the CompCert project.  One main interest of such theorem provers is the ability to extract directly the code from the proof. Unfortunately, their democratization suffers from a major drawback, the mismatch between equality in mathematics and in type theory. Thus, significant Coq developments have only been done by virtuosos playing with advanced concepts of computer science and mathematics.  Recently, an extension of type theory with homotopical concepts such as univalence is gaining traction because it allows for the first time to marry together expected principles of equality. But the univalence principle has been treated so far as a new axiom which breaks one fundamental property of mechanized proofs: the ability to compute with programs that make use of this axiom. The main goal of the CoqHoTT project is to provide a new generation of proof assistants with a computational version of univalence and use them as a base to implement effective logical model transformation so that the power of the internal logic of the proof assistant needed to prove the correctness of a program can be decided and changed at compile time—according to a trade-off between efficiency and logical expressivity. Our approach is based on a radically new compilation phase technique into a core type theory to modularize the difficulty of finding a decidable type checking algorithm for homotopy type theory. 
The impact of the CoqHoTT project will be very strong. Even if Coq is already a success, this project will promote it as a major proof assistant, for both computer scientists and mathematicians. CoqHoTT will become an essential tool for program certification and formalization of mathematics.","1498290","2015-06-01","2020-05-31"
"CoQuake","Controlling earthQuakes","Ioannis STEFANOU","ECOLE NATIONALE DES PONTS ET CHAUSSEES","According to the Centre for Research on the Epidemiology of Disasters (CRED), earthquakes are responsible for more than half of the total human losses due to natural disasters from 1994 to 2003. There is no doubt that earthquakes are lethal and costly. CoQuake proposes an alternative, ground-breaking approach for avoiding catastrophic earthquakes by inducing them at a lower energetic level. Earthquakes are a natural phenomenon that we cannot avoid, but –for the first time– in CoQuake I will show that it is possible to control them, hence reducing the seismic risk, fatalities and economic cost. CoQuake goes beyond the state-of-the-art by proposing an innovative methodology for investigating the effect and the controllability of various stimulating techniques that can reactivate seismic faults. It involves large-scale, accurate simulations of fault systems based on constitutive laws derived from micromechanical, grain-by-grain simulations under Thermo-Hydro-Chemo-Mechanical couplings (THMC), which are not calibrated on the basis of ad-hoc empirical and inaccurate constitutive laws. A pioneer experimental research programme and the design and construction of a new apparatus of metric scale, will demonstrate CoQuake’s proof-of-principle and it will help to explore the transition from aseismic to seismic slip. CoQuake is an interdisciplinary project as it takes knowledge from various fields of engineering, computational mechanics, geomechanics, mathematics and geophysics. CoQuake opens a new field and new line of research in earthquake mechanics and engineering, with a direct impact on humanity and science.","1499999","2018-06-01","2023-05-31"
"COrANE","Composition and Sources of Atmospheric Organic Aerosol and their Negative Health Effects","Markus Kalberer","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Atmospheric aerosol particles are key components of the earth’s climate system and are one of the major air pollution components. In both areas large uncertainties are associated with aerosol effects. The chemical composition is a major parameter determining the effects of aerosols on the climate and their health effects. A major but poorly defined fraction of the aerosol is organic material formed within the atmosphere (so-called Secondary Organic Aerosol, SOA). Only a comprehensive chemical analysis of SOA simulated in laboratory experiments can rigorously identify and quantify SOA sources. However, only a small minority of the SOA mass can be characterized on a molecular level due to fundamental limitations of conventional analytical-chemical techniques. Thus, there is a large uncertainty how accurate current laboratory experiments mimic atmospheric SOA. This uncertainly critically limits our ability to assess the role of aerosols in the climate system, to determine their toxicity and also constrains further improvements of legal limits for ambient particle concentrations.
The main SOA sources will be identified in this project in unprecedented detail by developing novel analytical techniques to characterize SOA comprehensively (mainly ultra-high resolution mass spectrometry). Generation of SOA in improved laboratory experiments and comparison with field samples will help to overcome the long-standing uncertainties described above.
Particle properties responsible for health effects are poorly understood, but oxidizing particle components are likely important in understanding particle-cell interactions. Compound classes in SOA will be quantified, which are potentially damaging biological tissue such as peroxides and radicals, using the strongly improved laboratory conditions to simulate accurately SOA. For these studies new, fast online spectroscopic techniques will be developed to accurately quantify these highly reactive and short-lived particle components.","1495851","2011-10-01","2017-09-30"
"CORFRONMAT","Correlated frontiers of many-body quantum mathematics and condensed matter physics","Nicolas ROUGERIE","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","One of the main challenges in condensed matter physics is to understand strongly correlated quantum systems. Our purpose is to approach this issue from the point of view of rigorous mathematical analysis. The goals are twofold: develop a mathematical framework applicable to physically relevant scenarii, take inspiration from the physics to introduce new topics in mathematics. The scope of the proposal thus goes from physically oriented questions (theoretical description and modelization of physical systems) to analytical ones (rigorous derivation and analysis of reduced models) in several cases where strong correlations play the key role.

In a first part, we aim at developing mathematical methods of general applicability to go beyond mean-field theory in different contexts. Our long-term goal is to forge new tools to attack important open problems in the field. Particular emphasis will be put on the structural properties of large quantum states as a general tool.

A second part is concerned with so-called fractional quantum Hall states, host of  the fractional quantum Hall effect. Despite the appealing structure of their built-in correlations, their mathematical study is in its infancy. They however constitute an excellent testing ground to develop ideas of possible wider applicability. In particular, we introduce and study a new class of many-body variational problems.

In the third part we discuss so-called anyons, exotic quasi-particles thought to  emerge as excitations of highly-correlated quantum systems. Their modelization gives rise to rather unusual, strongly interacting, many-body Hamiltonians with a topological content. Mathematical analysis will help us shed light on those, clarifying the characteristic properties that could ultimately be experimentally tested.","1056664","2018-01-01","2022-12-31"
"CORNET","Provably Correct Networks","Costin RAICIU","UNIVERSITATEA POLITEHNICA DIN BUCURESTI","Networks are the backbone of our society, but configuring them is error-prone and tedious: misconfigured networks result in headline grabbing network outages that affect many users and hurt company revenues while security breaches that endanger millions of customers. There are currently no guarantees that deployed networks correctly implement their operator’s policy.

Existing research has focused on two directions: a) low level analysis and instrumentation of real networking code prevents memory bugs in individual network elements, but does not capture network-wide properties desired by operators such as reachability or loop freedom; b) high-level analysis of network-wide properties to verify operator policies on abstract network models; unfortunately, there are no guarantees that the models are an accurate representation of the real network code, and often low-level errors invalidate the conclusions of the high-level analysis.

We propose to achieve provably correct networks by simultaneously targeting both low-level security concerns and network-wide policy compliance checking. Our key proposal is to rely on exhaustive network symbolic execution for verification and to automatically generate provably correct implementations from network models. Generating efficient code that is equivalent to the model poses great challenges that we will address with three key contributions:

a)	We will develop a novel theoretical equivalence framework based on symbolic execution semantics, as well as equivalence-preserving model transformations to automatically optimize network models for runtime efficiency.

b)	We will develop compilers that take network models and generate functionally equivalent and efficient executable code for different targets (e.g. P4 and C). 

c)	We will design algorithms that generate and insert runtime guards that ensure correctness of the network with respect to the desired policy even when legacy boxes are deployed in the network.","1325000","2018-01-01","2022-12-31"
"COSIRIS","Investigating the terrestrial carbon and water cycles with a multi-tracer approach","Ulrike Seibt","UNIVERSITE PIERRE ET MARIE CURIE - PARIS 6","The aim of COSIRIS is to isolate the simultaneous fluxes of photosynthesis and respiration of the terrestrial biosphere. With explicit knowledge of the component fluxes, we will: 1) test process based models of photosynthesis and respiration, 2) determine the sensitivity of each flux to environmental conditions, and 3) derive predictions of their responses to climate change. Specifically, COSIRIS aims to build a research facility to integrate a new tracer, carbonyl sulfide (COS) with CO2, water and their stable isotopes in a multi-tracer framework as a tool to separately investigate photosynthesis and respiration. In terrestrial ecosystems, CO2 is often taken up and released at the same time. Similar to CO2, COS is taken up during photosynthesis, but unlike CO2, concurrent COS emissions are small. Parallel COS and CO2 measurements thus promise to provide estimates of gross photosynthetic fluxes – impossible to measure directly at scales larger than a few leaves. The use of COS to derive CO2 fluxes has not been verified yet, but enough is known about their parallel pathways to suggest that COS, CO2 and its isotopes can be combined to yield powerful and unique constraints on gross carbon fluxes. COSIRIS will develop the expertise necessary to achieve this goal by providing: 1. an in-depth analysis of processes involved in COS uptake by vegetation, and of potentially interfering influences such as uptake by soil, 2. a novel process-based multi-tracer modelling framework of COS, CO2, water and their isotopes at the ecosystem scale, 3. extensive datasets on concurrent fluctuations of COS, CO2, water and their isotopes in ecosystems. This innovative approach promises advances in understanding and determining gross carbon fluxes at ecosystem to continental scales, particularly their variations in response to climate anomalies.","1822000","2008-07-01","2014-10-31"
"COSIWAX","Compound Specific Hydrogen Isotope Analyses of Leaf Wax n-Alkanes as a Novel Tool to Assess Plant and Ecosystem Water Relations Across new Spatial and Temporal Scales","Ansgar Kahmen","UNIVERSITAT BASEL","""Leaf wax n-alkanes are long-chained lipids that are vital components of plant cuticles. What makes leaf wax n-alkanes unique is that their stable hydrogen isotope composition (δD) contains information on precipitation and plant water relations. In addition, leaf wax n-alkanes are abundant in leaves, soils, sediments and even the atmosphere and can persist with their δD values over millions of years. With this exceptional combination of properties, leaf wax n-alkanes and their δD values are now being celebrated as the much-needed ecohydrological proxy that provides information on the hydrological cycle and plant water relations across spatial and temporal scales that range from leaves to biomes and from weeks to millions of years. Despite the enormous potential that leaf wax n-alkanes have as ecohydrological proxy for a range of different research areas, the exact type of hydrological information that is recorded in the δD values of leaf wax n-alkanes remains still unclear. This is because critical mechanisms that determine the δD values of leaf wax n-alkanes are not understood. This proposal will perform the experimental work that is now needed to resolve the key mechanisms that determine the δD values leaf wax n-alkanes. These experiments will set the basis to develop a new numerical model that will allow to ultimately test what the exact hydrological signal is that leaf wax n-alkanes record in their δD values: a mere hydrological signal reflecting the amount or origin of precipitation or, a plant-shaped signal indicating plant water relations such as evapotranspiration. Building on this new model, COSIWAX will set out to test the potential that leaf wax n-alkane δD values hold as new ecohydrological proxy for ecology and ecosystem sciences. If successful, COSIWAX will establish with this research leaf wax n-alkanes δD values as a new and innovative ecohydrological proxy that has extensive possible applications in paleoclimatology, ecology, earth system sciences.""","1496342","2011-10-01","2016-09-30"
"COSMASS","Constraining Stellar Mass and Supermassive Black Hole Growth through Cosmic Times: Paving the way for the next generation sky surveys","Vernesa Smolcic","FACULTY OF SCIENCE UNIVERSITY OF ZAGREB","Understanding how galaxies form in the early universe and how they evolve through cosmic time is a major goal of modern astrophysics. Panchromatic look-back sky surveys significantly advanced the field in the past decade, and we are now entering a 'golden age' of radio astronomy given an order of magnitude improved facilities like JVLA, ATCA and ALMA. I am leading two unique, state-of-the-art (JVLA/ATCA) radio surveys that will push to the next frontiers. The proposed ERC project will focus on the growth of stellar and black-hole mass in galaxies across cosmic time by: 1-probing various types of extremely faint radio sources over cosmic time, revealing the debated abundance of faint radio sources, 2-exploring star formation conditions at early cosmic times, allowing to access for the first time the dust-unbiased cosmic star formation history since the epoch of reionization, 3-performing the first census of high-redshift starbursting galaxies (SMGs), and their role in galaxy formation and evolution, and 4-performing a full census of galaxies hosting supermassive black holes (AGN), with different black-hole accretion modes, and their roles in galaxy evolution.
The exploitation of these radio sky surveys is essential for the preparation and success of the future large facilities like ASKAP, and SKA as they will 1-provide best predictions of the to-date uncertain cosmic radio background seen with the SKA, and 2-optimize photometric redshift estimates, essential for the success of the first ASKAP sky survey (EMU, >2016).
My radio surveys, expected to yield >100 refereed publications, carry an immense legacy value. The proposed ERC funding is essential for the success of these timely surveys, which I will conduct from Croatia. The ERC grant will allow me to lead my own research group working on this novel data, and to even more firmly establish myself as a leading survey scientist, and lead my group to internationally competitive levels, and enhance EU competitiveness.","1500000","2014-02-01","2019-01-31"
"COSMIC_DAWN","Cosmic Dawn – The Emergence of Black Holes and Galaxies 
in the Universe","Fabian Walter","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Studying quasars (actively accreting supermassive black holes) and galaxies at the earliest cosmic epochs is one of the prime objectives in modern astrophysics. How the first luminous sources formed and reionized the universe is one of the most fundamental open questions in cosmology. Likewise, a detailed characterization of the evolution of the cosmic molecular gas density, which lies at the heart of galaxy evolution, is a fundamental goal in observational astrophysics. The proposed program capitalizes on current and upcoming facilities and encompasses three aspects: The first (I) will use the recently started Pan-STARRS1 survey to identify a sizeable sample of z>7 quasars, i.e. when the universe was less than 0.8 Gyr old (<1/15th of today’s age), to statistically probe black hole growth in the Epoch of Reionization. The second project (II) will characterize the physical properties of z~6 and z~7 quasar hosts selected from project (I) and the SDSS / VISTA VIKING surveys using new observational NIR/(sub)millimeter capabilities, including ALMA (observations approved in cycle 0). The last project (III) will use the IRAM Plateau de Bure Interferometer, and ALMA, to embark on the first molecular deep field which will provide a complete ‘blind’ census of the molecular gas density through cosmic times (from z~1 to z~8, approved pilot program). The overarching goal of this ERC proposal is thus to greatly increase our understanding of the physical properties of the earliest massive objects in the universe, and to shed first light on the evolution of the cosmic molecular gas density in galaxies after the universe emerged from the cosmic ‘dark ages’. The PI is internationally recognized as a leader in molecular gas studies of high-redshift galaxies, and through his track record and access to the needed combination of research facilities, is uniquely positioned to successfully lead this program.","1439000","2012-11-01","2017-10-31"
"COSMICDAWN","Understanding the Origin of Cosmic Structure","Hiranya Vajramani Peiris","UNIVERSITY COLLEGE LONDON","The early universe is a “laboratory” for testing physics at very high energies, up to a trillion times greater than the energies reached by the Large Hadron Collider. The origin of structure in the universe is deeply tied to this extreme physics, which is imprinted in the primordial ripples seen in the cosmic microwave background (CMB). CMB data have thus far led the way in constraining early universe physics, and ESA’s Planck satellite is currently mapping the CMB at the highest precision ever achieved. However, next generation galaxy surveys – such as the Dark Energy Survey (DES), starting next year – will rival the CMB in their ability to unlock the secrets of the primordial universe.  I will use the Planck and DES data to rigorously test the theory of inflation, the dominant paradigm for the origin of cosmic structure, and to seek signatures of new physics that are likely to exist at these unexplored energies.
I have already played a leading role in bringing theory and robust data analysis together to understand the very early universe. This proposal aims, for the first time, to go beyond simply testing generic predictions of the inflationary paradigm, to gain a fundamental understanding of the physics responsible for the origin of cosmic structure. The keys to achieving this goal are: theoretical modelling at the cutting edge of fundamental physics (describing not just the inflationary period but also pre- and post-inflationary physics); advanced Bayesian and wavelet methods to extract reliable information from the data; a deep understanding of data limitations and control of systematics. The project will produce definitive results at the interface of cosmology and high energy physics, defining the frontiers of these fields well beyond the lifetimes of the surveys themselves.","1493066","2013-01-01","2018-12-31"
"COSMICEXPLOSIONS","The nature of cosmic explosions","Avishay Gal-Yam","WEIZMANN INSTITUTE OF SCIENCE LTD","Cosmic explosions, the violent deaths of stars, play a crucial role in many of the most interesting open questions in physics today. These events serve as “cosmic accelerators” for ultra-high-energy particles that are beyond reach for even to most powerful terrestrial accelerators, as well as distant sources for elusive neutrinos. Explosions leave behind compact neutron stars and black hole remnants, natural laboratories to study strong gravity. Acting as cosmic furnaces, these explosions driven the chemical evolution of the Universe Cosmic explosions trigger and inhibit star formation processes, and drive galactic evolution (“feedback”). Distances measured using supernova explosions as standard candles brought about the modern revolution in our view of the accelerating Universe, driven by enigmatic “dark energy”. Understanding the nature of cosmic explosions of all types is thus an extremely well-motivated endeavour. I have been studying cosmic explosions for over a decade, and since the earliest stages of my career, have followed an ambition to figure out the nature of cosmic explosions of all types, and to search for new types of explosions. Having already made several key discoveries, I now propose to undertake a comprehensive program to systematically tackle this problem.I review below the progress made in this field and the breakthrough results we have achieved so far, and propose to climb the next step in this scientific and technological ladder, combining new powerful surveys with comprehensive multi-wavelength and multi-disciplinary (observational and theoretical) analysis. My strategy is based on a combination of two main approaches: detailed studies of single objects which serve as keys to specific questions; and systematic studies of large samples, some that I have, for the first time, been able to assemble and analyze, and those expected from forthcoming efforts. Both approaches have already yielded tantalizing results.","1499302","2012-09-01","2017-08-31"
"COSMO@LHC","Cosmology at the CERN Large Hadron Collider","Geraldine Servant","EUROPEAN ORGANIZATION FOR NUCLEAR RESEARCH","The Large Hadron Collider (LHC), a 7 + 7 TeV proton-proton collider under completion at CERN, the European Laboratory for Particle Physics in Geneva, will take experiments into a new energy domain beyond the Standard Model of strong and electroweak interactions. As the LHC will unveil the mysteries of the electroweak symmetry breaking, this will also have far-reaching implications for cosmology. The aim of this project is to work out what we may learn about the Early Universe from discoveries at the LHC. This concerns in particular the two fundamental questions of the nature of the Dark Matter and the origin of the matter-antimatter asymmetry of the Universe. The LHC-Cosmology interplay has been a topic of active research in the last years. However, studies have essentially focussed on a single class of models: supersymmetry. The original and innovative directions of this project are: 1) To investigate dark matter particle physics models that have not been explored yet and confront theoretical predictions with existing and upcoming observational constraints. Measuring the properties of the dark matter will require a complementarity between the LHC searches and the other numerous ongoing dark matter experiments such as gamma ray telescopes, neutrino telescopes, cosmic positron detectors ... etc. 2) To work out the details of the electroweak phase transition in extensions of the Standard Model. One of the best-motivated mechanism for generating the baryon asymmetry of the universe relies on a first-order electroweak phase transition. Interestingly, this has strong implications for Gravity Wave physics. We will explore thoroughly how the planned gravity wave detector and space interferometer LISA, which turns out to be a completely independent window on the electroweak scale, could complement the information provided by the LHC. This project will also serve as a solid basis for future research at the Internatinal electron-positron Linear Collider.","800000","2008-07-01","2013-06-30"
"COSMO_SIMS","Astrophysics for the Dark Universe: Cosmological simulations in the context of dark matter and dark energy research","Oliver Jens Hahn","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The objective of this ambitious research proposal is to push forward the frontier of computational cosmology by significantly improving the precision of numerical models on par with the increasing richness and depth of surveys that aim to shed light on the nature of dark matter and dark energy.

Using new phase-space techniques for the simulation and analysis of dark matter, completely new insights into its dynamics are possible. They allow, for the first time, the accurate simulation of dark matter cosmologies with suppressed small-scale power without artificial fragmentation. Using such techniques, I will establish highly accurate predictions for the properties of dark matter and baryons on small scales and investigate the formation of the first galaxies in non-CDM cosmologies.
Baryonic effects on cosmological observables are a severe limiting factor in interpreting cosmological measurements. I will investigate their impact by identifying the relevant astrophysical processes in relation to the multi-wavelength properties of galaxy clusters and the galaxies they host. This will be enabled by a statistical set of zoom simulations where it is possible to study how these properties correlate with one another, with the assembly history, and how we can derive better models for unresolved baryonic processes in cosmological simulations and thus, ultimately, how we can improve the power of cosmological surveys.
Finally, I will develop a completely unified framework for precision cosmological initial conditions (ICs) that is scalable to both the largest simulations and the highest resolution zoom simulations. Bringing ICs into the ‘cloud’ will enable new statistical studies using zoom simulations and increase the reproducibility of simulations within the community.

My previous work in developing most of the underlying techniques puts me in an excellent position to lead a research group that is able to successfully approach such a wide-ranging and ambitious project.","1471382","2016-09-01","2021-08-31"
"COSMOIGM","The Intergalactic Medium as a Cosmological Tool","Matteo Viel","ISTITUTO NAZIONALE DI ASTROFISICA","The cosmoIGM proposal aims at investigating the role of the Intergalactic Medium (IGM) as a cosmological probe and  at exploiting  the many IGM-related sinergies between observational cosmology, galaxy formation and fundamental physics. The IGM is a unique cosmological observable as it probes 3/4 of the present age of the universe, it contains up to 80% of the baryons and is sensitive to scales that are not measured by other data. In the last decade, astronomical data sets have started to be widely used by the scientific community to address important physical issues such as: the nature of the dark matter and dark energy components and their evolution;  the physical properties of the baryonic matter; variation of fundamental constants; feedback processes by galaxies, etc. For example, results obtained from astronomical data are nowadays comparable to those obtained by ground based physics laboratories (e.g. neutrino masses).  This proposal will rely on observations of the IGM at high and low redshift and will interpret them by means of state-of-the-art computational facilities in order to firmly establish the (yet controversial) role of the IGM as a probe for cosmology and fundamental physics. Moreover, we aim at exploring the galaxy-IGM interplay at a crucial stage of the cosmic history when the universe was few Gyrs old and star forming galaxies were strongly affecting the dynamical, thermal and chemical properties of the IGM. The hosting institution,  Trieste Observatory, and the Trieste Area (ICTP, SISSA and Trieste University) have a long-standing expertise on the topics above. We foresee that the present interdisciplinary proposal will have a strong scientific impact and will help the P.I. to consolidate its independence and to create his first research team.","891400","2010-12-01","2016-11-30"
"COSMOLAB","Laboratory simulation of cosmological magnetic fields","Gianluca Gregori","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The advent of high-power laser systems in the past two decades has opened a new field of research where astrophysical environments can be scaled down to laboratory dimensions, yet preserving the essential physics. This is due to the invariance of the equations of ideal magneto-hydrodynamics (MHD) to a class of self-similar transformations. In this proposal, we will apply these scaling laws to investigate the dynamics of the high Mach number shocks arising during the formation of the large-scale structure of the Universe.  Although at the beginning of cosmic evolution matter was nearly homogenously distributed, today, as a result of gravitational instability, it forms a web-like structure made of filaments and clusters. Gas continues to accrete supersonically onto these collapsed structures, thus producing high Mach number shocks. It has been recently proposed that generation of magnetic fields can occur at these cosmic shocks on a cosmologically fast timescale via a Weibel-like instability, thus providing an appealing explanation to the ubiquitous magnetization of the Universe. Our proposal will thus provide the first experimental evidence of such mechanisms. We plan to measure the self-generated magnetic fields from laboratory shock waves using a novel combination of electron deflectometry, Faraday rotation measurements using THz lasers, and dB/dt probes. The proposed investigation on the generation of magnetic fields at shocks via plasma instabilities bears important general consequences. First, it will shed light on the origin of cosmic magnetic fields. Second, it would have a tremendous impact on one of the greatest puzzles of high energy astrophysics, the origin of Ultra High Energy Cosmic Rays. We plan to assess the role of charged particle acceleration via collisionless shocks in the amplification of the magnetic field as well as measure the spectrum of such accelerated particles. The experimental work will be carried both at Oxford U and at laser facilities.","1119690","2010-12-01","2015-11-30"
"COSMOS","Computational Simulations of MOFs for Gas Separations","Seda Keskin Avci","KOC UNIVERSITY","Metal organic frameworks (MOFs) are recently considered as new fascinating nanoporous materials. MOFs have very large surface areas, high porosities, various pore sizes/shapes, chemical functionalities and good thermal/chemical stabilities. These properties make MOFs highly promising for gas separation applications. Thousands of MOFs have been synthesized in the last decade. The large number of available MOFs creates excellent opportunities to develop energy-efficient gas separation technologies. On the other hand, it is very challenging to identify the best materials for each gas separation of interest. Considering the continuous rapid increase in the number of synthesized materials, it is practically not possible to test each MOF using purely experimental manners. Highly accurate computational methods are required to identify the most promising MOFs to direct experimental efforts, time and resources to those materials. In this project, I will build a complete MOF library and use molecular simulations to assess adsorption and diffusion properties of gas mixtures in MOFs. Results of simulations will be used to predict adsorbent and membrane properties of MOFs for scientifically and technologically important gas separation processes such as CO2/CH4 (natural gas purification), CO2/N2 (flue gas separation), CO2/H2, CH4/H2 and N2/H2 (hydrogen recovery). I will obtain the fundamental, atomic-level insights into the common features of the top-performing MOFs and establish structure-performance relations. These relations will be used as guidelines to computationally design new MOFs with outstanding separation performances for CO2 capture and H2 recovery. These new MOFs will be finally synthesized in the lab scale and tested as adsorbents and membranes under practical operating conditions for each gas separation of interest. Combining a multi-stage computational approach with experiments, this project will lead to novel, efficient gas separation technologies based on MOFs.","1500000","2017-10-01","2022-09-30"
"COSMOS","Game theoretic Control for Complex Systems of Systems","Sergio GRAMMATICO","TECHNISCHE UNIVERSITEIT DELFT","Modern society is based on large-scale, interconnected, complex infrastructures, e.g. power, transportation and communication systems, with network structure and interacting subsystems controlled by autonomous components and human users, generically called “agents”. These systems possess the features of “complex” systems of systems (C-SoS), such as rationality and autonomy of the agents, and require effective multi-agent coordination and control actions for their safe and efficient operation. Multi-agent optimization has attracted an extraordinary amount of research attention as a methodology to let agents cooperatively coordinate their actions, but it is inappropriate and ineffective for systems with noncooperative (selfish) agents, virtually all modern C-SoS.

A paradigm shift is necessary to ensure safe and efficient operation of complex systems with possibly noncooperative agents. With this aim, COSMOS shall embrace dynamic game theory and pursue a twofold scientific and technical objective: 1) to conceive a unifying framework for the analysis and control of complex, multi-agent, mixed cooperative and noncooperative, systems; 2) to provide automated computational methods for solving coordination, decision and control problems in C-SoS. To achieve these goals, COSMOS will adopt a novel operator-theoretic approach, and integrate methods within and across dynamic game theory, networked multi-agent systems and control, statistical learning, stochastic and mixed-integer optimization.","1499415","2019-02-01","2024-01-31"
"CosNeD","Radio wave propagation in heterogeneous media: implications on the electronics of Cosmic Neutrino Detectors","Alina Mihaela BADESCU","UNIVERSITATEA POLITEHNICA DIN BUCURESTI","Detection of cosmic neutrinos can answer very important questions related to some extremely energetic yet unexplained astrophysical sources such as: compact binary stars, accreting black holes, supernovae etc., key elements in understanding the evolution and fate of the Universe. Moreover, these particles carry the highest
energies per particle known to man, impossible to achieve in any present or foreseen man made accelerator devices thus their detection can test and probe extreme high energy physics. 
One of the newest techniques for measuring high energy cosmic neutrinos regards their radio detection in natural salt mines. A first and essential step is to determine experimentally the radio wave attenuation length in salt mines, and this will represent the main goal of this project. The results shall be used to estimate the implications on the construction of the detector. The outcome of this project may rejuvenate the radio detection in salt technique and be a compelling case for Romanian involvement. The same measurements can be used: to validate and improve previous work on theoretical simulation models of propagation in heterogeneous media –a regime not very well understood (which represents another goal of the project), and to study the behavior of classical antennas in non-conventional media (the third major goal).
The results to be obtained would be immediately relevant in determination of the key parameters that describe a cosmic neutrino detector, its performances and limitations. The events detected by such a   telescope will allow identification of individual sources indicating a step forward in “neutrino astronomy”. The extensive propagation and antenna behavior studies in heterogeneous media will be in the direct interest for the scientific community and have a prompt impact in telecommunications theory and industry.","185925","2016-11-01","2018-10-31"
"COSPSENA","Coherence of Spins in Semiconductor Nanostructures","Dominik Max Zumbühl","UNIVERSITAT BASEL","Macroscopic control of quantum states is a major theme in much of modern physics because quantum coherence enables study of fundamental physics and has promising applications for quantum information processing. The potential significance of quantum computing is recognized well beyond the physics community. For electron spins in GaAs quantum dots, it has become clear that decoherence caused by interactions with the nuclear spins is a major challenge.   We propose to investigate and reduce hyperfine induced decoherence with two complementary approaches: nuclear spin state narrowing and nuclear spin polarization. We propose a new projective state narrowing technique: a large, Coulomb blockaded dot measures the qubit nuclear ensemble, resulting in enhanced spin coherence times. Further, mediated by an interacting 2D electron gas via hyperfine interaction, a low temperature nuclear ferromagnetic spin state was predicted, which we propose to investigate using a quantum point contact as a nuclear polarization detector.   Estimates indicate that the nuclear ferromagnetic transition occurs in the sub-Millikelvin range, well below already hard to reach temperatures around 10 mK. However, the exciting combination of interacting electron and nuclear spin physics as well as applications in spin qubits give ample incentive to strive for sub-Millikelvin temperatures in nanostructures. We propose to build a novel type of nuclear demagnetization refrigerator aiming to reach electron temperatures of 0.1 mK in semiconductor nanostructures.   This interdisciplinary project combines Microkelvin and nanophysics, going well beyond the status quo. It is a challenging project that could be the beginning of a new era of coherent spin physics with unprecedented quantum control. This project requires a several year commitment and a team of two graduate students plus one postdoctoral fellow.","1377000","2008-06-01","2013-05-31"
"COSYM","Computational Symmetry for Geometric Data Analysis and Design","Mark Pauly","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The analysis and synthesis of complex 3D geometric data sets is of crucial importance in many scientific disciplines (e.g. bio-medicine, material science, mechanical engineering, physics) and industrial applications (e.g. drug design, entertainment, architecture). We are currently witnessing a tremendous increase in the size and complexity of geometric data, largely fueled by significant advances in 3D acquisition and digital production technology. However, existing computational tools are often not suited to handle this complexity.

The goal of this project is to explore a fundamentally different way of processing 3D geometry. We will investigate a new generalized model of geometric symmetry as a unifying concept for studying spatial organization in geometric data. This model allows exposing the inherent redundancies in digital 3D data and will enable truly scalable algorithms for analysis, processing, and design of large-scale geometric data sets. The proposed research will address a number of fundamental questions: What is the information content of 3D geometric models? How can we represent, store, and transmit geometric data most efficiently? Can we we use symmetry to repair deficiencies and reduce noise in acquired data? What is the role of symmetry in the design process and how can it be used to reduce complexity?

I will investigate these questions with an integrated approach that combines thorough theoretical studies with practical solutions for real-world applications.
The proposed research has a strong interdisciplinary component and will consider the same fundamental questions from different perspectives, closely interacting with scientists of various disciplines, as well artists, architects, and designers.","1160302","2011-02-01","2016-01-31"
"COTOFLEXI","Computational Modelling, Topological Optimization and Design of Flexoelectric Nano Energy Harvesters","Xiaoying ZHUANG","GOTTFRIED WILHELM LEIBNIZ UNIVERSITAET HANNOVER","Flexoelectricity is the generation of electric polarization under mechanical strain gradient or mechanical deformation due to the electric field gradient (converse flexo). It is a more general phenomenon than the linear change in polarization due to stress, the piezoelectric effect. Flexoelectricity exists in a wider range of centrosymmetric materials especially nontoxic materials useful for biomedical application. It grows dominantly in energy density at submicro- or nanoscale enabling self-powered nano devices such as body implants and small-scale wireless sensors. Among the emerging applications of flexoelectricity, energy harvesters are the basic front devices of wide technological impact. Despite the advantages offered by flexoelectricity, research in this field is still in germination. Experiments are limited in measuring, explaining and quantifying some key phenomena. Materials engineering and engineering of strain are the key challenges to bring energy harvesting structures/systems to become a viable technology. Accomplishment of this task pressingly requires a robust modelling tool that can assist the development of flexoelectric energy harvesters. Hence, the aim of the project is to develop a computational framework to support the characterization, design, virtual testing and optimization of the next generation nano energy harvesters. It will be able to (1) predict the energy conversion efficiency and output voltage influenced by layout and surface effects of structures in 3D, (2) to virtually test the performance with various vibrational dynamic conditions, and (3) to break through current designs of simple geometry for flexoelectric structures by optimization considering manufacturing constraints. Innovative metamaterial/3D folding energy harvesters expectantly outperforming current piezoelectric energy harvesters of the same size will be manufactured and tested.","1499938","2019-08-01","2024-07-31"
"CounterLIGHT","Interaction and Symmetry Breaking of Counterpropagating Light","Pascal Del Haye","NPL MANAGEMENT LIMITED","Light is generally expected to travel through media independent of its direction. Exceptions can be achieved eg. through polarization changes induced by magnetic fields (known as the Faraday effect) together with polarization-sensitive birefringent materials. However, light can also be influenced by the presence of a counterpropagating light wave. We have recently shown that this leads to the surprising consequence that light sent into tiny glass rings (microresonators) can only propagate in one direction, clockwise or counterclockwise, but not in both directions simultaneously. When sending exactly the same state of light (same power and polarization) into a microresonator, nonlinear interaction induces a spontaneous symmetry breaking in the propagation of light. In this proposal we plan to investigate the fundamental physics and a variety of ground-breaking applications of this effect. In one proposed application, this effect will be used for optical nonreciprocity and the realization of optical diodes in integrated photonic circuits that do not rely on magnetic fields (an important key element in integrated photonics). In another proposed experiment we plan to use the spontaneous symmetry breaking to demonstrate microresonator-based optical gyroscopes that have the potential to beat state-of-the-art sensors in both size and sensitivity. Additional research projects include experiments with all-optical logic gates, photonic memories, and near field sensors based on counterpropagating light states. Finally, we plan to demonstrate a microresonator-based system for the generation of dual-optical frequency combs that can be used for real-time precision spectroscopy in future lab-on-a-chip applications. On the fundamental physics side, our experiments investigate the interaction of counterpropagating light in a system with periodic boundary conditions. The fundamental nature of this system has the potential to impact other fields of science far beyond optical physics.","1500000","2018-03-01","2023-02-28"
"CoupledIceClim","Coupled climate and Greenland ice sheet evolution:past, present and future","Miren Vizcaino Trueba","TECHNISCHE UNIVERSITEIT DELFT","The Greenland ice sheet (GrIS) is losing mass at an increasing pace, in response to atmospheric and ocean forcing. The mechanisms leading to the observed mass loss are poorly understood. It is not clear whether the current trends will be sustained into the future, and how they are affected by regional and global climate variability. In addition, the impacts of Greenland deglaciation on the local and global climate are not well known. This project aims to explain the relationship between GrIS surface melt trends and climate variability, to determine the timing and impacts of multi-century deglaciation of Greenland, and to explain the relationship between ongoing and previous deglaciations during the last interglacial and the Holocene. For this purpose, we will use the Community Earth System Model (CESM), the first full-complexity global climate model to include interactive ice sheet flow and a realistic and physical-based simulation of surface mass balance (the difference between surface accumulation and losses from runoff and sublimation).  This tool will include for the first time a large range of temporal and spatial scales of ice sheet-climate interaction in the same model. Previous work has been done with oversimplified and/or uncoupled representations of ice sheet and climate processes, for instance with simplified ocean and/or atmospheric dynamics in Earth System Models of Intermediate Complexity, with fixed topography and prescribed ocean components in Regional Climate Models, or with highly parameterized snow albedo and/or melt schemes in General Circulation Models. This project will provide new insights into the coupling between the GrIS and climate change, will lead widespread integration of ice sheets as a new and indispensable component of complex Earth System Models, and will advance our understanding of present and past climate dynamics.","1677282","2016-06-01","2021-05-31"
"CoVeCe","Coinduction for Verification and Certification","Damien Gabriel Jacques Pous","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Software and hardware bugs cost hundreds of millions of euros every year to companies and administrations. Formal methods like verification provide automatic means of finding some of these bugs. Certification, using proof assistants like Coq or Isabelle/HOL, make it possible to guarantee the absence of bugs (up to a certain point).

These two kinds of tools are crucial in order to design safer programs and machines. Unfortunately, state-of-the art tools are not yet satisfactory. Verification tools often face state-explosion problems and require more efficient algorithms; certification tools need more automation: they currently require too much time and expertise, even for basic tasks that could be handled easily through verification.

In recent work with Bonchi, we have shown that an extremely simple idea from concurrency theory could give rise to algorithms that are often exponentially faster than the algorithms currently used in verification tools.

My claim is that this idea could scale to richer models, revolutionising existing verification tools and providing algorithms for problems whose decidability is still open.

Moreover, the expected simplicity of those algorithms will make it possible to implement them inside certification tools such as Coq, to provide powerful automation techniques based on verification techniques.  In the end, we will thus provide efficient and certified verification tools going beyond the state-of-the-art, but also the ability to use such tools inside the Coq proof assistant, to alleviate the cost of certification tasks.","1407413","2016-04-01","2021-03-31"
"COYOTE","Coherent Optics Everywhere: a New Dawn for Photonic Networks","Bernhard SCHRENK","AIT AUSTRIAN INSTITUTE OF TECHNOLOGY GMBH","The widespread adoption of the Internet and its influence on our daily life is unquestioned. Global Zettabyte traffic has rendered photonics as indispensable for the communication infrastructure. While direct signal detection has been dismissed in radio communications decades ago, it prevails in short- and medium-reach optics in virtue of its simplicity. In such an environment photonics can only rely on incremental improvements, whereas it desperately seeks for disruptive concepts.
COYOTE envisions a novel coherent homodyne transceiver concept for analogue signals and access to higher-order formats with efficiencies of 10 bits/symbol. On top of this, high-fidelity transport of multi-band 5G radio signals in the millimetre-wave range up to 100 GHz will be enabled by analogue coherent photonics while mitigating energy-hungry digital signal processing. COYOTE takes one more leap and dares the contradictory full-duplex data transmission in virtue of its novel reception engine to ultimately guarantee a lean solution with greatly simplified yet flexible “hardware”. 
The key asset of COYOTE’s coherent engine will be a locked laser with improved coherence characteristics together with a flexible modulator-detector element, which is capable to emulate direct-detection systems in a transparent way while giving birth to novel networking concepts. Exploration of the 3D Stokes and 2D quadrature spaces through a segmented receiver architecture will boost the spectral efficiency to >10 bits/s/Hz.
It is the lean and yet efficient coherent transceiver methodology of COYOTE that will remove the currently existing boundary between direct-detection and coherent systems in the midst of network reaches. By coherently “reviving” these telecom segments of integrated wireline-wireless access networks, optical interconnects for intra-datacentre connectivity and even quantum communication, an order-of-magnitude improvement in terms of spectral efficiency x reach product will be gained.","1500000","2019-01-01","2023-12-31"
"CPFTMW","New Applications of Broadband Rotational Spectroscopy","Nicholas Walker","UNIVERSITY OF NEWCASTLE UPON TYNE","""The recent invention of the chirped-pulse, Fourier transform microwave (CP-FTMW) spectrometer will allow application of rotational spectroscopy to a greatly expanded range of challenges over the next decade. The proposed work will apply the state-of-the-art CP-FTMW spectrometer at the University of Bristol to major themes in both fundamental and applied research. Palladium, platinum and nickel catalysts are of central importance in synthetic chemistry and the industrial production of chemicals. The microwave spectra of Mn...(C2H4), Mn...(C2H2), Mn-CCH and Mn-CH2 (M= Ni, Pd or Pt, n=1-3) will be measured to characterise structural and other changes induced in C2H4 and C2H2 by attachment to these metals. The results will inform understanding of the mechanisms of catalysis. The role and function of metal ions will be another major theme of the programme. Infrared-microwave (IR-MW) double resonance will be used to determine structures for (H2O)n...MCl and (H2O)n...MF (where M=Cu, Ag or Au and  n=1-6) to gain insight into the interactions that govern solvation shell formation. Copper ions have biological significance and govern the conformations adopted by proteins that include amyloid B-peptide, the production of which is associated with Alzheimer’s disease and cytochrome C oxidase which is important for respiration. IR-MW double resonance will be used to probe the structure of complexes where the ionic copper atom of a copper chloride molecule coordinates to glycine, imidazole, alanine, histidine and cysteine, respectively. The proposed work will provide precise data for modelling of interactions in protein active sites. Finally, technical innovations will be implemented to support applications of the instrument in chemical analysis. A GC-CP-FTMW (GC=gas chromatography) instrument will be constructed to allow analysis of the composition of wine and fruit juice with the aim of establishing CP-FTMW spectroscopy as a useful tool for commercial applications.""","1497862","2012-11-01","2017-10-31"
"CPROVER","Validation of Concurrent Software Across Abstraction Layers","Daniel Heinrich Friedrich Kroening","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The cost of software quality assurance (QA) dominates the cost of IT development and maintenance projects. QA is frequently on the critical path to market. Effective software QA is therefore decisive for the competitiveness of numerous industries that rely on IT, and essential for government tasks that rely heavily on IT.

This research programme will provide a pragmatic solution to the most pressing issue in software QA in mainstream software engineering: the use of concurrency. Programmers make use of numerous favors of concurrency in order to achieve better scalability, savings in power, increase reliability, and to boost performance. The need for software that makes diligent use of concurrent computational resources has been exacerbated by power-efficient multi-core CPUs, which are now widely deployed, but still unfertilized due to the lack of appropriate software. Concurrent software is particularly difficult to test, as bugs depend on particular interlavings between the sequential computations. Defects are therefore difficult to reproduce and diagnose, and often elude even very experienced programmers.

We propose to develop new, ground-braking reasoning and testing technology for this kind of software,
with the goal of cutting the staff effort in QA of concurrent effort in half. We will use a tightly integrated combination of scalable and performant testing technology and Model Checking and abstract interpretation engines to prune the search. Every aspect of the research programme is geared towards improving the productivity of the average application programmer. Our theories and reasoning technology will therefore be implemented in a seamless fashion within the existing, well-accepted programming environments Visual Studio and Eclipse, in close collaboration with Microsoft and IBM.","1368355","2011-12-01","2017-11-30"
"CRAMIS","Critical phenomena in random matrix theory and integrable systems","Tom Claeys","UNIVERSITE CATHOLIQUE DE LOUVAIN","The main goal of the project is to create a research group on critical phenomena in random matrix theory and integrable systems at the Université Catholique de Louvain, where the PI was recently appointed.
Random matrix ensembles, integrable partial differential equations and Toeplitz determinants will be the main research topics in the project. Those three models show intimate connections and they all share certain properties that are, to a large extent, universal. In the recent past it has been showed that Painlevé equations play an important and universal role in the description of critical behaviour in each of these areas. In random matrix theory, they describe the local correlations between eigenvalues in appropriate double scaling limits; for integrable partial differential equations such as the Korteweg-de Vries equation and the nonlinear Schrödinger equation, they arise near points of gradient catastrophe in the small dispersion limit; for Toeplitz determinants they describe phase transitions for underlying models in statistical physics.
The aim of the project is to study new types of critical behaviour and to obtain a better understanding of the remarkable similarities between random matrices on one hand and integrable partial differential equations on the other hand. The focus will be on asymptotic questions, and one of the tools we plan to use is the Deift/Zhou steepest descent method to obtain asymptotics for Riemann-Hilbert problems. Although many of the problems in this project have their origin or motivation in mathematical physics, the proposed techniques are mostly based on complex and classical analysis.","1130400","2012-08-01","2017-07-31"
"CRASH","CRyptographic Algorithms and Secure Hardware","François-Xavier Standaert","UNIVERSITE CATHOLIQUE DE LOUVAIN","Side-channel attacks are an important threat against cryptographic implementations in which an adversary takes advantage of physical leakages, such as the power consumption of a smart card, in order to recover secret information. By circumventing the models in which standard security proofs are obtained, they can lead to powerful attacks against a large class of devices. As a consequence, formalizing implementation security and efficiently preventing side-channel attacks is one of the most challenging open problems in modern cryptography. Physical attacks imply new optimization criteria, with potential impact on the way we conceive algorithms and the way we design circuits. By putting together mathematical and electrical engineering problems, just as they are raised in reality, the CRASH project is expected to develop concrete basements for the next generation of cryptographic algorithms and their implementation. For this purpose, three main directions will be considered. First, we will investigate sound evaluation tools for side-channel attacks and validate them on different prototype chips. Second, we will consider the impact of physical attacks on the mathematical aspects of cryptography, both destructively (i.e. by developing new attacks and advanced cryptanalysis tools) and constructively (i.e. by investigating new cipher designs and security proof techniques). Third, we will evaluate the possibility to integrate physical security analysis into the design tools of integrated circuits (e.g. in order to obtain “physical security aware” compilers). Summarizing, this project aims to break the barrier between the abstractions of mathematical cryptography and the concrete peculiarities of physical security in present microelectronic devices. By considering the system and algorithmic issues in a unified way, it is expected to get rid of the incompatibilities between the separate formalisms that are usually considered in order to explain these concurrent realities.","1498874","2011-10-01","2016-09-30"
"CREATES","Classifying the Range of Exoplanetary Atmospheres using Transmission and Emission Spectroscopy","David Kent Sing","THE UNIVERSITY OF EXETER","""Rarely in astrophysics are there opportunities to spectrally classify a completely new group of astrophysical objects.  This is the challenge facing the exoplanets christened “hot Jupiters”. The detection and subsequent spectroscopic information now achievable for a large number of these exoplanets are now allowing for detailed comparative exoplanetology.  This project uses a twofold approach to advance both the theory and observation of these exoplanets beyond their current limitations.  Hot Jupiter atmospheric spectra are built from two large observational survey programmes headed by Dr. Sing to obtain a vast amount of high quality data on transmission spectra.  One large programme uses the HST which alone will quadruple the number of broadband exoplanet transmission spectra.  The Hubble survey will be augmented by a large programme on the GTC telescope, where we will put efforts into pioneering multi-object spectroscopy, capable of delivering space-like quality spectra.  Both large programmes will be further complemented by followup observations, as well as existing near-IR spectroscopy.  This project will combine this plethora of data in a coherent fashion, enabling studies of nearly the entire planetary atmosphere.  Our observational efforts will be combined with a broad and inclusive theoretical modeling programme, where we will incorporate clouds and hazes, modelling the complete atmosphere in a self-consistent manner with a 3D global circulation model.  Our library of transmission spectra across the hot-Jupiter class will be used to address long outstanding and complex issues.  We will focus our efforts on two key areas, addressing why some hot Jupiters have hazes & clouds while others do not, and the outstanding issue on the presence or absence of stratospheres.   For the first time a comprehensive set of high quality exoplanet spectra will be available with which to inter-compare using the required set of theoretical tools.""","1495824","2013-11-01","2018-10-31"
"CREMA","Charge radius experiment with muonic atoms","Randolf Pohl","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","""A measurement of the 2S-2P transition frequencies (Lamb shift) in the muonic helium-3 and 4 ions by means of laser spectroscopy is proposed.  This will lead to a ten times more accurate determination of the root-mean-square (rms) charge radii of the He-3 and He-4 nuclei. The radius of the magnetic moment distribution inside the He-3 nucleus will result from the hyperfine structure in muonic 3He.
In the muonic helium ion, a single negative muon orbits the helium nucleus. The muon is a point-like lepton, just as the electron, except it is about 200 times heavier. This gives a factor of 200^3 = 10^7 enhancement of nuclear finite size effects on the energy levels of muonic vs. regular (electonic) Helium ions. Muonic helium is the ideal sytem to study the He nuclear size.
The CREMA project has four main aims:
(1) Solve the """"proton size puzzle"""" created by our recently completed muonic hydrogen project [R. Pohl et al., """"The size of the proton"""", Nature 466, 213 (2010)]. Our tenfold improvement of the proton charge radius resulted in a five sigma discrepancy with the 2006 CODATA value, which is mostly based on hydrogen spectroscopy.  This poses a serious challenge to bound-state QED, and may even point towards new physics. CREMA will help to clarify this.
(2) Absolute nuclear charge radii of all helium isotopes He-3,4,6,8 will result from CREMA.  The charge radius differences are precisely known, but the absolute size of the He-4 anchor nucleus can best be measured in muonic helium. Absolute charge radii are a more stringent benchmark for few-nucleon nuclear models than the radius difference.
(3) Test of bound-state QED: Spectroscopy of regular He+ ions is underway. He+ (Z=2) is more sensitive than hydrogen (Z=1) to higher-order QED contributions which scale as Z^5. An accurate He charge radius from CREMA is mandatory for this.
(4) An improved value of the Rydberg constant will result from the He+ spectroscopy only with the improved charge radius from CREMA.""","1499976","2011-11-01","2016-10-31"
"CriBLaM","Critical behavior of lattice models","Hugo DUMINIL-COPIN","INSTITUT DES HAUTES ETUDES SCIENTIFIQUES","Statistical physics is a theory allowing the derivation of the statistical behavior of macroscopic systems from the description of the interactions of their microscopic constituents. For more than a century, lattice models (i.e. random systems defined on lattices) have been introduced as discrete models describing the phase transition for a large variety of phenomena, ranging from ferroelectrics to lattice gas.

In the last decades, our understanding of percolation and the Ising model, two classical exam- ples of lattice models, progressed greatly. Nonetheless, major questions remain open on these two models.

The goal of this project is to break new grounds in the understanding of phase transition in statistical physics by using and aggregating in a pioneering way multiple techniques from proba- bility, combinatorics, analysis and integrable systems. In this project, we will focus on three main goals:

Objective A Provide a solid mathematical framework for the study of universality for Bernoulli percolation and the Ising model in two dimensions.
Objective B Advance in the understanding of the critical behavior of Bernoulli percolation and the Ising model in dimensions larger or equal to 3.
Objective C Greatly improve the understanding of planar lattice models obtained by general- izations of percolation and the Ising model, through the design of an innovative mathematical theory of phase transition dedicated to graphical representations of classical lattice models, such as Fortuin-Kasteleyn percolation, Ashkin-Teller models and Loop models.

Most of the questions that we propose to tackle are notoriously difficult open problems. We believe that breakthroughs in these fundamental questions would reshape significantly our math- ematical understanding of phase transition.","1499912","2018-09-01","2023-08-31"
"CRISP","Towards compressive information processing systems","Enrico Magli","POLITECNICO DI TORINO","This proposal targets the emerging frontier research field of compressive sampling (CS), and particularly its application in the framework of complex information processing systems, including several related innovative and unconventional aspects. Future systems will have to handle unprecedented amounts of information such as those generated in multiview video, medical and hyperspectral imaging applications, increasingly suffering from limited communication and computational resources. CS is a breakthrough technology that will have a profound impact on how these systems are conceived. It offers a viable and elegant solution, acquiring and representing an information signal through a small set of linear projections of it, allowing to dramatically reduce communication, storage and processing requirements, and is one of the topics that will dominate signal processing research in the next years. At the core of this research proposal is the concept of employing CS not only as a standalone tool, but inside an information processing system. The main challenge is to develop theory and algorithms that will allow to perform all signal manipulations typical of conventional systems directly on the linear measurements, as reconstructing the signal samples would be unfeasible due to excessive complexity. Such operations include compression, encryption, communication, reconstruction, signal analysis, information extraction and decision, and distributed signal processing, leading to a very multidisciplinary and technically challenging research agenda. Ultimately, our research aims at developing and demonstrating the fundamental tools that will fuel next-generation information processing systems with an order-of-magnitude better performance at a lower cost than today. Europe has several successful industries active in communications and signal processing. The future success of these sectors critically depends on the ability to innovate and integrate new technology.","1390000","2011-11-01","2017-06-30"
"CRITIQUEUE","Critical queues and reflected stochastic processes","Johannes S.H. Van Leeuwaarden","TECHNISCHE UNIVERSITEIT EINDHOVEN","Our primary motivation stems from queueing theory, the branch of applied probability that deals with congestion phenomena. Congestion levels are typically nonnegative, which is why reflected stochastic processes arise naturally in queueing theory. Other applications of reflected stochastic processes are in the fields of branching processes and random graphs.
We are particularly interested in critically-loaded queueing systems (close to 100% utilization), also referred to as queues in heavy traffic. Heavy-traffic analysis typically reduces complicated queueing processes to much simpler (reflected) limit processes or scaling limits. This makes the analysis of complex systems tractable, and from a mathematical point of view, these results are appealing since they can be made rigorous. Within the large
body of literature on heavy-traffic theory and critical stochastic processes, we launch two new research lines:
(i) Time-dependent analysis through scaling limits.
(ii) Dimensioning stochastic systems via refined scaling limits and optimization.
Both research lines involve mathematical techniques that combine stochastic theory with asymptotic theory, complex analysis, functional analysis, and modern probabilistic methods. It will provide a platform enabling collaborations between researchers in pure and applied probability and researchers in performance analysis of queueing systems. This will particularly be the case at TU/e, the host institution, and at
the affiliated institution EURANDOM.","970800","2010-08-01","2016-07-31"
"Crosstag","Unravelling cross-presentation pathways using a chemical biology approach","Sander Van kasteren","UNIVERSITEIT LEIDEN","Immune therapies are therefore currently being pursued to reinvigorate the immune reaction against tumours. This is not trivial, as the right type of immune cells must be activated against a tumour-specific antigen. One method to achieve this is by targeting tumour antigens to certain cross-presentation-promoting receptors on antigen presenting cells. The most intriguing of these is the mannose receptor (MR) as the method by which it does this is unknown.  
This glycoprotein-binding receptor appears to have two functions on APCs: general uptake-enhancement and, in certain isolated cases, cross-presentation-enhancment. What ligand parameters are important in causing cross-presentation enhancement is not known. Current tools, such as anti-MR antibodies and randomly glycosylated ligands fail to selectively enhance cross-presentation. The main aim of this proposal is to determine what structural parameters of the glycoprotein antigen result in enhanced cross-presentation upon MR-ligation. 
I will synthesise a library of biologically traceable single glycoform ligands - with controlled variation in glycan nature, stoichiometry and positioning - for the MR and study differences in uptake, routing and antigen presentation. 
A 2nd aim is to uncover what happens to the antigen after uptake by the MR. I.e. whether changes in antigen routing and proteolysis are responsible for enhanced cross presentation of different glycoforms. A 3rd aim is to develop a new method to study the kinetics of surface appearance of epitopes without T-cell reagents to quantify differences between glycoforms. 
With this approach I aim to gain new insight into methods for enhancing cross-presentation resulting in improved immune therapies against cancer. My background in carbohydrate and protein modification chemistry will provide the toolkit to synthesise the relevant reagents and my background in immunology will ensure the successful immunological validation of the synthetic single glycoforms.","1500000","2015-05-01","2020-04-30"
"CRYOMAT","Antifreeze GlycoProtein Mimetic Polymers","Matthew Ian Gibson","THE UNIVERSITY OF WARWICK","Fish living in polar oceans have evolved an elegant, macromolecular, solution to survive in sub-zero water: they secrete antifreeze (glyco)proteins (AFGPs) which have several ‘antifreeze’ effects, including ice recrystallization inhibition (IRI) - they slow the rate of ice crystal growth. Ice crystal growth is a major problem in settings as diverse as oil fields, wind turbines, road surfaces and frozen food. Analysis of the process of cryopreservation, whereby donor cells are frozen for later use, has revealed that ice recrystallization is a major contributor to cell death upon thawing. Enhanced cryopreservation methods are particularly needed for stem cell storage to maximize the use of this currently limited resource, but also to enable storage of clinically transfused cells such as platelets and red blood cells. AFGPs have thus far not found application in cryopreservation due to their low availability from natural sources, extremely challenging synthesis, indications of cytotoxicity, but more importantly they have a side effect of shaping ice crystals into needle-shapes which pierces cells’ membranes, killing them. The aim of this ambitious project is to take a multidisciplinary approach to develop synthetic polymers as tunable, scalable and accessible bio-mimetics of AFGPs, which specifically reproduce only the desirable IRI properties.  Precision synthetic and biological methods will be applied to access both vinyl- and peptide- based materials with IRI activity. The bio-inspired approach taken here will include detailed biophysical analysis of the polymer-ice interactions and translation of this understanding to real cryopreservation scenarios using blood-borne cells and human stem cells. In summary, this ambitious project takes inspiration from Nature's defense mechanisms that have evolved to allow life to flourish in extreme environments and will employ modern polymer chemistry to apply it to a real clinical problem; cryopreservation.","1496439","2015-06-01","2020-05-31"
"CRYSP","CRYSP: A Novel Framework for Collaboratively Building Cryptographically Secure Programs and their Proofs","Karthikeyan Bhargavan","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","The field of software security analysis stands at a critical juncture.
Applications have become too large for security experts to examine by hand,
automated verification tools do not scale, and the risks of deploying insecure software are too great to tolerate anything less than mathematical proof.
A radical shift of strategy is needed if programming and analysis techniques are to  keep up in a networked world where increasing amounts of governmental and individual information are generated, manipulated, and accessed through web-based software applications.

The basic tenet of this proposal is that the main roadblock to the security verification of a large program is not its size, but rather the lack of precise security specifications for the underlying libraries and security-critical application code. Since, large-scale software is often a collaborative effort, no single programmer knows all the design goals. Hence, this proposal advocates a collaborative specification and verification framework that helps teams of programmers write detailed security specifications incrementally and then verify that they are satisfied by the source program.

The main scientific challenge is to develop new program verification  techniques that can  be applied collaboratively, incrementally, and modularly to application and library code written in mainstream programming languages. The validation of this approach will be through substantial case studies. Our aim is to produce the first verified open source cryptographic protocol library and the first web applications with formal proofs of security.

The proposed project is bold and ambitious, but it is certainly feasible, and has the potential to change how software security is analyzed for years to come.","1406726","2010-11-01","2015-10-31"
"CRYSTENG-MOF-MMM","Crystal Engineering of Metal Organic Frameworks for application in Mixed Matrix Membranes","Jorge Gascon Sabate","TECHNISCHE UNIVERSITEIT DELFT","With this proposal, I seek to develop the gas separating membranes of the future. The overall aim is to produce composite membranes comprising engineered Metal Organic Framework (MOF) particles and polymers in the form of Mixed Matrix Membranes (MMMs). By applying these new membranes, energetically more efficient separations will be possible.
Despite the superior performance of membranes only based on crystalline materials like zeolites or MOFs, polymeric membranes rule the commercial scene thanks to their easy processing, high reproducibility and mechanical strength. However, the existing polymeric membrane materials are not optimal: improvements in permeability are always at the expense of selectivity and vice versa, while plasticization threatens their application at high pressures. This research aims at utilizing the best of both fields by combining the high selectivity of MOFs with the easy processing of polymers in the form of Mixed Matrix Membranes.
The main barrier to achieve this goal is the optimization of the MOF-polymer interaction and mass transport through the composite. This is very challenging because chemical compatibility, particle morphology and filler dispersion play a key role. Innovatively the project will be the first systematic study into this multi-scale phenomenon with investigations at all relevant interactions, including MOF particle tuning targeting the application in MMMs.
A thorough study on the synthesis of the selected MOF structures and on the performance of the composites will allow engineering MOFs at the molecular and particle levels, resulting in higher selectivity and faster transport. The use of flexible MOF structures will not only allow a better membrane processing but will also reduce polymer plasticization.
This research will deliver a new generation of mixed matrix membranes, outperforming the state of the art polymeric membranes.","1467510","2013-09-01","2018-08-31"
"CRYSYS","Crystallisation Systems Engineering –  Towards a next generation of intelligent crystallisation systems","Zoltan Kalman Nagy","LOUGHBOROUGH UNIVERSITY","The project proposes the development of an intelligent crystallisation system by combining state-of-the-art process analytical technologies and novel model-based and statistical feedback control approaches, to provide a fully integrated and adaptive system for efficient engineering of particulate products. The developed adaptive and robust control approaches will be incorporated in a Crystallisation Process Informatics System, to provide an intelligent decision support system, which triggers the suitable control algorithm taking into account the effect of crystallisation on the downstream processing units and final product properties. In this way crystallisation becomes a key intelligent “process actuator” in the whole production system, that manipulates final properties of the solid product taking into account operational, regulatory and economic constraints of the entire process, opening the way towards novel product engineering approaches. The project will bring the implementation of a new generation of integrated, intensified and intelligent crystallisation systems with drastically improved flexibility, predictability, stability and controllability. The system will be used for detailed evaluation of the current paradigm shift from batch to continuous processes in the pharmaceutical industries. Besides providing a breakthrough in crystallisation science the results could revolutionise the methods in which crystallisation will be designed and controlled in the future, yielding to the development of the emerging research field of Pharmaceutical Systems Engineering, by providing a comprehensive framework for the development of novel integrated pharmaceutical production units and product engineering technologies, for sustainable pharmaceutical production, with the aim of reducing time-to-market and increasing product quality, therefore providing considerable increase in quality of life, for example, by making new products available more quickly and at lower cost.","1263702","2011-09-01","2017-08-31"
"CSEM","The Collaborative Seismic Earth Model Project","Andreas FICHTNER","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Seismic tomography images of the Earth's interior are key to the characterisation of earthquakes, natural resource exploration, seismic risk assessment, tsunami warning, and studies of geodynamic processes. While tomography has drawn a fascinating picture of our planet, today's individual researchers can exploit only a fraction of the rapidly expanding seismic data volume. Applications relying on tomographic images lag behind their potential; fundamental questions remain unanswered: Do mantle plumes exist in the deep Earth? What are the properties of active faults, and how do they affect earthquake ground motion?

To address these questions and to ensure continued progress of seismic tomography in the 'Big Data' era, I propose new technological developments that enable a paradigm shift in Earth model construction towards a Collaborative Seismic Earth Model (CSEM). Fully accounting for the physics of wave propagation in the complex 3D Earth, the CSEM is envisioned to evolve successively through a systematic group effort of my team, thus going beyond the tomographic models that individual researchers may construct today.

I will develop the technological foundation of the CSEM and integrate these developments in studies of  large-earthquake rupture processes and the convective pattern of the Earth's mantle in relation to surface geology. The CSEM project will bridge the gap between regional and global tomography, and deliver the first multiscale model of the Earth where crust and mantle are jointly resolved. The CSEM will lead to a dramatic increase in the exploitable seismic data volume, and set new standards for the construction and reproducibility of tomographic Earth models.

Beyond this project, the CSEM will be openly accessible through the European Plate Observing System  (EPOS). It will then offer Earth scientists the unique opportunity to join forces in the discovery of multiscale Earth structure by systematically building on each other's results.","1367500","2017-01-01","2021-12-31"
"CSG","C° symplectic geometry","Lev Buhovski","TEL AVIV UNIVERSITY","""The objective of this proposal is to study """"continuous"""" (or C^0) objects, as well as C^0 properties of smooth objects, in the field of symplectic geometry and topology. C^0 symplectic geometry has seen spectacular progress in recent years, drawing attention of mathematicians from various background. The proposed study aims to discover new fascinating C^0 phenomena in symplectic geometry.

One circle of questions concerns symplectic and Hamiltonian homeomorphisms. Recent studies indicate that these objects possess both rigidity and flexibility, appearing in surprising and counter-intuitive ways. Our understanding of symplectic and Hamiltonian homeomorphisms is far from being satisfactory, and here we intend to study questions related to action of symplectic homeomorphisms on submanifolds. Some other questions are about Hamiltonian homeomorphisms in relation to the celebrated Arnold conjecture. The PI suggests to study spectral invariants of continuous Hamiltonian flows, which allow to formulate the C^0 Arnold conjecture in higher dimensions. Another central problem that the PI will work on is the C^0 flux conjecture.

A second circle of questions is about the Poisson bracket operator, and its functional-theoretic properties. The first question concerns the lower bound for the Poisson bracket invariant of a cover, conjectured by L. Polterovich who indicated relations between this problem and quantum mechanics. Another direction aims to study the C^0 rigidity versus flexibility of the L_p norm of the Poisson bracket. Despite a recent progress in dimension two showing rigidity, very little is known in higher dimensions.  The PI proposes to use combination of tools from topology and from hard analysis in order to address this question, whose solution will be a big step towards understanding functional-theoretic properties of the Poisson bracket operator.""","1345282","2017-10-01","2022-09-30"
"CSI.interface","A molecular interface science approach: Decoding single molecular reactions and interactions at dynamic solid/liquid interfaces","Markus Valtiner","TECHNISCHE UNIVERSITAET WIEN","After decades of truly transformative advancements in single molecule (bio)physics and surface science, it is still no more than a vision to predict and control macroscopic phenomena such as adhesion or electrochemical reaction rates at solid/liquid interfaces based on well-characterized single molecular interactions. How exactly do inherently dynamic and simultaneous interactions of a countless number of interacting “crowded” molecules lead to a concerted outcome/property on a macroscopic scale? 
Here, I propose a unique approach that will allow us to unravel the scaling of single molecule interactions towards macroscopic properties at adhesive and redox-active solid/liquid interfaces. Combining Atomic Force Microscopy (AFM) based single molecule force spectroscopy and macroscopic Surface Forces Apparatus (SFA) experiments CSI.interface will (1) derive rules for describing nonlinearities observed in complex, crowded (water and ions) and chemically diverse adhesive solid/liquid interfaces; (2) uniquely characterize all relevant kinetic parameters (interaction free energy and transition states) of electrochemical and adhesive reactions/interactions of single molecules at chemically defined surfaces as well as electrified single crystal facets and step edges. Complementary, (3) my team and I will build a novel molecular force apparatus in order to measure single-molecule steady-state dynamics of both redox cycles as well as binding unbinding cycles of specific interactions, and how these react to environmental triggers. 
CSI.interface goes well beyond present applications of AFM and SFA and has the long-term potential to revolutionize our understanding of interfacial interaction under steady state, responsive and dynamic conditions. This work will pave the road for knowledge based designing of next-generation technologies in gluing, coating, bio-adhesion, materials design and much beyond.","1499750","2016-07-01","2021-06-30"
"CSINEUTRONSTAR","The physics and forensics of neutron star explosions","Anna Louise Watts","UNIVERSITEIT VAN AMSTERDAM","Neutron stars offer a unique environment in which to develop and test theories of the strong force. Densities in neutron star cores can reach up to ten times the density of a normal atomic nucleus, and the stabilizing effect of gravitational confinement permits long-timescale weak interactions.  This generates matter that is neutron-rich, and opens up the possibility of stable states of strange matter, something that can only exist in neutron stars. Strong force physics is encoded in the Equation of State (EOS), the pressure-density relation.  This is linked to macroscopic observables such as mass M and radius R via the stellar structure equations. By measuring and inverting the M-R relation we can recover the EOS and diagnose the underlying dense matter physics.

This proposal focuses on a very promising technique for simultaneous measurement of M and R.  It exploits hotspots (burst oscillations) that form on the neutron star surface when material accreted from a companion star undergoes a thermonuclear explosion (a Type I X-ray burst).  As the star rotates, the hotspot gives rise to a pulsation.  Relativistic effects then encode information about M and R into the pulse profile.  However the mechanism that generates burst oscillations remains unknown, 18 years after their discovery.  This is frustrating in terms of our understanding of thermonuclear bursts.  It also leads to uncertainties in the precise form of the underlying surface emission pattern (a key factor in the pulse profile fitting process), which must be addressed to cement their reliability as diagnostics of M and R. 

This proposal has two objectives. Firstly, to resolve the burst oscillation mechanism via an ambitious programme of theoretical and observational analysis. Secondly, to ensure that burst oscillations are a robust tool for measurement of M and R by determining the effect of the surface pattern uncertainty on pulse profile fitting, independent of efforts to constrain the mechanism.","1499999","2015-06-01","2020-05-31"
"CSP-COMPLEXITY","Constraint Satisfaction Problems: Algorithms and Complexity","Manuel Bodirsky","TECHNISCHE UNIVERSITAET DRESDEN","The complexity of Constraint Satisfaction Problems (CSPs) has become a major common research focus of graph theory, artificial intelligence, and finite model theory. A recently discovered connection between the complexity of CSPs on finite domains to central problems in universal algebra led to additional activity in the area.

The goal of this project is to extend the powerful techniques for constraint satisfaction to CSPs with infinite domains. The generalization of CSPs to infinite domains enhances dramatically the range of computational problems that can be analyzed with tools from constraint satisfaction complexity. Many problems from areas that have so far seen no interaction with constraint satisfaction complexity theory can be formulated using infinite domains (and not with finite domains), e.g. in phylogenetic reconstruction, temporal and spatial reasoning, computer algebra, and operations research. It turns out that the search for systematic complexity classification in infinite domain constraint satisfaction often leads to fundamental algorithmic results.

The generalization of constraint satisfaction to infinite domains poses several mathematical challenges: To make the universal algebraic approach work for infinite domain constraint satisfaction we need fundamental concepts from model theory. Luckily, the new mathematical challenges come together with additional strong tools, such as Ramsey theory or results from model theory. The most important challgenges are of an algorithmic nature: finding efficient algorithms for significant constraint languages, but also finding natural classes of problems that can be solved by a given algorithm.","830316","2011-01-01","2015-12-31"
"CTO Com","Context- and Task-Oriented Communication","Michèle WIGGER","INSTITUT MINES-TELECOM","Emergence of a large number of distributed decision and control systems (e.g., in health care, transportation, and energy management), combined with increasing demands of traditional communications (e.g., due to multiview videos), create an imminent need for highly improved communication systems. We advocate that—combined with improvements in battery, antenna, and chip technologies—context- and/or task-oriented communication techniques will bring the desired breakthrough. Specifically, context- oriented techniques will greatly improve performance, because future networks have complex infrastructures (with cache-memories, cloud-RANs, etc.) allowing the terminals to collect side-informations about other terminals’ data or signals, and because many distributed decision systems rely on numerous devices with correlated measurements. Task-oriented techniques promise even larger gains, especially in distributed decision systems where decisions take value on a small range, and thus the traditional approach of communicating sequences of observed signals results in a huge overhead.
Information theory, and in particular distributed joint source-channel coding, provides a general framework for designing context-oriented communication techniques. Such a general framework  is missing for task-oriented communication. Previous results indicate that creative usages of information theory on its frontier to statistics and decision theory are well-suited for designing task-oriented communication techniques for applications as diverse as coordination of smart devices, distributed hypothesis testing, and clustering of data.
Our goal is to design context- and/or task-oriented communication techniques for these three applications and for cache-aided communication. Besides the high gains that our new  techniques bring directly to these applications, the complementarity of our applications and obtained results will facilitate a future general framework for context- and task-oriented communication.","1495288","2017-05-01","2022-04-30"
"CU-ANGIO","Prostate cancer localization by contrast-ultrasound angiogenesis imaging","Massimo Mischi","TECHNISCHE UNIVERSITEIT EINDHOVEN","Prostate cancer causes over 1/4 of new cancer cases and 1/10 of cancer deaths in western males. Efficient methods for early treatment are available. Many lives could therefore be saved by early cancer detection, but this is not viable due to the inadequacy of the available noninvasive diagnostics. Systematic biopsy is the only reliable detection technique, but it is hampered by high costs and causes serious discomfort and health risks because of its invasiveness. Moreover, precise cancer localization is not possible, impeding the use of available focal treatments.
This research will push the frontiers of prostate cancer diagnostics by a revolutionary method for localization of cancer angiogenesis (microvascular growth). Different from all methods for angiogenesis imaging, invariably based on the assessment of blood perfusion, I aim at quantifying the local dispersion dynamics of an intravascular tracer. Dispersion is the spreading process of the tracer within the vasculature, which I firmly believe to correlate much better than perfusion with microvascular architectures and, therefore, with cancer angiogenesis.
The assessment of local dispersion is challenging and will be pursued through an intravenous injection of an ultrasound contrast bolus and novel spatiotemporal analysis of the bolus passage through the prostate circulation, measured by three-dimensional ultrasound imaging.
If successful, the proposed method will represent a breakthrough for early noninvasive and accurate prostate cancer localization, precise focal treatment, and treatment follow-up, with strong potential for use for other types of cancers, such as breast cancer. Moreover, this method will facilitate further groundbreaking research in the therapeutic control of angiogenesis in several pathologies.
This exciting research builds on my multidisciplinary expertise in ultrasound contrast dilution methods and on consistent and successful collaborations with leading clinical and industrial partners.","1430955","2012-06-01","2018-05-31"
"Cu4Energy","Biomimetic Copper Complexes for Energy Conversion Reactions","Dennis Gerardus Hendrikus Hetterscheid","UNIVERSITEIT LEIDEN","Water oxidation (WO) and oxygen reduction (OR) are crucial reactions to produce and to consume solar fuels. It is important that WO and OR occur with very high catalytic rates with only a very small thermodynamic driving force (i.e. a small overpotential). In these terms, natural catalysts perform significantly better than the artificial systems. Especially the copper enzyme Laccase operates fast at a low overpotential. In principle one could use the same design principles used in the enzymatic systems to produce artificial catalysts for OR and WO. It is envisioned that for the most ideal OR and WO catalysts:

1. All redox reactions within the catalytic cycle should occur as close as possible to the thermodynamic potential where OR and WO become accessible.

2. Equilibria that are not coupled to redox reactions need to be biased for product formation.

3. Proton shuttles are necessary to manage proton transfer concerted with electron-transfer and electron-transfer coupled to O–O bond cleavage or O–O bond formation. 

In this proposal molecular copper catalysts for OR and WO are studied by means of a combined electrochemical and computational approach, taking in account the design principles above. Experiments will be carried out wherein the structure of the catalyst is linked to the observed catalytic activity and the potential energy surface of the catalytic cycle. The proposal is in particular focused on the rate-determining step of the catalytic reaction, as improvements here will directly lead to enhanced catalytic rates. A functional model system of Laccase will be designed to study the rate limiting proton-and-electron-coupled O–O bond scission reaction, which is the rate limiting step in OR by Laccase. 
The aim of the proposal is to significantly increase of fundamental understanding of the design principles for molecular OR and WO catalysts and to deliver new and very active molecular copper catalysts for OR and WO at the end of the project.","1500000","2015-05-01","2020-04-30"
"CUHL","Controlling Ultrafast Heat in Layered materials","Klaas-Jan TIELROOIJ","FUNDACIO INSTITUT CATALA DE NANOCIENCIA I NANOTECNOLOGIA","In this project I propose to take advantage of the enormous potential created by the recent material science revolution based on two-dimensional (2D) layered materials, by bringing it to the arena of nanoscale heat transport, where heat transport occurs on ultrafast timescales. This opens up a new research field of controllable ultrafast heat transport in layered materials. In particular, I will take advantage of the myriad of possibilities for miniature material and device design, with unprecedented controllability and versatility, offered by Van der Waals (VdW) heterostructures – stacks of different layered materials assembled on top of each other – and 1D systems of layered materials. 

Specifically, I will introduce novel device geometries based on VdW heterostructures for passively and actively controlling phonon modes and thermal transport. This will be measured mainly using time-domain thermoreflectance measurements. I will also develop novel time-resolved measurement techniques to follow heat spreading and coupling between different heat carriers: light, phonons, and electrons. These techniques will be mainly based on time-resolved infrared/Raman spectroscopy and photocurrent scanning microscopy. Moreover, I will study one-dimensional layered materials and assess their thermoelectric properties using electrical measurements. And finally, I will combine these results into hybrid devices with a photoactive layer, in order to demonstrate how phonon control allows for tuning of electrical and optoelectronic properties. 

The results of this project will have an impact on the major research fields of phononics, electronics and photonics, revealing novel physical phenomena. Additionally, the results are likely to be useful towards applications such as thermal management, thermoelectrics, photovoltaics and photodetection.","1475000","2018-12-01","2023-11-30"
"CURVATURE","Optimal transport techniques in the geometric analysis of spaces with curvature bounds","Andrea MONDINO","THE UNIVERSITY OF WARWICK","The unifying goal of the CURVATURE project is to develop new strategies and tools in order to attack fundamental questions in the theory of smooth and non-smooth spaces satisfying (mainly Ricci or sectional) curvature restrictions/bounds. 

The program involves analysis and geometry, with strong connections to probability and mathematical physics. The problems will be attacked by an innovative merging of geometric analysis and optimal transport techniques that already enabled the PI and collaborators to solve important open questions in the field.

The project is composed of three inter-connected themes:

Theme I investigates the structure of non smooth spaces with Ricci curvature bounded below and their link with
Alexandrov geometry. The goal of this theme is two-fold: on the one hand get a refined structural picture of
non-smooth spaces with Ricci curvature lower bounds, on the other hand apply the new methods to make progress in some long-standing open problems in Alexandrov geometry.

Theme II aims to achieve a unified treatment of geometric and functional inequalities for both smooth and
non-smooth, finite and infinite dimensional spaces satisfying Ricci curvature lower bounds. The approach
will be used also to establish new quantitative versions of classical geometric/functional inequalities for smooth Riemannian manifolds and to make progress in long standing open problems for both  Riemannian and sub-Riemannian manifolds.

Theme III will investigate optimal transport in a Lorentzian setting, where the Ricci curvature plays a key
role in Einstein's equations of general relativity. 

The three themes together will yield a unique unifying insight of smooth and non-smooth structures with curvature bounds.","1256221","2019-02-01","2024-01-31"
"CurvedSusy","Dynamics of Supersymmetry in Curved Space","Guido Festuccia","UPPSALA UNIVERSITET","Quantum field theory provides a theoretical framework to explain quantitatively natural phenomena as diverse as the fluctuations in the cosmic microwave background, superconductivity, and elementary particle interactions in colliders. Even if we use quantum field theories in different settings, their structure and dynamics are still largely mysterious. Weakly coupled systems can be studied perturbatively, however many natural phenomena are characterized by strong self-interactions (e.g. high T superconductors, nuclear forces) and their analysis requires going beyond perturbation theory. Supersymmetric field theories are very interesting in this respect because they can be studied exactly even at strong coupling and their dynamics displays phenomena like confinement or the breaking of chiral symmetries that occur in nature and are very difficult to study analytically.
Recently it was realized that many interesting insights on the dynamics of supersymmetric field theories can be obtained by placing these theories in curved space preserving supersymmetry. These advances have opened new research avenues but also left many important questions unanswered. The aim of our research programme will be to clarify the dynamics of supersymmetric field theories in curved space and use this knowledge to establish new exact results for strongly coupled supersymmetric gauge theories. The novelty of our approach resides in the systematic use of the interplay between the physical properties of a supersymmetric theory and the geometrical properties of the space-time it lives in. The analytical results we will obtain, while derived for very symmetric theories, can be used as a guide in understanding the dynamics of many physical systems. Besides providing new tools to address the dynamics of quantum field theory at strong coupling this line of investigation could lead to new connections between Physics and Mathematics.","1145879","2015-09-01","2020-08-31"
"CUTACOMBS","Cuts and decompositions: algorithms and combinatorial properties","Marcin PILIPCZUK","UNIWERSYTET WARSZAWSKI","In this proposal we plan to extend mathematical foundations of algorithms for various variants of the minimum cut problem within theoretical computer science.
Recent advances in understanding the structure of small cuts and tractability of cut problems resulted in a mature algorithmic toolbox for undirected graphs under the paradigm of parameterized complexity. In this position, we now aim at a full understanding of the tractability of cut problems in the more challenging case of directed graphs, and see opportunities to apply the aforementioned successful structural approach to advance on major open problems in other paradigms in theoretical computer science.
The specific goals of the project are grouped in the following three themes.

Directed graphs. Chart the parameterized complexity of graph separation problems in directed graphs and provide a fixed-parameter tractability toolbox, equally deep as the one in undirected graphs. Provide tractability foundations for routing problems in directed graphs, such as the disjoint paths problem with symmetric demands.

Planar graphs. Resolve main open problems with respect to network design and graph separation problems in planar graphs under the following three paradigms: parameterized complexity, approximation schemes, and cut/flow/distance sparsifiers. Recently discovered connections uncover significant potential in synergy between these three algorithmic approaches.

Tree decompositions. Show improved tractability of graph isomorphism testing in sparse graph classes. Combine the algorithmic toolbox of parameterized complexity with the theory of minimal triangulations to advance our knowledge in structural graph theory, both pure (focused on the Erdos-Hajnal conjecture) and algorithmic (focused on the tractability of Maximum Independent Set and 3-Coloring).","1228250","2017-03-01","2022-02-28"
"CUTTINGBUBBLES","Bubbles on the Cutting Edge","Niels Gerbrand Deen","TECHNISCHE UNIVERSITEIT EINDHOVEN","Many processes in the chemical, petrochemical and/or biological industries involve three phase gas-liquidsolid flows, where the solid material acts as a catalyst carrier, the gas phase supplies the reactants for the (bio-)chemical transformations and the liquid phase carries the product. In these processes the performance and operation of the reactor is mostly constrained by the interfacial mass transfer rate and the achievable insitu heat removal rate. A micro-structured bubble column reactor that significantly improves these crucial properties is proposed in this project. This novel type of reactor takes advantage of micro-structuring of the catalyst carrier in the form of a wire-mesh (see Figure 1).
The aim of the wire-mesh is i) to cut bubbles into smaller pieces leading to a larger interfacial area, ii) to enhance the bubble interface dynamics and mass transfer due to the interaction between the bubbles and the wires, and iii) to save costs in practical operation due to the smaller required reactor volume and the fact that
there is no need for an external filtration unit.
Cutting edge three-phase direct numerical simulation (DNS) tools and novel non-invasive optical (highspeed camera) techniques are used to study the micro-scale interaction between bubbles and a wire-mesh to gain understanding of the splitting and merging of bubbles and associated mass transfer characteristics. Furthermore, a proof-of-principle of the micro-structured reactor will be given through lab-scale experiments and macroscopic Euler-Lagrange numerical simulations, employing bubble-wire interaction closures based on the DNS simulations.
In addition to the novel reactor type, the project will generate a broad set of fundamental numerical and experimental research tools that can be used for the improvement of various gas-liquid-solid processes.
Several large companies (AkzoNobel, DSM, Sabic and Shell) have indicated their interest in the proposed
project and would like to be involved in a users committee.","1500000","2010-09-01","2015-08-31"
"CV-SUPER","Computer Vision for Scene Understanding from a first-person Perspective","Bastian Leibe","RHEINISCH-WESTFAELISCHE TECHNISCHE HOCHSCHULE AACHEN","""The goal of CV-SUPER is to create the technology to perform dynamic visual scene understanding from the perspective of a moving human observer. Briefly stated, we want to enable computers to see and understand what humans see when they navigate their way through busy inner-city locations. Our target scenario is dynamic visual scene understanding in public spaces, such as pedestrian zones, shopping malls, or other locations primarily designed for humans. CV-SUPER will develop computer vision algorithms that can observe the people populating those spaces, interpret and understand their actions and their interactions with other people and inanimate objects, and from this understanding derive predictions of their future behaviors within the next few seconds. In addition, we will develop methods to infer semantic properties of the observed environment and learn to recognize how those affect people’s actions. Supporting those tasks, we will develop a novel design of an object recognition system that scales up to potentially hundreds of categories. Finally, we will bind all those components together in a dynamic 3D world model, showing the world’s current state and facilitating predictions how this state will most likely change within the next few seconds. These are crucial capabilities for the creation of technical systems that may one day assist humans in their daily lives within such busy spaces, e.g., in the form of personal assistance devices for elderly or visually impaired people or in the form of future generations of mobile service robots and intelligent vehicles.""","1499960","2012-11-01","2017-10-31"
"CYFI","Cycle-Sculpted Strong Field Optics","Andrius Baltuska","TECHNISCHE UNIVERSITAET WIEN","The past decade saw a remarkable progress in the development of attosecond technologies based on the use of intense few-cycle optical pulses. The control over the underlying single-cycle phenomena, such as the higher-order harmonic generation by an ionized and subsequently re-scattered electronic wave packet, has become routine once the carrier-envelope phase (CEP) of an amplified laser pulse was stabilized, opening the way to maintain the shot-to-shot reproducible pulse electric field. Drawing on a mix of several laser technologies and phase-control concepts, this proposal aims to take strong-field optical tools to a conceptually new level: from adjusting the intensity and timing of a principal half-cycle to achieving a full-fledged multicolor Fourier synthesis of the optical cycle dynamics by controlling a multi-dimensional space of carrier frequencies, relative, and absolute phases. The applicant and his team, through their unique expertise in the CEP control and optical amplification methods, are currently best positioned to pioneer the development of an optical programmable “attosecond optical shaper” and attain the relevant multicolor pulse intensity levels of PW/cm2. This will enable an immediate pursuit of several exciting strong-field applications that can be jump-started by the emergence of a technique for the fully-controlled cycle sculpting and would rely on the relevant experimental capabilities already established in the applicant’s emerging group. We show that even the simplest form of an incommensurate-frequency synthesizer can potentially solve the long-standing debate on the mechanism of strong-field rectification. More advanced waveforms will be employed to dramatically enhance coherent X ray yield, trace the time profile of attosecond ionization in transparent bulk solids, and potentially control the result of molecular dissociation by influencing electronic coherences in polyatomic molecules.","980000","2012-01-01","2015-06-30"
"CytoChem","A Chemical Approach to Understanding Cell Division","Ulrike Sophie Eggert","KING'S COLLEGE LONDON","Many mechanisms underlying cytokinesis, the final step in cell division, remain poorly understood. The goal of my laboratory is to use chemical biology approaches to address some of the unanswered mechanistic questions by studying cytokinesis at the process, pathway and protein levels. I aim to discover small molecules that specifically target cytokinesis by different mechanisms because they are important tools to study the biology of cell division and could catalyze the discovery of therapeutics.

I am proposing here to use small molecules we discovered to study how the Rho pathway regulates cytokinesis. We will synthesize focused libraries around selected compounds to optimize their properties and to identify sites for affinity tags. I am proposing to identify our small molecules’ cellular targets using a combination of approaches, including a new strategy I designed that takes advantage of the fact that they target a discrete signalling pathway.

Rho signalling is involved in every step of cytokinesis, but there are many outstanding questions about how this occurs and which proteins are involved. We have completed a genome-wide RNAi screen that has revealed the identity of new proteins connected to Rho signalling. We will combine functional investigations into how these proteins participate in cytokinesis with our newly discovered small molecules. With this array of tools in hand, we expect to use imaging and other cell-based assays to gain of comprehensive understanding of the role of Rho signalling during cytokinesis and other Rho-dependent processes.","1499080","2012-10-01","2017-09-30"
"CZOSQP","Noncommutative Calderón-Zygmund theory, operator space geometry and quantum probability","Javier Parcet Hernandez","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","Von Neumann's concept of quantization goes back to the foundations of quantum mechanics
and provides a noncommutative model of integration. Over the years, von Neumann algebras
have shown a profound structure and set the right framework for quantizing portions of algebra,
analysis, geometry and probability. A fundamental part of my research is devoted to develop a
very much expected Calderón-Zygmund theory for von Neumann algebras. The lack of natural
metrics partly justifies this long standing gap in the theory. Key new ingredients come from
recent results on noncommutative martingale inequalities, operator space theory and quantum
probability. This is an ambitious research project and applications include new estimates for
noncommutative Riesz transforms, Fourier and Schur multipliers on arbitrary discrete groups
or noncommutative ergodic theorems. Other related objectives of this project include Rubio
de Francia's conjecture on the almost everywhere convergence of Fourier series for matrix
valued functions or a formulation of Fefferman-Stein's maximal inequality for noncommutative
martingales. Reciprocally, I will also apply new techniques from quantum probability in
noncommutative Lp embedding theory and the local theory of operator spaces. I have already
obtained major results in this field, which might be useful towards a noncommutative form of
weighted harmonic analysis and new challenging results on quantum information theory.","1090925","2010-10-01","2015-09-30"
"D3","Interpreting Drawings for 3D Design","Adrien BOUSSEAU","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","Designers draw extensively to externalize their ideas and communicate with others. However, drawings are currently not directly interpretable by computers. To test their ideas against physical reality, designers have to create 3D models suitable for simulation and 3D printing. However, the visceral and approximate nature of drawing clashes with the tediousness and rigidity of 3D modeling. As a result, designers only model finalized concepts, and have no feedback on feasibility during creative exploration.
Our ambition is to bring the power of 3D engineering tools to the creative phase of design by automatically estimating 3D models from drawings. However, this problem is ill-posed: a point in the drawing can lie anywhere in depth. Existing solutions are limited to simple shapes, or require user input to “explain” to the computer how to interpret the drawing. Our originality is to exploit professional drawing techniques that designers developed to communicate shape most efficiently. Each technique provides geometric constraints that help viewers understand drawings, and that we shall leverage for 3D reconstruction.
Our first challenge is to formalize common drawing techniques and derive how they constrain 3D shape. Our second challenge is to identify which techniques are used in a drawing. We cast this problem as the joint optimization of discrete variables indicating which constraints apply, and continuous variables representing the 3D model that best satisfies these constraints. But evaluating all constraint configurations is impractical. To solve this inverse problem, we will first develop forward algorithms that synthesize drawings from 3D models. Our idea is to use this synthetic data to train machine learning algorithms that predict the likelihood that constraints apply in a given drawing.
In addition to tackling the long-standing problem of single-image 3D reconstruction, our research will significantly tighten design and engineering for rapid prototyping.","1482761","2017-02-01","2022-01-31"
"D4PARTICLES","Statistical physics of dense particle systems in the absence of thermal fluctuations","Ludovic Berthier","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""Frontier research in statistical mechanics and soft condensed matter focuses on systems of ever-increasing complexity. Among these are systems where microscopic dynamics are not controlled by thermal fluctuations, either because the sources of the fluctuations have not a thermal origin, or because “microscopic” sources of fluctuations are altogether absent. Practical applications comprise everyday products such as paints or foodstuff which are soft solids composed of dense suspensions of particles that are too large for thermal fluctuations to play any role. Non-Brownian “active” matter, obtained when particles internally produce motion, represents another growing field with applications in biophysics and soft matter. Because these systems all evolve far from equilibrium, there exists no general framework to tackle these problems theoretically from a fundamental perspective. I will develop a radically new approach to lay the foundations of a detailed theoretical understanding of the physics of a broad but coherent class of materials evolving far from equilibrium. To go beyond phenomenology, I will carry theoretical research to elucidate the physics of particle systems that are simultaneously Dense, Disordered, Driven and Dissipative—D4PARTICLES. By combining numerical analysis of model systems to fully microscopic statistical mechanics analysis, my overall aim is to discover the general principles governing the physics of athermal particle systems far from equilibrium and to reach a complete theoretical understanding and obtain predictive tools regarding the phase behavior, structure and dynamics of D4PARTICLES. Reaching a new level of theoretical understanding of a broad range of materials will impact fundamental research by opening up statistical physics to a whole new class of complex systems and should foster experimental activity towards design and quantitative characterization of  large class of disordered solids and soft materials.""","1339800","2012-10-01","2017-09-30"
"DAMOC","Diabetes Approach by Multi-Organ-on-a-Chip","Javier RAMON","FUNDACIO INSTITUT DE BIOENGINYERIA DE CATALUNYA","Insulin secretion and insulin action are critical for normal glucose homeostasis. Defects in both of these processes lead to type 2 diabetes (T2D). Unravelling the mechanisms that lead to T2D is fundamental in the search of new molecular drugs to prevent and control this disease. Organ-on-a-chip devices offer new approaches for T2D disease modelling and drug discovery by providing biologically relevant models of tissues and organs in vitro integrated with biosensors. As such, organ-on-a-chip devices have the potential to revolutionize the pharmaceutical industry by enabling reliable and high predictive in vitro testing of drug candidates. The capability to miniaturize biosensor systems and advanced tissue fabrication procedures have enabled researchers to create multiple tissues on a chip with a high degree of control over experimental variables for high-content screening applications. The goal of this project is the fabrication of a biomimetic multi organ-on-a-chip integrated device composed of skeletal muscle and pancreatic islets for studying metabolism glucose diseases and for drug screening applications. Engineered muscle tissues and pancreatic islets are integrated with the technology to detect the glucose consumption, contraction induced glucose metabolism, insulin secretion and protein biomarker secretion of cells. We aim to design a novel therapeutic tool to test drugs with a multi organ-on-a-chip device. Such finding would improve drug test approaches and would provide for new therapies to prevent the loss of beta cell mass associated with T2D and defects in the glucose uptake in skeletal muscle.","1499554","2017-01-01","2021-12-31"
"DANCER","DAtacommunications based on NanophotoniC Resonators","John William Whelan-Curtin","CORK INSTITUTE OF TECHNOLOGY","A key challenge for the 21st century is, therefore to provide billions of people with the means to access, move and manipulate, what has become, huge volumes of information. The environmental and economic implications becoming serious, making energy efficient data communications key to the operation of today’s society.

In this project, the Principal Investigator will develop a new framework for optical interconnects and provide a common platform that spans Fibre-to-the-home to chip-to-chip links, even as far as global on-chip interconnects. The project is based on the efficient coupling of the Photonic Crystal resonators with the outside world. These provide the ultimate confinement of light in both space and time allowing orders of magnitude improvements in performance relative to the state of the art, yet in a simpler simple system- the innovator’s dream. New versions of the key components of optical links- light sources, modulators and photo-detectors- will be realised in this new framework providing a new paradigm for energy efficient communication.","1495450","2013-12-01","2019-05-31"
"DANSEINCELL","Modeling cytoplasmic trafficking and molecular delivery in cellular microdomains","David Holcman","ECOLE NORMALE SUPERIEURE","Cytoplasmic motion is a key determinant of organelle transport, protein-protein interactions, RNA transport and drug delivery, to name but a few cellular phenomena. Nucleic acid trafficking is important in antisense and gene therapy based on viral and synthetic vectors. This proposal is dedicated to the theoretical study of intracellular transport of proteins, organelles and DNA particles. We propose to construct a mathematical model to quantify and predict the spatiotemporal dynamics of complex structures in the cytosol and the nucleus, based on the physical characteristics and the micro-rheology of the environment (viscosity). We model the passive motion of proteins or DNA as free or confined diffusion, while for the organelle and virus motion, we will include active cytoskeleton-dependent transport. The proposed mathematical model of cellular trafficking is based on physical principles. We propose to estimate the mean arrival time and the probability of viruses and plasmid DNA to arrive to a nuclear pore. The motion will be described by stochastic dynamics, containing both a drift (along microtubules) and a Brownian (free diffusion) component. The analysis of the equations requires the development of new asymptotic methods for the calculation of the probability and the mean arrival time of a particle to a small hole on the nucleus surface. We will extend the analysis to DNA movement in the nucleus after cellular irradiation, when the nucleus contains single and double broken DNA strands (dbDNAs). The number of remaining DNA breaks determines the activation of the repair machinery and the cell decision to enter into apoptosis. We will study the dsbDNA repair machinery engaged in the task of finding the DNA damage. We will formulate and analyze, both numerically and analytically, the equations that link the level of irradiation to apoptosis. The present project belongs to the new class of initiatives toward a quantitative analysis of intracellular trafficking.","750000","2009-01-01","2014-06-30"
"DAPP","Data-centric Parallel Programming","Torsten Hoefler","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","We address a fundamental and increasingly important challenge in computer science: how to program large-scale heterogeneous parallel computers. Society relies on these computers to satisfy the growing demands of important applications such as drug design, weather prediction, and big data analytics. Architectural trends make heterogeneous parallel processors the fundamental building blocks of computing platforms ranging from quad-core laptops to million-core supercomputers; failing to exploit these architectures efficiently will severely limit the technological advance of our society. Computationally demanding problems are often inherently parallel and can readily be compiled for various target architectures. Yet, efficiently mapping data to the target memory system is notoriously hard, and the cost of fetching two operands from remote memory is already orders of magnitude more expensive than any arithmetic operation. Data access cost is growing with the amount of parallelism which makes data layout optimizations crucial. Prevalent parallel programming abstractions largely ignore data access and guide programmers to design threads of execution that are scheduled to the machine. We depart from this control-centric model to a data-centric program formulation where we express programs as collections of values, called memlets, that are mapped as first-class objects by the compiler and runtime system. Our holistic compiler and runtime system aims to substantially advance the state of the art in parallel computing by combining static and dynamic scheduling of memlets to complex heterogeneous target architectures. We will demonstrate our methods on three challenging real-world applications in scientific computing, data analytics, and graph processing. We strongly believe that, without holistic data-centric programming, the growing complexity and inefficiency of parallel programming will create a scaling wall that will limit our future computational capabilities.","1499672","2016-06-01","2021-05-31"
"DARKFRONTIER","Fundamental Physics at the Low Background Frontier","Jocelyn Monroe","ROYAL HOLLOWAY AND BEDFORD NEW COLLEGE","The nature of dark matter is one of the fundamental questions in physics today.  Direct signals for dark matter have remained elusive, indicating that multi-tonne scale detectors are needed to measure large numbers of dark matter interactions, while current efforts are at the 100 kg scale.  The foremost challenge is distinguishing dark matter signals from backgrounds, the most uncertain of which are from neutrons.  The research objective of this proposal is a world-leading dark matter search with a novel liquid argon (LAr) detector and a new analysis approach to measuring neutron backgrounds in-situ.
The DEAP/CLEAN program of single-phase LAr detectors is a new direction for dark matter searches.  It draws on successful, proven approaches of solar neutrino physics to building low-background detectors that scale simply to multi-tonne target masses.  Demonstration of this approach by the current 100 kg stage (MiniCLEAN) will break new ground for future experiments.  At the 100 tonne scale, such a detector would be a new kind of observatory for fundamental physics at the low background frontier, testing predicted properties of dark matter, neutrinos, supernovae, and stellar evolution.  Success depends critically on demonstrating the required background suppression.
This proposal addresses the key challenges of dark matter detection in two new ways, with the novel single-phase effort for multi-tonne scalability, and by developing new methods to overcome neutron backgrounds.  The tasks of this proposal are: (i) to develop a measurement of the in-situ neutron background in LAr; (ii) to develop an active neutron veto for in-situ measurement of the cosmogenic neutron background, beginning with a measurement of the flux and energy spectrum in an existing prototype; and, (iii) to lead the dark matter search, using the measured backgrounds. The MiniCLEAN dark matter sensitivity is a factor of 20 beyond current experimental results, with great potential for discovery.","1063174","2011-09-01","2017-08-31"
"DarkGRA","Unveiling the dark universe with gravitational waves: Black holes and compact stars as laboratories for fundamental physics","Paolo PANI","UNIVERSITA DEGLI STUDI DI ROMA LA SAPIENZA","In recent years, our theoretical understanding of the strong-field regime of gravity has grown in parallel with the observational confirmations that culminated in the landmark detection of gravitational waves (GWs). This synergy of breakthroughs at the observational, technical, and conceptual level offers the unprecedented opportunity to merge traditionally disjoint areas, and to make strong gravity a precision tool to probe fundamental physics. 

The aim of the DarkGRA project is to investigate novel effects related to strong gravitational sources -such as black holes (BHs) and compact stars- that can be used to turn these objects into cosmic labs, where matter in extreme conditions, particle physics, and the very foundations of Einstein's theory of gravity can be put to the test. In this context, we propose to explore some outstanding, cross-cutting problems in fundamental physics: the existence of extra light fields, the limits of classical gravity, the nature of BHs and of spacetime singularities, and the effects of dark matter near compact objects. Our ultimate goal is to probe fundamental physics in the most extreme gravitational settings and to devise new approaches for detection, complementary to laboratory searches. This groundbreaking research program -located at the interface between particle physics, astrophysics and gravitation- is now made possible by novel techniques to scrutinize astrophysical compact objects, by current and future GW and X-ray detectors, and by the astonishing precision of pulsar timing. If supported by a solid theoretical framework, these new observations can potentially lead to surprising discoveries and paradigm shifts in our understanding of the fundamental laws of nature at all scales.","1337481","2017-10-01","2022-09-30"
"DARKJETS","Discovery strategies for Dark Matter and new phenomena in hadronic signatures with the ATLAS detector at the Large Hadron Collider","Caterina Doglioni","LUNDS UNIVERSITET","The Standard Model of Particle Physics describes the fundamental components of ordinary matter and their interactions. Despite its success in predicting many experimental results,  the Standard Model fails to account for a number of interesting phenomena. One phenomenon of particular interest is the large excess of unobservable (Dark) matter in the Universe. This excess cannot be explained by Standard Model particles. A compelling hypothesis is that Dark Matter is comprised of particles that can be produced in the proton-proton collisions from the Large Hadron Collider (LHC) at CERN. 

Within this project, I will build a team of researchers at Lund University dedicated to searches for signals of the presence of Dark Matter particles. The discovery strategies employed seek the decays of particles that either mediate the interactions between Dark and Standard Model particles or are produced in association with Dark Matter. These new particles manifest in detectors as two, three, or four collimated jets of particles (hadronic jets).

The LHC will resume delivery of proton-proton collisions to the ATLAS detector in 2015. Searches for new, rare, low mass particles such as Dark Matter mediators have so far been hindered by constraints on the rates of data that can be stored. These constraints will be overcome through the implementation of a novel real-time data analysis technique and a new search signature, both introduced to ATLAS by this project. The coincidence of this project with the upcoming LHC runs and the software and hardware improvements within the ATLAS detector is a unique opportunity to increase the sensitivity to hadronically decaying new particles by a large margin with respect to any previous searches. The results of these searches will be interpreted within a comprehensive and coherent set of theoretical benchmarks, highlighting the strengths of collider experiments in the global quest for Dark Matter.","1268076","2016-02-01","2021-01-31"
"DASMT","Domain Adaptation for Statistical Machine Translation","Alexander Fraser","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","Rapid translation between European languages is a cornerstone of good governance in the EU, and of great academic and commercial interest. Statistical approaches to machine translation constitute the state-of-the-art. The basic knowledge source is a parallel corpus, texts and their translations. For domains where large parallel corpora are available, such as the proceedings of the European Parliament, a high level of translation quality is reached. However, in countless other domains where large parallel corpora are not available, such as medical literature or legal decisions, translation quality is unacceptably poor. 

Domain adaptation as a problem of statistical machine translation (SMT) is a relatively new research area, and there are no standard solutions. The literature contains inconsistent results and heuristics are widely used. We will solve the problem of domain adaptation for SMT on a larger scale than has been previously attempted, and base our results on standardized corpora and open source translation systems.

We will solve two basic problems. The first problem is determining how to benefit from large out-of-domain parallel corpora in domain-specific translation systems. This is an unsolved problem. The second problem is mining and appropriately weighting knowledge available from in-domain texts which are not parallel. While there is initial promising work on mining, weighting is not well studied, an omission which we will correct. We will scale mining by first using Wikipedia, and then mining from the entire web.

Our work will lead to a break-through in translation quality for the vast number of domains with less parallel text available, and have a direct impact on SMEs providing translation services. The academic impact of our work will be large because solutions to the challenge of domain adaptation apply to all natural language processing systems and in numerous other areas of artificial intelligence research based on machine learning approaches.","1228625","2015-12-01","2020-11-30"
"dasQ","Atomic-Scale Dynamics of Quantum Materials","Sebastian Loth","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Quantum materials exhibit strong electron-electron interaction, which gives rise to such remarkable phenomena as high temperature superconductivity and colossal magnetoresistance. These materials mark one of the frontiers of modern condensed matter physics: a new class of solids where many-body physics dominates. By understanding quantum materials a new generation of devices may become available, greatly boosting our ability to handle information or harvest energy. 
A key difficulty is that correlated-electron materials present inherent complexity on multiple length and timescales, with static and dynamic inhomogeneities that determine cooperativity and the emergence of collective behavior.
The goal of the dasQ proposal is to resolve the microscopic dynamics of quantum materials in the presence of atomic scale heterogeneity.
Ultrafast pump probe spectroscopy at THz wavelength will be combined with scanning tunneling microscopy. Strong enhancement of THz radiation in the STM’s tunnel junction enables simultaneous atomic spatial resolution and picosecond time resolution. We will explore methods to control charge order locally by tip interaction, atom manipulation and coherent driving with THz fields. Atomically-resolved pump-probe spectroscopy will quantify nanometer-sized variations in quasiparticle lifetimes across inhomogeneous phases. Furthermore, the microscopic mechanism of charge density wave capture at singular pinning sites will be addressed. These experiments will impact many aspects of correlated-electron materials; one of the stated goals is to resolve how cooper pairing is modified locally when charge order competes with superconductivity. 
The success of the dasQ project will create new experiments that interact with many-body phases at the intrinsic length scale of charge correlation and will identify opportunities for scaling of electronic devices using quantum materials.","1988100","2015-06-01","2020-05-31"
"DASTCO","Developing and Applying Structural Techniques for Combinatorial Objects","Paul Joseph Wollan","UNIVERSITA DEGLI STUDI DI ROMA LA SAPIENZA","The proposed project will tackle a series of fundamental problems in discrete mathematics by studying labeled graphs, a generalization of graphs which readily apply to problems beyond graph theory.  To achieve these goals will require both developing new graph theoretic tools and techniques as well as further expounding upon known methodologies.

The specific problems to be studied can be grouped into a series of semi-independent projects.  The first focuses on signed graphs with applications to a conjecture of Seymour concerning 1-flowing binary matroids and a related conjecture on the intregality of polyhedra defined by a class of binary matrices.  The second proposes to develop a theory of minors for directed graphs.  Finally, the project looks at topological questions arising from graphs embedding in a surface and the classic problem of efficiently identifying the trivial knot. The range of topics considered will lead to the development of tools and techniques applicable to questions in discrete mathematics beyond those under direct study.

The project will create a research group incorporating graduate students and post doctoral researchers lead by the PI.  Each area to be studied offers the potential for ground-breaking results at the same time offering numerous intermediate opportunities for scientific progress.","850000","2011-12-01","2017-09-30"
"DAUBOR","Design and Applications of Unconventional Borylation Reactions","Mariola Tortosa Manzanares","UNIVERSIDAD AUTONOMA DE MADRID","""Boronic esters are versatile synthetic intermediates for the preparation of a wide range of organic molecules. The recent approval of the anti-cancer agent Velcade, the first boronic acid containing drug commercialized, further confirms the status of boronic acid derivatives as an important class of compounds in chemistry and medicine. This proposal aims to develop three new unconventional approaches for the synthesis of boronic esters.
The first one is based on the use of copper (low price and low toxicity) to promote unknown borylation reactions. Our method is an important step forward in that it proceeds using catalytic quantities of copper and allows the formation of a C-B bond along with a C-C or a C-N bond in a single catalytic cycle. Additionally, a copper-catalyzed borylation reaction is proposed as the key tool to solve the total synthesis of nigricanoside A, a potent antimitotic agent. The total synthesis of this natural product could have an impact in cancer research similar to that found for taxol or epothilones.
The second approach deals with the development of borylation reactions under metal-free conditions. I propose to use bifunctional catalysts to promote the dual activation of B-B bonds and suitable electrophiles. This approach constitutes an unconventional way to synthesize boronic esters and has no precedent in the literature.
Finally, the third section of this proposal branches into riskier territory. I propose to use Lewis-base/diboron adducts to generate organoboryl radicals. If successful, the potential impact will be very high and will certainly open unexplored ways in boron chemistry.
The copper-catalyzed, metal-free, and radical approaches proceed by mechanistically distinct pathways and can give rise to complementary reactivity and selectivity partners. New findings in these areas would represent a significant step in the industrial and academic synthesis of boronic esters.""","1495200","2014-02-01","2019-01-31"
"DCBIF","Flight dynamics and control of birds and insects","Graham Keith Taylor","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Insects bristle with sensors, but how do they exploit this rich sensory information to achieve their extraordinary stability and manoeuvrability? Bird and insect wings deform in flight, and have passively deployable structures such as feathers and flaps, but how do they exploit these features when aircraft designers shy away from aeroelasticity? Birds fly without a vertical tailfin, but how do they maintain yaw stability when most aircraft require one to fly safely? Questions such as these drive my research on bird and insect flight dynamics. My research is unique in using the engineering tools of flight dynamics and control theory to analyse physiological and biomechanical data from real animals. One research track will use measurements of the forces and torques generated by insects flying tethered in a virtual-reality flight simulator to parameterise their equations of motion, in order to model the input-output relationships of their sensorimotor control systems. A second research track will measure the detailed wing kinematics and deformations of free-flying insects in order to analyse the effects of aeroelasticity on flight manoeuvres. A third research track will measure the wing and tail kinematics of free-flying birds using onboard wireless video cameras, and use system identification techniques to model how these affect the body dynamics measured using onboard instrumentation. Applying these novel experimental techniques will allow me to make and test quantitative predictions about flight stability and control. This highly interdisciplinary research bridges the fields of physiology and biomechanics, with significant feeds to and from engineering. My research will break new ground, developing novel experimental techniques and theoretical models in order to test and generate new hypotheses of adaptive function. Its broader impacts include the public interest in all things flying, and potential military and civilian applications in flapping micro-air vehicles.","1954565","2008-06-01","2014-05-31"
"DE-CO2","Quantifying CO2 emissions from tropical deforestation to ‘close’ the global carbon budget","Guido Van Der Werf","STICHTING VU","The land and oceans have mitigated climate change by taking up about half of the anthropogenic CO2 emitted since the industrial revolution. However, these ‘sinks’ are predicted to lose their efficiency. Globally, the combined sink strength of the land and ocean can be calculated indirectly as the difference between anthropogenic emissions – from fossil fuel burning and deforestation – and the atmospheric CO2 increase. However, large uncertainty in the deforestation term masks out potential changes in sink strength contained in the better-constrained fossil fuel and atmospheric terms. This creates the need for a new accurate approach to quantify emissions from deforestation and its variability over the past decades.

I propose to quantify deforestation emissions from the novel fire perspective. A substantial share of deforestation emissions stems from burning vegetation, and this focus enables validation of emissions by comparing atmospheric enhancements of fire-emitted carbon monoxide (CO) with satellite-derived concentrations of CO. The proposed multidisciplinary work will follow three steps: 1) quantify net emissions from fires and decomposition in deforestation and degradation regions, combining satellite data with biogeochemical modelling, 2) validate these emissions by combining newly measured CO:CO2 ratios and the isotopic signature of CO2 downwind of deforestation regions, atmospheric chemistry transport modelling, and satellite-derived CO concentrations, and 3) use relations between fire emissions and visibility reported at airports as a novel way to extend the new deforestation emissions estimates back in time before high-quality satellite observations were available. The new approach will lead to the first constrained, monthly resolved estimate of deforestation emissions. Applying the global CO2 mass balance equation will then provide a better quantitative understanding of the (changing) sink capacity of the Earth's oceans and land surface.","1500000","2011-11-01","2016-10-31"
"DEBRIS","Debris in extrasolar planetary systems","Mark Charles Wyatt","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","This proposal concerns the debris discs of nearby stars; ie, discs of asteroids, comets and dust. Such dust can be imaged, providing clues to the underlying planetary system. Debris images have already predicted planets later confirmed in direct imaging. Most debris lies in cold outer (~100AU) regions of planetary systems, but a growing number of stars have hot dust in regions where terrestrial planets are expected (few AU). This proposal aims learn about the planetary systems of nearby stars through study of their debris discs. Specific focus is on the frontier area of characterisation and modelling of dust within planetary systems, which is important for the design of missions to detect habitable planets, a high priority goal for the next decade. The PI has played a significant role in debris disc studies, and proposes to consolidate an independent research team in Cambridge. The proposal covers 3 studies supported by 3 PDRAs. Specific objectives are: 1) Debris disc observations: Carry out survey for cold debris around unbiased sample of nearest 500 stars with Herschel and SCUBA2. Follow-up bright discs with high resolution imaging using ALMA and JWST to characterise sub-structure from planets and search for dust at multiple radii. Pioneer survey for hot dust using polarisation and interferometry. 2) Debris disc modelling: Develop new model to follow the interplay between collisions, radiation pressure, P-R drag, sublimation, disintegration, and dynamical interactions with planets. Use model to consider nature of small particle halos, resonant ring structures formed by terrestrial planets, and level of cometary dust scattered into inner regions. 3) Debris disc origin: Demonstrate constraints placed on planet formation models through studies of dust from Earth-moon forming impacts, effect of planetesimals on late-stage planetary dynamics, population synthesis explaining planets and debris, constraints on primordial size and stirring of debris.","1497920","2012-01-01","2016-12-31"
"DECCA","Devices, engines and circuits: quantum engineering with cold atoms","Jean-Philippe BRANTUT","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Over the last decade, cold atomic gases have become one of the best controlled quantum system. This novel, synthetic material can be shaped at the microscopic level to mimic a wide range of models, and simulate the universal physics that these models describe. This project pioneers a new approach to quantum simulations, jumping from cold atoms materials into the realm of devices: systems carved out of cold gases, separated by interfaces, connected to each other and allowing for a controlled driving.

At the heart of this approach is the study of transport of atoms at the quantum level. Our devices will allow for the measurement of the universal conductance of quantum critical systems or other many-body states. They will feature interfaces and contacts where new types of localized states emerge, such as the one proposed to explain the long-standing question of the “0.7 anomaly” in quantum point contacts. They will also allow for a new type of engineering, where currents of particles, spin or entropy can be controlled and directed in order to perform operations such as cooling.

This research will be possible thanks to the development of a new apparatus, capable of detecting in a non-destructive way tiny atomic currents, such as the one driven through single mode quantum conductors. It will combine an optical cavity for high efficiency optical detection, and high optical resolution optics allowing for manipulations and patterning at the scale of the wave function of individual particles.","1454258","2017-02-01","2022-01-31"
"DECCAPAC","Design and Exploitation of C-C and C-H Activation Pathways in Asymmetric Catalysis","Nicolai Cramer","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Synthesizing organic molecules in high purity with designed properties is of utmost importance for pharmaceutical applications and material- and polymer sciences including the efficient production of enantiopure compounds and the compliance with ecological concerns and sustainability. The efficiency of all reaction classes has improved over the past decades. However, the basic principle and execution did not change: The target molecule is disconnected into donor and acceptor synthons and appropriate functional groups need to be introduced and adjusted to carry out the envisioned coupling. These additional steps decrease the yield and efficiency, are costly in time, resources and produce waste. The introduction of new functionalities by direct C-H or C-C bond activation is a unique and highly appealing strategy. The range of substrates is virtually unlimited, including hydrocarbons, small molecules and polymers. Such  dream reactions  avoid any pre-functionalization, shorten synthetic routes, make unsought disconnections possible and allow for a more efficient usage of our dwindling resources. Despite recent progress in the activations of  inert  bonds, narrow scopes, poor reactivities and harsh conditions hamper most general practical applications. Especially, enantioselective activations are a longstanding challenge. The outlined project seeks to address these issues by the development and exploitation of new catalytic enantioselective C-H and C-C functionalizations of broadly available organic substrates, using chiral Rh- and Pd- catalysts, additionally supported by automated screening and computational techniques. These reactions will be then applied in the streamlined synthesis of pharmaceutically relevant scaffolds and of compounds for organic electronics.","1499500","2011-02-01","2016-01-31"
"DeCO-HVP","Decouple Electrochemical Reduction of Carbon Dioxide to High Value Products","Kathryn Ellen TOGHILL","LANCASTER UNIVERSITY","This programme aims to convert carbon dioxide into high value hydrocarbon products using carbon neutral electrochemical methods. High value products are materials that may be used as carbon based chemical feedstocks and as synthetic fuels, reducing the ever-present demand on oil and natural gas to fulfil these needs. The project is within the remit of an international ambition to valorise carbon dioxide waste and reduce environmentally harmful greenhouse gas generation, as opposed to stopping at carbon capture and sequestration. This proposal outlines an alternative route to carbon dioxide utilisation (CDU), in which a mediated approach that decouples the electrochemical reduction from the catalytic process is explored. Novel bimetallic catalysts will be synthesised and studied, meditating electron donating solutions will be generated, and a robust and comprehensive analytical arrangement will be implemented to allow total identification and quantification of the wide range of possible products. 
Electrocatalytic CO2 reduction is one of the key approaches to CDU, as it has a direct pathway to carbon neutral renewable electricity. Nonetheless it is a field that has shown minimal progress in the past 30 years. A paradigm shift is necessary in the approach to electrochemical CO2 reduction, where conventional heterogeneous interfacial catalysis is limited by mass transport, passivation, and CO2 solubility. This proposal outlines the use of electron donating mediators generated separately to the catalysed chemical reduction of CO2, such that the electrolyte becomes the electrode. This opens a whole new avenue for catalyst research, and here target bimetallic catalysts that suppress side reactions and promote high value product synthesis are described.","1499994","2018-10-01","2023-09-30"
"DECORE","Deep Earth Chemistry of the Core","James Badro","INSTITUT DE PHYSIQUE DU GLOBE DE PARIS","Core formation represents the major chemical differentiation event on the terrestrial planets, involving the separation of a metallic liquid from the silicate matrix that subsequently evolves into the current silicate crust and mantle. The generation of the Earth’s magnetic field is ultimately tied to the segregation and crystallization of the core, and is an important factor in establishing planetary habitability. The processes that control core segregation and the depths and temperatures at which this process took place are poorly understood, however. We propose to study those processes. Specifically, the density of the core is lower than would be expected for pure iron, indicating that a light component (O, Si, S, C, H) must be present. Similarly, the Earth’s mantle is richer in iron-loving (“siderophile”) elements, e.g, V, W, Mo, Ru, Pd, etc., than would be expected based upon low pressure metal-silicate partitioning data. Solutions to these problems are hampered by the pressure range of existing experimental data, < 25 GPa, equivalent to ~700 km in the Earth. We propose to extend the accessible range of pressures and temperatures by developing protocols that link the laser-heated diamond anvil cell with analytical techniques such as (i) the NanoSIMS, (ii) the focused ion beam device (FIB), (iii) and transmission and secondary electron microscopy, allowing us to obtain quantitative data on element partitioning and chemical composition at extreme conditions relevant to the Earth’s lower mantle. The technical motivation follows from the fact that the real limitation on trace element partitioning studies at ultra high-pressure has been the grain size of the phases produced at high P-T, relative to the spatial resolution of the analytical methods available to probe the experiments; we can bridge the gap by combining state-of-the-art laser heating experiments with new nano-scale analytical techniques.","1509200","2008-11-01","2013-10-31"
"DEDIGROWTH","Dedicated growth of novel 1-dimensional materials for emerging nanotechnological applications","Nicole Grobert","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","This proposal aims to establish growth systematics for catalytically grown nanomaterials, such as nanoparticles, nanorods, carbon and hetero-atomic nanotubes. At present there is no clear understanding of the formation mechanism of these structures. Hence, the control over their properties, a vital aspect for technological applications of nanomaterials, is limited and remains difficult. Therefore, the main target of this proposal is the controlled production of new carbon and non-carbon-based nanomaterials with the focus on achieving structural control of the nanomaterials at the atomic level. An essential step towards the controlled generation of such new nanomaterials is a comprehensive understanding of the growth reactions and the role of the metal catalyst involved in the synthesis process. To achieve this, we will use in-situ techniques to study the chemical environment in the reactor during growth and state-of-the-art electron microscopy to reveal the chemical composition of the resulting catalyst particles and structures with atomic resolution. This data will provide information on how the nanostructure may have formed. Theoretical calculations and modelling of atomic scale processes of the catalyst reactivity will be used to draw a consistent picture of the functioning of the catalyst. An improved understanding of the functioning of the catalyst will allow us to estimate how the catalyst particles and reaction conditions have to be modified in order to enhance or to suppress certain products. A new high-throughput synthesis method together with the systematic variation of the growth parameters, such as cluster particle size and composition, temperature, gas pressure and precursor, will be used to generate a nanomaterials growth library. This nanomaterials library will be made available on the Internet for use by other researchers in planning their experiments.","1276038","2010-02-01","2016-01-31"
"DEDOM","Development of Density Functional Theory methods for Organic Metal Interaction","Fabio Della Sala","CONSIGLIO NAZIONALE DELLE RICERCHE","First principles Density-Functional Theory (DFT) methods have been widely applied for computing electronic and optical properties of different systems. Recently theoretical modeling of metal-organic interfaces received a much attention due to their importance in different nanoscience fields. However, common (i.e. local and semi-local) approximations to the exchange-correlation (XC) functional of DFT show several shortcomings in describing metal-organic energy-levels alignment and thus charge-transfer. Aim of the DEDOM (DEvelopment of Density functional theory methods for Organic Metal interaction) project is to elaborate new theoretical methods beyond the current state-of-the-art for the description of the electronic and optical properties of organic molecules linked or deposited on metal surfaces or metal nanoparticles. This task includes: i) the development of new and efficient XC functionals, based on optimized effective potential (OEP) and including exact-exchange and correlation from many-body theory, to obtain an accurate description of charge-transfer between organic molecules and metal surfaces; ii) the investigation of optical properties, including light-emission, of organic molecules on metal surfaces using Time-Dependent DFT; iii) the description of metals using Green’s functions and multi-scale approaches to investigate metal-induced modification of the optical properties of organic molecules, including fluorescence quenching or enhancement due to the coupling of electronic excitations to plasmons. The DEDOM project is theoretically and technically extremely challenging due to the use of unconventional orbital-dependent XC-functionals and it requires a strong interdisciplinary effort, joining solid-state physics, theoretical chemistry, electromagnetic engineering and implementation of advanced computational techniques. If successful, it will represent a major progress in the theoretical description of organic-metal interfaces.","1250000","2008-07-01","2013-12-31"
"DEEP TIME","Dynamic Earth Evolution and Paleogeography through Tomographic Imaging of the Mantle","Karin Sigloch","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","DEEP TIME will unearth a record of geological time that is buried thousands of kilometres deep. The seafloor that covers two-thirds of the earth's surface is a tiny fraction of all seafloor created during its history – the rest has sunk back into the viscous mantle. Slabs of subducted seafloor carry a record of surface history: how continents and oceans were configured over time and where their tectonic plate boundaries lay. DEEP TIME will follow former surface oceans as far back in time as the convecting mantle system will permit, by imaging subducted slabs down to the core with cutting-edge seismological techniques. Current tectonic plate reconstructions incorporate little if any of this deep structural information, which probably reaches back 300+ million years; they are based on present-day seafloor, which constrains only the past 100-150 million years.
DEEP TIME will match deep slab structure to the geological surface record of subduction – volcanic arcs and other crustal slivers that stayed afloat, survived collisions, and form the world’s largest mountain belts. Integrating these two direct records of subduction, the project will
* Add paleo-trenches to existing plate reconstructions and extend them 2-3 times longer into the past. 
* Produce a 3-D atlas of the mantle that matches subducted seafloor with paleo-oceans inferred by land geology. 
* Rigorously test the hypothesis of vertical slab sinking, which may yield an absolute mantle reference frame.
Tomographic models and geological land records will be synthesized into quantitative and testable paleogeographic reconstructions that complement and extend existing ones, especially in paleo-oceanic areas. This is likely to transform our understanding of the earth’s physical surface environment and biosphere during Mesozoic times, as well as the formation of natural resources. It also will put observational constraints on elusive mantle rheologies. Nearly every subdiscipline of the earth sciences could benefit.","1438846","2015-08-01","2020-07-31"
"DEEPSEA","Parallelism and Beyond: Dynamic Parallel Computation for Efficiency and High Performance","Umut Acar","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","We propose to radically extend the frontiers of two major themes in
computing, parallelism and dynamism, and develop a novel paradigm of
computing:  dynamic-parallelism.  To this end, we will follow two
lines of research.  First, we will develop techniques for extracting
efficiency and high performance from parallel programs written in
high-level programming languages.  Second, we will develop the
dynamic-parallelism model, where computations can respond to a wide
variety of dynamic changes to their data automatically and
efficiently, by developing novel abstractions (calculi), high-level
programming-language constructs, and compilation techniques.  The
research will culminate in a language that extends the C programming
language with support for parallel and dynamic-parallel programming.

The proposal is motivated by urgent needs driven by the advent of
multicore chips, which is making parallelism mainstream, and the
increasing ubiquity of software, which requires applications to
operate on highly dynamic data.  These advances demand parallel and
highly dynamic software, which remains too difficult and labor
intensive to develop.  The urgency is further underlined by the
increasing data and problem sizes---online data grows
exponentially, doubling every few years---that require similarly
powerful advances in performance.

The proposal will achieve profound impact by dramatically simplifying
the development of high-performing dynamic and dynamic-parallel
software.  As a result, programmer productivity and software quality
including correctness, reliability, performance, and resource (e.g.,
time and energy) consumption will improve significantly.  The proposal
will not only open new research opportunities in parallel computing,
programming languages, and compilers, but also in other fields where
parallel and dynamic problems abound, e.g., algorithms, computational
biology, geometry, graphics, machine learning, and software systems.","1076570","2013-06-01","2018-05-31"
"DeepSPIN","Deep Learning for Structured Prediction in Natural Language Processing","André Filipe TORRES MARTINS","INSTITUTO DE TELECOMUNICACOES","Deep learning is revolutionizing the field of Natural Language Processing (NLP), with breakthroughs in machine translation, speech recognition, and question answering. New language interfaces (digital assistants, messenger apps, customer service bots) are emerging as the next technologies for seamless, multilingual communication among humans and machines.

From a machine learning perspective, many problems in NLP can be characterized as structured prediction: they involve predicting structurally rich and interdependent output variables. In spite of this, current neural NLP systems ignore the structural complexity of human language, relying on simplistic and error-prone greedy search procedures. This leads to serious mistakes in machine translation, such as words being dropped or named entities mistranslated. More broadly, neural networks are missing the key structural mechanisms for solving complex real-world tasks requiring deep reasoning.

This project attacks these fundamental problems by bringing together deep learning and structured prediction, with a highly disruptive and cross-disciplinary approach. First, I will endow neural networks with a ""planning mechanism"" to guide structural search, letting decoders learn the optimal order by which they should operate. This makes a bridge with reinforcement learning and combinatorial optimization. Second, I will develop new ways of automatically inducing latent structure inside the network, making it more expressive, scalable and interpretable. Synergies with probabilistic inference and sparse modeling techniques will be exploited. To complement these two innovations, I will investigate new ways of incorporating weak supervision to reduce the need for labeled data.

Three highly challenging applications will serve as testbeds: machine translation, quality estimation, and dependency parsing. To maximize technological impact, a collaboration is planned with a start-up company in the crowd-sourcing translation industry.","1436000","2018-02-01","2023-01-31"
"DEEPVISION","Information-age microscopy for deep vision imaging of biological tissue","Ivo Micha Vellekoop","UNIVERSITEIT TWENTE","Modern biology could not exist without the optical microscope. Hundreds of years of research have seemingly developed microscopes to perfection, with one essential limitation: in turbid biological tissue, not even the most advanced microscope can penetrate deeper than a fraction of a millimetre. At larger depths light scattering prevents the formation of an image. DEEP VISION takes a radically new approach to microscopy in order to lift this final limitation.
Microscopes are based on the idea that light propagates along a straight line. In biological tissue, however, this picture is naive: light is scattered by every structure in the specimen. Since the amount of ‘non-scattered’ light decreases exponentially with depth, a significant improvement of the imaging depth is fundamentally impossible, unless scattered light itself is used for imaging.
In 2007, Allard Mosk and I pioneered the field of wavefront shaping. The game-changing message of wavefront shaping is that scattering is not a fundamental limitation for imaging: using a spatial light modulator, light can be focused even inside the most turbid materials, if ‘only’ the correct wavefront is known.
DEEP VISION aims to initiate a fundamental change in how we think about microscopy: to use scattered light rather than straight rays for imaging. The microscope of the future is no longer based on Newtonian optics. Instead, it combines new insights in scattering physics, wavefront shaping, and compressed sensing to extract all useful information from a specimen.
Whereas existing microscopes are ignorant to the nature of the specimen, DEEP VISION is inspired by information theory; imaging revolves around a model that integrates observations with statistical a-priori information about the tissue. This model is used to calculate the wavefronts for focusing deeper into the specimen. Simulations indicate that my approach will penetrate at least four times deeper than existing microscopes, without loss of resolution.","1500000","2016-03-01","2021-02-28"
"DEFTPORE","Deformation control on flow and transport in soft porous media","Christopher MacMinn","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Fluid flows through soft porous media are ubiquitous across nature and industry, from methane bubbles rising through lakebed and seabed sediments to nutrient transport in living cells and tissues to the manufacturing of paper products and many composites. Despite their ubiquity, flow and transport in these systems remain at the frontier of our ability to measure and model. A defining feature of soft porous media is that they can experience deformations that transform the pore structure. This has profound implications for the transport and mixing of solutes and the simultaneous flow of multiple fluid phases, both of which are strongly coupled to the pore structure. The goal of this project is to shed new light on flow and transport in soft porous media by studying a series of three canonical flow problems (tracer transport, miscible viscous fingering, and two-phase flow) across soft adaptations of three classical model systems (a soft-walled Hele Shaw cell, a quasi-2D packing of soft beads, and a cylindrical 3D “core” of soft beads). These flow problems and model systems have been thoroughly studied in the context of stiff porous media, allowing us to leverage decades of previous work and focus exclusively on the new behaviour introduced by “softness”. We will collect an extensive set of new, high-resolution experimental observations in each of these model systems, and we will reconcile these observations with mathematical models based on the traditional approach of upscaled constitutive functions. By updating this traditional approach to account for deformation, we will provide a new, pragmatic class of continuum models that capture the leading-order features of flow and transport in soft porous media. Our results will jumpstart the field of flow and transport in soft porous media, breaking open a vast new realm of research questions and applications around understanding, predicting, and controlling these complex systems.","1482862","2019-02-01","2024-01-31"
"DEGAS","Deciphering the Evolution of Galaxies and the Assembly of Structure: Probing the Growth of Non-Linear Structure in the Dark Universe with Statistical Analyses of Galaxy Surveys","Iohn Peder Ragnar Norberg","UNIVERSITY OF DURHAM","I propose to measure the growth of non-linear structure in the dark universe to answer two fundamental questions in cosmology: Is the Cold Dark Matter structure formation theory compatible with the galaxy distribution on group scales? Is the accelerating expansion of the Universe caused by Dark Energy? This frontier research probes two key components of our standard cosmological model. This study is fundamental for understanding structure formation and galaxy evolution, leading to possible ground-breaking changes in our comprehension of gravitational physics.

I will tackle this ambitious research plan by exploiting my extensive knowledge of galaxy survey analyses and propose to critically test our standard model by measuring three key properties: the shape and evolution of the Cold Dark Matter halo mass function; the efficiency of galaxy formation in Local Group sized systems; the evolution of the growth of structure. To achieve those decisive goals, I will build the DEGAS Team, an inter-disciplinary unit dedicated to solve photometric and spectroscopic survey systematics, to develop optimal clustering statistics for imaging surveys and to create a large variety of state-of-the-art mock Universes to interpret the statistical analyses. The techniques developed will be applied to two world-leading galaxy surveys: GAMA, a multi-wavelength redshift survey of which I am a founder and co-PI, and Pan-STARRS PS1, a unique 3/4-sky imaging survey. Using innovative clustering statistics accounting for individual photometric redshift distributions and statistically robust methods for halo mass function estimates, my DEGAS Team will provide the ultimate test for structure formation models, gain key insights on galaxy evolution and present novel constraints on the nature of gravity.","1256696","2011-01-01","2016-12-31"
"DELPHI","DELPHI: a framework to study Dark Matter and the emergence of galaxies in the epoch of reionization","Pratika DAYAL","RIJKSUNIVERSITEIT GRONINGEN","Our Universe started as a dark featureless sea of hydrogen, helium, and dark matter of unknown composition about 13 and a half billion years ago. The earliest galaxies lit up the Universe with pinpricks of light, ushering in the era of  ‘cosmic dawn’. These galaxies represent the primary building blocks of all subsequent galaxies and the sources of the first (hydrogen ionizing) photons that could break apart the hydrogen atoms suffusing all of space starting the process of ‘cosmic reionization’. By virtue of being the smallest bound structures in the early Universe, these galaxies also provide an excellent testbed for models wherein Dark Matter is composed of warm, fast moving particles as opposed to the sluggish heavy particles used in the standard Cold Dark Matter paradigm.

Exploiting the power of the latest cosmological simulations as well as semi-analytic modelling rooted in first principles, DELPHI will build a coherent and predictive model to answer three of the key outstanding questions in physical cosmology:
- how did the interlinked processes of galaxy formation and reionization drive each other?
- what were the physical properties of early galaxies and how have they evolved through time to give rise to  the galaxy properties we see today?
- what is the nature (mass) of the mysterious Dark Matter  that makes up 80% of the matter content in the Universe?

The timescale of the ERC represents an excellent opportunity for progress on these fundamental questions: observations with cutting-edge instruments (e.g. the Hubble and Subaru telescopes) are providing the first tantalising glimpses of early galaxies assembling in an infant Universe, required to pin down theoretical models. The realistic results obtained by DELPHI will also be vital in determining survey strategies and exploiting synergies between forthcoming key state-of-the-art instruments such as the European-Extremely Large Telescope, the James Webb Space Telescope and the Square Kilometre Array.","1500000","2017-03-01","2022-02-28"
"DELPHI","Computing Answers to Complex Questions in Broad Domains","Jonathan Berant","TEL AVIV UNIVERSITY","The explosion of information around us has democratized knowledge and transformed its availability for
people around the world. Still, since information is mediated through automated systems, access is bounded
by their ability to understand language.
Consider an economist asking “What fraction of the top-5 growing countries last year raised their co2 emission?”.
While the required information is available, answering such complex questions automatically is
not possible. Current question answering systems can answer simple questions in broad domains, or complex
questions in narrow domains. However, broad and complex questions are beyond the reach of state-of-the-art.
This is because systems are unable to decompose questions into their parts, and find the relevant information
in multiple sources. Further, as answering such questions is hard for people, collecting large datasets to train
such models is prohibitive.
In this proposal I ask: Can computers answer broad and complex questions that require reasoning over
multiple modalities? I argue that by synthesizing the advantages of symbolic and distributed representations
the answer will be “yes”. My thesis is that symbolic representations are suitable for meaning composition, as
they provide interpretability, coverage, and modularity. Complementarily, distributed representations (learned
by neural nets) excel at capturing the fuzziness of language. I propose a framework where complex questions
are symbolically decomposed into sub-questions, each is answered with a neural network, and the final answer
is computed from all gathered information.
This research tackles foundational questions in language understanding. What is the right representation
for reasoning in language? Can models learn to perform complex actions in the face of paucity of data?
Moreover, my research, if successful, will transform how we interact with machines, and define a role for
them as research assistants in science, education, and our daily life.","1499375","2019-04-01","2024-03-31"
"DELPHINS","DESIGN AND ELABORATION OFMULTI-PHYSICS INTEGRATED NANOSYSTEMS","Thomas Ernst","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","The innovation of DELPHINS application will consist in building a generic multi-sensor design platform for embedded multi-gas-analysis-on-chip, based on a global modelling from the individual NEMS sensors to a global multiphysics NEMS-CMOS VLSI (Very large Scale Integration) system. The latter constitute a new research field with many potential applications such as in medicine (specific diseases recognition) but also in security (toxic and complex air pollutions), in industry (perfumes, agribusiness) and environment control. As an example, several studies in the last 10 years have demonstrated that some specific combination of biomarkers in breath above a given threshold could indicate early stage of diseases. More generally, patterns of breathing gas could constitute a virtual fingerprint of specific pathologies. NEMS (Nano-Electro-Mechanical Systems) based sensor is one of the most promising technologies to get the required resolutions and sensitivities for few molecules detection. We will focus on the analytical module of the system (sensing part + embedded electronics processing) that will include ultra-dense (more than thousands) NEMS arrays with state-of the art CMOS transistors. We will obtain integrated nano-oscillators individually addressed within an innovative architecture inspired from memory and imaging technologies. Few molecules sensitivity will be achieved thanks to suspended resonant nanowires co-integrated locally with their closed-loop and reading electronics. This would make possible the analysis of complex gases within an integrated portable system, which does not exist yet.","1723206","2009-11-01","2014-10-31"
"DEMONS","Deciphering Eruptions by Modeling Outputs of Natural Systems","Alain Burgisser","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Active volcanoes emit high temperature gases that modify the chemical composition of the Earth’s atmosphere. It is crucial to be able to quantify the contribution of volcanogenic gases to the atmosphere so that the global atmospheric effects of a major eruption can be predicted and so that volcanogenic effects can be discriminated from anthropogenic emissions. At the scale of one volcano, monitoring of gas plumes is a major tool in volcanic risk management. Volcanologists have long measured gas composition and fluxes between and during eruptions and often noted a decoupling between degassing flux and magmatic flux. In parallel, experimental petrologists are now able to calculate the gas composition that is in equilibrium with the magma at depth. However, when the calculated gas composition is compared to that measured at the surface, a general disagreement arises. As a result, it is currently impossible to determine whether a plume is generated in response to passive degassing or to magma ascent. This is a serious drawback as these processes have opposite implications for volcanic activity. Such difficulties are mainly due to the fact that the interplay between degassing mechanisms and gas chemistry has not been addressed. To improve the application of volcanic gas analyses to understanding global geochemical budgets and for the mitigation of volcanic risk, we propose to link deep magmatic processes and surface emissions. Our objective is to model the quantity and composition of volcanic gases as a function of the petrology of the magma at depth and the eruptive regime, and compare those calculations with new measures of plumes at active volcanoes. We will achieve this by modeling the chemical kinetics of degassing in volcanic conduits by using a combination of experimental, field, and numerical approaches. We anticipate building a tool linking flux and composition of gases to eruptive regime, thus opening the door to inverse modeling of volcanic gas observations.","1364478","2008-09-01","2012-12-31"
"DEPENDABLECLOUD","Towards the dependable cloud:
Building the foundations for tomorrow's dependable cloud computing","Rodrigo Seromenho Miragaia Rodrigues","INESC ID - INSTITUTO DE ENGENHARIADE SISTEMAS E COMPUTADORES, INVESTIGACAO E DESENVOLVIMENTO EM LISBOA","Cloud computing is being increasingly adopted by individuals, organizations, and governments. However, as the computations that are offloaded to the cloud expand to societal-critical services, the dependability requirements of cloud services become much higher, and we need to ensure that the infrastructure that supports these services is ready to meet these requirements. In particular, this proposal tackles the challenges that arise from two distinctive characteristic of the cloud infrastructure.

The first is that non-crash faults, despite being considered highly unlikely by the designers of traditional systems, become commonplace at the scale and complexity of the cloud infrastructure. We argue that the current ad-hoc methods for handling these faults are insufficient, and that the only principled approach of assuming Byzantine faults is too pessimistic. Therefore, we call for a new systematic approach to tolerating non-crash, non-adversarial faults. This requires the definition of a new fault model, and the construction of a series of building blocks and key protocol elements that enable the construction of fault-tolerant cloud services.

The second issue is that to meet their scalability requirements, cloud services spread their state across multiple data centers, and direct users to the closest one. This raises the issue that not all operations can be executed optimistically, without being aware of concurrent operations over the same data, and thus multiple levels of consistency must coexist. However, this puts the onus of reasoning about which behaviors are allowed under such a hybrid consistency model on the programmer of the service.  We propose a systematic solution to this problem, which includes a novel consistency model that allows for developing highly scalable services that are fast when possible and consistent when necessary, and a labeling methodology to guide the programmer in deciding which operations can run at each consistency level.","1076084","2012-10-01","2018-01-31"
"DESERTSTORMS","Desert Storms - Towards an Improved 
Representation of Meteorological Processes in 
Models of Mineral Dust Emission","Peter Knippertz","KARLSRUHER INSTITUT FUER TECHNOLOGIE","This project aims at revolutionizing the way the emission of mineral dust from natural soils is treated in numerical models of the Earth system. Dust significantly affects weather and climate through its influences on radiation, cloud microphysics, atmospheric chemistry and the carbon cycle via the fertilization of ecosystems. To date, quantitative estimates of dust emission and deposition are highly uncertain. This is largely due to the strongly nonlinear dependence of emissions on peak winds, which are often underestimated in models and analysis data. The core objective of this project is therefore to explore ways of better representing crucial meteorological processes such as daytime downward mixing of momentum from nocturnal low-level jets, convective cold pools and small-scale dust devils and plumes in models. To achieve this, we shall undertake (A) a detailed analysis of observations including station data, measurements from recent field campaigns, analysis data and novel satellite products, (B) a comprehensive comparison between output from a wide range of global and regional dust models, and (C) extensive sensitivity studies with regional and large-eddy simulation models in realistic and idealized set-ups to explore effects of resolution and model physics. In contrast to previous studies, all evaluations will be made on a process level concentrating on specific meteorological phenomena. Main deliverables are guidelines for optimal model configurations and novel parameterizations that link gridscale quantities with probabilities of winds exceeding a given threshold within the gridbox. The results will substantially advance our quantitative understanding of the global dust cycle and reduce uncertainties in predicting climate, weather and impacts on human health.","1355025","2010-10-01","2015-09-30"
"DESI_JEDI-IMAGING","Development of mass spectrometric techniques for 3D imaging and in-vivo analysis of biological tissues","Zoltan Takats","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","Recent development of atmospheric pressure desorption ionization methods has opened a unique area of application for analytical mass spectrometry. Most of these methods do not require any modification of samples, and this feature, together with the minimal invasiveness of these methods allows direct analytical interrogation of biological tissues, even the real-time, in-vivo observation of biochemical processes. The proposed research focuses on the development of atmospheric pressure desorption ionization mass spectrometric methods for the characterization of biological tissues. The first question to answer is aimed at the nature of information which can be obtained, using a variety of desorption ionization methods including desorption electrospray ionization and jet desorption ionization methods. Preliminary results show, that APDI-MS methods provide information on lipids, metabolic compounds, drugs and certain proteins. First task of the proposed research is to implement a chemical imaging system, which is capable of producing 3D concentration distribution functions for various constituents of tissue samples. The developed methodology will be used to tackle fundamental pathophysiological problems including development of various malignant tumors. A database will be created for the unequivocal identification of various tissues including healthy and malignant tissue samples. In-vivo applications of MS will also be developed. JeDI-MS,similarly to water jet surgery, also utilizes high velocity water jet can directly be used as an intelligent scalpel. Real-time in-situ tissue identification has the potential of revolutionizing cancer surgery, since this way the amount of removed tissue can be minimized, while the tumor removal efficiency is maximized. The identical experimental platform can also be used to gather real-time in-situ metabolic information, which can help to understand pathophysiological changes.","1750000","2008-09-01","2013-08-31"
"DIAL","Diamond Lasers: Revolutionising Laser Engineering","Alan John Kemp","UNIVERSITY OF STRATHCLYDE","Over the last three years, synthetic single crystal diamond with high optical quality has become available for the first time. The time is thus ripe to exploit this unique material for laser engineering. Building on their pioneering work characterising, modelling and experimentally proving this material, this team will explore novel means to harness its the extraordinary properties – a thermal conductivity that is one to two orders of magnitude greater than conventional solid-state laser materials, an extremely high rigidity, excellent resistance to mechanical stress, a wide transparency window, and very good Raman gain properties. The thermal conductivity of diamond, in particular, has the potential to revolutionise solid-state laser design. To date, the design of a solid-state laser has largely been driven by the need to manage heat – the use of diamond can remove this requirement leading to simpler and more compact designs for high performance lasers. This programme will focus on introducing laser gain to structures based on novel high optical quality diamond. Four principal approaches will be examined:
1. Developing high thermal conductivity hybrid structures by sandwiching thin slices of laser gain material between layers of diamond.
2. Using the high Raman gain in diamond to develop high performance diamond Raman lasers
3. Exploiting optically efficient, room-temperature colour centres in diamond to develop a revolutionary suite of broadly tuneable and ultrafast visible lasers.
4. Exploring the direct doping of diamond with laser ions, building on the rapid recent progress in diamond synthesis.
Encompassing laser physics, materials science and device engineering, this programme will balance risk and reward to help position Europe as the world-leader in laser engineering. The lasers developed will be important tools in vital sectors such as science (e.g. biological imaging), energy (e.g. wind speed sensing) and medicine (e.g. treating vascular lesions).","1479707","2011-10-01","2016-09-30"
"DIAMOND","Discovery and Insight with Advanced Models Of  Nanoscale Dimensions","Joost Herman Bert Vandevondele","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Generating knowledge about new materials and obtaining insight in their properties at the nanoscale level are highly relevant to the scientific objectives of the EU. Here, I propose to advance the current state of the art in atomistic modeling of complex systems. I aim at providing and establishing new tools that will allow for the description of large multi-component/multi-phase systems at experimental temperature and pressure with predictive power and controlled error. Generality and ease of use will be key. Building upon my experience, I have identified two clear needs that I will address. One need is a capable implementation, i.e. suitable for large condensed phase systems, of electronic structure theories that go beyond traditional DFT. Powerful linear scaling methods with excess accuracy are essential to validate, on the complex systems themselves, the use of DFT. The second need is an automatic approach for extracting empirical models from raw electronic structure data. Empirical methods are essential to perform simulations that are multiscale in time, space, and accuracy. This automatic approach must be able to generate models beyond the intuition and patience of an individual scientist using advanced optimization methods such as genetic algorithms or neural networks. Models must have a built-in estimate of their quality. The latter feature will allow for enhancing/correcting these empirical approaches automatically with first principles calculations whenever necessary. Massively parallel computing will be the enabling technology. In line with my track record, I will establish these new methods by demonstrating their potential through challenging applications. Example applications will be in diverse fields, including sustainable energy production, catalysis, environment and health. By making these tools freely and openly available to both academia and industry the benefit for the community as a whole will be significant.","1728576","2011-11-01","2016-10-31"
"DIBOSON","Direct and Indirect Searches for New Physics with Diboson Final States at ATLAS","Samira Hassani","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","The Large Hadron Collider (LHC) at the European Organisation for Nuclear Research (CERN) promises a major step forward in the understanding of the fundamental nature of matter. Four large experiments at the LHC are complementary addressing the question of the origin of our Universe by searching for the so-called New Physics.
The ”Standard Model” (SM), the theory that reflects our understanding of elementary particles and their fundamental interactions, has been extensively studied and experimentally verified to an unprecedented precision over the past decades. Despite its impressive success, there are many unanswered questions; which suggest that there is a more fundamental theory which incorporates New Physics. It is expected that at the LHC either New Physics beyond the SM will be discovered or excluded up to a very high energies, thus our view of the fundamental structure of the Universe will be challenged and probably revolutionized in the coming years.
The ATLAS  experiment is dedicated to address the key issue of ElectroWeak Symmetry Breaking (EWSB) and linked to this the search for the Higgs boson as well as the search for Physics beyond the Standard Model. The analysis proposed here is measurement and searches for New Physics in diboson processes . The New Physics effects in the diboson sector will be observed either directly, as in the case of new particle production decaying to diboson, e.g., new vector bosons
and extra-dimensions, or indirectly through deviations from the SM predictions of observable such as cross sections and asymmetries. Triple gauge boson self-coupling (TGC) are extremely sensitive to New Physics, thus a very powerful tool for indirect searches for New Physics contributions through loop corrections.
At the LHC, the unprecedented center-of-mass energy and luminosity will allow to measure the TGC with a high accuracy and to probe regions that are inaccessible at previous experiments even with modest amounts of data.","904190","2011-12-01","2016-11-30"
"DIFFOP","Nonlinear Data and Signal Analysis with Diffusion Operators","Ronen TALMON","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","Nowadays, extensive collection and storage of massive data sets have become a routine in multiple disciplines and in everyday life. These large amounts of intricate data often make data samples arithmetic and basic comparisons problematic, raising new challenges to traditional data analysis objectives such as filtering and prediction. Furthermore, the availability of such data constantly pushes the boundaries of data analysis to new emerging domains, ranging from neuronal and social network analysis to multimodal sensor fusion. The combination of evolved data and new domains drives a fundamental change in the field of data analysis. Indeed, many classical model-based techniques have become obsolete since their models do not embody the richness of the collected data. Today, one notable avenue of research is the development of nonlinear techniques that transition from data to creating representations, without deriving models in closed-form. The vast majority of such existing data-driven methods operate directly on the data, a hard task by itself when the data are large and elaborated. The goal of this research is to develop a fundamentally new methodology for high dimensional data analysis with diffusion operators, making use of recent transformative results in manifold and geometry learning. More concretely, shifting the focus from processing the data samples themselves and considering instead structured data through the lens of diffusion operators will introduce new powerful “handles” to data, capturing their complexity efficiently. We will study the basic theory behind this nonlinear analysis, develop new operators for this purpose, and devise efficient data-driven algorithms. In addition, we will explore how our approach can be leveraged for devising efficient solutions to a broad range of open real-world data analysis problems, involving intrinsic representations, sensor fusion, time-series analysis, network connectivity inference, and domain adaptation.","1260000","2019-02-01","2024-01-31"
"DiGGeS","Discrete Groups and Geometric Structures","Fanny Solveig KASSEL","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Discrete subgroups of Lie groups, whose study originated in Fuchsian differential equations and crystallography at the end of the 19th century, are the basis of a large aspect of modern geometry. They are the object of fundamental theories such as Teichmüller theory, Kleinian groups, rigidity theories for lattices, homogeneous dynamics, and most recently Higher Teichmüller theory. They are closely related to the notion of a geometric structure on a manifold, which has played a crucial role in geometry since Thurston. In summary, discrete subgroups are a meeting point of geometry with Lie theory, differential equations, complex analysis, ergodic theory, representation theory, algebraic geometry, number theory, and mathematical physics, and these fascinating interactions make the subject extremely rich.

In real rank one, important classes of discrete subgroups of semisimple Lie groups are known for their good geometric, topological, and dynamical properties, such as convex cocompact or geometrically finite subgroups. In higher real rank, discrete groups beyond lattices remain quite mysterious. The goal of the project is to work towards a classification of discrete subgroups of semisimple Lie groups in higher real rank, from two complementary points of view. The first is actions on Riemannian symmetric spaces and their boundaries: important recent developments, in particular in the theory of Anosov representations, give hope to identify a number of meaningful classes of discrete groups which generalise in various ways the notions of convex cocompactness and geometric finiteness. The second point of view is actions on pseudo-Riemannian symmetric spaces: some very interesting geometric examples are now well understood, and recent links with the first point of view give hope to transfer progress from one side to the other. We expect powerful applications, both to the construction of proper actions on affine spaces and to the spectral theory of pseudo-Riemannian manifolds","1049182","2017-09-01","2022-08-31"
"DIGT","Diffeomorphism Invariant Gauge Theories, Asymptotic Safety and Geometry","Kirill Krasnov","THE UNIVERSITY OF NOTTINGHAM","The aim of the proposed research is to develop a new description of gravity in four spacetime dimensions. This will (i) serve as a new tool to investigate the conjecture that four-dimensional quantum gravity may be ultra-violet complete (asymptotically safe); (ii) provide new techniques for the problem of classification of geometric structures on four-manifolds. To this end we shall study a certain large class of diffeomorphism invariant SU(2) gauge theories. The low-energy physics of these theories is known to be indistinguishable from that of general relativity (GR). At high energies, they provide an interesting type of deformations of GR, with the key property that the number of propagating degrees of freedom is the same as in general relativity. To test the asymptotic safety conjecture we shall perform perturbative one-loop computations to determine how these theories are renormalized by quantum corrections and then study the resulting renormalization group flow. The same class of theories will also be used to solve some of fundamental conjectures about the geometric structures on four-manifolds. The most optimistic scenario results will prove the asymptotic safety in four-dimensional quantum gravity and explicitly describe the physics occurring around the ultra-violet fixed point. The impact of this on theoretical physics will be comparable to the impact of the 1973 discovery of asymptotic freedom on the high energy physics. The work on the proposal will involve some of the world leading scientists as collaborators and advisors. The project will be carried out in the School of Mathematical Sciences at the University of Nottingham, one of the major mathematics research centres in the UK. Regular long-term research visits to our group by leading scientists, as well as three meetings planned will establish the PI Nottingham research group as one of the leading European centres in the subject area.","1222830","2012-01-01","2017-06-30"
"DIMENSION","High-Dimensional Phenomena and Convexity","Boaz Binyamin Klartag","WEIZMANN INSTITUTE OF SCIENCE LTD","High-dimensional problems with a geometric flavor appear in quite a few branches of mathematics, mathematical physics and theoretical computer science.  A priori, one would think that the diversity and the rapid increase of the number of configurations would make it impossible to formulate general, interesting theorems that apply to large classes of high-dimensional geometric objects. The underlying theme of the proposed project is that the contrary is often true. Mathematical developments of the last decades indicate that high dimensionality, when viewed correctly, may create remarkable order and simplicity, rather than complication. For example, Dvoretzky's theorem demonstrates that any high-dimensional convex body has nearly-Euclidean sections of a high dimension. Another example is the central limit theorem for convex bodies due to the PI, according to which any high-dimensional convex body has approximately Gaussian marginals. There are a number of strong motifs in high-dimensional geometry, such as the concentration of measure, which seem to compensate for the vast amount of different possibilities. Convexity is one of the ways in which to harness these motifs and thereby formulate clean, non-trivial theorems. The scientific goals of the project are to develop new methods for the study of convexity in high dimensions beyond the concentration of measure, to explore emerging connections with other fields of mathematics, and to solve the outstanding problems related to the distribution of volume in high-dimensional convex sets.","998000","2013-01-01","2018-12-31"
"DIMO6FIT","DIMO6FIT: Extending the Standard Model -- Global Fits of Optimal Variables in Diboson Production","Kristin LOHWASSER","THE UNIVERSITY OF SHEFFIELD","The status quo of particle physics after the first data taking at the Large Hadron Collider is: a light Higgs particle has been discovered that is perfectly compatible with the electroweak Standard Model (SM). While this is undoubtedly a historic step in particle physics, it is not entirely satisfactory, as in its current state the SM leaves many questions unanswered. 

If the Standard Model of today is just the low energy theory of more complex phenomena, then these phenomena will become manifest in modifications of the cross sections and differential distributions of known processes. These modifications can be described by higher dimensional operators, which are general extensions of the SM and can be tested using precision measurements of diboson production processes.

The DIMO6Fit project will focus on measuring those production processes most sensitive to the new physics effects, using innovative analysis techniques aimed at significantly reducing the debilitating limitations in current measurements. I will set up a novel combined global fit for determining the higher dimensional operators coherently based on the LHC measurements. 
The full determination of the higher dimensional operators will be the first global precision test of general extensions to the SM. The ERC Starting Grant will make it possible to bring together a team that will conduct more efficient measurements then today at the ATLAS experiment, that will establish the framework for new precision tests, and will generate results of yet unforeseeable potential. With DIMO6FIT I will establish an exciting programme aiming at determining the higher dimensional operators, which will help uncover new physics and elucidate its nature. These novel studies will form a unique and significant contribution to the understanding of the fundamental interactions of known and possibly yet unknown particles.","1497000","2017-02-01","2022-01-31"
"DINAMIX","Real-time diffusion NMR analysis of mixtures","Jean-Nicolas DUMEZ","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Chemical samples often come as solution mixtures. While advanced analytical methods exist for samples at equilibrium, the information on components and their interactions that may be accessed for the frequent and important case of out-of-equilibrium mixtures is much more limited. The DINAMIX project will tackle this challenge and provide detailed, molecular-level information on out-of-equilibrium mixtures. The proposed concept relies on diffusion nuclear magnetic resonance (NMR) spectroscopy, a powerful method that separates the spectra of mixtures’ components and identifies interactions, in correlation with structural insight provided by NMR observables. While classic experiments require several minutes, spatial encoding (SPEN) in principle makes it possible to acquire data orders of magnitude faster, in less than a second. The PI has recently demonstrated that SPEN diffusion NMR is a general concept, with the potential to provide real-time information on out-of-equilibrium mixtures. These include a vast range of systems undergoing chemical change, as well as the important class of “hyperpolarised” solution mixtures generated by dissolution dynamic nuclear polarisation (D-DNP). D-DNP indeed provides dramatic NMR sensitivity enhancements of up to 4 orders of magnitude, which however last only for a short time in solution. In the DINAMIX project, we will develop i/ novel robust and accurate real-time diffusion NMR methods, ii/ advanced algorithms for data processing and analysis, iii/ protocols for sensitive component identification. We will exploit the resulting methodology for mechanistic investigations into catalytic organic and enzymatic reactions. The real-time diffusion NMR analysis of systems that are out-of-chemical equilibrium, far-from-spin-equilibrium or both will provide transformative insight on mixtures, with applications in chemical synthesis, supramolecular and polymer science, structural biology, and microstructural studies in materials and in vivo.","1499307","2019-02-01","2024-01-31"
"DINOPRO","From Protist to Proxy:
Dinoflagellates as signal carriers for climate and carbon cycling during past and present extreme climate transitions","Appy Sluijs","UNIVERSITEIT UTRECHT","I propose to develop and apply a novel method for the integrated reconstruction of past changes in carbon cycling and climate change. This method will be based on combining a well-established sensitive paleoclimate proxy with a recent discovery: the stable carbon isotopic composition (δ13C) of marine dinoflagellates (algae) and their organic fossils (dinocysts) reflects seawater carbonate chemistry, particularly pCO2. Biological (culture) experiments will lead to new insights in dinoflagellate carbon acquisition, and enable quantification of the effect of carbon speciation on dinoflagellate δ13C. The rises in CO2 concentrations during the last century, and at the termination of the last glacial period will be used to test and calibrate the new method. The δ13C of fossil dinoflagellate cysts will subsequently be used to reconstruct surface ocean pCO2 and ocean acidification during a past analogue of rapidly rising carbon dioxide concentrations, 55 million years ago. My research will shed new light on processes such as ocean acidification and the marine carbon cycle as a whole. Past analogues of rapid carbon injection can aid in the quantification of climate change and identification of vulnerable biological groups, critical to identify ‘tipping points’ in system Earth. The study of dinoflagellate carbon isotopes comprises the initiation of a new research field and will provide constraints on ocean acidification in the past and its consequences in the future.","1498800","2010-09-01","2016-08-31"
"DIOCLES","Discrete bIOimaging perCeption for Longitudinal Organ modElling and computEr-aided diagnosiS","Nikolaos Paragyios","ECOLE CENTRALE DES ARTS ET MANUFACTURES","Recent hardware developments from the medical device manufacturers have made possible non-invasive/in-vivo acquisition of anatomical and physiological measurements. One can cite numerous emerging modalities (e.g. PET, fMRI, DTI). The nature (3D/multi-phase/vectorial) and the volume of this data make impossible in practice their interpretation from humans. On the other hand, these modalities can be used for early screening, therapeutic strategies evaluation as well as evaluating bio-markers for drugs development. Despite enormous progress made on the field of biomedical image analysis still a huge gap exists between clinical research and clinical use.  The aim of this proposal is three-fold. First we would like to introduce a novel biomedical image perception framework for clinical use towards disease screening and drug evaluation. Such a framework is expected to be modular (can be used in various clinical settings), computationally efficient (would not require specialized hardware), and can provide a quantitative and qualitative anatomo-pathological indices.  Second, leverage progress made on the field of machine learning along with novel, efficient, compact representation of clinical bio-markers toward computer aided diagnosis. Last, using these emerging multi-dimensional signals, we would like to perform longitudinal modelling and understanding the effects of aging to a number of organs and diseases that do not present pre-disease indicators such as brain neurological diseases, muscular diseases, certain forms of cancer, etc.

Such a challenging and pioneering effort lies on the interface of medicine (clinical context), biomedical imaging (choice of signals/modalities), machine learning (manifold representations of heterogeneous multivariate variables), discrete optimization (computationally efficient inference of higher-order models), and bio-medical image inference (measurement extraction and multi-modal fusion of heterogeneous information sources).","1500000","2011-09-01","2016-08-31"
"DIRECT-fMRI","Sensing activity-induced cell swellings and ensuing neurotransmitter releases for in-vivo functional imaging sans hemodynamics","Noam Shemesh","FUNDACAO D. ANNA SOMMER CHAMPALIMAUD E DR. CARLOS MONTEZ CHAMPALIMAUD","Functional-Magnetic Resonance Imaging (fMRI) has transformed our understanding of brain function due to its ability to noninvasively tag ‘active’ brain regions. Nevertheless, fMRI only detects neural activity indirectly, by relying on slow hemodynamic couplings whose relationships with underlying neural activity are not fully known.
 We have recently pioneered two unique MR approaches: Non-Uniform Oscillating-Gradient Spin-Echo (NOGSE) MRI and Relaxation Enhanced MR Spectroscopy (RE MRS). NOGSE-MRI is an exquisite microstructural probe, sensing cell sizes (l) with an unprecedented l^6 sensitivity (compared to l^2 in conventional approaches); RE MRS is a new spectral technique capable of recording metabolic signals with extraordinary fidelity at ultrahigh fields. 
This proposal aims to harness these novel concepts for mapping neural activity directly, without relying on hemodynamics. The specific objectives of this proposal are:
(1) Mapping neural activity via sensing cell swellings upon activity (μfMRI): we hypothesize that NOGSE can robustly sense subtle changes in cellular microstructure upon neural firings and hence convey neural activity directly. 
(2) Probing the nature of elicited activity via detection of neurotransmitter release: we posit that RE MRS is sufficiently sensitive to robustly detect changes in Glutamate and GABA signals upon activation. 
(3) Network mapping in optogenetically-stimulated, behaving mice: we propose to couple our novel approaches with optogenetics to resolve neural correlates of behavior in awake, behaving mice.  
Simulations for μfMRI predict >4% signal changes upon subtle cell swellings; further, our in vivo RE MRS experiments have detected metabolites with SNR>50 in only 6 seconds. Hence, these two complementary –and importantly, hemodynamics-independent– approaches will represent a true paradigm shift: from indirect detection of neurovasculature couplings towards direct and noninvasive mapping of neural activity in vivo.","1787500","2016-03-01","2021-02-28"
"DIRECTDELIVERY","Controlled fusion of liposomes and cells: a new pathway for direct drug delivery","Alexander Kros","UNIVERSITEIT LEIDEN","Inspired by the natural membrane fusion machinery, the aim of this research line is to design a synthetic analogue in order to: 1) Understand the process of the peptide-controlled fusion of two membranes at the atomic, molecular and mesoscopic level. 2) Developing a new generic method for the controlled delivery of any (bio)molecule directly into the cytoplasm of a cell thereby omitting endocytotic pathways. This new paradigm opens many new applications in the fields of functional proteomics, genomics and siRNA-technology. Studying, imitating and dissecting processes from Nature and applying the underlying principles has been highly successful approach for many years and opened up new lines of research and applications which were previously unimagineable. Examples are the aptamer and antibody technology. I will use this learning-from-Nature approach to design synthetic analogues of the membrane fusion machinery to create new functions and/or applications which are currently non-existent. Membrane fusion is a key process in all living cells as it facilitates the transport of molecules between and within cells. A primary mechanism by which molecules are conveyed to the appropriate location is to encapsulate them in liposomes that deliver the cargo by fusing with the lipid membrane of the target cell or compartment. I will use synthetic analogues of the membrane fusion machinery to induce the controlled fusion between 1) specific liposomes and 2) liposome-cell. This approach opens up a new paradigm for the direct introduction of (bio)molecule into the cytoplasm of living cells omitting the endocytotic pathways for which the applications are only limited by one s imagination.","1392262","2009-10-01","2014-09-30"
"DIRECTEDINFO","Investigating Directed Information","Haim Permuter","BEN-GURION UNIVERSITY OF THE NEGEV","This research investigates a new measure that arises in information theory
called directed information. Recent advances, including our preliminary results, shows that
directed information arises in communication as the maximum rate that can be transmitted reliably
in channels with feedback. The directed information is multi-letter expression and therefore very
hard to optimize or compute.

Our plan is first of all to find an efficient methodology for optimizing the measure using the
dynamic programming framework and convex optimization tools. As an important by-product of
finding the fundamental limits is finding coding schemes that achieves the limits. Second, we
plan to find new roles for directed information in communication, especially in networks with
bi-directional communication and in data compression with causal conditions. Third, encouraged by
a preliminary work on interpretation of directed information in economics and estimation theory,
we plan to show that directed information has interpretation in additional fields such as
statistical physics. We plan to show that there is duality relation between different fields with
causal constraints. Due to the duality insights and breakthroughs in one problem will lead to new
insights in other problems. Finally, we will apply directed information as a statistical
inference of causal dependence. We will show how to estimate and use the directed information
estimator to measure causal inference between two or more process. In particular, one of the
questions we plan to answer  is the influence of industrial activities (e.g., $\text{CO}_2$
volumes) on the global warming.

Our main focus will be to develop a deeper understanding of the mathematical properties of
directed information, a process that is instrumental to each problem. Due to their theoretical
proximity and their interdisciplinary nature, progress in one problem will lead to new insights
in other problems.  A common set of mathematical tools developed in","1224600","2013-08-01","2019-07-31"
"DIREVOLFUN","Directed Evolution of Function within Chemical Systems: Adaptive Capsules and Polymers","Jonathan Russell Nitschke","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","A signature trait of living systems is their ability to dynamically adjust to features of their environments, adapting to stay alive, and evolving to take better advantage of the resources in their environments. This proposed research aims to synthesise new chemical systems that are capable of adaptation and evolution, with the achievement of specified functions being used as the benchmarks by which we may be judged to have succeeded in setting the direction for our systems  evolution. Two parallel lines of inquiry will be followed. First, we will build upon results that we have recently published in Science[1] to create a series of new molecular capsules that are capable of dynamically adapting to different guest molecules. These capsules will serve as sensors and as enzyme-like catalysts through the use of transition-state-analogue guests. Second, we will prepare new metal-containing conjugated polymers through self-assembly, which will be capable of dynamically exchanging building blocks in solution. These polymers will have potential applications as electrically-conductive materials, with functional properties that may be tuned and optimised by the application of evolutionary pressures.

The success of these studies will thus create novel materials with uses as self-assembled sensors, catalysts, and electrical conductors. We will also shed light upon the question of how chemical systems may be induced to evolve under selective pressure. These studies thus have long-term bearing upon the questions of how living systems evolved from pre-biological mixtures of molecules.

[1] P. Mal, B. Breiner, K. Rissanen, J.R. Nitschke, Science 2009, 324, 1697-1699.","1357006","2011-01-01","2016-12-31"
"DISCOTEX","Distributional Compositional Semantics for Text Processing","Stephen Clark","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","""The notion of meaning is central to many areas of Computer Science, Artificial Intelligence (AI), Linguistics, Philosophy, and Cognitive Science. A formal account of the meaning of natural language utterances is crucial to AI, since an understanding of natural language is at the heart of much intelligent behaviour. More specifically, Natural Language Processing (NLP) --- the branch of AI concerned with the automatic processing, analysis and generation of text --- requires a model of meaning for many of its tasks and applications.

There have been two main approaches to modelling the meaning of language in NLP. The first, the ``compositional"""" approach, is based on classical ideas from Philosophy and Mathematical Logic, and includes formal accounts of how the meaning of a sentence can be determined from the relations of words in a sentence. The second, more recent approach focuses on the meanings of the words themselves. This is the ``distributional"""" approach to lexical semantics and is based on the idea that the meanings of words can be determined by considering the contexts in which words appear in text.

The ambitious idea in this proposal is to exploit the strengths of the two approaches, by developing a unified model of distributional and compositional semantics, and exploiting it for NLP tasks and
applications.  The aim is to make the following fundamental contributions:

1. advance the theoretical study of meaning in Linguistics, Computer Science and AI;
2. develop new meaning-sensitive approaches to NLP applications which can be robustly applied to naturally occurring text.

The claim is that language technology based on ``shallow"""" approaches is reaching its performance limit, and the next generation of language technology requires a more sophisticated, but robust, model of meaning, which this project will provide.""","1087930","2012-09-01","2017-08-31"
"DISCOVER","Design of Mixed Anion Inorganic Semiconductors for Energy Conversion","David SCANLON","UNIVERSITY COLLEGE LONDON","Multi-component systems offer the chemical and structural flexibility necessary to meet the needs of next-generation energy conversion. The vast majority of work in the field has focused on mixed-metal compounds. DISCOVER will computationally explore mixed-anion compounds. These are complex systems that provide significant technical challenges for atomistic and electronic structure modelling. Currently, structure-property relationships are poorly developed and there is a distinct lack of understanding of order-disorder transitions. Crucially, no systematic approach has been established for designing new combinations which can be tailored to match the criteria for technological applications. 

This project aims to utilize advanced computational techniques to: (i) understand trends in existing mixed anion systems, and (ii) to employ state of the art crystal structure prediction codes to investigate novel ternary and quaternary mixed-anion compositions. The structure-property information emanating from this analysis will allow us to develop design principles for mixed anion semiconductors, which we will use to predict prototype systems for energy conversion. Promising candidates will be experimentally tested through a collaborative network of experts in the field. This ambitious project will push the boundaries of computational materials design, through the use of both classical and electronic structure simulation techniques for bulk, surface and excited states calculations.

The principle outcome will be a novel understanding of how to controllably design mixed anion semiconductors for technological applications, which will drive this material class to the forefront of materials science, while establishing my group at the frontier of computational materials science.","1499998","2018-02-01","2023-01-31"
"DisDyn","Distributed and Dynamic Graph Algorithms and Complexity","Danupon NA NONGKAI","KUNGLIGA TEKNISKA HOEGSKOLAN","This project aims to (i) resolve challenging graph problems in distributed and dynamic settings, with a focus on connectivity problems (such as computing edge connectivity and distances), and (ii) on the way develop a systematic approach to attack problems in these settings, by thoroughly exploring relevant algorithmic and complexity-theoretic landscapes. Tasks include


- building a hierarchy of intermediate computational models so that designing algorithms and proving lower bounds can be done in several intermediate steps, 

- explaining the limits of algorithms by proving conditional lower bounds based on old and new reasonable conjectures, and

- connecting techniques in the two settings to generate new insights that are unlikely to emerge from the isolated viewpoint of a single field.


The project will take advantage from and contribute to the developments in many young fields in theoretical computer science, such as fine-grained complexity and sublinear algorithms. Resolving one of the connectivity problems will already be a groundbreaking result. However, given the approach, it is likely that one breakthrough will lead to many others.","1500000","2017-02-01","2022-01-31"
"DISPATCH Neuro-Sense","Distributed Signal Processing Algorithms for Chronic Neuro-Sensor Networks","Alexander BERTRAND","KATHOLIEKE UNIVERSITEIT LEUVEN","The possibility to chronically monitor the brain 24/7 in daily-life activities would revolutionize human-machine interactions and health care, e.g., in the context of neuroprostheses, neurological disorders, and brain-computer interfaces (BCI). Such chronic systems must satisfy challenging energy and miniaturization constraints, leading to modular designs in which multiple networked miniature neuro-sensor modules form a ‘neuro-sensor network’ (NSN).

However, current multi-channel neural signal processing (NSP) algorithms were designed for traditional neuro-sensor arrays with central access to all channels. These algorithms are not suited for NSNs, as they require unrealistic bandwidth budgets to centralize the data, yet a joint neural data analysis across NSN modules is crucial.

The central idea of this project is to remove this algorithm bottleneck by designing novel scalable, distributed NSP algorithms to let the modules of an NSN jointly process the recorded neural data through in-network data fusion and with a minimal exchange of data.

To guarantee impact, we mainly focus on establishing a new non-invasive NSN concept based on electroencephalography (EEG). By combining multiple ‘smart’ mini-EEG modules into an ‘EEG sensor network’ (EEG-Net), we compensate for the lack of spatial information captured by current stand-alone mini-EEG devices, without compromising in ‘wearability’. Equipping such EEG-Nets with distributed NSP algorithms will allow to process high-density EEG data at viable energy levels, which is a game changer towards high-performance chronic EEG for, e.g., epilepsy monitoring, neuroprostheses, and BCI.

We will validate these claims in an EEG-Net prototype in the above 3 use cases, benefiting from ongoing collaborations with the KUL university hospital. In addition, to demonstrate the general applicability of our novel NSP algorithms, we will validate them in other emerging NSN types as well, such as modular or untethered neural probes.","1489656","2019-01-01","2023-12-31"
"DISPEQ","Qualitative study of nonlinear dispersive equations","Nikolay Tzvetkov","UNIVERSITE DE CERGY-PONTOISE","We plan to further improve the understanding of the nonlinear dispersive wave propagation phenomena. In particular we plan to develop tools allowing to make a statistical description of the corresponding flows and methods to study transverse stability independently of the very particular arguments based on the inverse scattering. We also plan to study critical problems in strongly non Euclidean geometries.","880270","2010-10-01","2015-09-30"
"DLGAPS","Dynamics of Lie group actions on parameter spaces","Barak Weiss","TEL AVIV UNIVERSITY","There are many parallels between Lie group actions on homogeneous spaces and the action of $\SL_2(\R)$ and its subgroups on strata of translation or half-translation surfaces. I propose to investigate these two spaces in parallel, focusing on the dynamical
behavior, and more specifically, the description of orbit-closures.
I intend to utilize existing and emerging measure rigidity results, and to develop new topological
approaches. These should also shed light on the geometry and topology of the spaces. I propose to apply results concerning these spaces to the study of diophantine approximations (approximation on fractals), geometry of numbers (Minkowski's conjecture), interval exchanges, and rational billiards.","850000","2011-10-01","2016-09-30"
"DLT","Deep Learning Theory: Geometric Analysis of Capacity, Optimization, and Generalization for Improving Learning in Deep Neural Networks","Guido Francisco MONTUFAR CUARTAS","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Deep Learning is one of the most vibrant areas of contemporary machine learning and one of the most promising approaches to Artificial Intelligence. Deep Learning drives the latest systems for image, text, and audio processing, as well as an increasing number of new technologies. The goal of this project is to advance on key open problems in Deep Learning, specifically regarding the capacity, optimization, and regularization of these algorithms. The idea is to consolidate a theoretical basis that allows us to pin down the inner workings of the present success of Deep Learning and make it more widely applicable, in particular in situations with limited data and challenging problems in reinforcement learning. The approach is based on the geometry of neural networks and exploits innovative mathematics, drawing on information geometry and algebraic statistics. This is a quite timely and unique proposal which holds promise to vastly streamline the progress of Deep Learning into new frontiers.","1500000","2018-07-01","2023-06-30"
"DMAP","Data Mining Algorithms in Practice","Flavio Chierichetti","UNIVERSITA DEGLI STUDI DI ROMA LA SAPIENZA","Data Mining algorithms are a cornerstone of today's Internet-related services and products. We aim to tackle some of the most important problems in Data Mining --- our goal is to develop a systematic theoretical understanding of certain simple algorithms that, in spite of being at the core of today's web industry, are not yet well understood in terms of their properties and performances, and to develop new simple algorithms for fundamental problems in this domain that have so far escaped a satisfactory solution.","1137500","2016-02-01","2021-01-31"
"DODECIN","Construction of a Molecular Crane Based on the Flavoprotein Dodecin","Gilbert Nöll","UNIVERSITAET SIEGEN","The flavoprotein dodecin from the halophilic organism Halobacterium salinarum binds not only native but also artificial flavins with high affinities in their oxidized state. Reduction of the flavins induces the dissociation of the holocomplex into apododecin and free flavin. Based on these unique binding characteristics, a molecular crane shall be developed that is able to pick up and to release molecular objects through a switch of the electric potential. For this purpose, a single flavin has to be linked to the conductive tip of an atomic force microscope via a molecular wire-like subunit (flavin molecular wire AFM tip/electrode). On the basis of such an electrochemically switchable molecular crane, it will be possible to bind and release single molecules of dodecin apoprotein or even larger molecular assemblies attached to apododecin serving as molecular junction. While the construction of a molecular crane for the transport of single molecules is the main goal, the successful realization of this project fundamentally depends on the synthesis and characterization of molecular wire-like subunits, which can be used to attach redox-active proteins to surfaces in an electrochemically switchable state. Thus, functionalized single-walled carbon nanotubes or organic p-electron systems will be examined with respect to their ability to serve as molecular wire. Surface modification protocols have to be developed and modified surfaces will be investigated by a combination of atomic force microscopy, surface plasmon resonance spectroscopy, and electrochemical methods. The results of these studies will be of general interest for the construction of molecular switches, devices, and transport systems, and for the development of amperometric biosensors and biofuel cells.","1100000","2009-11-01","2015-10-31"
"DOPING-ON-DEMAND","Doping on Demand: precise and permanent control of the Fermi level in nanocrystal assemblies","Arjan Houtepen","TECHNISCHE UNIVERSITEIT DELFT","The aim of the work proposed here is to develop a completely new method to electronically dope assemblies of semiconductor nanocrystals (a.k.a quantum dots, QDs), and porous semiconductors in general. External dopants are added on demand in the form of electrolyte ions in the voids between QDs. These ions will be introduced via electrochemical charge injection, and will subsequently be immobilized by (1) freezing the electrolyte solvent at room temperature or (2) chemically linking the ions to ligands on the QD surface, or by a combination of both. Encapsulating doped QD films using atomic layer deposition will provide further stability. This will result in stable doped nanocrystal assemblies with a constant Fermi level that is controlled by the potential set during electrochemical charging. 

QDs are small semiconductor crystals with size-tunable electronic properties that are considered promising materials for a range of opto-electronic applications. Electronic doping of QDs remains a big challenge even after two decades of research into this area. At the same time it is highly desired to dope QDs in a controlled way for applications such as LEDs, FETs and solar cells. This research project will provide unprecedented control over the doping level in QD films and will provided a major step in the optimization of optoelectronic devices based on QDs. The “Doping-on-Demand” approach will be exploited to develop degenerately doped, low-threshold QD lasers that can be operated under continuous wave excitation, and QD laser diodes that use electrical injection of charge carriers. The precise control of the Fermi-level will further be used to optimize pin junction QD solar cells and to develop, for the first time, QD pn junction solar cells with precise control over the Fermi levels.","1497842","2016-01-01","2020-12-31"
"DoRES","Direct measurements of key nuclear Reactions for the creation of Elements in Stars","Claudia Lederer","THE UNIVERSITY OF EDINBURGH","The evolution of the universe has left an imprint in the form of the chemical elements. Understanding the cosmic origins of the elements remains a major challenge for science. The abundances of elements we see in our solar system, distant stars, meteorites, and in stellar explosions provide us with clues about how the elements came to be produced in a variety of different processes and stellar environments. To unravel these mysteries we need to understand the nuclear reactions producing and destroying the elements. New generation accelerator facilities and instrumentation are being developed in Europe which will enable many of these reactions to be measured directly for the first time, and with high precision. This offers the prospect of a major step forward in the field in the next few years. Many of the key reactions involve unstable nuclei, studied experimentally either by using radioactive beams or targets. These unstable nuclei play a critical role in high temperature stellar environments, most notably stellar explosions. Reactions can occur on the unstable nucleus before it has decayed thereby strongly altering the path of subsequent element synthesis. The proposal is sub-divided into 5 themes, concerning production of the heavy elements in neutron capture reactions, destruction of the cosmic gamma-ray emitter 26Al in core collapse supernovae, neutron source reactions in  stars, the puzzle of high abundances of proton-rich heavy isotopes, and the origin of nature’s least abundant isotope 180mTa. Experiments will initially be performed using neutron beams from the upgraded n_TOF facility at CERN including the high flux EAR-2 beam line, and using radioactive beams from the upgraded HIE-ISOLDE facility at CERN. In the later phase of the proposal experiments will also be performed using the new ultra-high intensity neutron beam facility FRANZ at Frankfurt, and with radioactive beams injected into heavy ion storage rings to be installed at GSI and CERN.","1495479","2016-06-01","2021-05-31"
"DOS","Drugging the Undruggable: Discovery of Protein-Protein Interaction Modulators Using Diversity-Oriented Synthesis","David Spring","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","This proposal aims to exploit diversity-oriented synthesis in order to lay the scientific and technological foundations for the development of enzyme inhibition by protein-protein interaction (PPI) modulation as a tool for chemical biology and molecular therapeutics. We will deploy diversity-oriented synthesis lead discovery to explore concepts for PPI modulation in important enzyme families. This work will yield new chemical entities with a spectrum of properties directed against candidate macromolecular interactions important in the regulation of enzymes that mediate key biological pathways. The proposed work has the potential to transform current approaches to drug discovery, and to radically extend the repertoire of tools available for chemical biology. It will help to address the problem of identifying small-molecule inhibitors of PPIs, widely accepted to be of major fundamental and practical significance to biomedical science.","1499723","2012-01-01","2016-12-31"
"DPI","Deep Packet Inspection to Next Generation Network Devices","Anat Bremler-Barr","HERZLIYA","Deep packet inspection (DPI) lies at the core of contemporary Network Intrusion Detection/Prevention Systems and Web Application Firewall. DPI aims to identify various malware (including spam and viruses), by inspecting both the header and the payload of each packet and comparing it to a known set of patterns. DPI are often performed on the critical path of the packet processing, thus the overall performance of the security tools is dominated by the speed of DPI.

Traditionally, DPI considered only exact string patterns. However, in modern network devices patterns are often represented by regular expressions due to their superior expressiveness. Matching both exact string and regular expressions are well-studied area in Computer Science; however all well-known solutions are not sufficient for current network demands: First, current solutions do not scale in terms of speed, memory and power requirements. While current network devices work at 10-100 Gbps and have thousands of patterns, traditional solutions suffer from exponential memory size or exponential time and induce prohibitive power consumption.  Second, non clear-text traffic, such as compressed traffic, becomes a dominant portion of the Internet and is clearly harder to inspect.

In this research we design new algorithms and schemes that cope with today demand. This is evolving area both in the Academia and Industry, where currently there is no adequate solution.

We intend to use recent advances in hardware to cope with these demanding requirements. More specifically, we plan to use Ternary Content-Addressable Memories (TCAMs), which become standard commodity in contemporary network devices.  TCAMs can compare a key against all rules in a memory in parallel and thus provide high throughput.  We believ","990400","2010-11-01","2016-10-31"
"DPMP","Dependable Performance on Many-Thread Processors","Lieven Eeckhout","UNIVERSITEIT GENT","Contemporary microprocessors seek at improving performance through thread-level parallelism by co-executing multiple threads on a single microprocessor chip. Projections suggest that future processors will feature multiple tens to hundreds of threads, hence called many-thread processors. Many-thread processors, however, lead to non-dependable performance: co-executing threads affect each other s performance in unpredictable ways because of resource sharing across threads. Failure to deliver dependable performance leads to missed deadlines, priority inversion, unbalanced parallel execution, etc., which will severely impact the usage model and the performance growth path for many important future and emerging application domains (e.g., media, medical, datacenter).

DPMP envisions that performance introspection using a cycle accounting architecture that tracks per-thread performance, will be the breakthrough to delivering dependable performance in future many-thread processors. To this end, DPMP will develop a hardware cycle accounting architecture that estimates single-thread progress during many-thread execution. The ability to track per-thread progress enables system software to deliver dependable performance by assigning hardware resources to threads depending on their relative progress. Through this cooperative hardware-software approach, this project addresses a fundamental problem in multi-threaded ad multi/many-core processing.","1389000","2010-10-01","2016-09-30"
"DRAGNET","""DRAGNET: A high-speed, wide-angle camera for catching extreme astrophysical events""","Jason William Thomas Hessels","STICHTING ASTRON, NETHERLANDS INSTITUTE FOR RADIO ASTRONOMY","""Looking up on a starry night, it’s easy to imagine that the Universe is unchanging. In reality, however, the Universe is teeming with activity: there are massive explosions from accreting black holes, bright radio flashes from ultra-magnetic pulsars, and likely other spectacles that have so far escaped our prying eyes. These fleeting events can happen faster than the blink of an eye and, importantly, they trace the most extreme astrophysical phenomena. Catching these rare performances poses a major challenge for observational astronomers, but the scientific payoff is well worth the effort.

With this proposal, I will mould the Low-Frequency Array (LOFAR) telescope into DRAGNET, the world's premier high-speed, wide-angle camera for radio astronomy. Radio waves are a unique and powerful way of investigating the most extreme astrophysical processes. With DRAGNET I will characterize the rate of fast radio transients, i.e. astrophysical bursts lasting less than a second, and search for new astrophysical phenomena in this largely unexplored domain. This has the potential to give us transformative insight into the extremes of gravity and dense matter. Alongside this, I will simultaneously monitor hundreds of radio-emitting neutron stars (pulsars) on a regular basis. This will allow me to understand why some neutron stars pulse regularly, while others show rapid switches in their emission properties. This will address the physics behind the strongest magnetic fields in the Universe.

I have led the construction of LOFAR's high-time-resolution observing capabilities; in this project I will capitalize on that investment and do cutting-edge science that is beyond the reach of any other existing telescope. Simply put, this project will establish a world-leading research group in the emerging field of fast radio transients and will crystallize the wide-field radio telescope as an essential tool for unveiling the bustling activity that makes our Universe so interesting to study.""","1964587","2014-01-01","2018-12-31"
"DRANOEL","Deciphering RAdio NOn-thermal Emission on the Largest scales","Annalisa BONAFEDE","ALMA MATER STUDIORUM - UNIVERSITA DI BOLOGNA","This proposal aims to understand  the origin of the radio emission detected in the most massive objects in our Universe: galaxy clusters.
The extreme physical conditions in the intra-cluster medium of galaxy clusters are beyond anything achievable in any laboratory on Earth. The space in between the galaxies is filled with an extremely hot and diluted gas that hosts the largest-scale magnetic fields known so far. A big challenge of modern astrophysics is understanding the origin of radio emission spread over huge swathes in some clusters. This emission is a mystery because it requires relativistic electrons moving around magnetic field lines, but both the origin of the magnetic fields and of the electrons are unknown. Absolutely fundamental to the understanding of the radio emission are a detailed knowledge of the magnetic fields and of the energy spectrum of the emitting particles.
We are stepping into a new era of observational astronomy, in which surveys will be conducted rather than single pointed observations. This survey era is changing our approach to observational data. It enables to perform all-sky studies but calls for numerical and technological efforts for the data handling.
Taking advantage of the advent of new radio and X-ray facilities, such as LOFAR, the JVLA, ASKAP, and  eROSITA, this project wants to understand the origin of the radio emission, its evolution and its connections with the cluster dynamics.
We have today the unprecedented opportunity to discover the physical processes at work in these unique environments, that link the micro-physical processes at work in galaxy clusters with the clusters' macro-physics. The proposed  study will address fundamental questions not restricted to the physics of galaxy clusters but having impact on several inter-connected physical disciplines, such as cosmology, astro-particle physics and  plasma physics.","1496250","2017-09-01","2022-08-31"
"DROEMU","DROPLETS AND EMULSIONS: DYNAMICS AND RHEOLOGY","Mauro Sbragaglia","UNIVERSITA DEGLI STUDI DI ROMA TOR VERGATA","The applications of micro- and nanofluidics are now numerous, including lab-on-chip systems based upon micro-manipulation of discrete droplets, emulsions of interest in food and medical industries (drug delivery), analytical separation techniques of biomolecules, such as proteins and DNA, and facile handling of mass-limited samples. The problems involved contain diverse nano- and microstructures with a variety of lifetimes, touching atomistic scales (contact lines, thin films), mesoscopic collective behaviour (emulsions, glassy, soft-jammed systems) and hydrodynamical spatio-temporal evolutions (droplets and interface dynamics) with complex rheology and strong non-equilibrium properties. The interplay of the dynamics at the different scales involved still remains to be fully understood.
The fundamental research I address in this project aims to set up the unified framework for the characterization and modelling of interfaces in confined geometries by means of an innovative micro- and nanofluidic numerical platform.
The main challenging and ambitious questions I intend to address in my project are: How the stability of micro- and nanodroplets is affected by thermal gradients? Or by boundary corrugation and modulated wettability? Or by complex rheological properties of the dispersed and/or continuous phases? How these effects can be tuned to design new optimal devices for emulsions production? What are the rheological properties of these new soft materials? How confinement in small structures changes the bulk emulsion properties? What is the molecular-hydrodynamical mechanism at the origin of contact line slippage? How to realistically model the fluid-particle interactions on the molecular scale?
The strength of the project lies in an innovative and state-of-the-art numerical approach, based on mesoscopic Lattice Boltzmann Models, coupled to microscopic molecular physics, supported by theoretical modelling, lubrication theory and experimental validation.","1170924","2011-12-01","2016-11-30"
"DROPCELLARRAY","DropletMicroarrays: Ultra High-Throughput Screening of Cells in 3D Microenvironments","Pavel Levkin","KARLSRUHER INSTITUT FUER TECHNOLOGIE","High-throughput (HT) screening of live cells is crucial to accelerate both fundamental biological research and discovery of new drugs. Current methods for HT cell screenings, however, either require a large number of microplates, are prone to cross-contaminations and are limited to adherent cells (cell microarrays), or are not compatible with adherent cells as well as with spatial indexing (droplet microfluidics). We recently demonstrated the use of superhydrophobic-superhydrophilic microarrays to create high-density arrays of microdroplets or hydrogel micropads. We propose here to develop a new platform for HT cell screening experiments using the unique properties of the superhydrophilic microarrays separated by superhydrophobic thin barriers. The new technology will allow us to perform up to 300K cell experiments in parallel using a single chip. Individual cell experiments will be performed in thousands of completely isolated microdroplet at defined locations on the chip. This will enable spatial indexing, time-lapse measurements and screening of either adherent or non-adherent cells. Parallel manipulations within individual microreservoirs, such as washing, addition of chemical libraries, or staining will be developed to open new possibilities in the field of live cell studies. Superhydrophobic barriers will allow complete isolation of the microreservoirs, thus preventing cross-contamination and cell migration. We will also develop a technology for the HT screening of cells in 3D hydrogel micropads. We will use these methods to gain better understanding of how different parameters of the 3D cell microenvironment influence various aspects of cell behavior. The project will require the development of new technological tools which can later be applied to a wide range of cell screening experiments and biological problems. Our long term aim is to replace the outdated microplate technology with a more powerful and convenient method for cell screening experiments.","1499820","2014-02-01","2019-01-31"
"DRY-2-DRY","Do droughts self-propagate and self-intensify?","Diego González Miralles","UNIVERSITEIT GENT","Droughts cause agricultural loss, forest mortality and drinking water scarcity. Their predicted increase in recurrence and intensity poses serious threats to future global food security. Several historically unprecedented droughts have already occurred over the last decade in Europe, Australia and the USA. The cost of the ongoing Californian drought is estimated to be about US$3 billion. Still today, the knowledge of how droughts start and evolve remains limited, and so does the understanding of how climate change may affect them.

Positive feedbacks from land have been suggested as critical for the occurrence of recent droughts: as rainfall deficits dry out soil and vegetation, the evaporation of land water is reduced, then the local air becomes too dry to yield rainfall, which further enhances drought conditions. Importantly, this is not just a 'local' feedback, as remote regions may rely on evaporated water transported by winds from the drought-affected region. Following this rationale, droughts self-propagate and self-intensify.

However, a global capacity to observe these processes is lacking. Furthermore, climate and forecast models are immature when it comes to representing the influences of land on rainfall. Do climate models underestimate this land feedback? If so, future drought aggravation will be greater than currently expected. At the moment, this remains largely speculative, given the limited number of studies of these processes.

I propose to use novel in situ and satellite records of soil moisture, evaporation and precipitation, in combination with new mechanistic models that can map water vapour trajectories and explore multi-dimensional feedbacks. DRY-2-DRY will not only advance our fundamental knowledge of the mechanisms triggering droughts, it will also provide independent evidence of the extent to which managing land cover can help 'dampen' drought events, and enable progress towards more accurate short-term and long-term drought forecasts.","1465000","2017-02-01","2022-01-31"
"DualitiesHEPTH","Dualities in Super-symmetric Gauge Theories, String Theory and Conformal Field Theories","Luis Fernando Alday","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The aim of the present proposal is to establish a research team developing and exploiting dualities arising in super-symmetric gauge theories, string theory and conformal field theories.  These will also have many applications outside these fields. The overarching aims of the team will be: To develop established dualities into computational tools for physical quantities such as the S-matrix, correlation functions and partition functions. The construction of explicit examples of new dualities. To use such dualities to gain new insights into the mathematical structure of the theories involved.

The proposal brings together researchers with different areas of expertise: super-symmetric gauge theories, string theories, conformal field theories, integrable systems and special functions. We divide it into two strands:

Strand I. Deals with the AdS/CFT correspondence, scattering amplitudes and correlation functions. The main objectives are to compute scattering amplitudes of planar maximally-super symmetric Yang-Mills to all values of the coupling; extend these computations to the non-planar case; compute efficiently correlation functions in this theory.

Strand II. Deals with new and exciting correspondences between four dimensional super-symmetric theories and two dimensional conformal field theories. We aim to find more examples of 4d/2d correspondences and to develop the established ones (and new ones) into efficient computational tools which will be used, for instance, to compute correlation functions in 2d Conformal Toda theories and other CFT's and even physical quantities in theories that do not admit a Lagrangian description. Progress in the first part of this strand will be used to understand the elusive 6d (2,0) theory. Furthermore, we will actively look for common mathematical structures between strands I and II.","1414258","2012-12-01","2017-11-30"
"DUST-IN-THE-WIND","Dust in the wind — a new paradigm for inflow and outflow structures around supermassive black holes","Sebastian Florian Hoenig","UNIVERSITY OF SOUTHAMPTON","Active galactic nuclei (AGN) represent the active growing phases of supermassive black holes. For the first time, we are able to resolve the dusty gas on parsec scales and directly test our standard picture of these objects. While this “unification scheme” relates the parsec-scale IR emission with a geometrically-thick disk, I have recently found that the bulk of the dust emission comes from the polar region of the alleged disk where gas is blown out from the vicinity of the black hole. Along with these polar features, the compactness of the dust distribution seems to depend on the accretion state of the black hole. Neither of these findings have been predicted by current models and lack a physical explanation. 
To explain the new observations, I proposed a revision to the AGN unification scheme that involves a dusty wind driven by radiation pressure. Depending on their masses, velocities, and frequency, such dusty winds might play a major role in self regulating AGN activity and, thus, impact the interplay between host and black hole evolution. However, as of now we do not know if these winds are ubiquitous in AGN and how they would work physically. Upon completion of the research program, I want to
• characterise the pc-scale mass distribution, its kinematics, and the connection to the accretion state of the AGN,
• have a physical explanation of the dusty wind features and constrain its impacts on the AGN environment, and
• have established dust parallax distances to several nearby AGN, as a multi-disciplinary application of the constraints on the dust distribution.
For that, I will combine the highest angular resolution observations in the IR and sub-mm to create the first pc-scale intensity, velocity, and density maps of a sample of 11 AGN. I will develop a new model that combines hydrodynamic simulations with an efficient treatment of radiative transfer to simulate dusty winds. Finally, direct distances to 12 AGN with a combined 3% precision will be measured.","1475171","2016-05-01","2021-04-30"
"Dust2Planets","Unveiling the role of X-rays in protoplanetary disks via laboratory astrophysics","Lisseth Gavilan","UNIVERSITE PARIS-SUD","The arrival of ALMA and JWST could revolutionize our understanding of planet formation from the observations of protoplanetary disks. But in order to interpret such observations, better models fed by robust laboratory data are urgently needed. However, laboratory experiments designed to study cosmic matter have mostly focused on the first stages of stellar evolution, where molecular clouds are irradiated by ultraviolet (UV) photons from OB stars. The subsequent protoplanetary stage, where young stars vigorously emit X-rays, has been rarely addressed by experiments. Yet X-rays have a larger penetration depth in solids than UV photons, and could enable important photochemical pathways in the evolution of protoplanetary matter. In this project, we aim to quantify the impact of X-rays on protoplanetary dust via laboratory astrophysics. Our goal is to give closure to the question: how do X-rays impact disk evolution and early planet formation?

This project will go beyond the state-of-the-art in two directions: via the laboratory simulation of the X-ray spectrum of T Tauri stars, and by pioneering the use of heterogeneous analogs to protoplanetary dust. We will perform a coupled study of both the dust and gas phases following irradiation to quantify the full impact of X-rays. Complex organic molecules resulting from X-ray irradiation and desorption will be compared to cometary and ALMA detections to clarify the disk-comet connection. Analysis of the X-irradiated solids will elucidate the physico-chemical mechanisms of dust growth, key to the evolution of primordial seeds to planetesimals. X-ray photochemical rates on both the dust and gas phases will be consolidated in a new X-ray Astrochemical Database (XRAD). Our laboratory data will shed light on the photochemical evolution of protoplanetary disks and more generally, on other X-ray Dominated Regions in the universe.","1499876","2019-10-01","2024-09-30"
"DustPrints","Dusting for the Fingerprints of Planet Formation","Tilman David BIRNSTIEL","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","With close to 2000 detected planets, it is striking that we still do not
understand how planets form. Their building blocks form in gas disks
around young stars, where colliding dust grains form ever-larger
aggregates. But this growth is not without limits: larger particles
quickly drift towards the star and collide at speeds that shatter them
to pieces, long before gravity can bind them together. The mechanisms
involved in the assembly and transport of these building blocks remain
some of the biggest mysteries of planet formation.

Solids in protoplanetary disks evolve differently than the gas, but not
independent of it. Observations allow us to directly probe particle
growth – the first stage of planet formation. But the gas-solids
coupling also enables us to probe the gas disk structure indirectly –
just like we cannot see the wind, but we see leaves being moved by it.
With this proposal I want to answer some of the key questions of planet
formation: (1) What mechanisms drive disk evolution? (2) What role do
solids play in the transport of volatiles and the pre-biotic building
blocks of life? We will for the first time couple detailed models of the
evolution of solids in protoplanetary disks with chemical models on the
one side and with hydrodynamical simulations on the other. We aim to
derive the unique observable fingerprints of these processes and link
those predictions to upcoming observations.

With the advent of the ALMA observatory, the prospects of finding these
fingerprints are excellent. ALMA will allow us to test our predictions
through a wide range of observables at unprecedented sensitivity and
resolution, including dust continuum emission, chemical abundance
patterns, and isotopic ratios in disks, comets, and our solar system.
With our work designed to interpret these observations, we will set the
stage for a future understanding of protoplanetary disks and planet
formation.","1435088","2017-03-01","2022-02-28"
"DYBHO","The dynamics of black holes:  testing the limits of Einstein's theory","Vitor Manuel Dos Santos Cardoso","INSTITUTO SUPERIOR TECNICO","From astrophysics to high-energy physics and quantum gravity, black holes (BHs) have acquired an ever increasing role in fundamental physics, and are now part of the terminology of many important branches of theoretical and observational physics. It has been established that supermassive BHs lurk at the center of many galaxies and provide fertile ground for stellar growth and evolution. Millions of stellar-mass BHs populate the galaxies, and power violent processes such as gamma-ray bursts, etc. In high-energy physics, the gauge/gravity duality has created a powerful framework for the study of strongly coupled gauge theories and found applications in connection with the experimental program on heavy ion collisions at RHIC and LHC, among many others. As emphasized by Maldacena and Witten, BHs play a special role in the correspondence: confinement in QCD may be related via the Hawking-Page phase transition to BHs in anti-de Sitter (AdS).

Given the central role that BHs have been claiming in physics, a major task for theoreticians
is to understand processes in which they are involved. With the advent of techniques to evolve BH spacetimes numerically, the field is undergoing a phase transition from a promising branch of general relativity to one of the most exciting fields in 21st century research that will open up unprecedented opportunities to expand and test our understanding of fundamental physics and the universe.

This project aims at evolving numerically BHs in generic backgrounds, in a fully non-linear framework. We intend to generalize all the machinery developed in the last 30 years for asymptotically flat,  (3+1) dimensional spacetimes to other geometries and field equations.
This allows a number of fundamental questions to be tackled, from tests of the cosmic censorship to an understanding of the stability and phase diagrams of  these objects and
how different field equations can impact on gravitational-wave emission","915000","2010-12-01","2015-11-30"
"DYCOCA","DYNAMIC COVALENT CAPTURE: Dynamic Chemistry for Biomolecular Recognition and Catalysis","Leonard Jan Prins","UNIVERSITA DEGLI STUDI DI PADOVA","Molecular recognition plays a fundamental role in nearly all chemical and biological processes. The objective of this research project is to develop new methodology for studying and utilizing the noncovalent recognition between two molecular entities, focussing on biomolecular receptors and catalysts. A dynamic covalent capture strategy is proposed, characterized by the following strongholds. The target itself self-selects the best component out of a combinatorial library. The approach has a very high sensitivity, because molecular recognition occurs intramolecularly, and is very flexible, which allows for an easy implementation in very diverse research areas simply by changing the target. The dynamic covalent capture strategy is strongly embedded in the fields of supramolecular chemistry and (physical) organic chemistry. Nonetheless, the different work programmes strongly rely on the input from other areas, such as combinatorial chemistry, bioorganic chemistry, catalysis and computational chemistry, which renders the project highly interdisciplinary. Identified targets are new synthetic catalysts for the selective cleavage of biologically relevant compounds (D-Ala-D-Lac, cocaine and acetylcholine, and in a later stage peptides and DNA/RNA). Applicative work programmes are dedicated to the dynamic imprinting of monolayers on nanoparticles for multivalent recognition and cleavage of biologically relevant targets in vivo and to the development of new screening methodology for measuring chemical equilibria and, specifically, for the discovery of new HIV-1 fusion inhibitors.","1400000","2009-10-01","2014-09-30"
"DYMO","Dynamic dialogue modelling","Milica GASIC","UNIVERSITAT DES SAARLANDES","With the prevalence of information technology in our daily lives, our ability to interact with machines in increasingly simplified and more human-like ways has become paramount.  Information is becoming ever more abundant but our access to it is limited not least by technological restraints.  Spoken dialogue systems address this issue by providing an intelligent speech interface that facilitates swift, human-like acquisition of information.

The advantages of speech interfaces are already evident from the rise of personal assistants such as Siri, Google Assistant, Cortana or Amazon Alexa. In these systems, however, the user is limited to a simple query, and the systems attempt to provide an answer within one or two turns of dialogue. To date, significant parts of these systems are rule-based  and do not readily scale to changes in the domain of operation. Furthermore, rule-based systems can be brittle when speech recognition errors occur.

The vision of this project is to develop novel dialogue models that provide natural human-computer interaction beyond simple information-seeking dialogues and that continuously evolve as they are being used by exploiting both dialogue and non-dialogue data. Building such robust and intelligent spoken dialogue systems poses serious challenges in artificial intelligence and machine learning.  The project will tackle four bottleneck areas that require fundamental research: automated knowledge acquisition, optimisation of complex behaviour, realistic user models and sentiment awareness.  Taken together, the proposed solutions have the potential to transform the way we access information in areas as diverse as e-commerce, government, healthcare and education.","1499956","2019-09-01","2024-08-31"
"DYNA-MIC","Deep non-invasive imaging via scattered-light acoustically-mediated computational microscopy","Ori Katz","THE HEBREW UNIVERSITY OF JERUSALEM","Optical microscopy, perhaps the most important tool in biomedical investigation and clinical diagnostics, is currently held back by the assumption that it is not possible to noninvasively image microscopic structures more than a fraction of a millimeter deep inside tissue. The governing paradigm is that high-resolution information carried by light is lost due to random scattering in complex samples such as tissue. While non-optical imaging techniques, employing non-ionizing radiation such as ultrasound, allow deeper investigations, they possess drastically inferior resolution and do not permit microscopic studies of cellular structures, crucial for accurate diagnosis of cancer and other diseases.
I propose a new kind of microscope, one that can peer deep inside visually opaque samples, combining the sub-micron resolution of light with the penetration depth of ultrasound. My novel approach is based on our discovery that information on microscopic structures is contained in random scattered-light patterns. It breaks current limits by exploiting the randomness of scattered light rather than struggling to fight it.
We will transform this concept into a breakthrough imaging platform by combining ultrasonic probing and modulation of light with advanced digital signal processing algorithms, extracting the hidden microscopic structure by two complementary approaches: 1) By exploiting the stochastic dynamics of scattered light using methods developed to surpass the diffraction limit in optical nanoscopy and for compressive sampling, harnessing nonlinear effects. 2) Through the analysis of intrinsic correlations in scattered light that persist deep inside scattering tissue.
This proposal is formed by bringing together novel insights on the physics of light in complex media, advanced microscopy techniques, and ultrasound-mediated imaging. It is made possible by the new ability to digitally process vast amounts of scattering data, and has the potential to impact many fields.","1500000","2016-04-01","2021-03-31"
"DYNAMIC MINVIP","Dynamic Minimal prior knowledge for model based Computer Vision and Scene Analysis","Bodo Rosenhahn","GOTTFRIED WILHELM LEIBNIZ UNIVERSITAET HANNOVER","Efficient solutions for open problems in computer vision are often achieved with the help of suitable prior knowledge, e.g. stemming from labeled databases, physical simulation or geometric invariances. Yet it has been largely neglected to analyse the minimal amount of prior knowledge, needed to satisfactory solve computer vision tasks. Even more important, there is need to steer the amount of priors in a dynamic fashion. Especially for scene analysis, database knowledge can become so large and complex, that it cannot be integrated efficiently for optimization. On the other hand, there exist geometric priors which are efficient and compact, but they have to be integrated and exploited explicitly in vision systems. As a consequence there is need to develop methods to conclude from (statistical) database knowledge to geometric prior knowledge and therefore to achieve compressed priors which contain the relevant information from a given database.  Besides the efficient regularization during scene analysis, specific tasks require to treat the amount of priors dynamically, e.g. to maintain individualities of patterns or to avoid a bias from a given database. Our beyond state-of-the art research will focus on answering the following questions:

1)	How to limit statistical prior knowledge to geometric priors for solving markerless Motion Capture dynamically with sufficient accuracy ?
2)	How to stabilize tracking without introducing a database bias, or to enforce individuality ?
3) 	How to extract (geometric) motion characteristics for individual motion transfer and interpretation ?

Advancing minimal dynamic prior knowledge means to seek for the essence and granularity of priors. This will have a profound impact well beyond computer vision (e.g. for cognitive sciences or robotics). We strongly believe that we have the necessary competence to pursue this project. Preliminary results have been well received by the community","1430000","2011-10-01","2016-09-30"
"Dynamic Nano","Dynamic Nanoplasmonics","Na Liu","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","The key component of nanoplasmonics is metals. When metal nanoparticles are placed in close proximity, the possibility of shaping and controlling near-field and far-field optical properties expands enormously. Near-field coupling between metal nanoparticles is extremely sensitive to nanometer conformational changes. Such strong dependence on conformation provides unique opportunities in manipulating optical response on the nanoscale. Simultaneously, it also raises significant challenges in realization of dynamic plasmonic systems, which can exhibit immediate conformational changes upon a regulated physical or chemical control input.  
Also importantly, plasmonic nanostructures can serve as an efficient far-field to near-field transformer, converting optical radiation into strong localized electromagnetic fields. This unprecedented ability enables probing local dynamic changes on the nanoscale that are extremely crucial in nanocatalysis and phase transitions of nanomaterials, where many unanswered questions abound. 

In my proposal, I would like to develop a new generation of dynamic nanoplasmonic building blocks for biology, chemistry, and materials science. These plasmonic building blocks either can exhibit dynamic structural changes themselves or can be integrated with functional materials, where dynamic events take place. I will utilize both bottom-up and top-down nanotechniques to advance the perspective of plasmonics towards synthetic plasmonic machinery as well as on-chip dynamic plasmonic devices with both tailored optical response and active functionality. With such plasmonic building blocks, long-standing questions in protein dynamics, chiral sensing, dynamic light matter interaction, gas-phase catalysis, and phase transitions on the nanoscale will be addressed. My proposed methods will allow for unprecedented resolution on optically disseminating dynamic behavior and revolutionary multidisciplinary experiments that were not possible to perform before.","1499700","2015-04-01","2020-03-31"
"DYNAMICMARCH","Dynamics of Multiple, Interacting and Concurrent Markov Chains","Thomas Sauerwald","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Markov chains are fundamental processes and have been studied in nearly every scientific discipline in the past 100 years. In computer science, reversible Markov chains, also known as random walks, form the basis of many efficient randomised algorithms which have been successfully applied to a variety of complex sampling, learning and optimisation problems.

Nowadays an increasing number of algorithms and processes on networks are based on multiple (concurrent and possibly dependent) random walks, including algorithms for packet routing, content search, graph clustering, link prediction and website ranking. This trend will be reinforced by the steady growth of large-scale networks and massive data sets. However, the existing theory of single random walks is not well-suited to cope with the complexity inherent to multiple random walks, and even for independent walks many fundamental questions remain open.

The goal of this proposal is to develop a systematic and rigorous study of multiple random walks. First, we will analyse this random process via commonly used metrics such as hitting times, cover times, mixing times as well as new quantities which are unique to multiple random walks. Then we will connect these quantities to structural properties of the underlying graph. Finally, these insights will be applied to the design of new efficient randomised algorithms for large graphs and distributed networks.","1400789","2016-05-01","2021-04-30"
"DYNAMIQS","Relaxation dynamics in closed quantum systems","Marc Cheneau","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Statistical mechanics, a century-old theory, is probably one of the most powerful constructions of physics. It predicts that the equilibrium properties of any system composed of a large number of particles depend only on a handful of macroscopic parameters, no matter how the particles interact with each other. But the question of how many-body systems relax towards such equilibrium states remains largely unsolved. This problem is especially acute for quantum systems, which evolve in a much larger mathematical space than the classical space-time and obey non-local equations of motion. Despite the formidable complexity of quantum dynamics, recent theoretical advances have put forward a very simple picture: the dynamics of closed quantum many-body systems would be essentially local, meaning that it would take a finite time for correlations between two distant regions of space to reach their equilibrium value. This locality would be an emergent collective property, similar to spontaneous symmetry breaking, and have its origin in the propagation of quasiparticle excitations. The fact is, however, that only few observations directly confirm this scenario. In particular, the role played by the dimensionality and the interaction range is largely unknown. The concept of this project is to take advantage of the great versatility offered by ultracold atom systems to investigate experimentally the relaxation dynamics in regimes well beyond the boundaries of our current knowledge. We will focus our attention on two-dimensional systems with both short- and long-range interactions, when all previous experiments were bound to one-dimensional systems. The realisation of the project will hinge on the construction on a new-generation quantum gas microscope experiment for strontium gases. Amongst the innovative techniques that we will implement is the electronic state hybridisation with Rydberg states, called Rydberg dressing.","1500000","2016-05-01","2021-04-30"
"DYNAMO","Dynamics and assemblies of colloidal particles
under Magnetic and Optical forces","Pietro Tierno","UNIVERSITAT DE BARCELONA","Control of microscale matter through selective manipulation of colloidal building blocks will unveil novel scientific and technological avenues expanding current frontiers of knowledge in Soft Matter systems. I propose to combine state-of-the-art micromanipulation techniques based on magnetic and optical forces to transport, probe and assemble colloidal matter with single particle resolution in real time/space and otherwise unreachable capabilities. In the first part of the project, I will use paramagnetic colloids as externally controllable magnetic inclusions to probe the structural and rheological properties of optically assembled colloid crystals and glasses. In the second part, I will realize a new class of anisotropy patchy magnetic colloids, characterized by selective, directional and reversible interactions and employ these remotely addressable units to realize gels and frustrated crystals (static case), active jamming and synchronization via hydrodynamic coupling (dynamic case).

DynaMO project will power a basic experimental research embracing a variety of apparently different systems ranging from deterministic ratchets, viscoelastic crystals, glasses, patchy colloidal gels, frustrated crystals, active jamming, and hydrodynamic waves. The ERC grant will allow me to establish a young and dynamic research group of interdisciplinary nature focused on these issues and aimed at performing high quality research and training/inspiring talented researchers in innovative and challenging scientific projects.","1309320","2014-01-01","2018-12-31"
"DYNAP","Dynamic Penetrating Peptide Adaptamers","Javier Montenegro Garcia","UNIVERSIDAD DE SANTIAGO DE COMPOSTELA","The aim of this proposal is to identify, at the molecular level, the minimal topological and structural motifs that govern the membrane translocation of short peptides. A covalent reversible bond strategy will be developed for the synthesis of self-adaptive penetrating peptides (adaptamers) for targeted delivery.

It is known that the recently developed therapeutic technologies (i.e. gene therapy, chemotherapy, hyperthermia, etc.) cannot reach their expected potential due to limitations in the current delivery strategies, which hinder the efficient targeting of the appropriate tissues, cells and organelles. Despite the enormous therapeutic potential of short penetrating peptides, these molecules suffer from drawbacks such as toxicity, instability to protease digestion and lack of specificity.

Dynamic covalent chemistry has significant synthetic advantages. In the proposed research, peptide scaffolds with clickable reversible groups (e.g. hydrazide) will be conjugated with collections of aldehydes to afford self-adaptive biomimetic transporters, whose secondary structure and penetrating properties will be systematically characterized by biophysical, cell-biology and pattern recognition techniques.

The versatility of dynamic supramolecular “peptide adaptamers” with precisely positioned protein ligands will be explored for multivalent specific recognition, protein transport, cell targeting of drugs and probes and membrane epitoping.

Additionally, we propose to synthesise dynamic and environmentally sensitive fluorescent probes for biocompatible membrane labelling and uptake signalling.

The resulting discoveries of this research will allow the formulation of novel transfecting reagents for gene therapy, selective platforms for drug-delivery and the development of dynamic fluorescent membrane probes. The potential results of this proposal will shake the fields of drug-delivery and non-viral gene transfection and will resolve the limitations of the current approaches.","1492525","2016-02-01","2021-01-31"
"DYNCORSYS","Real-time dynamics of correlated many-body systems","Philipp Werner","UNIVERSITE DE FRIBOURG","""Strongly correlated materials exhibit some of the most remarkable phenonomena found in condensed matter systems. They typically involve many active degrees of freedom (spin, charge, orbital), which leads to numerous competing states and complicated phase diagrams. A new perspective on correlated many-body systems is provided by the nonequilibrium dynamics, which is being explored in transport studies on nanostructures, pump-probe experiments on correlated solids, and in quench experiments on ultra-cold atomic gases.
An advanced theoretical framework for the study of correlated lattice models, which can be adapted to nonequilibrium situations, is dynamical mean field theory (DMFT). One aim of this proposal is to develop """"nonequilibrium DMFT"""" into a powerful tool for the simulation of excitation and relaxation processes in interacting many-body systems. The big challenge in these simulations is the calculation of the real-time evolution of a quantum impurity model. Recently developed real-time impurity solvers have, however, opened the door to a wide range of applications. We will improve the efficiency and flexibility of these methods and develop complementary approaches, which will extend the accessible parameter regimes. This machinery will be used to study correlated lattice models under nonequilibrium conditions. The ultimate goal is to explore and qualitatively understand the nonequilibrium properties of """"real"""" materials with active spin, charge, orbital and lattice degrees of freedom.
The ability to simulate the real-time dynamics of correlated many-body systems will be crucial for the interpretation of experiments and the discovery of correlation effects which manifest themselves only in the form of transient states. A proper understanding of the most basic nonequilibrium phenomena in correlated solids will help guide future experiments and hopefully lead to new technological applications such as ultra-fast switches or storage devices.""","1493178","2012-02-01","2017-01-31"
"DYNRIGDIOPHGEOM","Dynamics of Large Group Actions, Rigidity, and Diophantine Geometry","Oleksandr Gorodnyk","UNIVERSITY OF BRISTOL","In our project we address several fundamental questions regarding ergodic-theoretical properties of actions of large groups. The problems that we plan to tackle are not only of central importance in the abstract theory of dynamical systems, but they also lead to solutions of a number of open questions in Diophantine geometry such as the Batyrev--Manin and Peyre conjectures on the asymptotics and the distribution of rational points on algebraic varieties, a generalisation of the Oppenheim conjecture on distribution of values of polynomial functions, a generalisation of Khinchin and Dirichlet theorems on Diophantine approximation in the setting of homogeneous varieties, and estimates on the number of integral points (with almost prime coordinates satisfying polynomial and congruence equations. The proposed research is expected to imply profound connections between diverse areas of mathematics simultaneously enriching each of them. For instance, we expect to establish a precise relation between the generalised Ramanujan conjecture in the theory of automorphic forms and the order of Diophantine approximation on algebraic varieties. We also plan to use our results on counting lattice points to derive estimates on multiplicities of automorphic representations and prove results in direction of Sarnak&apos;s density hypothesis. We investigate the problem of distribution of orbits, raised by Arnold and Krylov in sixties, the problem of multiple recurrence, pioneered by Furstenberg in seventies, and the problem of rigidity of group actions, formulated by Zimmer in eighties. We plan to compute the asymptotic distribution of orbits for actions on general homogeneous spaces, to establish multiple recurrence for large classes of actions of nonamenable groups, to prove isomorphism and factor rigidity of homogeneous actions and rigidity of actions under perturbations.","630000","2010-02-01","2016-01-31"
"E-CONTROL","""Electric-Field Control of Magnetic Domain Wall Motion and Fast Magnetic Switching: Magnetoelectrics at Micro, Nano, and Atomic Length Scales""","Sebastiaan Van Dijken","AALTO KORKEAKOULUSAATIO SR","""The aim of the proposed research is to study electric-field induced magnetic phenomena in thin-film ferromagnetic-ferroelectric heterostructures. In particular, the project addresses ferroic order competition and magnetoelectric coupling dynamics at micro, nano, and atomic length scales.

The first part of the project focuses on the dynamics of coupled ferromagnetic-ferroelectric domains and electric-field induced magnetic domain wall motion at sub-nanosecond time scales. For simultaneous imaging of both ferroic domain responses to ultra-short electric-field pulses, the construction of a time-resolved polarization microscope is proposed. The second part relates to finite-size scaling of ferroic domain correlations in continuous films and electric-field control of magnetic effects in patterned nanostructures. Here, the aim is to elucidate the competition between magnetoelectric coupling at ferromagnetic-ferroelectric interfaces and the relevant energy scales within the bulk of ferroic materials. Moreover, electric-field induced domain wall motion in magnetic nanowires is pursued as a viable low-power alternative to current-driven spin-torque effects. Finally, the third part of E-CONTROL aims at visualization of magnetoelectric coupling effects with atomic precision. For this frontier study, the development of in situ transmission electron microscopy (TEM) techniques is proposed. The new measurement method enables the application of local electric fields on cross-sectional specimen during TEM analysis and this is bound to provide unique insights in strain-mediated and charge-modulated coupling mechanisms between ferromagnetic and ferroelectric thin films.""","1499465","2012-10-01","2017-09-30"
"E-DNA-T-PEP","Engineering DNA transfer into Cells by Precision in Electroporation","Pouyan Boukany","TECHNISCHE UNIVERSITEIT DELFT","The proposal aims to understand and control the transport of DNA in electroporation process at the molecular/subcellular level such that more efficient and safer non-viral gene delivery can be achieved. The introduction of naked DNA into living cell via non-viral routes is the safest approach in gene therapy. Electroporation is the electrical disruption of biological membranes to introduce naked DNA into the cell. Due to our lack of information about fundamentals of electropores formation and DNA electrotransfer, electroporation methods still suffer from low transfection efficiency, random uptake and excessive cell damage.
The main barriers to achieving this goal are: i) understanding the creation of electropores at molecular level; ii) understanding the underlying mechanism of DNA transport across the membrane of a cell during and after electric pulses and iii) controlling the electrotransfer of DNA through these pores into a cell at molecular level. It is almost impossible to overcome these barriers based on our current rudimentary understanding of cell electroporation.
The successful outcome of this project will significantly aid the development of gene delivery into living cells, which will lead to electroporation-based therapies in the near future.To this end, I will apply a multidisciplinary approach, combining disciplines as physical chemistry, transport phenomena, DNA dynamics, biophysics and cell biology. To unveil the entire electroporation process, innovatively I will employ the integrated atomic force microscopy with micro/nanofluidics to visualize the evolution of pore size/density at the membrane level. Furthermore, to understand the DNA electrotransfer, I will study how DNA interacts with electropores and moves through them using optical tweezers and single-molecule FRET.  Finally, I will dissect the role of cytoskeleton on the transport of DNA, by mapping out the relationship between the viscoelasticity of cell and location of DNA inside the cell.","1481410","2013-10-01","2018-09-30"
"E-GAMES","Surface Self-Assembled Molecular Electronic Devices: Logic Gates, Memories and Sensors","Marta Mas Torrent","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","Organic electronic devices, such as organic field-effect transistors (OFETs), are raising an increasing interest for their potential in large area coverage and low cost applications. Also, the use of single molecules as active electronic components offers great prospects for the miniaturization of devices and for their compatibility with biological systems. Within this framework, e-GAMES goals are:
1) Molecular logic gates for the storage and transmission of magnetic and optical information and for locally controlling surface wettability. The two huge limitations that hinder the application of molecules in logic gates are: i) Fabrication of devices on a solid support, ii) Concatenation of logic gates. I plan to overcome these drawbacks employing self-assembled monolayers of bistable electroactive molecules. These systems could also be used in the fabrication of surfaces with tunable wettability properties, of high interest in microfluidics and for biosensors.
2) Ambipolar organic field-effect transistors with donor-acceptor systems and their exploitation in light, temperature or pressure sensors, and/or memory devices.
Intramolecular electron transfer in organic semiconductors designed for preparing ambipolar OFETs will be explored for the first time. This phenomenon will be exploited for the fabrication of light, pressure or temperature stimuli-responsive OFETs bringing innovative perspectives to the field.
3) Organic/inorganic hybrid devices based on field-effect transistors for sensing environmentally hazardous carbon nanoparticles.
Carbon-based nanoparticles are being increasingly used in many applications despite their recognized toxicity. The grounds for the development of a new generation of nanotechnological low-cost and selective sensors based on transistors functionalized with organic sensing molecular monolayers for the detection of such materials will be developed, contributing towards the improvement of citizens’ safety and environmental preservation.","1499675","2012-12-01","2018-09-30"
"E-MARS","Evolution of Mars","Cathy Monique Quantin","UNIVERSITE LYON 1 CLAUDE BERNARD","The primary questions that drive the Mars exploration program focus on life. Has the Martian climate ever been favorable for life development? Such scenario would imply a distinct planetary system from today with a magnetic flied able to retain the atmosphere. Where is the evidence of such past climate and intern conditions? The clues for answering these questions are locked up in the geologic record of the planet. The volume of data acquired in the past 15 years by the 4 Martian orbiters (ESA and NASA) reach the petaoctet, what is indecent as regard to the size of the Martian community. e-Mars propose to built a science team composed by the PI, Two post-doctorates, one PhD student and one engineer  to exploit the data characterizing the surface of Mars. e-Mars proposes the unprecedented approach to combine topographic data, imagery data in diverse spectral domain and hyperspectral data from multiple orbiter captors to study the evolution of Mars and to propose pertinent landing sites for next missions. e-Mars will focus on three scientific themes: the composition of the Martian crust to constraint the early evolution of the planet, the research of possible habitable places based on evidence of past liquid water activity from both morphological record and hydrated mineral locations, and the study of current climatic and geological processes driven by the CO2 cycle. These scientific themes will be supported by three axis of methodological development: the geodatabase management via Geographic Information Systems (G.I.S.)., the automatic hyperspectral data analysis and the age estimates of planetary surface based on small size crater counts.","1392000","2011-11-01","2017-10-31"
"E-MOBILE","Enhanced Modeling and Optimization of Batteries Incorporating Lithium-ion Elements","Mathieu Maurice Luisier","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","""Developing rechargeable batteries with larger storage capacity, higher output power, faster charge/discharge time, and longer calendar lifetime could significantly impact the economical and environmental future of the European Union. New generations of lithium-ion batteries (LIBs) based on nanostructured electrodes are perfect candidates to supply all-electrical vehicles and favor the usage of renewable energies instead of fossil fuels. Hence, the global LIB revenue is expected to expand from $11 billion in 2011 up to $50 billion in 2020. The goal of this project is therefore to provide an advanced simulation and optimization platform to design LIBs with improved performance and increase the competitiveness of Europe in this domain. The proposed computer aided design (CAD) tool must satisfy three key requirements in order to reach this ambitious objective: (i) computational efficiency, (ii) results accuracy, and (iii) automated predictability. Massively parallel computing has been identified as the enabling technology to handle the first requirement. The second one will be addressed by implementing a state-of-the-art device operation model relying on a multi-scale resolution of the battery electrodes, a detailed description of the electron and ion motions, a material parametrization derived from ab-initio quantum transport techniques, and a validation of the approach through comparisons with experimental measurements. Finally, to meet the last requirement, the operation model will be coupled to a genetic algorithm optimizer capable of automatically predicting the LIB configuration that best matches pre-defined performance targets. The resulting CAD tool will be released as an open source package so that the entire battery community can benefit from it.""","1492800","2013-10-01","2018-09-30"
"e-See","Single electron detection in Transmission Electron Microscopy","Martien DEN HERTOG","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The ultimate goal of device miniaturization is to rely on a single charge provided by a single dopant atom: solotronics. Currently the gate length in a transistor cannot be reduced beyond 10-12 nm, as variability between nominally identical devices reaches unacceptable levels. Elaborate quantum transport experiments can monitor the presence and spin state of a single charge, but do not provide information about location and distribution (wavefunction) of the charge or the local chemical and crystallographic environment. The latter, however, determine why the charge is present at a specific location with a particular distribution. Scanning probe techniques can measure charges but are restricted to the near surface region. In contrast, the phase of an electron in transmission electron microscopy (TEM) can probe the sample volume and is sensitive to charge. The target of the e-See project is the first real time observation of the wavefunction associated to a single electron charge in the volume of a device with atomic resolution. I aim to implement low temperature quantum transport experiments in a TEM to allow simultaneous electrical manipulation of this charge. Combined visualization and manipulation of a single charge trapped by Coulomb blockade in a transistor will (i) identify the origins of device variability, and (ii) show how the local properties of the sample affect localization of a single charge and its wavefunction. The project impact involves understanding of variability, improving device design and creation of a new research field on low temperature electrical in situ TEM experiments. It will provide the tool to visualize a single charge wavefunction in any device, enabling ultimate device engineering: deterministic 3D atomic scale control of the position of charge localization. To this end, I will use electron holography and scanning TEM, develop a low temperature electrical TEM sample holder, and novel sample preparation.","1998958","2018-10-01","2023-09-30"
"E3ARTHS","Exoplanets and Early Earth Atmospheric Research: THeories and Simulations","Franck Selsis","UNIVERSITE DE BORDEAUX","This program is dedicated to the simulation and characterization of Extrasolar Terrestrial Planet (ETP) atmospheres. Thanks to new generation codes, the team E3ARTHS aims to provide a top expertise in a key domain of astrobiology: the origin, evolution and identification of habitable worlds, and the quest for biomarkers on Earth-like planets. The team will also revisit early Earth models for a better understanding of the context of the origins of life, in the light of recent works on Earth formation, impact history and Solar evolution. The observable signatures of an ETP and its ability to sustain life are determined by atmospheric properties: chemistry, radiative transfer, climate. Although these processes are usually treated separately, they evolve in a tightly coupled scheme under the influence of astrophysical, geophysical and, if present, biological mechanisms. Eventually, realistic planetary environments will thus have to be modeled with self-consistent 3D tools, involving a multidisciplinary and international approach. Although ambitious by today's standards, such enterprise is a necessary counterpart of the planned ETP searches, and is required to study the discovered planets. Observatories like Darwin/TPF and ELTs will provide direct information on ETPs within 10-15 years. Ongoing transit searches (CoRoT, and Kepler), and radial-velocity surveys, are on the verge of detecting ETPs. In this context, E3ARTHS can become one of the cores in European theoretical research on ETPs, in close interaction with observation programs. Since his PhD, F. Selsis has developed his own research on ETPs, which already had important implications for the design of instruments for TEP search and characterization. His plan is now to take this research at the next level by creating a dedicated team that will integrate new tools such as 3D climate, photochemical and radiative transfer codes, produce virtual observations of ETPs, and study their potential for life.","719759","2008-10-01","2013-09-30"
"EARLY EARTH","Early Earth evolution: chemical differentiation vs. mantle mixing","Maud Boyet","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Although short-lived chronometers have yielded a precise chronology of the Early Earth differentiation, there is insufficient data available on the chemical fractionation related to these processes to model the Early Earth’s differentiation. 142Nd isotope data suggest that a reservoir enriched in rare earth elements (REE) has existed since 4.53 Ga, but has not been sampled since its formation. A key question is whether such a reservoir could remain hidden for more than 4.5 Gyr in the convective mantle. The first goal of this project is to test whether the REE alternatively could be stored in the core. Information on the mantle composition and the extent of chemical differentiation in the Early Earth will be also obtained by measurement of Sm-Nd, Pt-Re-Os and Lu-Hf radiogenic systems of Archean samples. This work will provide valuable information on (1) the redox state of the Early Earth, (2) the nature of the precursor material forming the Earth, the chronology of Earth's differentiation relative to the Moon formation, and (4) for reconstructing a model for terrestrial magma ocean crystallization. This proposal will provide the possibility of tackling a topic from a number of angles, using new instrumentation. New approaches and collaborations will be combined in order to constrain the most realistic model of the early Earth evolution.","453286","2008-08-01","2012-11-30"
"EARLY EARTH","Early Earth Dynamics: Pt-Re-Os isotopic constraints on Hadean-Early Archean mantle evolution","Ambre Luguet","RHEINISCHE FRIEDRICH-WILHELMS-UNIVERSITAT BONN","This project aims to directly constrain the melting history and composition of the mantle of the Earth
for the first 750 Ma of its history. So far, our limited knowledge hinges on isolated detrital zircons from
Archean crustal rocks. They indicate crustal extraction as early as 4.4 Ga with peaks at 4.0 and 4.3 Ga but
reveal conflicting models for the composition of the Hadean mantle. Both the timing and extent of these
early crust formation events and the composition of the Hadean mantle have crucial implications for our
understanding of the Early Earth’s chemical evolution and dynamics as well as crustal growth and thermal
cooling models. Sulfides (BMS) and platinum group minerals (PGM) may hold the key to these fundamental
issues, as they are robust time capsules able to preserve the melting record of their mantle source over
several billion years.
I propose to perform state-of-the-art in-situ Pt-Re-Os isotopic measurements on an extensive
collection of micrometric BMS and PGM from Archean cratonic peridotites and chromite deposits, and
paleoplacers in Archean sedimentary basins. For the first time, < 20 μm minerals will be investigated for Pt-
Re-Os. The challenging but high-resolution micro-drilling technique will be developed for in-situ sampling
of the PGM and BMS with subsequent high-precision 187Os-186Os isotopic measurements by NTIMS. This
highly innovative project will be the first to constrain Hadean Earth history from the perspective of the
Earth’s mantle. By opening a new window towards high-precision geochemical exploration for micrometric
minerals, this project will have long-term implications for the understanding of the micro to nano-scale
heterogeneity of isotopic signatures in the Earth’s mantle and in extra-terrestrial materials.","1306743","2010-10-01","2016-09-30"
"EARLYEARTH","Accretion and Differentiation of Terrestrial Planets","Maria Schoenbaechler","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","This proposal aims to constrain the late accretion history of the Earth and the differentiation of the earliest silicate reservoirs in planets. Highly siderophile elements (HSE) constrain the late accretion of material onto the Earth; a process that potentially delivered water to Earth. During core formation, HSE strongly partition into metal. Once core formation ceases, newly accreted HSE-rich material will significantly contribute to the HSE budget of the Earth’s mantle. The HSE are more abundant in the Earth’s mantle than predicted from low temperature partitioning experiments and feature nearly chondritic relative abundances. This implies a significant late accretion of chondritic material (“the late veneer”). This idea is challenged by high pressure/temperature experiments indicating that the HSE were left in the behind in the mantle during core formation, thereby calling into question the late veneer. To address this issue, I propose the setup of new isotopic tracers and utilize (i) nucleosynthetic anomalies and (ii) stable isotope systematics of the HSE to determine the origin of HSE in the Earth’s mantle. Unravelling this issue is a major advance in understanding planetary accretion. Formation of the earliest silicate reservoirs probably occurred contemporary to late accretion. Global differentiation in terrestrial silicate reservoirs may have taken place within the first 30 million years of the Earth’s formation based on Sm-Nd isotope data. This timing has been debated on various grounds. The 92Nb-92Zr decay system is a potentially powerful chronometer to further constrain this issue. Its usefulness, however, has been hindered by uncertainties of the initial 92Nb abundance in the solar system. I propose to obtain unequivocal evidence from old differentiated meteorites to settle this debate. The results will have implications for understanding early silicate differentiation on asteroids and - depending on the initial 92Nb abundance - the Earth and Mars.","1994545","2012-04-01","2017-12-31"
"EARTH CORE STRUCTURE","Thermal and compositional state of the Earth's inner core from seismic free oscillations","Arwen Fedora Deuss","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The core, comprising the innermost parts of the Earth, is one of the most dynamic regions of our planet. The inner core is solid, surrounded by a liquid iron alloy. Inner core solidification combined with motions in the fluid outer core drive the geodynamo which generates Earth's magnetic field. Solidification of the inner core also supplies some of the heat that drives mantle convection and subsequently plate tectonics at the surface of the Earth. The thermal and compositional structure of the inner core is thus key to understanding the inner workings of our planet. No direct samples can be taken of the core and our knowledge of the thermal and compositional state of the Earth's outer and inner core relies on seismology. Ray theoretical studies using short period body waves are the most commonly used seismological data; these have led to observations of a large range of anomalous structures in the Earth's inner core, including anistropy, layers and hemispherical variations. However, due to uneven station and earthquake distribution, the robustness and global distribution of these features is still controversial. Long period seismic free oscillations, on the other hand, are able to provide global constraints, but lack of appropriate theory has prevented more complicated structures from being studied using normal modes. Thus, many fundamental questions regarding the thermal history of the core and geodynamo remain unanswered. Here, I propose to develop a comprehensive seismic inner core model, employing fully-coupled normal mode theory for the first time and using data from large earthquakes such as the Sumatra-Andaman event of 26 December 2006. This will dramatically change our current ideas of structure in the inner core. Using a novel combination of fluid dynamics and mineral physics I will interpret the thermal and compositional structure found at the centre of our planet, which in turn are fundamental to understand its geodynamo and magnetic field.","1202744","2008-10-01","2014-09-30"
"EARTHBLOOM","Earth’s first biological bloom: An integrated field, geochemical, and geobiological examination of the origins of photosynthesis and carbonate production 3 billion years ago","Stefan Victor LALONDE","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The origin of oxygenic photosynthesis is one of the most dramatic evolutionary events that the Earth has ever experienced. At some point in Earth’s first two billion years, primitive bacteria acquired the ability to harness sunlight, oxidize water, release O2, and transform CO2 to organic carbon, and all with unprecedented efficiency. Today, oxygenic photosynthesis accounts for nearly all of the biomass on the planet, and exerts significant control over the carbon cycle. Since 2 billion years ago (Ga), it has regulated the climate of our planet, ensuring liquid water at the surface and enough oxygen to support complex life. The biological and geological consequences of oxygenic photosynthesis are so great that they effectively underpin what we think of as a habitable planet. Understanding the origins of photosynthesis is a paramount scientific challenge at the heart of some of humanity’s greatest questions: how did life evolve? how did Earth become a habitable planet? EARTHBLOOM addresses these questions head-on through the first comprehensive scientific study of Earth’s first blooming photosynthetic ecosystem, preserved as Earth’s oldest carbonate platform. This relatively unknown, >450m thick deposit, comprised largely of 2.9 Ga fossil photosynthetic structures (stromatolites), is one of the most important early Earth fossil localities ever identified, and EARTHBLOOM is carefully positioned for major discovery. EARTHBLOOM will push the frontier of field data collection and sample screening using new XRF methods for carbonate analysis. EARTHBLOOM will also push the analytical frontier in the lab by applying the most sensitive metal stable isotope tracers for O2 at ultra-low levels (Mo, U, and Ce) coupled with novel isotopic “age of oxidation” constraints. By providing new constraints on atmospheric CO2, ocean pH, oxygen production, and nutrient availability, EARTHBLOOM is poised to redefine Earth’s surface environment at the dawn of photosynthetic life.","1848685","2017-02-01","2022-01-31"
"EBDD","Beyond structure: integrated computational and experimental approach to Ensemble-Based Drug Design","Julien Michel","THE UNIVERSITY OF EDINBURGH","""Although protein dynamics plays an essential role in function, it is rarely considered explicitly in current structure-based approaches to drug design. Here I propose the computer-aided design of ligands by modulation of protein dynamics, or equivalently, protein structural ensembles. The detailed understanding of ligand-induced perturbations of protein dynamics that will result from this study is crucial not just to accurately predicting binding affinities and tackling """"undruggable"""" targets, but also to understanding protein allostery.

Three major aims will be pursued during this project.

First, I will combine concepts from chemoinformatics and non-equilibrium thermodynamics to detect  cryptic """"druggable"""" small molecule binding sites in computed structural ensembles. New computational methods will be developed to predict how binding at these putative sites is likely to influence protein function. This will enable rational approaches to allosteric control of protein function.

Second, new classes of non-equilibrium sampling algorithms will be developed to improve by 2-3 orders of magnitude the speed of computation of protein/ligand structural ensembles by molecular simulations. This will enable routine consideration of protein flexibility in ligand optimisation problems.

Third, I will address with the above methods a frontier problem in molecular recognition: the rational design of protein isoform-specific ligands. To achieve this goal, I will integrate computation with experiments and focus efforts on the therapeutically relevant cyclophilin protein family. Experimental work will involve the use of purchased or custom-synthesized competitive and allosteric ligands in enzymatic assays, calorimetry and crystal structure analyses.

Overall, this project proposes fundamental advances in our ability to quantify and engineer protein-ligand interactions, therefore expanding opportunities for the development of future small molecule therapeutics.""","1382202","2013-12-01","2018-11-30"
"EC","Extremal Combinatorics","Peter Keevash","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Combinatorics forms a challenging and fundamental part of pure mathematics, but is in the happy position of being relatively accessible to a wider audience. One of its most exciting and rapidly developing branches is Extremal Combinatorics, which has a wide range of direct applications both to other areas of mathematics and other academic disciplines. Thus it makes its influence felt indirectly when the theoretical power it brings to these disciplines is in turn used for more practical applications. The proposed project addresses a range of important problems at the frontier of Extremal Combinatorics, principally those motivated by a question of Turan, an open problem that mathematicians have battled with for over sixty years, which has led to many developments in the theory of graphs and hypergraphs. Recently there has been a lot of progress in this area, so it is an exciting topic for future research. The PI has identified some key intermediate goals to pursue for this first objective, and also for a second objective involving various ways to extend the scope of this area, including a rainbow variant that has impressive potential applications in additive number theory. A third area being studied is the theory of set systems with restricted intersections, which has a rich history in combinatorics, and has also found applications to computer science, particular in the theories of complexity and communication. It is also closely connected to the concepts of trace and VC-dimension, which play a central role in many areas of statistics, discrete and computational geometry and learning theory. The PI will co-ordinate a research team of two postdocs and one doctoral student with clearly defined goals that will bring this project to fruition over a five-year period.","780000","2010-01-01","2015-12-31"
"EC","Extremal Combinatorics","Oleg Pikhurko","THE UNIVERSITY OF WARWICK","A typical problem of Extremal Combinatorics is to maximise or minimise a certain parameter given some combinatorial restrictions. This area experienced a remarkable growth in the last few decades, having a wide range of applications that include results in number theory, algebra, geometry, logic, information theory, and theoretical computer science. There are also many practical fields that were greatly influenced by ideas from Extremal Combinatorics such as, for example, analysis of large networks, ranking of web-pages, or shotgun cloning of DNA fragments.

The Principal Investigator (PI for short) will work on a number of extremal problems, with the main directions being the Tur\'an function (maximising the size of a hypergraph without some fixed forbidden subgraphs), the Rademacher-Tur\'an problem (minimising the density of F-subgraphs given the edge density), and Ramsey numbers (quantitative bounds on the maximum size of a monochromatic substructure that exists for every colouring). These are fundamental and general questions that go back at least as far as the 1940s but remain wide open despite decades of active attempts. During attacks on these notoriously difficult problems, mathematicians developed a number of powerful general methods. PI will work on extending and sharpening these techniques as well as on finding ways of applying the recently introduced concepts of (hyper)graph limits and flag algebras to concrete extremal problems. Since these concepts deal with some approximation to the studied problem, one important aspect of the project is to develop methods for obtaining exact results from asymptotic calculations (for example, via the stability approach).

The support by means of a 5-year research grant will enable PI to consolidate his research and build a group in Extremal Combinatorics.","1129919","2012-10-01","2018-07-31"
"ECAP","Efficient Cryptographic Arguments and Proofs","Jens Groth","UNIVERSITY COLLEGE LONDON","""Privacy and verifiability are fundamental security goals that often conflict with each other. In elections we want to verify that the final tally is correct without violating the voters’ privacy; companies are audited but do not want financial statements to disclose the details of their business strategies; people identifying themselves do not want their personal information to be abused in identity theft, etc.

Zero-knowledge proofs allow the verification of facts with minimal privacy loss. More precisely, a zero-knowledge proof is a protocol that allows a prover to convince a verifier about the truth of a statement in a manner that does not disclose any other information. The ability to combine verification and privacy makes zero-knowledge proofs extremely useful; they are used in numerous cryptographic protocols.

The purpose of this proposal is to establish a research group dedicated to the study of zero-knowledge proofs. A main focus of the group will be to improve efficiency. Zero-knowledge proofs can be very complex and in many security applications the zero-knowledge proofs are the main performance bottleneck. This leads to a significant cost in terms of time and money; or if the cost is too high it may force users to use insecure schemes without zero-knowledge proofs.

Our vision will be to reduce the cost of zero-knowledge proofs so much that instead of being expensive protocols components they become so cheap that their cost is insignificant compared to other protocol components. This will make existing cryptographic protocols that rely on zero-knowledge proofs faster and also broaden the range of security applications where zero-knowledge proofs can be used.""","1346074","2012-10-01","2017-09-30"
"ECHO","Practical Imaging and Inversion of Transient Light Transport","Matthias HULLIN","RHEINISCHE FRIEDRICH-WILHELMS-UNIVERSITAT BONN","The automated analysis of visual data is a key enabler for industrial and consumer technologies and of immense economic
and social importance. Its main challenge is in the inherent ambiguity of images due to the very mechanism of image
capture: light reaching a pixel on different paths or at different times is mixed irreversibly. Consequently, even after
decades of extensive research, problems like deblurring or descattering, geometry/material estimation or motion tracking
are still largely unsolved and will remain so in the foreseeable future.
Transient imaging (TI) tackles this problem by recording ultrafast optical echoes that unmix light contributions by the total
pathlength. So far, TI used to require high-end measurement setups. By introducing computational TI (CTI), we paved the
way for a lightweight capture of transient data using consumer hardware. We showed the potential of CTI in scenarios like
robust range measurement, descattering and imaging of objects outside the line of sight – tasks that had been considered
difficult to impossible so far.
The ECHO project is rooted in computer graphics and computational imaging. In it, we will overcome the practical limitations that are hampering a large-scale deployment of TI: the time required for data capture and to reconstruct the
desired information, both in the order of seconds to minutes, a lack of dedicated image priors and of quality guarantees for
the reconstruction, the limited accuracy and performance of forward models and the lack of ground-truth data and
benchmark methods.
Over the course of ECHO, we will pioneer advanced capture setups and strategies, signal formation models, priors and numerical
methods, for the first time enabling real-time reconstruction and analysis of transient light transport in complex and dynamic
scenes. The methodology developed in this far-reaching project will turn TI from a research technology into a family of
practical tools that will immediately benefit many applications.","1525840","2018-12-01","2023-11-30"
"ECNANO","Electrochemistry in fluidic nanodevices: From fundamentals to integrated sensor platforms","Serge Joseph Guy Lemay","UNIVERSITEIT TWENTE","I propose to explore the frontiers of electrochemistry at the nanometer scale by developing new experimental approaches based on lithographically fabricated fluidic nanodevices. This will allow groundbreaking experiments on a broad range of fundamental topics including double layer structure, screening in ionic liquids, nanoscale hydrodynamics and the dielectric response of single macromolecules. It will also lay the foundations for new analytical techniques based on electrochemical single molecule recognition and targeted at integration with state-of-the-art electronics on a single chip. The latter combination could potentially bring about a revolution in (bio)sensing technology on a scale comparable to those which have already taken place in computing and communications. My first focus will be on nanofabricating sub-femtolitre channels and chambers in which single or small numbers of redox-active molecules can be detected and manipulated using electrochemistry at pairs of embedded electrodes. Simultaneously, I will explore the capabilities electrochemical impedance spectroscopy using nanoelectrodes at frequencies up to 200 MHz. Such a combination of ultra-short length scales and high frequencies has heretofore remained inaccessible and will be made possible here by using electrodes that form an intrinsic part of an integrated detection circuit. This research has a truly exploratory character, as few investigators so far have attempted to combine nanofluidics, modern microelectronics and electrochemistry. Doing so will test our microscopic understanding of electrochemical processes, enable new classes of experiments, and push the limits of electrochemistry as an analytical method. There is thus a high likelihood that further new concepts and applications will emerge over the course of this multidisciplinary program.","1500000","2011-09-01","2016-08-31"
"ECOMAGICS","Electric Control of Magnetization Dynamics","Georg Woltersdorf","MARTIN-LUTHER-UNIVERSITAET HALLE-WITTENBERG","In this proposal a new electric field based approach for the control of magnetization dynamics is discussed. The advantage of using electric fields compared to magnetic fields is twofold: (i) electric fields are easy to confine in nano-structures (screening), and (ii) no current flow is required which may allow for the development of new spintronic devices with ultra low power consumption.
Physically the application of an electric field to an ultrathin ferromagnetic material gives rise to modification of the wave-function overlap at the interface between a ferromagnetic metal and a dielectric. This electronic tuning causes a modified occupation of the d-orbitals at the interface and leads to electrically induced anisotropies. Hence external electric fields generate internal magnetic fields. Due to the modified orbital moment also a large voltage induced effect on the Gilbert damping is expected in magnetization dynamic experiments.  In principle these fields can be applied even on ultrafast time scales. This will be explored when rf-electric fields are used to drive internal magnetic fields in the GHz frequency range to generate spin-waves.  Furthermore this technique will be used to excite monochromatic spin-waves with wave-vectors well in the exchange dominated regime in order to study their propagation properties.
I propose to use the spin-wave Doppler effect in order to break the intrinsic mirror symmetry required in for four-magnon scattering processes. In this way the resonance saturation may be tuned electrically to much larger values. Moreover electrically driven surface acoustic waves will be used to generate spin-waves which will be manipulated by an electric current using the spin-wave Doppler effect.

The research described in the proposal is likely to have a large impact as a shift to electric field controlled spintronic devices is favorable on small length scales.  In addition the power consumption of these devices may be reduced significantly.","1495860","2012-01-01","2017-10-31"
"EDECS","Exploring Dark Energy through Cosmic Structures: Observational Consequences of Dark Energy Clustering","Pier Stefano Corasaniti","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Understanding the nature of Dark Energy (DE) in the Universe is the central challenge of modern cosmology. Einstein’s Cosmological Constant (Λ) provides the simplest explanation fitting the available cosmological data thus far. However, its unnaturally tuned value indicates that other hypothesis must be explored. Furthermore, current observations do not by any means rule out alternative models in favor of the simplest “concordance” ΛCDM. In the absence of theoretical prejudice, observational tests have mainly focused on the DE equation of state. However, the detection of the inhomogeneous nature of DE will provide smoking-gun evidence that DE is dynamical, ruling out Λ. This key aspect has been mostly overlooked so far, particularly in the optimization design of the next generation of surveys dedicated to DE searches which will map the distribution of matter in the Universe with unprecedented accuracy. The success of these observations relies upon the ability to model the non-linear gravitational processes which affect the collapse of Dark Matter (DM) at small and intermediate scales. Therefore, it is of the highest importance to investigate the role of DE inhomogeneities throughout the non-linear evolution of cosmic structure formation. To achieve this, we will use specifically designed high-resolution numerical simulations and analytical methods to study the non-linear regime in different DE models. The hypothesis to be tested is whether the intrinsic clustering of DE can alter the predictions of the standard ΛCDM model. We will investigate the observational consequences on the DM density field and the properties of DM halos. The results will have a profound impact in the quest for DE and reveal new observable imprints on the distribution of cosmic structures, whose detection may disclose the ultimate origin of the DE phenomenon.","1468800","2012-04-01","2017-08-31"
"EDEQS","ENTANGLING AND DISENTANGLING EXTENDED QUANTUM SYSTEMS IN AND OUT OF EQUILIBRIUM","Pasquale Calabrese","SCUOLA INTERNAZIONALE SUPERIORE DI STUDI AVANZATI DI TRIESTE","""It is nowadays well established that many-body quantum systems in one and two spatial dimensions exhibit unconventional collective behavior that gives rise to intriguing novel states of matter. Examples are topological states exhibiting nonabelian statistics in 2D and spin-charge separated metals and Mott insulators in 1D. An important focus of current research is to characterize both equilibrium and non-equilibrium dynamics of such systems. The latter has become experimentally accessible only during the last decade and constitutes one of the main frontiers of modern theoretical physics. In recent years it has become clear that entanglement is a useful concept for characterizing different states of matter as well as non-equilibrium time evolution.

One main aim of this proposal is to utilize entanglement measures to fully classify states of matter in low dimensional systems. This will be achieved by carrying out a systematic study of the entanglement of several disconnected regions in 1D quantum critical systems. In addition, entanglement measures will be used to benchmark the performance of numerical algorithms based on tensor network states (both in 1D and 2D) and identify the """"optimal"""" algorithm for finding the ground state of a given strongly correlated many-body system.

The second main aim of this proposal is to utilize the entanglement to identify the most important features of the the non equilibrium time evolution after a """"quantum quench"""", with a view to solve exactly the quench dynamics in strongly interacting integrable models. A particular question we will address is which observables """"thermalize"""", which is an issue of tremendous current experimental and theoretical interest. By combining analytic and numerical techniques we will then study the non equilibrium dynamics of non integrable models, in order to quantify the effects of integrability.""","1108000","2011-09-01","2016-08-31"
"EffectiveTG","Effective Methods in Tame Geometry and Applications in Arithmetic and Dynamics","Gal BINYAMINI","WEIZMANN INSTITUTE OF SCIENCE LTD","Tame geometry studies structures in which every definable set has a
finite geometric complexity. The study of tame geometry spans several
interrelated mathematical fields, including semialgebraic,
subanalytic, and o-minimal geometry. The past decade has seen the
emergence of a spectacular link between tame geometry and arithmetic
following the discovery of the fundamental Pila-Wilkie counting
theorem and its applications in unlikely diophantine
intersections. The P-W theorem itself relies crucially on the
Yomdin-Gromov theorem, a classical result of tame geometry with
fundamental applications in smooth dynamics.

It is natural to ask whether the complexity of a tame set can be
estimated effectively in terms of the defining formulas. While a large
body of work is devoted to answering such questions in the
semialgebraic case, surprisingly little is known concerning more
general tame structures - specifically those needed in recent
applications to arithmetic. The nature of the link between tame
geometry and arithmetic is such that any progress toward effectivizing
the theory of tame structures will likely lead to effective results
in the domain of unlikely intersections. Similarly, a more effective
version of the Yomdin-Gromov theorem is known to imply important
consequences in smooth dynamics.

The proposed research will approach effectivity in tame geometry from
a fundamentally new direction, bringing to bear methods from the
theory of differential equations which have until recently never been
used in this context. Toward this end, our key goals will be to gain
insight into the differential algebraic and complex analytic structure
of tame sets; and to apply this insight in combination with results
from the theory of differential equations to effectivize key results
in tame geometry and its applications to arithmetic and dynamics. I
believe that my preliminary work in this direction amply demonstrates
the feasibility and potential of this approach.","1155027","2018-09-01","2023-08-31"
"EFMA","Equidistribution, fractal measures and arithmetic","Peter Pal VARJU","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The subject of this proposal lies at the crossroads of analysis, additive combinatorics, number theory and fractal geometry exploring equidistribution phenomena for random walks on groups and group actions and regularity properties of self-similar, self-affine and Furstenberg boundary measures and other kinds of stationary measures. Many of the problems I will study in this project are deeply linked with problems in number theory, such as bounds for the separation between algebraic numbers, Lehmer's conjecture and irreducibility of polynomials.

The central aim of the project is to gain insight into and eventually resolve problems in several main directions including the following. I will address the main challenges that remain in our understanding of the spectral gap of averaging operators on finite groups and Lie groups and I will study the applications of such estimates. I will build on the dramatic recent progress on a problem of Erdos from 1939 regarding Bernoulli convolutions. I will also investigate other families of fractal measures. I will examine the arithmetic properties (such as irreducibility and their Galois groups) of generic polynomials with bounded coefficients and in other related families of polynomials.

While these lines of research may seem unrelated, both the problems and the methods I propose to study them are deeply connected.","1334109","2018-10-01","2023-09-30"
"EGGS","The first Galaxies","Johan Peter Uldall Fynbo","KOBENHAVNS UNIVERSITET","The goal of this project is to discover the first galaxies that formed after the Big Bang. The astrophysics of galaxy formation is deeply fascinating. From tiny density fluctuations of quantum mechanical nature believed to have formed during an inflationary period a tiny fraction of a second after the Big Bang during structure slowly formed through gravitational collapse. This process is strongly dependent on the nature of the dominant, but unknown form of matter - the dark matter. In the project proposed here I will study the epoch of first galaxy formation and the subsequent few billion years of cosmic evolution using gamma-ray bursts and Lyman-α (Lyα) emitting galaxies as probes. I am the principal investigator on two observational projects utilizing these probes. In the first project, I will over three years starting October 2009 be using the new X-shooter spectrograph on the European Southern Observatory Very Large Telescope to build a sample of ~100 gamma-ray bursts with UV/optical/near-IR spectroscopic follow-up. The objective of this project is to measure primarily metallicities, molecular content, and dust content of the gamma-ray burst host galaxies. I am primarily interested in the redshift range from 9 to 2 corresponding to about 500 million years to 3 billions years after the Big Bang. In the 2nd project we will use the new European Southern Observatory survey telescope VISTA. I am co-PI of the Ultra-VISTA project that over the next 5 years starting December 2009 will create an ultradeep image (about 2000 hr of total integration time) of a piece of sky known as the COSMOS field. I am responsible for the part of the project that will use a narrow-band filter to search for Lyα emitting galaxies at a redshift of 8.8 (corresponding to about 500 million years after the Big Bang) - believed to correspond to the epoch of formation of some of the very first galaxies.","1002000","2011-11-01","2016-10-31"
"EIGER","Exploring the Inception of Galaxies and the Epoch of Reionization","Ross James Mclure","THE UNIVERSITY OF EDINBURGH","Studying the nature of the first generation of galaxies to form in the Universe is central to efforts to understand the earliest phases of galaxy evolution and the physical  processes driving cosmic reionization. Building on my recent success investigating galaxy evolution at redshifts z>6,  I propose to recruit and lead the research team necessary to fully exploit my involvement in two Hubble Space Telescope (HST) imaging programmes focused on the high-redshift Universe. The first of these is a new, ultra-deep, proprietary imaging programme in the Hubble Ultra-Deep Field (on which I am co-PI) which will deliver the deepest near-IR image ever obtained and the first robust sample of z>9 galaxies. This dataset will produce the definitive measurement of the faint-end of the high-redshift galaxy luminosity function in the pre-JWST era, a key observational
constraint  necessary for understanding reionization. The second HST programme is the on-going, wide-area, CANDELS imaging survey, which will provide the first statistically significant sample of massive galaxies at redshifts 6<z<8, many of which will be suitable for spectroscopic follow-up.  Consequently, I intend to assemble a team with the necessary skills to take full advantage of my leading position in these two key imaging datasets and to exploit opportunities for spectroscopic follow-up with the next generation of multi-object optical/near-IR spectrographs. Finally, I also propose to recruit the necessary expertise to accurately interpret the new observational results within the context of the latest spectral synthesis and galaxy formation models. In summary, the aim of this proposal is to build a research team  with the interdisciplinary skills necessary to successfully exploit the latest observational datasets, interpret them within the context of the latest theoretical predictions, and  thereby attempt to construct a fully consistent framework describing high-redshift galaxy evolution.","1176273","2012-12-01","2016-11-30"
"ELASTIC-TURBULENCE","Purely-elastic flow instabilities and transition to elastic turbulence in microscale flows of complex fluids","Manuel António Moreira Alves","UNIVERSIDADE DO PORTO","Flows of complex fluids, such as many biological fluids and most synthetic fluids, are common in our daily life and are very important from an industrial perspective. Because of their inherent nonlinearity, the flow of complex viscoelastic fluids often leads to counterintuitive and complex behaviour and, above critical conditions, can prompt flow instabilities even under low Reynolds number conditions which are entirely absent in the corresponding Newtonian fluid flows.
The primary goal of this project is to substantially expand the frontiers of our current knowledge regarding the mechanisms that lead to the development of such purely-elastic flow instabilities, and ultimately to understand the transition to so-called “elastic turbulence”, a turbulent-like phenomenon which can arise even under inertialess flow conditions. This is an extremely challenging problem, and to significantly advance our knowledge in such important flows these instabilities will be investigated in a combined manner encompassing experiments, theory and numerical simulations. Such a holistic approach will enable us to understand the underlying mechanisms of those instabilities and to develop accurate criteria for their prediction far in advance of what we could achieve with either approach separately. A deep understanding of the mechanisms generating elastic instabilities and subsequent transition to elastic turbulence is crucial from a fundamental point of view and for many important practical applications involving engineered complex fluids, such as the design of microfluidic mixers for efficient operation under inertialess flow conditions, or the development of highly efficient micron-sized energy management and mass transfer systems.
This research proposal will create a solid basis for the establishment of an internationally-leading research group led by the PI studying flow instabilities and elastic turbulence in complex fluid flows.","994110","2012-10-01","2018-01-31"
"ELDORADO","Electrophilicity-Lifting Directed by Organochalcogen Redox-Auxiliaries and Diversiform Organocatalysis","Alexander BREDER","UNIVERSITAET REGENSBURG","The implementation of viable practices for the ecologically cognizant production and consumption of energy and renewable resources rank among the most pressing societal challenges of the 21st century. Against this background, the design and development of innovative concepts for the sustainable use of energy and energy-rich compounds from regenerative sources becomes a matter of profound technological and scientific pertinence. A promising approach that has been put forward in the context of chemical synthesis is the application of visible light as an inexpensive source of energy and air as an abundant and gratuitous oxidant for the derivatization of certain hydrocarbons. Despite the enormous economic and ecological benefits associated with the use of light and air as integral components of redox reactions, the realization of such processes is strikingly limited to very isolated applications. Consequently, this methodological deficit represents a momentous opportunity for modern chemical sciences to lastingly transform the routine lines of action for the oxidative manipulation of organic molecules. A key issue that needs to be taken into consideration for the design of efficient light-driven aerobic oxidation protocols is the identification of proper catalyst systems that allow for the site- and chemoselective activation of individual bonds within polyatomic frameworks. In this regard, the prime objective of the proposed research program is the rational design of non-metallic and in part cooperative catalysis regimes as enabling technologies for the electrophilic activation of non-aromatic carbon–carbon multiple- and carbon-chalcogen single bonds to facilitate a wide and diverse array of heretofore unprecedented oxidative coupling-, addition-, and rearrangement reactions. To demonstrate its utility in a superordinate context, this methodological concept will be applied in highly modular enantioselective syntheses of biologically relevant polyketide natural products.","1499954","2019-08-01","2024-07-31"
"ELECTRIC","Chip Scale Electrically Powered Optical Frequency Combs","Bart Johan KUYKEN","UNIVERSITEIT GENT","In ELECTRIC, I will integrate electrically powered optical frequency combs on mass manufacturable silicon chips. This will allow for making use of all the advantageous properties of these light sources in real-life situations.
Optical frequency combs are light sources with a spectrum consisting of millions of laser lines, equally spaced in frequency. This equifrequency spacing provides a link between the radio frequency band and the optical frequency band of the electromagnetic spectrum. This property has literally revolutionized the field of frequency metrology and precision laser spectroscopy. Recently, their application field has been extended. Amongst others, their unique properties have been exploited in precision distant measurement experiments  as well as optical waveform and microwave synthesis demonstrators. Moreover, so called “dual-comb spectroscopy” experiments have demonstrated broadband Fourier Transform Infrared  spectroscopy with ultra-high resolution and record acquisition speeds. However, most of these demonstrations required large bulky experimental setups which hampers wide deployment.
 
I will build frequency combs on optical chips that can be mass-manufactured. Unlike the current chip scale Kerr comb based solutions they do not need to be optically pumped with a powerful continuous wave laser and can have a narrower comb spacing. The challenge here is two-fold. First, we need to make electrically powered integrated low noise oscillators. Second, we need to lower the threshold of current on-chip nonlinear optical interactions by an order of magnitude to use them in on-chip OFC generators. 

Specifically I will achieve this goal by: 
•	Making use of ultra-efficient nonlinear optical interactions based on soliton compression in dispersion engineered III-V waveguides and plasmonic enhanced second order nonlinear materials. 
•	Enhance the performance of ultra-low noise silicon nitride mode locked lasers with these nonlinear components.","1391250","2018-02-01","2023-01-31"
"ELECTROCHEMBOTS","MAGNETOELECTRIC CHEMONANOROBOTICS FOR CHEMICAL AND BIOMEDICAL APPLICATIONS","Salvador Pané Vidal","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","""The ability to generate electric fields at small scales is becoming increasingly important in many fields of research including plasmonics-based sensing, micro- and nanofabrication, microfluidics and spintronics.  The localized generation of electrical fields at extremely small scales has the potential to revolutionize conventional methods of electrically stimulating cells.  The objective of this proposal is the development of miniaturized untethered devices capable of delivering electric currents to cells for the stimulation of their vital functions. To this end, we propose the construction of micro- and nanoscale magnetoelectric structures that can be triggered using external magnetic fields.  These small devices will consist of composite hybrid structures containing piezoelectric and magnetostrictive layers. By applying an oscillating magnetic field in the presence of a DC bias field, the magnetostrictive element will deform, thereby generating stress in a piezoelectric shell, which in turn will become electrically polarized. Small devices capable of wirelessly generating electric fields offer an innovative way of studying the electrical and electrochemical stimulation of cells.  For example, by concentrating electric fields at specific locations in a cell, the behavior of protein membrane components such as cell adhesion molecules or transport proteins can be altered to modulate the stiction of proliferating cells or ion channel gating kinetics.""","1491701","2013-09-01","2018-08-31"
"ELECTROLITH","Electrical Petrology: tracking mantle melting and volatiles cycling using electrical conductivity","Fabrice Olivier Gaillard","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Melting in the Earth’s mantle rules the deep volatile cycles because it produces liquids that concentrate and redistribute volatile species. Such redistributions trigger volcanic degassing, magma emplacement in the crust and hydrothermal circulation, and other sorts of chemical redistribution within the mantle (metasomatism). Melting also affects mantle viscosities and therefore impacts on global geodynamics. So far, experimental petrology has been the main approach to construct a picture of the mantle structure and identify regions of partial melting.
Magnetotelluric (MT) surveys reveal the electrical properties of the deep Earth and show highly conductive regions within the mantle, most likely related to volatiles and melts. However, melting zones disclosed by electrical conductivity do not always corroborate usual pictures deduced from experimental petrology. In 2008, I proposed that small amount of melts, very rich in volatiles species and with unusual physical properties, could reconcile petrological and geophysical observations. The broadening of this idea is however limited by (i) the incomplete knowledge of both petrological and electrical properties of those melts and (ii) the lack of petrologically based models to fit MT data. ELECTROLITH will fill this gap by treating the following points:
- How volatiles in the H-C-S-Cl-F system trigger the beginning of melting and how it affects mantle conductivity?
- What are the atomic structures and the physical properties of such volatile-rich melts?
- How can such melts migrate in the mantle and what are the relationships with deformation?
- What are the scaling procedures to integrate lab-scale observations into a petrological scheme that could decipher MT data in terms of melt percolation models, strain distributions and chemical redistributions in the mantle
ELECTROLITH milestone is therefore a reconciled perspective of geophysics and petrology that will profoundly enrich our vision of the mantle geodynamics","1051236","2011-11-01","2017-10-31"
"ELECTRON4WATER","Three-dimensional nanoelectrochemical systems based on low-cost reduced graphene oxide: the next generation of water treatment systems","Jelena RADJENOVIC","FUNDACIO INSTITUT CATALA DE RECERCA DE L'AIGUA","The ever-increasing environmental input of toxic chemicals is rapidly deteriorating the health of our ecosystems and, above all, jeopardizing human health. Overcoming the challenge of water pollution requires novel water treatment technologies that are sustainable, robust and energy efficient. ELECTRON4WATER proposes a pioneering, chemical-free water purification technology: a three-dimensional (3D) nanoelectrochemical system equipped with low-cost reduced graphene oxide (RGO)-based electrodes. Existing research on graphene-based electrodes has been focused on supercapacitor applications and synthesis of defect-free, superconductive graphene. I will, on the contrary, use the defective structure of RGO to induce the production of reactive oxygen species and enhance electrocatalytic degradation of pollutants. I will investigate for the first time the electrolysis reactions at 3D electrochemically polarized RGO-coated material, which offers high catalytic activity and high surface area available for electrolysis. This breakthrough approach in electrochemical reactor design is expected to greatly enhance the current efficiency and achieve complete removal of persistent contaminants and pathogens from water without using any chemicals, just by applying the current. Also, high capacitance of RGO-based material can enable further energy savings and allow using intermittent energy sources such as photovoltaic panels. These features make 3D nanoelectrochemical systems particularly interesting for distributed, small-scale applications. This project will aim at: i) designing the optimum RGO-based material for specific treatment goals, ii) mechanistic understanding of (electro)catalysis and (electro)sorption of persistent pollutants at RGO and electrochemically polarized RGO, iii) understanding the role of inorganic and organic matrix and recognizing potential process limitations, and iv) developing tailored, adaptable solutions for the treatment of contaminated water.","1493734","2017-05-01","2022-04-30"
"ELECTRONOPERA","Electron dynamics to the Attosecond time scale and Angstrom length scale on low dimensional structures in Operation","Anders Mikkelsen","LUNDS UNIVERSITET","We will develop and use imaging techniques for direct probing of electron dynamics in low dimensional structures with orders of
magnitude improvements in time and spatial resolution. We will perform our measurements not only on static structures, but on
complex structures under operating conditions. Finally as our equipment can also probe structural properties from microns to
single atom defects we can directly correlate our observations of electron dynamics with knowledge of geometrical structure. We
hope to directly answer central questions in nanophysics on how complex geometric structure on several length-scales induces
new and surprising electron dynamics and thus properties in nanoscale objects.
The low dimensional semiconductors and metal (nano) structures studied will be chosen to have unique novel properties that will
have potential applications in IT, life-science and renewable energy.
To radically increase our diagnostics capabilities we will combine PhotoEmission Electron Microscopy and attosecond XUV/IR
laser technology to directly image surface electron dynamics with attosecond time resolution and nanometer lateral resolution.
Exploring a completely new realm in terms of timescale with nm resolution we will start with rather simple structure such as Au
nanoparticles and arrays nanoholes in ultrathin metal films, and gradually increase complexity.
As the first group in the world we have shown that atomic resolved structural and electrical measurements by Scanning Tunneling
Microscopy is possible on complex 1D semiconductors heterostructures. Importantly, our new method allows for direct studies of
nanowires in devices.
We can now measure atomic scale surface chemistry and surface electronic/geometric structure directly on operational/operating
nanoscale devices. This is important both from a technology point of view, and is an excellent playground for understanding the
fundamental interplay between electronic and structural properties.","1419120","2010-10-01","2015-09-30"
"ELITE","Early Life Traces, Evolution, and Implications for Astrobiology","Emmanuelle J Javaux","UNIVERSITE DE LIEGE","Tracking the early traces of life preserved in very old rocks and reconstructing the major steps of its evolution is an exciting and most challenging domain of research. How amazing it is to have a cell that is 1.5 or 3.2 billion years old under a microscope! From these and other disseminated fragments of life preserved along the geological timescale, one can build the puzzle of biosphere evolution and rising biological complexity. The possibility that life may exist beyond Earth on other habitable planets lies yet at another scale of scientific debates and popular dreams. We have the chance now to live at a time when technology enable us to study in the finest details the very old record of life, or to land on planets with microscope and analytical tools, mimicking a geologist exploring extraterrestrial rocky outcrops to find traces of water and perhaps life. There is still a lot to be done however, to solve major questions of life evolution on Earth, and to look for unambiguous life traces, on Earth or beyond. The project ELiTE aims to provide key answers to some of these fundamental questions.
Astrobiology studies the origin, evolution and distribution of life in the Universe, starting with life on Earth, the only biological planet known so far. The ambitious objectives of the project ELiTE are the following:

1) The identification of Early traces of life and their preservation conditions, in Precambrian rocks of established age

2) The characterization of their biological affinities, using innovative approaches comprising micro to nanoscale morphological, ultrastructural and chemical analyses of fossil and recent analog material

3) The determination of the timing of major steps in evolution. In particular, the project ELiTE aims to decipher two major and inter-related steps in early life evolution and the rise of biological complexity: the evolution of cyanobacteria, responsible for Earth oxygenation and ancestor of the chloroplast, influencing drastically the evolution of life and the planet Earth, and the evolution of the domain Eucarya since LECA (Last Eucaryotic Universal Ancestor).

4) The determination of causes of observed pattern of evolution in relation with the environmental context (oxygenation, impacts, glaciations, tectonics, nutrient availability in changing ocean chemistry) and biological innovations and interactions (ecosystems evolution).

Objective 1 has implications for the search for unambiguous traces of life on Earth and beyond Earth. Objectives 2 to 4 have implications for the understanding of causes and patterns of biological evolution and rise of complexity in Earth life. Providing answers to these most fundamental questions will have major impact on our understanding of early life evolution, with implications for the search for life beyond Earth.","1470736","2013-01-01","2018-12-31"
"EllipticPDE","Regularity and singularities in elliptic PDE's: beyond monotonicity formulas","Xavier ROS-OTON","UNIVERSITAT ZURICH","One of the oldest and most important questions in PDE theory is that of regularity. A classical example is Hilbert's XIXth problem (1900), solved by De Giorgi and Nash in 1956. During the second half of the XXth century, the regularity theory for elliptic and parabolic PDE's experienced a huge development, and many fundamental questions were answered by Caffarelli, Nirenberg, Krylov, Evans, Nadirashvili, Friedman, and many others. Still, there are problems of crucial importance that remain open.

The aim of this project is to go significantly beyond the state of the art in some of the most important open questions in this context. In particular, three key objectives of the project are the following. First, to introduce new techniques to obtain fine description of singularities in nonlinear elliptic PDE's. Aside from its intrinsic interest, a good regularity theory for singular points is likely to provide insightful applications in other contexts. A second aim of the project is to establish generic regularity results for free boundaries and other PDE problems. The development of methods which would allow one to prove generic regularity results may be viewed as one of the greatest challenges not only for free boundary problems, but for PDE problems in general. Finally, the third main objective is to achieve a complete regularity theory for nonlinear elliptic PDE's that does not rely on monotonicity formulas. These three objectives, while seemingly different, are in fact deeply interrelated.","1335250","2019-01-01","2023-12-31"
"ELNOX","Elemental nitrogen oxidation – A new bacterial process in the nitrogen cycle","Heide Schulz-Vogt","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","The largest reservoir for nitrogen on earth is the atmosphere that contains 78 percent nitrogen gas. Until now the only known biological process interacting with elemental nitrogen is the bacterial reduction of nitrogen to ammonia for the build up of biomass (nitrogen fixation). This reaction requires energy and is only carried out in the absence of other nitrogen sources, such as ammonia or nitrate. Thermodynamically, the oxidation of nitrogen to nitrate with oxygen releases reasonable amounts of energy, but no bacterium using this redox couple has been known until today. We have isolated a marine bacterium, which is capable of growing in the dark with nitrogen gas as electron donor and oxygen as electron acceptor while forming nitrate. As this microorganism can also use carbondioxide as a carbon source it basically lives of air. While oxidizing atmospheric nitrogen gas the bacterium releases large amounts of nitrate and thereby enhances the amount of fixed nitrogen available for other organisms. At the moment the apparent flux of elemental nitrogen to the ocean by bacterial nitrogen fixation is much smaller than the loss of nitrogen through bacterial denitrification, suggesting that we are missing a major input of nitrogen. This newly discovered physiology of nitrogen oxidation could close this large gap in our understanding of the nitrogen cycle. The amount of biological available nitrogen determines the amount of biomass that can be build up by living organisms. Therefore, it is crucial to know the nitrogen flux into the biosphere, to understand the balances in the carbon cycle. In this project I propose to study this new bacterial physiology in order to understand, which factors control the activity of nitrogen oxidizing bacteria. We need to know how widespread these bacteria are, to estimate their influence on the global nitrogen cycle, and I propose to investigate the interactions between nitrogen oxidizers and other relevant bacteria of the nitrogen cycle.","1450673","2008-07-01","2013-06-30"
"EMAGIN2D","Electrical control of magnetism in multiferroic 2D materials","Efrén NAVARRO-MORATALLA","UNIVERSITAT DE VALENCIA","The avenue of magnetism in the field of 2D materials has marked the ultimate milestone in the discovery of one-atom-thick classes of materials. Bulk ferromagnets and antiferomagnets now have their 2D counterparts and are at one’s provision for the realization of imagination-limited artificial layered structures. At the same time, this awaited breakthrough has brought in new conundrums that demand investigation. This project is driven by the exploration of the limits of van der Waals 2D magnets from both a fundamental physics and a materials science and devices point of view. Firstly, it addresses fundamental key questions regarding spin order at the true 2D limit, which remain a mystery to the date. Here, the great variety of magnetic anisotropies exhibited by the transition metal halides will shed new light to the subtle equilibrium of interactions in few-layer magnets. Secondly, the project will invoke the control of the magnetic ground states and spin textures in true 2D magnets via electrical manipulation. Electric fields will assist in tuning the magnetic coupling and critical behaviour and the spatial manipulation of spin topologies. Anticipated breakthroughs will be the enhancement of the critical temperature in semiconducting single layer magnets towards room temperature 2D magnetism and the realization of single-layer multiferroic 2D materials. Thirdly, the field effect electrical control of magnetism in designer van der Waals and lateral heterostructures will allow for an enhanced magneto-electric coupling, yielding functional devices for effective charge-to-spin transduction that hold promise in spintronics. The proposal will achieve success by an integral approach to research, through the combination of the study of solid-state growth techniques together with the implementation of state-of-the-art deterministic manipulation of 2D materials in inert conditions and the use high resolution magnetism probes to test hybrid magnetic-optoelectronic devices.","1500000","2018-12-01","2023-11-30"
"EMATTER","New materials for energy production and sustainable energy use","Stoyan Smoukov","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The proposed research is in the field of nanofiber materials, focusing on the development of functional nanofibers for the complementary purposes of energy production and sustainable energy use.  Significant opportunities exist in these areas, stemming from the development of several methods in the last decade for higher capacity nanofiber production, as well as the strategic need to find alternatives to current production of energy and its uses. Nanofibers are expected to bring revolutionary advances to these and many other fields of science and technology, including catalysis, filtration, protein separations, tissue engineering, and flexible electronics.  We will work on creating such materials with potential applications in multi-exciton photovoltaics and catalysis for energy production.  For sustainable energy use, we will develop bioinspired responsive materials and architectures, which would store energy, release it on demand, and act as life-like, efficient, and autonomous entities.  Fundamental questions we will address in the research include: How do we tailor semiconductor band structures, as well as achieve nanoscale morphologies for efficient dissociation of photogenerated excitons? Can we develop general predictive rules for the conditions needed to fabricate nanofibers from any polymer solution by liquid shear processing?  Can the molecular crystallinity and porosity be controlled in the fibers? What are the simplest life-like, autonomous devices that could be made with synthetic materials?

This work will include extensive solution-based synthesis, processing, structural and chemical characterization (by optical and electron microscopy, small angle X-rays), physical property measurements (mechanical, optical, electronic), device fabrication and assembly, and computer simulations. Most of the facilities needed for the research are available in Cambridge, and some will be arranged for through external collaborations.","1963835","2012-02-01","2018-01-31"
"EMERGE","Enzyme Driven Molecular Nanosystems","Rein V Ulijn","UNIVERSITY OF STRATHCLYDE","Functional nanomaterials are predicted to have an enormous impact on some of the most pressing issues of 21st century society, including next-generation health care and energy related technologies. Bottom-up approaches, using self-assembly principles, are increasingly considered to be the most appropriate routes for their synthesis. Indeed, Science magazine highlighted  How far can we push chemical self-assembly?  as one of the 25 biggest questions that face scientific inquiry over the next quarter century. Despite significant advances in recent years, it is still a major challenge to access precisely defined nano-structures in the laboratory, especially if these do not represent the global free energy minimum (i.e. are asymmetric, multifunctional, compartmentalized and/or dynamic). The biological world provides numerous outstanding examples of highly complex functional nano-scale architectures with attractive features such as defect repair, adaptability, molecular recognition and programmability. It is the objective of this ERC Starting Grant to develop and exploit the concept of (bio-)catalytic self-assembly, a bio-inspired approach for bottom-up synthesis of complex nanomaterials. We will explore three unique features of these systems (i) spatiotemporal control, (ii) catalytic amplification, either towards or away from equilibrium and the tempting vision of (iii) dynamic systems with emergent properties. In our approach we aim to encompass the entire spectrum from fundamental understanding to eventual societal benefit. Alongside the fundamental aims, we wish to put our methodologies to use, in collaboration with experts in these fields, to develop novel functional materials towards applications in next-generation biomaterials and gel-phase supramolecular (opto-) electronic materials.","1500000","2011-01-01","2015-12-31"
"Emergent-BH","Emergent spacetime and maximally spinning black holes","Monica Guica","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","One of the greatest challenges of theoretical physics is to understand the fundamental nature of gravity and how it is reconciled with quantum mechanics. Black holes indicate that gravity is holographic, i.e. it is emergent, together with some of the spacetime dimensions, from a lower-dimensional field theory. The emergence mechanism has just started to be understood in certain special contexts, such as AdS/CFT. However, very little is known about it for the spacetime backgrounds relevant to the real world,  due mainly to our lack of knowledge of the underlying field theories.

My goal is to uncover the fundamental nature of spacetime and gravity in our universe by: i) formulating and working out the properties of the relevant lower-dimensional field theories and ii) studying the mechanism by which spacetime and gravity emerge from them. I will adress the first problem by concentrating on the near-horizon regions of maximally spinning black holes, for which the dual field theories  greatly simplify and can be studied using a combination of conformal field theory and string theory methods. To study the emergence mechanism, I plan to adapt the tools  that were succesfully used to understand  emergent gravity in anti de-Sitter (AdS) spacetimes  - such as holographic quantum entanglement and conformal bootstrap - to  non-AdS, more realistic spacetimes.","1495476","2016-09-01","2021-08-31"
"EMIL","Exceptional Materials via Ionic Liquids","Anja-Verena Mudring","RUHR-UNIVERSITAET BOCHUM","Novel and improved nanomaterials with luminescent properties shall be synthesized in ionic liquids (ILs). In this approach the advantages of ionic liquids in nanoparticles synthesis (high nucleation rate, excellent electrosteric nanoparticles (NP) stabilization, morphology control, tunable properties) shall be combined with two unconventional synthesis methods that again take advantage of unique IL properties to obtain unprecedented compounds. Using a completely new and unconventional approach by evaporating metals, intermetallic phases or metal oxides and fluorides under high vacuum (negligible vapour pressure, low flammabilitly of ILs!) into ionic liquids goes far beyond the state of art of nanoparticle synthesis and is expected to have a high technological impact and should offer a way to highly thermodynamically unstable reaction product. Secondly, microwave (MW) irradiation (high polarizability and conductivity of IL ions makes them excellent MW acceptors) of appropriate metal salt/IL solutions should not only lead to NP/IL systems but the reaction of two NP/IL solutions should again lead to otherwise non-accessible reaction products. In combination, new materials with improved properties will be gained. For example, ILs will improve the lifetime of luminescent rare earth (RE)-based systems due to the weaker covalent RE solvent interaction.  Analysis and property determinations of the systems under investigation will involve a variety of aspects of chemistry, physics and materials science.","999848","2008-09-01","2013-08-31"
"EMRCC","Effective methods in rigid and crystalline cohomology","Alan George Beattie Lauder","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The purpose of the project is to develop methods for computing with the rigid and crystalline cohomology of varieties over finite fields. The project will focus on two main problems. First, the fast computation of the Galois action. Second, the effective computation of the cycle class map, and the inverse problem of explicitly recovering algebraic cycles from Galois-invariant cohomology classes (c.f. the Tate conjecture). Research on the first problem would be a natural extension of on-going work of the Prinicipal Investigator and others. By contrast the second problem is entirely new, at least in the context of computational number theory. The overall goal of the project is to provide methods and software which will extend the range of application of computational number theory within the mathematical sciences.","750000","2008-10-01","2013-09-30"
"EN-LUMINATE","Enhancing and Tuning Electroluminescence with Nanoantennas","Jana Zaumseil","RUPRECHT-KARLS-UNIVERSITAET HEIDELBERG","""Being able to enhance and tune the interaction of a light wave with a molecule or nanoparticle on a fundamental level opens up an exciting range of applications such as more efficient solar cells, more sensitive photon detectors and brighter emitters for lighting applications. Nanoplasmonics promises to offer this level of control. Taking the current knowledge on nanoantennas a step further we will integrate them in organic and carbon-nanotube light-emitting devices to improve and tune their emission in unprecedented ways. As our testing platform we will use light-emitting field-effect transistors (LEFETs). Their planar structure, where the light emission zone can be positioned at any point allows for easy and controlled incorporation of plasmonic structures without interfering with charge transport. LEFETs can be made from a wide range of semiconducting materials. We will apply nanoantennas in LEFETs to 1) Enhance electroluminescence of high mobility organic semiconductors 2) Tune excitation decay and transition selection rules in organic semiconductors and 3) Enhance photo- and electroluminescence of single-walled carbon nanotubes. All of these materials offer high carrier mobilities and therefore high currents but have very low fluorescence efficiencies that can be  improved substantially by nanoantennas. We will study the influence of nanoantennas on the fundamental emission properties of these different types of emitters. At the same time we will improve their efficiency in light-emitting devices and thus enable new and innovative applications.""","1496684","2012-12-01","2017-11-30"
"EnBioN","Engineering the Biointerface of Nanowires to Direct Stem Cell Differentiation","Ciro CHIAPPINI","KING'S COLLEGE LONDON","ENBION will engineer a platform to direct the differentiation of stem cells by developing principles for the rational design of the biointerface of nanowires. 
It is increasingly evident that efficient tissue regeneration can only ensue from combining the regenerative potential of stem cells with regulatory stimuli from gene therapy and niche engineering. Yet, despite significant advances towards integrating these technologies, the necessary degree of control over cell fate remains elusive. 
Vertical arrays of high aspect ratio nanostructures (nanowires) are rapidly emerging as promising tools to direct cell fate. Thanks to their unique biointerface, nanowires enable gene delivery, intracellular sensing, and direct stimulation of signalling pathways, achieving dynamic manipulation of cells and their environment. 
This broad manipulation potential highlights the importance and timeliness of engineering nanowires for regenerative medicine. However, developing a nanowire platform to direct stem cell fate requires design principles based on the largely unknown biological processes governing their interaction with cells. Enabling localized, vector-free gene therapy through efficient transfection relies on understanding the still debated mechanisms by which nanowires induce membrane permeability. Directing cell reprogramming requires understanding the largely unexplored mechanosensory processes and the resulting epigenetic effects arising from the direct interaction of nanowires with multiple organelles within the cell. Engineering the cell microenvironment requires yet undeveloped strategies to localize signalling and transfection with a resolution comparable to the lengthscale of cells.
ENBION will develop this critical knowledge and integrate it into guidelines for dynamic manipulation of cells. Beyond the nanowire platform, the principles highlighted by this unique interface can guide the development of nanomaterials with improved control over cellular processes.","1495430","2018-02-01","2023-01-31"
"END2ENDSECURITY","Practical design and analysis of certifiably secure protocols - theory and tools for end-to-end security","Michael Backes","UNIVERSITAT DES SAARLANDES","State-of-the-art technologies struggle to keep pace with possible security vulnerabilities. The lack of a consistent methodology and tools for analyzing security protocols throughout the various stages of their design hinders the detection and prevention of vulnerabilities and comprehensive protocol analysis. Moreover, state-of-the-art verification tools typically only address particular narrow aspects of a protocol&apos;s security and require expert knowledge; hence they do not help protocol designers. The challenge is to guarantee end-to-end security - from high-level specifications of the desired security requirements, to a specification of a security protocol that relies on innovative cryptographic primitives, to a secure, executable program. This proposal addresses key steps of this challenge: our goal is to develop a general methodology for automatically devising security protocols and programs based on high-level specifications of selected security requirements and protocol tasks. This includes developing a user-friendly interface for specifying the protocol&apos;s intended behavior and high-level security requirements, devising suitable abstract protocols, selecting suitable cryptographic instantiations, and generating a secure, streamlined implementation. This methodology will also include novel verification techniques that complement all design phases along with a theory which propagates verification results from phase to phase with the ultimate goal of certified end-to-end security. This includes developing type systems for analyzing abstract protocols, a general framework for conducting cryptographic proofs, and techniques for reasoning about executable code. The tools we develop should be automated and usable by non-experts.","1074807","2010-02-01","2013-10-31"
"ENERGYBIOCATALYSIS","Understanding and Exploiting Biological Catalysts for Energy Cycling: Development of Infrared Spectroelectrochemistry for Studying Intermediates in Metalloenzyme Catalysis","Kylie Alison Vincent","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Advanced catalysts for energy cycling will be essential to a future sustainable energy economy. Interconversion of water and hydrogen allows solar and other  green  electricity to be stored in transportable form as H2 - a fuel for electricity generation on demand. Precious metals (Pt) are the best catalysts currently available for H2 oxidation in fuel cells. In contrast, readily available Ni/Fe form the catalytic centres of robust enzymes used by micro-organisms to oxidise or produce H2 selectively, at rates rivalling platinum. Metalloenzymes also efficiently catalyse redox reactions of the nitrogen and carbon cycles. Electrochemistry of enzyme films on a graphite electrode provides a direct route to studying and exploiting biocatalysis, for example a fuel cell that produces electricity from dilute H2 in air using an electrode modified with hydrogenase. Understanding structures and complex chemistry of enzyme active sites is now an important challenge that underpins exploitation of enzymes and design of future catalysts. This project develops sensitive IR methods for metalloenzymes on conducting surfaces or particles. Ligands with strong InfraRed vibrational signatures (CO, CN-) are exploited as probes of active site chemistry for hydrogenases and carbon-cycling enzymes. The proposal unites physical techniques (surface vibrational spectroscopy, electrochemistry), microbiology (mutagenesis, microbial energy cycling), inorganic chemistry (reactions at unusual organometallic centres) and technology development (energy-catalysis) in addressing enzyme chemistry. Understanding the basis for the extreme catalytic selectivity of enzymes will contribute to knowledge of biological energy cycling and provide inspiration for new catalysts.","1373322","2011-02-01","2016-01-31"
"ENERGYMAPS","Revealing the electronic energy landscape of multi-layered (opto)electronic devices","Yana VAYNZOF","RUPRECHT-KARLS-UNIVERSITAET HEIDELBERG","Modern optoelectronic (OE) devices such as light-emitting or photovoltaic diodes offer exciting opportunities for the future. A wide range of materials has been utilized in these devices, including among others: organic materials, inorganic quantum dots and hybrid perovskites. While the functionality, performance and device physics vary strongly from material to material and device to device, all OE devices depend on the energy levels of their individual components and the interaction of the electronic states at the various heterointerfaces. Lacking a method to map the energy levels in a device, energy level diagrams reported for most devices consist of a combination of individual energy levels for each material, which neglect interactions between the materials (that may cause interfacial dipoles and/or band bending) and do not represent the true energetic landscape. Despite this, they are routinely used for interpretation of device performance and physics.
This project aims to map the energy levels in real functional devices: revealing the true nature of buried interfaces, multilayers and contacts, and to answer fundamental long-standing questions in the field of OE, such as the origin of photovoltage losses and energetics of injection/extraction contacts of devices. We will develop and utilize a “Ultra-violet photoemission spectroscopy (UPS) depth profiling” technique based on the combination of UPS with Ar gas cluster ion beam (GCIB) etching that induces minimal surface damage, on a wide range of organic, inorganic and hybrid materials and devices. We will reveal the true energy level landscapes of devices and monitor their evolution throughout the device lifetime. Furthermore, we will explore the possibility to expand the use of GCIB etching beyond UPS as a new nanofabrication technique. These studies will open new frontiers in OE research and would allow the development of novel interface engineering approaches, device architectures and material design rules.","1497931","2017-09-01","2022-08-31"
"ENIGMA","EXPLORING NONLINEAR DYNAMICS IN GRAPHENE NANOMECHANICAL SYSTEMS","Farbod Alijani","TECHNISCHE UNIVERSITEIT DELFT","Micro and nanomechanical systems are being adopted in billions of products, that address a wide range of sensor and actuator applications in modern technology. The advent of graphene, and the ability to fabricate single atom thick membranes, promises further device downscaling, enabling ultimate sensing capabilities that until recently seemed utopical. But, these atomically thin membranes are in essence nonlinear and exhibit nonlinear dynamic behavior at forces of only a few pN, which needs to be understood to harness their full potential.
Although the field of nonlinear dynamics dates back several centuries, its implications at the atomic scale have remained relatively unexplored. Thermal fluctuations due to Brownian motion and nanoscale forces become dominant at this scale, and when combined with graphene’s exotic elasticity, give rise to phenomena that are not observed before, and cannot be explained by classical approaches. Our poor understanding of these complex features at the same time, have made characterization of graphene very challenging. An example is its bending modulus that is evaluated orders of magnitude higher than theoretical predications, by the available experimental methods.
In this project, I aim at providing full understanding of nonlinearities of these one atom thick membranes, not only to unveil the enigmatic behavior of graphene but also to improve current nanomaterial characterization methods. The distinguishing feature of my methodology is that on the one side, it will be based on atomistic simulations combined with modal order reduction techniques, to predict the complexities at the single atom level; on the other side, experimental nonlinear dynamic data will be analyzed for evaluating nonlinear effects and extracting material properties using nonlinear resonances in the MHz range. My methodology will have the potential to serve as the next generation of characterization techniques for nanomaterial science and nanomechanics communities.","1422598","2018-11-01","2023-10-31"
"ENLIGHT","The interplay between quantum coherence and environment in the photosynthetic electronic energy transfer and light-harvesting: a quantum chemical picture","Benedetta Mennucci","UNIVERSITA DI PISA","Photon energy absorption and electronic energy transfer (EET) represents the first fundamental step in both natural and artificial light-harvesting systems. The most striking example is photosynthesis, in which plants, algae and bacteria are able to transfer the absorbed light to the reaction centers in proteins with almost 100% quantum efficiency. Recent two-dimensional spectroscopic measurements suggest that the role of the environment (a protein or a given embedding supramolecular architecture) is fundamental in determining both the dynamics and the efficiency of the process. What is still missing in order to fully understand and characterize EET is a new theoretical and computational approach which can reproduce the microscopic dynamics of the process based on an accurate description of the playing actors, i.e. the transferring pigments and the environment. Such an approach is a formidable challenge due to the large network of interactions which couples all the parts and makes the dynamics of the process a complex competition of random fluctuations and coherences. Only a strategy based upon an integration of computational models with different length and time scales can achieve the required completeness of the description. This project aims at achieving such an integration by developing completely new theoretical and computational tools based on the merging of quantum mechanical methods, polarizable force fields and dielectric continuum models. Such a strategy in which the fundamental effects of polarization between the pigments and the environment will be accounted for in a dynamically coupled way will allow to simulate the full dynamic process of light harvesting and energy transfer in complex multichromophoric supramolecular systems.","1300000","2011-09-01","2016-08-31"
"ENLIGHTMENT","Photonic Electrodes for Enhanced Light Management in Optoelectronic Devices","Antonio Agustin Mihi Cervelló","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","Nanostructured dielectric and metallic photonic architectures can concentrate the electric field through resonances, increase the light optical path by strong diffraction and exhibit many other interesting optical phenomena that cannot be achieved with traditional lenses and mirrors. The use of these structures within actual devices will be most beneficial for enhanced light absorption in thin solar cells, photodetectors and to develop new sensors and light emitters. However, emerging optoelectronic devices rely on large area and low cost fabrication routes such as roll to roll or solution processing, to cut manufacturing costs and increase the production throughput. If the exciting properties exhibited by the photonic structures are to be implemented in these devices, then they too have to be processed in a similar fashion as the devices they intend to improve. This research plan is aimed to develop photonic electrodes that will enhance light matter interaction based on wave optics phenomena while being fabricated with techniques fully compatible with today’s mass production approaches, allowing seamless integration of wave optics components in current devices. The objectives of this proposal are: 1) to investigate the fundaments of the enhanced light-matter interaction observed in devices that use wave optics elements. 2) To develop fabrication routes for large area and low cost photonic and plasmonic structures using techniques similar to those employed in industry, so they could be easily incorporated in technologies such as roll to roll. 3) To fabricate prototype solar cells, photodetectors and sensors on top of photonic electrodes, demonstrating improved performance without deterioration of other figures of merit in the device. The results of the research plan will advance the state of the art in nanophotonics structures, providing the path towards a new generation of large-scale and low-cost photonic architectures.","1500000","2015-12-01","2020-11-30"
"ENOLCAT","Emulating Nature: Reaction Diversity and Understanding through Asymmetric Catalysis","Andrew David Smith","THE UNIVERSITY COURT OF THE UNIVERSITY OF ST ANDREWS","The remarkable way that Nature prepares complex natural products has always been a source of inspiration to scientists, stimulating the development of new synthetic methods and strategies, as elegantly demonstrated by biomimetic approaches to total synthesis. Similarly, the performance and specificity of enzymes, perfected though evolution, offer ideals of selectivity and specificity that synthetic chemists aspire to. This proposal aims to develop an internationally leading research programme inspired by Nature’s ability to selectively generate diverse products from simple materials with exquisite levels of regio- and enantiocontrol. We aspire to synthetically emulate the elegant behaviour of Nature’s building blocks, such as co-enzyme A, in their ability to generate synthetic diversity (such as polyketides and alkaloids) from a simple and common starting material. Using this blueprint, we intend to selectively control the synthesis of a diverse range of bespoke stereodefined carbo- and heterocycles from readily available starting materials using simple man-made catalysts. We specifically aim to develop new strategies within the field of organic catalysis, focused upon the development of methods for the in situ catalytic generation of chiral ammonium enolates from carboxylic acids and their employment in catalysis. We also propose to develop a comprehensive mechanistic understanding of these processes.  In preliminary work we have delineated a simple and efficient approach to this problem by employing chiral isothioureas as asymmetric catalysts, and we aim to build on the insight provided by these studies to develop this powerful concept into a generally applicable synthetic strategy.","1497005","2011-10-01","2017-06-30"
"ENREMOS","Enantioselective Reactions on Model Chirally Modified Surfaces","Swetlana Schauermann","CHRISTIAN-ALBRECHTS-UNIVERSITAET  ZU KIEL","Imparting chirality to non-chiral metal surfaces by adsorption of chiral modifiers is a highly promising route to create effective heterogeneously catalyzed processes for production of enantiopure pharmaceuticals. A molecular-level understanding of enantioselective processes on chiral surfaces is an importance prerequisite for the rational design of new enantiospecific catalysts. With the research outlined in this proposal we are aiming at a fundamental level understanding of the structure of chirally modified surfaces, the bonding of the prochiral substrate on the chiral media and the details of the kinetics and dynamics of enantioselective surface reactions. A full mechanistic picture can be obtained if these aspects will be understood both on the extended single crystal surfaces, mimicking a local interaction of the modifier-substrate complexes with a metal, as well as on the small chirally modified nanoparticles that more accurately resemble the structural properties and high catalytic activity of practically relevant powdered supported catalyst. To achieve these atomistic insights, we propose to apply a combination of ultrahigh vacuum (UHV) based methods for studying reaction kinetics and dynamics (multi-molecular beam techniques) and in-situ surface spectroscopic and microscopic tools on well-defined model surfaces consisting of metal nanoparticles supported on thin single crystalline oxide films. Complementary, the catalytic behaviour of these chirally modified model surfaces will be investigated under ambient pressure conditions with enantiospecific detection of the reaction products that will enable detailed atomistic insights into structure-reactivity relationships.","1589736","2014-01-01","2018-12-31"
"ENSENA","Entanglement from Semiconductor Nanostructures","Gregor Weihs","UNIVERSITAET INNSBRUCK","At the interface between quantum optics and semiconductors we find a rich field of investigation with huge potential for quantum information processing communication technologies. Entanglement is one of the most fascinating concepts in quantum physics research as well as an important resource for quantum information processing.

This project will develop novel sources of entangled photon pairs with semiconductor nanostructures. In particular, we will use the scattering of microcavity exciton-polaritons as an extremely strong optical nonlinearity for the generation of entanglement with properties that are difficult to achieve with the traditional methods. Further we will work with individual semiconductor quantum dots to create controlled single entangled pairs and explore the interfacing of quantum dots to flying qubits.

The long term vision of this research is to create integrated sources of entanglement that can be combined with laser sources, passive optical elements, and even detectors in order to realize the quantum optics lab on a chip.","1259726","2011-01-01","2015-12-31"
"ENTANGLED-TM-ALKANE","Entangled pincer ligand architectures and their application in the transition-metal-mediated activation of alkanes","Adrian Benjamin Chaplin","THE UNIVERSITY OF WARWICK","The selective transformation of alkanes is an area of contemporary importance with wide-ranging implications for organic synthesis and the effective use of petroleum resources. While homogeneous transition metal catalysis is a potentially powerful means for achieving this objective, the fundamental organometallic chemistry of alkane activation reactions has proven to be exceedingly difficult to investigate due to the weakly interacting nature of alkanes. To address this knowledge gap and provide the foundation for future advancement of the field, ENTANGLED-TM-ALKANE outlines a systematic approach for the study of pivotal sigma–alkane complex intermediates; nominally transient and extremely reactive metal-alkane adducts formed through coordination of an intact C–H bond to the metal centre. Inspired from supramolecular chemistry, the approach involves the innovative use of systems containing alkane substrates held in close proximity to reactive metal centres through mechanical entanglement within supporting tridentate macrocyclic ‘pincer’ ligands (i.e. alkane based [2]rotaxanes and [2]catenanes). Through the interwoven topology of these systems, problematic dissociation reactions of sigma–alkane complexes will be circumvented, facilitating isolation and ultimately enabling their structure and reaction chemistry to be probed in much greater detail than has been previously possible. The project objectives are to: (a) develop and use new synthetic (supramolecular) methodologies for the preparation of these mechanically interlocked metal-alkane assemblies; (b) systematically investigate the organometallic chemistry of the metal centre and its interaction with the entangled alkane; and through variation of the macromolecules’ components (macrocycle donors and geometry, alkane, metal), (c) compile a definitive and unprecedented body of qualitative and quantitative structure-activity relationships for the activation alkanes using transition metals.","1521137","2015-04-01","2020-03-31"
"EntangleUltraCold","Entanglement in Strongly Correlated Quantum Many-Body Systems with Ultracold Atoms","Daniel Guenther GREIF","RUPRECHT-KARLS-UNIVERSITAET HEIDELBERG","Entanglement plays a central role for strongly correlated quantum many-body systems and is considered to be the root for a number of surprising emergent phenomena in solids such as high-temperature superconductivity or fractional Quantum Hall states. Entanglement detection in these systems is an important target of current research, but has so far remained elusive owing to the fine control required and high demands on statistical sampling.

The goal of this project is to realize strongly correlated quantum systems close to the ground state using quantum annealing of ultracold fermionic atoms, and to study the character, strength and role of entanglement. We will construct a novel type of cold atom experiment, which makes use of optical tweezers and Raman sideband cooling. This will allow a 100-fold improvement in the experimental repetition rate compared to conventional experiments and allow reaching the ground state in systems of up to 7x7 sites. The flexibility of the moving optical tweezers will facilitate implementing entanglement measures, including concurrence, quantum-state tomography and entanglement entropy. Our primary research objective is studying entanglement in the doped Hubbard model, where a variety of strongly correlated systems are expected, as well as the role of entanglement in thermalizing out-of-equilibrium samples. In a later stage we will focus on frustrated systems in triangular lattices and honeycomb geometries, and also interacting topological states.

Our experiments will have a far-reaching impact on condensed matter research, as it will be the first platform for experimental exploration of the role of entanglement in strongly correlated fermionic many-body systems. Our insights will be beyond the capabilities of numerical simulations and we envision that the project will lead to a better understanding of complex quantum phenomena, and may ultimately drive the discovery of novel quantum materials.","1787564","2019-08-01","2024-07-31"
"ENTERAPIC","Energy-Efficient Multi-Terabit/s Photonic Interconnects","Christian Gunter Koos","KARLSRUHER INSTITUT FUER TECHNOLOGIE","The rapid growth of data traffic requires radically new approaches for high-speed data transmission to increase the bandwidth and power efficiency by orders of magnitude. The proposed research aims at novel system and device concepts for low-energy high-capacity optical interconnects in data centers. Data rates of 10 Tbit/s and beyond are envisaged by coherent multicarrier transmission. Parametric frequency conversion in high-Q Kerr-nonlinear resonators will be used to generate broadband combs of frequency-locked optical carriers. Integrated silicon photonic systems will allow for power-efficient multichannel modulation and detection. Novel reconfigurable optical signal processors will avoid excessive digital post-processing and hence reduce overall energy consumption.","1498800","2012-01-01","2016-12-31"
"ENTROPHASE","Entropy formulation of evolutionary phase transitions","Elisabetta Rocca","UNIVERSITA DEGLI STUDI DI PAVIA","The ground-breaking nature of the project relies on the possibility of opening new horizons
with a novel  mathematical formulation of physical problems.
The project aim is indeed to obtain relevant mathematical results in order to
get further insight into new models for phase transitions and the
corresponding evolution PDE systems. The new approach presented here turns
out to be particularly helpful within the investigation of issues like as existence, uniqueness,
control, and long-time behavior of the solutions for such evolutionary PDEs.

Moreover, the importance of the opportunity to apply such new theory to  phase transitions lies
in the fact that such phenomena arise in a variety of applied problems like, e.g.,
melting and freezing in solid-liquid mixtures, phase changes in solids, crystal growth, soil freezing,
damage in elastic materials, plasticity, food conservation, collisions, and so on. From
the practical viewpoint, the possibility to describe these phenomena in a quantitative way
has deeply influenced the technological
development of our society, stimulating the related mathematical interest.","659785","2011-04-01","2017-03-31"
"enzC-Hem","Creating Versatile Metallo-Enzyme Environments for Selective C-H Activation Chemistry: Lignocellulose Deconstruction and Beyond","Anthony GREEN","THE UNIVERSITY OF MANCHESTER","The availability of a versatile catalytic platform to precisely target and functionalize individual C-H bonds in complex organic molecules would revolutionize our synthetic strategies, leading to streamlined routes to high value chemicals and supporting the development of a ‘greener’ chemical industry. Although an impressive range of C-H functionalizations can be achieved with small transition metal complexes, site selectivity is often determined by features of the substrate, and not by the catalyst. A general approach to achieve the more aspirational ‘catalyst controlled’ transformations requires molecular recognition elements within the catalyst which: a) allow precise substrate orientation and b) can be tuned to alter selectivity. In principle, these requirements could be perfectly addressed by protein catalysts which can be readily adapted via laboratory evolution. However, enzyme engineering strategies are currently limited to Nature’s twenty amino acid alphabet, severely limiting the range of metal co-ordination environments, and thus catalytic activities, that are accessible within proteins. 
In enzC-Hem, I will exploit advanced protein engineering technology available in my laboratory to install ‘chemically programmed’ ligands and/or noble metal co-factors into selected enzyme scaffolds. I will show that the resulting C-H activation catalysts can be systematically optimized via directed evolution with an expanded genetic code using modern ultra-high throughput methods (>100 variants per second), yielding biocatalysts with augmented selectivity/activity profiles. Thus my approach merges the broad range of C-H functionalizations accessible with small molecule catalysts with precise control of selectivity provided by proteins. The biocatalysts developed will address major global challenges in biotechnology and synthetic chemistry, from enhancing lignocellulose derived biofuel production to revealing novel bioactive molecules via late-stage functionalizations.","1492424","2018-01-01","2022-12-31"
"EOS","Enzyme catalysis in organic solvents","Damien Laage","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Enzymes are remarkably efficient catalysts and their recent use in non-aqueous organic solvents is opening a tremendous range of applications in synthetic chemistry: since, surprisingly, most enzymes do not denature in these non-natural environments, new reactions involving e.g. water-insoluble reagents can be catalyzed, while unwanted degradation side reactions are suppressed.
However, a key challenge for these applications is to overcome the greatly reduced catalytic activity compared to aqueous conditions. Empirically, adding activators such as salts or small amounts of water dramatically enhances the activity, but the underlying mechanisms have remained elusive, thus preventing a rational optimization.
Through analytic modeling and numerical simulations, our project will provide the first atomic-scale detailed description of enzyme catalysis in organic solvents, including the key role of the environment. We will then use this unprecedented molecular insight to design rigorous new procedures for the rational engineering of systems with dramatically enhanced activities, both through optimized choices of solvents and additives, and through targeted protein mutations.
Specifically, we will first rigorously establish the influence of enzyme flexibility on catalytic activity through an original model accounting for the dynamic disorder arising from conformation fluctuations. Second, we will provide the first molecular explanation of the commonly invoked “lubricating” action of added water. Third, the underlying mechanism of the much employed salt-induced activation will be determined, probably calling for a radical change from the currently used picture of a water-mediated action.
Far-reaching practical impacts are expected for the numerous industrial syntheses already employing biocatalysis in non-aqueous media.","1390800","2012-01-01","2017-12-31"
"EPIC","Energy transfer Processes at gas/wall Interfaces under extreme Conditions","Brian PETERSON","THE UNIVERSITY OF EDINBURGH","In the future, high-efficiency (low CO2) vehicles will be powered in part by reinvented internal combustion (IC) engines that are “downsized” and operate with new combustion modes. These engine concepts are subject to problems such as increased transient heat transfer and flame quenching in small passages. Near-wall transient heat transfer is not well-understood in engine environments; the gas is not constant in pressure, temperature, or velocity such that physical processes quickly digress from established theory. EPIC is uniquely placed to address these problems. A novel constant-volume chamber, offering realistic engine passages but with optical access, and which emulates the pressure/temperature time curve of a real engine, will be developed. This chamber will make it possible to measure the highly transient and highly variable processes at the gas/wall interface (including a highly dynamic flame front) for single- and two-wall passages. Measurements will be made using a suite of advanced laser diagnostics; a novel aspect of the proposed work as they have not been used in combination to study such a problem before. Hybrid fs/ps rotational coherent Raman (i.e. CARS) in a line format will provide transient gas temperature and species profiles normal to the wall surface in high-risk/high-gain packages. PIV/PTV measurements will further elucidate flow dynamics at the surface. Planar OH-LIF will help interpret CARS measurements and provide necessary details of flame transport and quenching. As the flame approaches the surface, phosphor thermometry will measure wall temperature and heat flux to elucidate the highly dynamic inter-coupling between flame and wall. EPIC will provide substantial breakthroughs in knowledge by measuring unsteady boundary layer development and understanding its influence on flame quenching for single- and two-wall surfaces. As such, EPIC will provide the fundamental knowledge that supports cleaner combustion technology for the future.","1499351","2017-12-01","2022-11-30"
"EPIDELAY","Delay differential models and transmission dynamics of infectious diseases","Gergely Röst","SZEGEDI TUDOMANYEGYETEM","The aim of this project is to develop and analyse infinite dimensional dynamical models for the transmission dynamics and propagation of infectious diseases. We use an integrated approach which spans from the abstract theory of functional differential equations to the practical problems of epidemiology, with serious implications to public health policy, prevention, control and mitigation strategies in cases such as the ongoing battle against the nascent H1N1 pandemic.

Delay differential equations are one of the most powerful mathematical modeling tools and they arise naturally in various applications from life sciences to engineering and physics, whenever temporal delays are important. In abstract terms, functional differential equations describe dynamical systems, when their evolution depends on the solution at prior times.
The central theme of this project is to forge strong links between the abstract theory of delay differential equations and practical aspects of epidemiology. Our research will combine competencies in different fields of mathematics and embrace theoretical issues as well as real life applications.

In particular, the theory of equations with state dependent delays is extremely challenging, and this field is at present on the verge of a breakthrough. Developing new theories in this area and connecting them to relevant applications would go far beyond the current research frontier of mathematical epidemiology and could open a new chapter in disease modeling.","796800","2011-05-01","2016-12-31"
"EPOQUE","Engineering post-quantum cryptography","Peter SCHWABE","STICHTING KATHOLIEKE UNIVERSITEIT","""Our digital society critically relies on protection of data and communication against espionage and cyber crime. Underlying all protection mechanisms is cryptography, which we are using
daily to protect, for example, internet communication or e-banking. This protection is threatened by the dawn of universal quantum computers, which will break large parts of the
cryptography in use today. Transitioning current cryptographic algorithms to crypto that resist attacks by large quantum computers, so called """"post-quantum cryptography"""", is possibly the
largest challenge applied cryptography is facing since becoming a domain of public research in the second half of the last century. Large standardization bodies, most prominently ETSI and
NIST, have started efforts to evaluate concrete proposals of post-quantum crypto for standardization and deployment. NIST's effort follows in the tradition of successful public """"crypto
competitions"""" with strong involvement by the academic cryptographic community. It is expected to run through the next 5 years.
This project will tackle the engineering challenges of post-quantum cryptography following two main research directions. The first direction investigates implementation characteristics of
submissions to NIST for standardization. These include speed on various platforms, code size, and RAM usage. Furthermore we will study so-called side-channel attacks and propose suitable
countermeasures. Side-channel attacks use information such as timing or power consumption of cryptographic devices to obtain secret information. The second direction is about protocol
integration. We will examine how different real-world cryptographic protocols can accommodate the drastically different performance characteristics of post-quantum cryptography, explore
what algorithms suit best the requirements of common usage scenarios of these protocols, and investigate if changes to the high-level protocol layer are advisable to improve overall system
performance.""","1500000","2018-10-01","2023-09-30"
"EPSILON","Elliptic Pdes and Symmetry of Interfaces and Layers for Odd Nonlinearities","Enrico Valdinoci","FORSCHUNGSVERBUND BERLIN EV","The scope of this project is to perform an analytical study of the geometric properties of the interafaces arising in the scalar Ginzburg-Landau-Allen-Cahn equation, with particular attention to possible 1D symmetries.

Also, we would like to analyze the cases in which the operator is singular, degenrate, subelliptic or fractional and to obtain results for PDEs in manifold and in inverse overdetermined problems, since all these models share some important features with classical semilinear PDEs and possess a wide range of potential applications.

To achieve our goals, we would like to build a small, mobile and specialized team of young researchers with outstanding professional skills, specialized in the above subjects, which
has a long history together, new upcoming projects and a network to spread out to.","850000","2012-01-01","2016-12-31"
"EQUALIS","EQualIS : Enhancing the Quality of Interacting Systems","Patricia Bouyer-Decitre","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The ubiquitous use of computerized systems, and their increasing
complexity, demand formal evidences of their correctness. While
current formal-verification techniques have already been applied to a
number of case studies, they are not sufficient yet to fully analyze
several aspects of complex systems such as communication networks,
embedded systems or industrial controllers.  There are three important
characteristics of these systems which need to be tackled:
- the rich interaction that crucially constrains the behaviour of
such systems is poorly taken into account in the actual models;
- the imprecisions and uncertainty inherent to systems that are
implemented (e.g. on a digital processor), or which interact via a
network, or which control physical equipments, are mostly ignored
by the verification process;
- the deployment of large interacting systems emphasizes the lack for
a modular approach to the synthesis of systems.

The goal of this project is to develop a systematic approach to the
formal analysis of interacting systems.  We will use models from game
theory for properly taking into account the interaction in those
systems, and will propose quantitative measures of correctness and
quality, that will take into account the possible perturbations in the
systems. The core of the project will be the development of various
algorithms for synthesizing high-quality interactive systems. We will
be particularly attached to the modularity of the approach and to the
development of efficient algorithms. The EQualIS project will deeply
impact the design and verification of interacting systems, by
providing a rich framework, that will increase our confidence in the
analysis of such systems.","1497431","2013-01-01","2019-02-28"
"ERACHRON","Eradicating Chronic Infections","Sara SATTIN","UNIVERSITA DEGLI STUDI DI MILANO","""Given the alarming progression of chronic and relapsing infections in the last decades, and the even more alarming predictions for the upcoming years, it is urgent for chemists to be able to provide new molecular tools to study, and ultimately solve, these complex biological problems. Bacterial persisters are an elusive """"dormant"""" phenotype that play a pivotal role in chronic infections, with mechanisms that remain to be fully unravelled. Current knowledge suggests that bacterial persisters are not genetically resistant to antibiotic treatment; they simply appear to shut down through a cascade of biochemical events called the stringent response (SR), becoming insensitive to current drugs. This subpopulation remains unaffected during the time of pharmacological treatment and represents a reservoir that sustains pathogen survival and resurgence. The goal of this project is to fill the knowledge gap between persisters formation and infection eradication, providing the community with potent and selective small molecular tools that can be used to challenge complementary survival mechanisms.
I will adopt a combined approach targeting a specific cellular trigger of the persister phenotype with small molecules designed ad hoc in order to switch it off. The target is a bacterial protein involved in the SR cascade, whose activity is proposed to be allosterically regulated. Coordination propensity analysis of the dynamic behaviour of the target will highlight regulation sites exploitable to modulate and control the protein activity. Structure-based design, virtual fragment screening and chemical synthesis will operate in synergy. Experimental screening methodologies intrinsically rich in structural information, such as those based on NMR spectroscopy, will be privileged.
The overarching goal is to identify molecules able to prevent the insurgence of the """"dormant"""" drug-tolerant state and, possibly, revert the persisters already present to the """"awake"""" drug-sensitive phenotype.

""","1500000","2018-02-01","2023-01-31"
"ERBIUM","Ultracold Erbium: Exploring Exotic Quantum Gases","Francesca Ferlaino","UNIVERSITAET INNSBRUCK","Ultracold quantum gases have exceptional properties and offer an ideal test-bed to elucidate intriguing phenomena of modern quantum physics. My project proposes to use a new exotic element to study strong dipolar effects in quantum gases. For its appealing properties, we choose erbium (Er), a rare-earth metal that has hardly been explored until now.  This species is strongly magnetic and comparatively heavy.  Due to these characteristics, we expect the quantum system to be of extreme dipolar character and to exhibit a large number of magnetic Feshbach resonances, necessary to manipulate the low-energy scattering properties. Moreover, this element has a very rich energy level spectrum, which could open up the way to establish novel laser cooling schemes, and it has numerous isotopes, one of them having a fermionic character. Remarkably, none of the species so far used in ultracold quantum gas experiments offers such a unique combination of properties!  By using Erbium, we will be in an optimal position to produce a strongly dipolar atomic gases of bosons and fermions with tunable contact interaction. First important goals of the ERBIUM project include: Extensive study of Er scattering properties,  realization of the first Bose-Einstein condensates and degenerate Fermi gases of erbium atoms, study of dipolar effects in atomic system, production of strongly polar weakly-bound Er molecules and study their properties in a two-dimensional trapping environment. We also have a long-term vision for the ERBIUM project: we will mix heavy erbium atoms with much lighter lithium atoms to produce atomic mixtures with extreme mass imbalance.","1076442","2011-01-01","2015-12-31"
"ERGODICNONCOMPACT","Ergodic theory on non compact spaces","Omri Moshe Sarig","WEIZMANN INSTITUTE OF SCIENCE LTD","The proposal is to look for, and investigate, new ergodic theoretic types of behavior for dynamical systems which act on non compact spaces. These could include transience and non-trivial ways of escape to infinity, critical phenomena similar to phase transitions, and new types of measure rigidity. There are potential applications to smooth ergodic theory (non-uniform hyperbolicity), algebraic ergodic theory (actions on homogeneous spaces), and probability theory (weakly dependent stochastic processes).","539479","2009-10-01","2014-09-30"
"ERIKLINDAHLERC2007","Multiscale and Distributed Computing Algorithms for Biomolecular Simulation and Efficient Free Energy Calculations","Erik Lindahl","KUNGLIGA TEKNISKA HOEGSKOLAN","The long-term goal of our research is to advance the state-of-the-art in molecular simulation algorithms by 4-5 orders of magnitude, particularly in the context of the GROMACS software we are developing. This is an immense challenge, but with huge potential rewards: it will be an amazing virtual microscope for basic chemistry, polymer and material science research; it could help us understand the molecular basis of diseases such as Creutzfeldt-Jacob, and it would enable rational design rather than random screening for future drugs. To realize it, we will focus on four critical topics:  • ALGORITHMS FOR SIMULATION ON GRAPHICS AND OTHER STREAMING PROCESSORS: Graphics cards and the test Intel 80-core chip are not only the most powerful processors available, but this type of streaming architectures will power many supercomputers in 3-5 years, and it is thus critical that we design new “streamable” MD algorithms.  • MULTISCALE MODELING: We will develop virtual-site-based methods to bridge atomic and mesoscopic dynamics, QM/MM, and mixed explicit/implicit solvent models with water layers around macromolecules.  • MULTI-LEVEL PARALLEL & DISTRIBUTED SIMULATION: Distributed computing provides virtually infinite computer power, but has been limited to small systems. We will address this by combining SMP parallelization and Markov State Models that partition phase space into transition/local dynamics to enable distributed simulation of arbitrary systems.  • EFFICIENT FREE ENERGY CALCULATIONS: We will design algorithms for multi-conformational parallel sampling, implement Bennett Acceptance Ratios in Gromacs, correction terms for PME lattice sums, and combine standard force fields with polarization/multipoles, e.g. Amoeba. We have a very strong track record of converting methodological advances into applications, and the results will have impact on a wide range of fields from biomolecules and polymer science through material simulations and nanotechnology.","992413","2008-09-01","2013-08-31"
"ERQUAF","Entanglement and Renormalisation for Quantum Fields","Jutho Jan J HAEGEMAN","UNIVERSITEIT GENT","Over the past fifteen years, the paradigm of quantum entanglement has revolutionised the understanding of strongly correlated lattice systems. Entanglement and closely related concepts originating from quantum information theory are optimally suited for quantifying and characterising quantum correlations and have therefore proven instrumental for the classification of the exotic phases discovered in condensed quantum matter. One groundbreaking development originating from this research is a novel class of variational many body wave functions known as tensor network states. Their explicit local structure and unique entanglement features make them very flexible and extremely powerful both as a numerical simulation method and as a theoretical tool.

The goal of this proposal is to lift this “entanglement methodology” into the realm of quantum field theory. In high energy physics, the widespread interest in entanglement has only been triggered recently due to the intriguing connections between entanglement and the structure of spacetime that arise in black hole physics and quantum gravity. During the past few years, direct continuum limits of various tensor network ansätze have been formulated. However, the application thereof is largely unexplored territory and holds promising potential. This proposal formulates several advancements and developments for the theoretical and computational study of continuous quantum systems, gauge theories and exotic quantum phases, but also for establishing the intricate relation between entanglement, renormalisation and geometry in the context of the holographic principle. Ultimately, these developments will radically alter the way in which to approach some of the most challenging questions in physics, ranging from the simulation of cold atom systems to non-equilibrium or high-density situations in quantum chromodynamics and the standard model.","1499375","2017-02-01","2022-01-31"
"ESCQUMA","Exploring Strongly Correlated Quantum Matter 
with Cold Excited Atoms","Igor Walter Lesanovsky","THE UNIVERSITY OF NOTTINGHAM","The understanding of quantum matter in and out of equilibrium is among the biggest challenges of modern physics. Despite decades of research fundamental questions, such as the precise
workings behind rather ubiquitous materials such as high temperature superconductors are still unresolved. At the same time there is a new generation of experiments approaching which realises and probes quantum matter with novel and exotic interactions at an unprecedented level of precision. This has already highlighted new avenues of research but also demands for radically new theoretical approaches which lie outside the scope of just a single traditional physical discipline. Novel and in particular multidisciplinary lines of thinking are required to tackle this immense challenge. Such new research will not solely be delivering invaluable insights into currently unresolved problems but rather form a new basis for the understanding of quantum matter from a multidisciplinary perspective. This will open up new horizons for fundamental research and at the same time will pave the way for future technologies and materials which rely on non-equilibrium phenomena or quantum matter. This research proposal takes on this challenge by setting up a broad theoretical research programme which is multipronged and multidisciplinary and which directly connects to the most recent research efforts in ultra cold atomic physics. Here currently a step change is taking place where new experiments explore strongly correlated quantum physics within gases of excited atoms – so-called Rydberg atoms.  Exploiting this unique moment we will develop a framework for the description of the equilibrium and non-equilibrium properties of these complex and very versatile quantum systems. This system-specific research approach has the advantage that theoretical predictions can be verified experimentally and applied in practice almost immediately, leading to research attacking the frontiers of current knowledge.","1492000","2014-01-01","2018-12-31"
"ESKIN","Stretchable Electronic Skins","Stephanie Perichon Ep Lacour","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Future electronic systems will be soft and elastic.  I propose to explore the materials, technology and integration of stretchable electronic systems, which will transform at will, evenly coat a spherical lens, or smoothly interface with a delicate biological organ.  Electronics will be anywhere as well as everywhere.  The proposed programme has the potential to emulate yet another revolution in the microelectronics industry and trigger transformations in the biomedical sector.
The ESKIN programme is an ambitious and highly interdisciplinary endeavour requiring expertise at the frontier of engineering, material sciences, biotechnology and neuroscience.  Stretchability in an electronic system is its ability to negotiate mechanical deformations without letting them interfere with its electrical functionality.  This is a novel and challenging demand on electronic device technology, which has, to date, mainly pushed for smaller scale fabrication and increased performance.  Furthermore the natural compliance of biological tissues and cells calls for softer electronic biomedical interfaces.  Overcoming the  hard to soft  mechanical mismatch will, without doubt, open up new horizons in biomedical research and its related industries.
The manufacture of stretchable electronic skins will then require working out the underlying science and technology for active device materials on soft, elastic substrates.  This capability will further be implemented to demonstrate various  soft and elastic electronic systems  ranging from stretchable displays to long-term neural implants.  My philosophy is to exploit as much as possible current micro/nanofabrication techniques available for  hard surfaces  but to tailor them to  soft surfaces , optimizing and improving them where needed, in order to ensure rapid transition to worldwide distributed consumer and healthcare products.","1499738","2011-03-01","2016-02-29"
"ETMCECS","Enantioselective Transition Metal Catalysis for Efficient Chemical Synthesis","Hon Wai Lam","THE UNIVERSITY OF NOTTINGHAM","Organic molecules of all shapes and sizes are required for a multitude of applications in numerous settings, such as in the biomedical, pharmaceutical, and agrochemical industries (among others). To meet this demand, organic synthesis is faced with the challenge of converting simple, readily available chemical building blocks into more complex structures in as rapid, efficient, and cost-effective a manner as possible. As such, increasing the efficiency of organic synthesis provides enormous benefits to society, quality of life, and a sustainable future.

In this proposal, we outline a program aimed at the design, development, and application of new asymmetric transition metal-catalyzed reactions, where a chiral catalyst will control which particular enantiomer of a chiral product is formed. This feature is absolutely vital, since the action of chiral functional molecules within a chiral environment (such as in biological systems) is critically dependent upon their three-dimensional shape, and hence their enantiomeric composition. Several sub-project areas (each based around transition metal ions for which our group has had prior expertise) are presented, which target compounds from simpler chemical building blocks (copper- and rhodium-catalyzed reactions) to those of higher complexity (nickel-catalyzed domino reactions). During the course of this research, we anticipate that a host of useful discoveries will be made that will positively impact the discipline of organic synthesis for the ultimate benefit of society.","1498892","2011-01-01","2015-12-31"
"ETOPEX","Engineering Topological Phases and Excitations in Nanostructures with Interactions","Jelena KLINOVAJA","UNIVERSITAT BASEL","The main goal of this theory project is to propose engineered topological phases emerging only in strongly interacting systems and to identify the most feasible systems for experimental implementation. First, we will focus on setups hosting topological states localized at domain walls in one-dimensional channels such as parafermions, which are a new class of non-Abelian anyons and most promising candidates for topological quantum computing schemes. Second, in the framework of weakly coupled wires and planes, we will develop schemes for novel fractional topological phases in two- and three-dimensional interacting systems. To achieve these two goals, my team will identify necessary ingredients such as strong electron-electron interactions, helical magnetic order, or crossed Andreev proximity-induced superconductivity and address each of them separately. Later, we combine them to lead us to the desired topological phases and states. On our way to the main goal, as test cases, we will also study non-interacting analogies of the proposed effects such as Majorana fermions and integer topological insulators and pay close attention to the rapid experimental progress to come up with the most feasible proposals. We will study transport properties, scanning tunneling and atomic force microscopy. Especially for systems driven out of equilibrium, we will develop a Floquet-Luttinger liquid technique. We will explore the stability of engineered topological phases, error rates of topological qubits based on them, and computation schemes allowing for a set of universal qubit gates. We will strive to find a reasonable balance between topological stability and experimental
feasibility of setups. Our main theoretical tools are Luttinger liquid techniques (bosonization and renormalization group), Green functions, Floquet formalism, and numerical simulations in non-interacting test models.","1158403","2018-01-01","2022-12-31"
"EURECAT","Smart Systems for Small Molecule Activation and Sustainable Homogeneous Catalysis","Jarl Ivar Van Der Vlugt","UNIVERSITEIT VAN AMSTERDAM","This proposal addresses currently unresolved fundamental questions concerning the activation and functionalization in homogeneous catalysis of challenging, inherently unreactive substrates like NH3, CO2 or water. Responsive ligands in combination with earth-abundant first row transition metals may hold the key to selective bifunctional activation of these small molecules. This innovative, bioinspired concept, utilizing ligand-metal cooperativity to enhance, tune and control the reactivity of base metals, mimics and circumvents the use of expensive and/or toxic 2nd and 3rd row transition metals. A comprehensive tool-box of readily accessible smart ligand systems with cooperative, redox noninnocent or adaptive features will be used for stoichiometric and ultimately catalytic reactivity studies with Fe, Co, Ni and Cu. This will result in mechanistic understanding of novel pathways for selective N-H and O-H activation on well-defined mono- and dinuclear cooperative complexes. Coupling and insertion reactions with alkenes to yield efficient C-N and C-O bond forming processes and CO2 functionalization will be investigated. This fundamental knowledge is then applied for unprecedented cooperative catalysis with first row transition metals. The project will ultimately result in important leads for the direct intermolecular hydroamination of alkenes, including with ammonia, which is one of the top-ten challenges in catalysis, and the efficient, sustainable production of carboxylic acids from CO2 and alkenes. Also relevant contributions to the oxidative activation of water for hydration reactions with alkynes are foreseen. When successful, my initiatives will enable significant breakthroughs in the design of unique, smart ligand systems for the cooperative activation and functionalization of small molecules with base metals. An ERC starting grant greatly strengthens my position in the emerging field of cooperative homogeneous catalysis.","1498471","2012-01-01","2016-12-31"
"EURO-LAB","Experiment to Unearth the Rheological Oceanic Lithosphere-Asthenosphere Boundary","Catherine Ann Rychert","UNIVERSITY OF SOUTHAMPTON","Plate tectonics has been a fundamental tenet of Earth Science for nearly 50 years, but fundamental questions remain, such as where is the base of the plate and what makes a plate, “plate-like?” A better understanding of the transition from the rigid lithospheric plate to the weaker mantle beneath – the rheological lithosphere-asthenosphere boundary (LAB) - has important implications for the driving forces of plate tectonics, natural hazard mitigation, mantle dynamics, the evolution of the planet, and climate change. There are many proxies used to estimate the depth and nature of the base of tectonic plates, but to date no consensus has been reached. For example, temperature is known to have a strong effect on the mechanical behaviour of rocks. However, it has also been suggested that the chemical composition of the plate provides additional strength or that melt weakens the mantle beneath the plate.

We are at a critical juncture where large-scale efforts using geophysical, geochemical, and geological techniques are being launched to better understand the definition of the tectonic plate. The simple and short history of the ocean plate makes it the ideal location to advance our understanding. However, imaging the oceanic LAB has proved challenging given the remoteness of the oceans and associated difficulties in instrumentation. Most observations come from only one ocean, the Pacific, from indirect, remote observations, at different areas and scales.

I propose a large-scale effort to systematically image an oceanic plate beneath the Atlantic from birth at ridge to 40 My old seafloor. I will deploy ocean bottom seismometers (OBS) and magnetotelluric (MT) instruments, and I will image the plate at a range of resolution scales (laterally and in depth) and sensitivities to physical and chemical properties. This large, focused, interdisciplinary effort will finally determine the processes and properties that make a plate strong
and define it.","1827855","2016-04-01","2021-03-31"
"EUROPIUM","The origin of heavy elements: a nuclear physics and astrophysics challenge","Almudena Arcones Segovia","TECHNISCHE UNIVERSITAT DARMSTADT","Where in the universe are heavy elements synthesized? How are these elements produced? These are two exciting and interdisciplinary questions in nuclear astrophysics today and will be investigated in my ERC project EUROPIUM. The favored astrophysical sites are neutrino-driven winds following core-collapse supernovae and neutron star mergers, where extreme conditions enable the rapid neutron capture process (r-process). We will perform long-time multidimensional simulations of these two scenarios and combine them with nucleosynthesis calculations. In neutron star mergers, the radioactive decay of neutron-rich nuclei triggers an electromagnetic signal known as kilonova. This was potentially observed in 2013 after a short gamma ray burst, associated with a neutron star merger. We will simulate the neutrino- and viscous-driven ejecta from the disk that forms after the merger around the central compact object. In addition, we will investigate supernova neutrino-driven winds that produce lighter heavy elements from strontium to silver. We will explore the impact of rotation, improved microphysics, and magnetic fields on the wind evolution and nucleosynthesis. Because the synthesis of lighter heavy elements elements occurs closer to stability, the nuclear physics uncertainties will be reduced by experiments in the near future. This will uniquely allow us to combine observations and nucleosynthesis calculations to constrain the astrophysical conditions and gain new insights into core-collapse supernovae. In nuclear physics, a new era for extreme neutron-rich isotopes is starting with new experimental facilities. Based on our simulations, we will study the impact of the nuclear physics input (nuclear masses, beta decays, neutron captures, and fission) going beyond the state-of-the-art by providing r-process abundances with uncertainties. Comparing our results with forefront observations of the oldest stars will in turn provide new insights about the origin of heavy elements.","1446875","2016-05-01","2021-04-30"
"EUVPLASMA","Laser-driven plasma sources of extreme ultraviolet light for nanolithography","Oscar VERSOLATO","STICHTING NEDERLANDSE WETENSCHAPPELIJK ONDERZOEK INSTITUTEN","Moore’s law is not dead. Keeping it alive is of significant importance to society and to the economy. The prediction that the number of transistors in computer and memory chips doubles every two years, has pushed innovative, disruptive technologies, enabling the smartphone and driving tomorrow’s green automotive industries. It changes society.

The density of elements realized on a chip is defined by one essential step in their production: lithography. Moore’s law thus provides a challenge to science and industry to develop beyond state of the art lithographic technologies. This challenge is being met by introducing extreme ultraviolet (EUV) lithography in high-volume manufacturing. This is happening right now. Generating the required EUV light – from tin-microdroplet-based laser-driven plasma sources – of sufficient power, reliability, and stability, presents a formidable, multi-faceted task, combining industrial innovations with attractive scientific questions.

My proposal addresses this EUV source challenge through the following objectives: (1) create insight into tin-droplet deformation and fragmentation for optimal target preparation through laser-pulse impact, the first step of the two-step sequence used to produce EUV light; (2) provide understanding of the myriad of atomic plasma processes responsible for the emission of EUV light in the second step of the process; (3) understand and push the fundamental limit of this plasma-conversion of laser light into EUV light; and (4) explain and control how the laser-produced plasma expands. Each of these objectives has a significant potential impact in its own field of science and technology. This proposal as a whole has a further goal, namely to use the knowledge gained to transition from the CO2-laser technology currently in use for driving EUV sources to the superior, modern, solid-state lasers, to achieve the industrial dream of a plasma EUV light source that is one order of magnitude brighter.","1500000","2019-02-01","2024-01-31"
"EVERYSOUND","Computational Analysis of Everyday Soundscapes","Tuomas Oskari Virtanen","TAMPEREEN KORKEAKOULUSAATIO SR","Sounds carry a large amount of information about our everyday environment and physical events that take place in it. For example, when a car is passing by, one can perceive the approximate size and speed of the car. Sound can easily and unobtrusively be captured e.g. by mobile phones and transmitted further – for example, tens of hours of audio is uploaded to the internet every minute e.g. in the form of YouTube videos. However, today's technology is not able to recognize individual sound sources in realistic soundscapes, where multiple sounds are present, often simultaneously, and distorted by the environment.
The ground-breaking objective of EVERYSOUND is to develop computational methods which will automatically provide high-level descriptions of environmental sounds in realistic everyday soundscapes such as street, park, home, etc. This requires developing several novel methods, including joint source separation and robust pattern classification algorithms to reliably recognize multiple overlapping sounds, and a hierarchical multilayer taxonomy to accurately categorize everyday sounds. The methods are based on the applicant's internationally recognized and awarded expertise on source separation and robust pattern recognition in speech and music processing, which will allow now tackling the new and challenging research area of everyday sound recognition.
The results of EVERYSOUND will enable searching for multimedia based on its audio content, which is not possible with today's technology. It will allow mobile devices, robots, and intelligent monitoring systems to recognize activities in their environments using acoustic information. Producing automatically descriptions of vast quantities of audio will give new tools for geographical, social, cultural, and biological studies to analyze sounds related to human, animal, and natural activity in urban and rural areas, as well as multimedia in social networks.","1500000","2015-05-01","2020-04-30"
"EVODIS","Exploiting vortices to suppress dispersion and reach new separation power boundaries","Wim De Malsche","VRIJE UNIVERSITEIT BRUSSEL","The 21st century is expected to develop towards a society depending ever and ever more on (bio-)chemical measurements of fluids and matrices that are so complex they are well beyond the current analytical capabilities. Incremental improvements can no longer satisfy the current needs of e.g. the proteomics field, requiring the separation of tens of thousands of components. The pace of progress in these fields is therefore predominantly determined by that of analytical tools, whereby liquid chromatography is the most prominent technique to separate small molecules as well as macromolecules, based on differential interaction of each analyte with support structures giving it a unique migration velocity. To improve its performance, a faster transport between these structures needs to be generated. Unfortunately the commonly pursued strategy, relying on diffusion and reducing the structure size, has come to its limits due to practical limitations related to packing and fabrication of sub-micron support structures, pressure tolerance and viscous heating.
A ground-breaking step to advance chromatographic performance to another level would be to accelerate mass transport in the lateral direction, beyond the rate of diffusion only. To meet this requirement, an array of microstructures and local electrodes can be defined to create lateral electroosmotic vortices in a pressure-driven column, aiming to accelerate the local mass transfer in an anisotropic fashion. The achievement of ordered arrays of vortices is intimately linked to this requirement, which is also of broader importance for mixing, anti-fouling of membrane and reactor surfaces, enhanced mass transfer in reactor channels, emulsification, etc. Understanding and implementing anisotropic vortex flows will therefore not only revolutionize analytical and preparative separation procedures, but will also be highly relevant in all flow systems that benefit from enhanced mass transfer.","1460688","2016-03-01","2021-02-28"
"EvoluTEM","Illuminating Atomic Scale Processes in Liquids and Gases","Sarah- Jane HAIGH","THE UNIVERSITY OF MANCHESTER","EvoluTEM: Illuminating Atomic Scale Processes in Liquids and Gases

Objective 1: To build new capability in atomic resolution environmental imaging and analysis. 
Objective 2: To apply this platform to synthesise new photonic nanomaterials with enhanced performance.

The vision is to design, construct, and make available the next generation of multifunctional in situ specimen holders for the scanning /transmission electron microscope (S/TEM). This new experimental resource will enable ground-breaking characterisation of complex nanoscale reactions under realistic and relevant environmental conditions using a lab-on-a-chip configuration. By providing a platform with unparalleled atomic scale imaging and simultaneous elemental analysis capabilities, as well as flexible in situ (temperature, pressure, and illumination) environments, this effort will provide an experimental module for a wide range of breakthrough in situ nanomaterials experiments. Motivating this work is the goal of being able to fully characterize the synthesis of novel photonic 2D materials, optoelectronic nanoparticles, and photoactive organic-inorganic perovskites. This research could lead to a new level of mechanistic understanding, providing knowledge to realize routes for the production of new nanostructures, with properties that can be optimally tailored for photonic applications (photovoltaics, light emission or optoelectronics). This ambitious research program is only possible because of the principal investigators outstanding electron microscopy expertise, coupled with the world leading nanofabrication capabilities and in situ imaging facilities at the University of Manchester. The project has been structured into five work packages (WPs) with each having well-defined milestones and deliverables.","1755279","2017-04-01","2022-03-31"
"EXAGAL","Hydrodynamical Simulations of Galaxy Formation at the Peta- and Exascale","Volker Springel","HITS GGMBH","Numerical simulations of galaxy formation provide a powerful technique for calculating the non-linear evolution of cosmic structure formation. In fact, they have played an instrumental role in establishing the current standard cosmological model known as LCDM. However, unlocking the predictive power of current petaflop and future exaflop computing platforms requires a targeted effort in developing new numerical methods that excel in accuracy, parallel scalability, and in physical fidelity to the processes relevant in galaxy formation. A new moving-mesh technique for hydrodynamics recently developed by us provides a significant opportunity for a paradigm shift in cosmological simulations of structure formation, replacing the established smoothed particle hydrodynamics technique with a much more accurate and flexible approach. Building on the first successes with this method, we here propose a comprehensive research program to apply this novel numerical framework in a new generation of hydrodynamical simulations of galaxy formation that aim to greatly expand the physical complexity and dynamic range of theoretical galaxy formation models. We will perform the first simulations of individual galaxies with several tens of billion hydrodynamical resolution elements and full adaptivity, allowing us to resolve the interstellar medium in global models of galaxies with an unprecedented combination of spatial resolution and volume. We will simultaneously and self-consistently follow the radiation field in galaxies down to very small scales, something that has never been attempted before. Through cosmological simulations of galaxy formation in representative regions of the Universe, we will shed light on the connection between galaxy formation and the large-scale distribution of gas in the Universe, and on the many facets of feedback processes that regulate galactic star formation, such as energy input from evolving and dying stars or from accreting supermassive black holes.","1488000","2013-02-01","2018-07-31"
"EXASCALEPLASMATURB","Turbulence in Laboratory and Astrophysical Plasmas:
Tackling Key Unsolved Problems via Peta- to Exascale Computing","Frank Jenko","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Plasma turbulence is a ubiquitous phenomenon, influencing the dynamics in most of the visible universe and playing a crucial role in countless laboratory experiments of basic or applied plasma science. Yet, various fundamental aspects of this prototypical nonlinear process involving many degrees of freedom and leading to self-organization far from thermodynamic equilibrium are only poorly understood at present. The aim of this project is to tackle a number of longstanding unsolved problems related to plasma turbulence by means of extreme computing approaching the exascale. For this purpose, a novel generation of numerical tools will be developed and run on some of the largest supercomputers in the years to come, breaking new ground both scientifically as well as computationally. The three main goals are to develop simulation capabilities for peta- to exascale computations of plasma turbulence, to strive to unravel the general nature of plasma turbulence, and to better grasp the creation of magnetic fields in turbulent plasmas together with their effects on energetic particles. Exploiting the fact that the same basic processes are operative in laboratory plasmas as well as throughout the plasma universe, and involving a critical mass of scientists with a strong background in plasma physics, astrophysics, and applied mathematics, the project is well set up for frontier research based on fruitful interactions between these neighbouring fields. Most importantly, it will lead to a new level of understanding of turbulence in laboratory and astrophysical plasmas and help pioneer the use of the next generation(s) of supercomputers – both of which will be of wide benefit.","1450000","2011-11-01","2015-12-31"
"EXC3ITE","EXploring Chemistry, Composition and Circulation in the stratosphere with InnovativeTEchnologies","Johannes Christian Laube","UNIVERSITY OF EAST ANGLIA","It is ozone that primarily heats and therefore creates the stratosphere. Human emissions of ozone-depleting substances (ODSs) have however led to dramatic stratospheric ozone losses for decades. This global problem is ongoing and of renewed concern due to recent unexpected changes. It is also likely affecting the nature of the stratosphere itself, with implications for global health and economy. In addition, emissions of greenhouse gases have been proposed to lead to a long-term acceleration of the stratospheric overturning circulation. In summary, significant stratospheric changes are to be expected from both, ozone losses and global warming.
Indications for such changes have been reported, but there are substantial uncertainties and limitations connected with these studies. In addition, current technologies to explore stratospheric composition and chemistry are very expensive and often offer only infrequent data. There is clearly a need for new and improved tools to correctly detect and quantify changes from observations.
This project will open 3 novel avenues to explore stratospheric chemistry, composition and circulation:
1) A newly developed low-cost technology to retrieve and analyse air from the stratosphere. This will be a new way to derive budgets of all important and newly emerging ODSs directly in the stratosphere; while at the same time providing observations of many strong greenhouse gases.
2) I have found new evidence for substantial past changes in stratospheric chemistry and circulation. An unprecedented investigation of stratospheric air archives spanning 40 years and >50 trace gases will allow new insights into these changes 
3) New diagnosis tools and a detailed comparison with state-of-the-art models will identify the implications for future climate.
The EXC3ITE project will result in a breakthrough in the understanding of stratospheric changes which are of high importance for society through their impact on climate prediction and ozone recovery.","1496439","2016-04-01","2021-03-31"
"ExclusiveHiggs","Search for New Physics in First and Second Generation Quark Yukawa Couplings through Rare Exclusive Decays of the Observed Higgs Boson","Konstantinos NIKOLOPOULOS","THE UNIVERSITY OF BIRMINGHAM","Following the discovery of a Higgs boson with a mass of about 125 GeV, a detailed set of property measurements has confirmed that it plays a central role in the spontaneous breaking of the electroweak symmetry.
Nevertheless, its role in the generation of fermion mass, in particular of the first and second generation, is still unclear. In the Standard Model (SM) this is implemented in an ad hoc manner through Yukawa interactions, and many beyond-the-SM theories offer rich phenomenology and exciting prospects for the discovery of New Physics in this sector.

This project will attack - for the first time - in a systematic and comprehensive way the experimentally most unconstrained sector of the SM: the couplings of the light-quarks (up, down, charm and strange) to the Higgs boson, including possible flavour-violating interactions.  The rare exclusive Higgs boson decays to a meson and a photon or Z boson, which is a novel and unique approach, will be searched for with the ATLAS detector at the CERN Large Hadron Collider (LHC).  At the same time, an extensive set of measurements of analogous rare exclusive decays of the W and Z bosons will be performed, further enhancing the scientific value of the proposed research programme.

The expected branching ratio sensitivity of 10^{-6} for the Higgs boson decays, and 10^{-9} for the W and Z boson decays will probe viable New Physics models, and in several cases will reach and surpass the SM predictions. This project will lead to a profound extension of the ATLAS and LHC physics output, going beyond what was previously considered possible. It will open a new line of research in the Higgs sector, providing relevant input to many different areas of frontier research, including particle cosmology and planning for possible future particle physics facilities.","1499945","2017-03-01","2022-02-28"
"EXO-ATMOS","Exploring the Plurality of New Worlds: Their Origins, Climate and Habitability","Jean-Michel  Lucien-Bernard Desert","UNIVERSITEIT VAN AMSTERDAM","Recent surveys have revealed an amazing, and yet unexplained, diversity of planets orbiting other stars. The key to understanding and exploiting this diversity is to study their atmospheres. This is because exoplanets’ atmospheres are unique laboratories that hold the potential to transform our understanding of planet formation, physics, and habitability. This is a new opportunity to place the Solar System and the Earth’s ecosystem in a broader context; one of the main goals of modern astrophysics. 

The aim of this proposal is to leverage exoplanet detections, as well as observational capabilities and theoretical frameworks, to deepen and broaden our understanding of planetary physics. This project will transform the field of exoplanet atmospheres by contributing to three major advances. We will: i) push exoplanet characterization new frontiers by providing the largest in-depth study of atmospheres through the measurements of precise spectra, and the retrieval of their composition, in order to constrain their origins; ii) reveal for the first time global exo-climate through a novel method to probe atmospheric structure and dynamics; and iii) pioneer an innovative approach that uses robotic small telescopes to estimate the impact of stellar radiation on atmospheres, with a particular focus on their habitability. Theses objectives will be achieved via an ambitious portfolio of cutting-edge observations, combined with state-of-the-art modelling for their interpretation. Their accomplishment would be a major breakthrough, culminating in a comprehensive comparative exoplanetology, which in turn will open up new key discoveries in planetary formation and evolution. Our expertise will also enable predictions on conditions for habitability and direct the search atmospheric biosignatures with upcoming capabilities. The impact of our discoveries will go well beyond the scientific community since the quest of our origins is of interest to mankind.","2000000","2016-03-01","2021-02-28"
"ExoAI","Deciphering super-Earths using Artificial Intelligence","Ingo WALDMANN","UNIVERSITY COLLEGE LONDON","The discovery of extrasolar planets - i.e. planets orbiting other stars - has fundamentally transformed our understanding of planets, solar systems and our place in the Milky Way. Recent discoveries have shown that planets between 1-2 R are the most abundant in our galaxy, so called super-Earths. Yet, they are entirely absent from our own solar system. Their nature, chemistry, formation histories or climate remain very much a mystery. Estimates of their densities suggest a variety of possible planet types and formation/evolution scenarios but current degeneracies cannot be broken with mass/radius measures alone. Spectroscopy of their atmospheres can provide vital insight. Recently, the first atmosphere around a super-Earth, 55 Cnc e, was discovered, showcasing that these worlds are far more complex than simple densities allow us to constrain. 
To achieve a more fundamental understanding, we need to move away from the status quo of treating individual planets as case-studies and analysing data ‘by hand’. A globally encompassing, self-consistent and self-calibrating approach is required. Here, I propose to move the field a significant step towards this goal with the ExoAI (Exoplanet Artificial Intelligence) framework. ExoAI will use state-of-the-art neural networks and Bayesian atmospheric retrieval algorithms applied to big-data. Given all available data of an instrument, ExoAI will autonomously learn the best calibration strategy, intelligently recognise spectral features and provide a full quantitative atmospheric model for every planet observed. This uniformly derived catalogue of super-Earth atmospheric models, will move us on from the individual case-studies and allow us to study the larger picture. We will constrain the underlying processes of planet formation/migration and bulk chemistries of super-Earths. The algorithm and the catalogue of atmospheric and instrument models will be made freely available to the community.","1500000","2018-01-01","2022-12-31"
"EXOEARTHS","EXtra-solar planets and stellar astrophysics: towards the detection of Other Earths","Nuno Miguel Cardoso Santos","CENTRO DE INVESTIGACAO EM ASTRONOMIA E ASTROFISICA DA UNIVERSIDADE DO PORTO","The detection of more than 300 extrasolar planets orbiting other solar-like stars opened the window to a new field of astrophysics. Many projects to search for Earth-like planets are currently under way, using a huge battery of telescopes and instruments. New instrumentation is also being developed towards this goal for use in both ground- and space-based based facilities. Since planets come as an output of the star formation process, the study of the stars hosting planets is of great importance. The stellar-planet connection is strengthened by the fact that most of the exoplanets were discovered using a Doppler radial-velocity technique, where the gravitational influence of the planet on the star and not the planet itself is actually measured. This project aims at doing frontier research to explore i) in unique detail the stellar limitations of the radial-velocity technique, as well as ways of reducing them, having in mind the detection of Earth-like planets and ii) to develop and apply software packages aiming at the study of the properties of the planet-host stars, having in mind the full characterization of the newfound planets, as well as understanding planet formation processes. These goals will improve our capacity to detect, study, and characterize new very low mass extra-solar planets. EXOEarths further fits into the fact that I am currently Co-PI of the project for a new high-resolution ultra-stable spectrograph for the VLT. The results of this project are crucial to fully exploit this new instrument. They will be also of extreme importance to current state-of-the-art planet-search projects aiming at the discovery of other Earths, in particular those making use of the radial-velocity method.","928090","2009-10-01","2014-12-31"
"EXOWATER","Chemical EXchanges On WATER-rich worlds: Experimentation and numerical modelling","Gabriel Tobie","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The present project is dedicated to the characterization of chemical exchanges within water-rich bodies including icy moons of Jupiter and Saturn as well as exoplanets that may be discovered in a near future.  Recent spacecraft missions, Galileo (1996-2003) and Cassini-Huygens (2004-today), have revealed that complex chemical exchanges between their warm silicate inner core and their water-rich outer layer have occur on Enceladus, Europa and Titan.  Similar exchange processes are also likely to occur within water-rich planets outside our Solar System. Here I propose to combine experimental investigations and numerical modelling to quantify the degree of interaction between seafloors, oceans, ice shells, and surfaces, atmospheres of water-rich worlds. This innovative approach will provide the first complete description of exchange processes on water-rich bodies and will constrain the conditions for which such water-rich environments are favourable for the development of life.

The proposed sophisticated modeling of interactions between the interior and surface will provide  precious tools for the interpretation  of Galileo/Cassini observations and will significantly improve our current understanding of planetary processes.  The output of these numerical simulations will also help for the definition of measurements that should be done by future exploration missions (EJSM and TSSM) in order to constrain the composition and size of icy moon s ocean.

The detection of water-rich around other stars is within our reach. When the first detections of a water-rich planet and the first identification of atmospheric components will occur, my proposed modelling efforts will provide a theoretical framework for the data interpretation in term of physical and chemical conditions of their ocean and atmosphere. This will provide key constraints to define if a detected planet outside our Solar System is a good candidate for harbouring life.","1481400","2011-01-01","2015-12-31"
"exoZoo","High definition and time-resolved studies of exoplanet atmospheres: a new window on the extreme diversity of the exoplanet zoo","Jayne BIRKBY","UNIVERSITEIT VAN AMSTERDAM","The ongoing search for a second Earth has revealed an astounding diversity in the population of planets orbiting other stars. This eclectic zoo of other worlds shows little similarity with our own solar system. The key to understanding this diversity lies in exoplanet atmospheres, which hold vital information on their formation histories and evolutionary pathways. To access them, I propose three ambitious new windows into exoplanet characterization that will: i) Deliver the largest homogenous study of exoplanet atmospheres at high spectral resolution, and provide a novel framework that combines multi-resolution data to deliver precise atmospheric measurements. This will place unprecedented constraints on the physical causes of exoplanet diversity and is a game changer in understanding their origins. ii) Perform the first robust study of the reflective properties of exoplanet atmospheres at high spectral resolution, constraining their evolution and demonstrating how to interpret, and crucially, optimise high-resolution observations for the upcoming biomarker hunt with the extremely large telescopes. This may be our only way to characterize the nearest habitable worlds. iii) Pioneer an innovative photometric monitoring technique for exoplanet atmospheres and use it to measure their rotation, detect giant storms, and hunt for occulting moons to begin surveying their demographics. The achievement of these three main objectives will be a ground-breaking step forward in our understanding of exoplanets and Earth’s place amongst them, bringing us ever closer to answering the question: are we alone?","1500000","2019-09-01","2024-08-31"
"EXPLOREMAPS","Combinatorial methods, from enumerative topology to random discrete structures and compact data representations","Gilles Schaeffer","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""Our aim is to built on recent combinatorial and algorithmic progress to attack a series of deeply connected problems that have independantly surfaced in enumerative topology, statistical physics, and data compression. The relation between these problems lies in the notion of """"combinatorial map"""", the natural discrete mathematical abstraction of objects with a 2-dimensional structures (like geographical maps, computer graphics' meshes, or 2d manifolds). A whole new set of properties of these maps has been uncovered in the last few years under the impulsion of the principal investigator. Rougly speaking, we have shown that classical graph exploration algorithms, when correctly applied to maps, lead to remarkable decompositions of the underlying surfaces. Our methods resort to algorithmic and enumerative combinatorics. In statistical physics, these decompositions offer an approach to the intrinsec geometry of discrete 2d quantum gravity: our method is here the first to outperform the celebrated """"topological expansion of matrix integrals"""" of Brezin-Itzykson-Parisi-Zuber. Exploring its implications for the continuum limit of these random geometries is our great challenge now. From a computational geometry perspective, our approach yields the first encoding schemes with asymptotically optimal garanteed compression rates for the connectivity of triangular or polygonal meshes. These schemes improve on a long series of heuristically efficient but non optimal algorithms, and open the way to optimally compact data structures. Finally we have deep indications that the properties we have uncovered extend to the realm of ramified coverings of the sphere. Intriguing computations on the fundamental Hurwitz's numbers have been obtained using the ELSV formula, famous for its use by Okounkov et al. to rederive Kontsevich's model. We believe that further combinatorial progress here could allow to bypass the formula and obtaine an elementary explanation of these results.""","750000","2008-07-01","2013-06-30"
"EXPLORERS","EXPLORERS Exploring epigenetic robotics: raising intelligence in machines","Pierre-Yves Oudeyer","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","In spite of considerable work in artificial intelligence, machine learning, and pattern recognition in the past 50 years, we have no machine capable of adapting to the physical and social environment with the flexibility, robustness and versatility of a 6-months old human child. Instead of trying to simulate directly the adult s intelligence, EXPLORERS proposes to focus on the developmental principles that give rise to intelligence in infants by re-implementing them in machines. Framed in the developmental/epigenetic robotics research agenda, and grounded in research in developmental psychology, its main target is to build robotic machines capable of autonomously learning and re-using a variety of skills and know-how that were not specified at design time, and with initially limited knowledge of the body and of the environment in which it will operate. This implies several fundamental issues: How can a robot discover its body and its relationships with the physical and social environment? How can it learn new skills without the intervention of an engineer? What internal motivations shall guide its exploration of vast spaces of skills? Can it learn through natural social interactions with humans? How to represent the learnt skills and how can they be re-used? EXPLORERS attacks directly those questions by proposing a series of fundamental scientific and technological advances, including computational intrinsic motivation systems for learning basic sensorimotor skills reused for grounded acquisition of the meaning of new words. This project not only addresses fundamental scientific questions, but also relates to important societal issues: personal home robots are bound to become part of everyday life in the 21st century, in particular as helpful social companions in an aging society. EXPLORERS objectives converge to the challenges implied by this vision: robots will have to be able to adapt and learn new skills in the unknown homes of users who are not engineers.","1572215","2009-12-01","2015-05-31"
"EXPROTEA","Exploring Relations in Structured Data with Functional Maps","Maksims OVSJANIKOVS","ECOLE POLYTECHNIQUE","We propose to lay the theoretical foundations and design efficient computational methods for analyzing, quantifying and exploring relations and variability in structured data sets, such as collections of geometric shapes, point clouds, and large networks or graphs, among others. Unlike existing methods that are tied and often limited to the underlying data representation, our goal is to design a unified framework in which variability can be processed in a way that is largely agnostic to the underlying data type. 
In particular, we propose to depart from the standard representations of objects as collections of primitives, such as points or triangles, and instead to treat them as functional spaces that can be easily manipulated and analyzed. Since real-valued functions can be defined on a wide variety of data representations and as they enjoy a rich algebraic structure, such an approach can provide a completely novel unified framework for representing and processing different types of data. Key to our study will be the exploration of relations and variability between objects, which can be expressed as operators acting on functions and thus treated and analyzed as objects in their own right using the vast number of tools from functional analysis in theory and numerical linear algebra in practice. 
Such a unified computational framework of variability will enable entirely novel applications including accurate shape matching, efficiently tracking and highlighting most relevant changes in evolving systems, such as dynamic graphs, and analysis of shape collections. Thus, it will permit not only to compare or cluster objects, but also to reveal where and how they are different and what makes instances unique, which can be especially useful in medical imaging applications. Ultimately, we expect our study to create to a new rigorous, unified paradigm for computational variability, providing a common language and sets of tools applicable across diverse underlying domains.","1499845","2018-01-01","2022-12-31"
"EXQFT","Exact Results in Quantum Field Theory","Zohar Komargodski","WEIZMANN INSTITUTE OF SCIENCE LTD","Quantum field theory (QFT) is a unified conceptual and mathematical framework that encompasses a veritable cornucopia of physical phenomena, including phase transitions, condensed matter systems, elementary particle physics, and (via the holographic principle) quantum gravity. QFT has become the standard language of modern theoretical physics.

Despite the fact that QFT is omnipresent in physics, we have virtually no tools to analyze from first principles many of the interesting systems that appear in nature. (For instance, Quantum Chromodynamics, non-Fermi liquids, and even boiling water.)

Our main goal in this proposal is to develop new tools that would allow us to make progress on this fundamental problem. To this end, we will employ two strategies.
First, we propose to study in detail systems that possess extra symmetries (and are hence simpler). For example, critical systems often admit the group of conformal transformations. Another example is given by theories with Bose-Fermi degeneracy (supersymmetric theories). We will explain how we think significant progress can be achieved in this area. Advances here will allow us to wield more analytic control over relatively simple QFTs and extract physical information from these models.  Such information can be useful in many areas of physics and lead to new connections with mathematics. Second, we will study general properties of renormalization group flows. Renormalization group flows govern the dynamics of QFT and understanding their properties may lead to substantial developments. Very recent progress along these lines has already led to surprising new results about QFT and may have direct applications in several areas of physics. Much more can be achieved.

These two strategies are complementary and interwoven.","1158692","2013-09-01","2018-08-31"
"EXTENDFRET","Extended fluorescence resonance energy transfer with plasmonic nanocircuits","Jerome Wenger","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Förster fluorescence resonance energy transfer (FRET) is one of the most popular methods to measure distance, structure, association, and dynamics at the single molecule level. However, major challenges are limiting FRET in several fields of physical and analytical sciences: (i) a short distance range below 8 nm, (ii) a concentration range in the nanomolar regime, and (iii) generally weak detected signals.
At the interface between physical chemistry and nano-optics, the proposal objective is to extend the effectiveness of single molecule FRET using plasmonic nanocircuits to: (i) perform FRET on a range up to 20 nm, (ii) detect a single FRET pair in a solution of micromolar concentration, and (iii) improve the statistical distribution in FRET measurements.
To meet its ambitious goals, the proposal introduces plasmonic nanocircuits to tailor the light-molecule interaction at the nanoscale. Energy transfer between donor and acceptor fluorophores is efficiently mediated through intense surface plasmon modes to extend the FRET distance range and improve the fluorescence signal. Moreover, the nanocircuits will be combined with recent innovations in biophotonics: stimulated emission of acceptor fluorescence, full dynamic analysis, and fluidic nanochannels.
The scientific breakthroughs and project impacts will open new horizons for proteomics, enzymology, genomics and photonics. For elucidating molecular structure, the long range FRET will enable understanding the folding structure of large DNA or protein molecules. For assessing chemical reactions, achieving single molecule analysis at micromolar concentration is essential to monitor relevant kinetics, reveal sample heterogeneity, and detect rare and/or transient species. For analytical chemistry, nanocircuits are ideal for sensitive biosensing on a chip. For photonics, nanocircuits can realize key components for optical information processing at the nanoscale.","1477942","2012-01-01","2016-12-31"
"EXTPRO","Quasi-Randomness in Extremal Combinatorics","Asaf Shapira","TEL AVIV UNIVERSITY","Combinatorics is an extremely fast growing mathematical discipline. While it started as a collection of isolated problems that
were tackled using ad-hoc arguments it has since grown into a mature discipline which both incorporated into it deep tools from other mathematical areas, and has also found applications in other mathematical areas such as Additive Number Theory, Theoretical Computer Science, Computational Biology and Information Theory.

The PI will work on a variety of problems in Extremal Combinatorics which is one of the most active subareas within Combinatorics with spectacular recent developments. A typical problem in this area asks to minimize (or maximize) a certain parameter attached to a discrete structure given several other constrains. One of the most powerful tools used in attacking problems in this area uses the so called Structure vs Randomness phenomenon. This roughly means that any {\em deterministic} object can be partitioned into smaller quasi-random objects, that is, objects that have properties we expect to find in truly random ones. The PI has already made significant contributions in this area and our goal in this proposal is to obtain further results of this caliber by tackling some of the hardest open problems at the forefront of current research. Some of these problems are related to the celebrated Hypergraph and Arithmetic Regularity Lemmas, to Super-saturation problems in Additive Combinatorics and Graph Theory, to problems in Ramsey Theory, as well as to applications of Extremal Combinatorics to problems in Theoretical Computer Science. Another major goal of this proposal is to develop new approaches and techniques for tackling problems in Extremal Combinatorics.

The support by means of a 5-year research grant will enable the PI to further establish himself as a leading researcher in Extremal Combinatorics and to build a vibrant research group in Extremal Combinatorics.","1221921","2015-03-01","2020-02-29"
"ExTrA","Exoplanets in Transit and their Atmosphere","Xavier Bonfils","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Since the discoveries of giant planets outside our Solar System, over 800 extra-solar planets have been detected and several thousands candidates are awaiting confirmation. They have revolutionized planetary science, by placing our once unique solar system into context. The subset of extrasolar planets that transit their parent star have had most impact on our understanding of their planetary structure and atmospheric physics: they are the only ones for which one can simultaneously measure mass and radius, and therefore infer internal composition. The few that transit a host star bright enough for detailed spectroscopic follow-up provide, in addition, observational information on the composition and physics of extrasolar planetary atmospheres.
Much interest is now focused on finding and characterizing terrestrial mass planets, ideally in the habitable zone of their host stars. The present ERC project offers a novel method to dramatically improve the precision of both the detection and the characterization of exoplanets. The method makes use of multi-object spectrographs to add spectroscopic resolution on traditional differential photometry. This enables the fine correction of the atmospheric variations that would otherwise hinder ground-based observations.
We propose to setup small-size telescopes equipped with a multi-object near-IR spectrograph and observe 800 M dwarfs. This will be the most sensitive survey for Earth-size planets transiting bright nearby stars. It shall yield dozens exo-Earths amenable to atmospheric characterization, including several habitable exo-Earths.
To perform their atmospheric characterization, we also propose to apply the technique of differential spectro-photometry with multi-object spectrographs available on large telescopes. Our observations will represent a step forward in transmission spectroscopy and prepare for the identification of bio-markers in exo-Earth atmospheres with the future ELTs.","2000000","2014-07-01","2019-06-30"
"ExtreFlow","Extreme deformation of structured fluids and interfaces. Exploiting ultrafast collapse and yielding phenomena for new processes and formulated products","Valeria Garbin","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","The increasing demand for environmentally friendly, healthier, and better performing formulated products means that the process industry needs more than ever predictive models of formulation performance for rapid, effective, and sustainable screening of new products. Processing flows and end use produce deformations that are extreme compared to what is accessible with existing experimental methods. As a consequence, the effects of extreme deformation are often overlooked without justification.

Extreme deformation of structured fluids and soft materials is an unexplored dynamic regime where unexpected phenomena may emerge. New flow-induced microstructures can arise due to periodic forcing that is much faster than the relaxation timescale of the system, leading to collective behaviors and large transient stresses.

The goal of this research is to introduce a radically innovative approach to explore and characterize the regime of extreme deformation of structured fluids and interfaces. By combining cutting-edge techniques including acoustofluidics, microfluidics, and high-speed imaging, I will perform pioneering high-precision measurements of macroscopic stresses and evolution of the microstructure. I will also explore strategies to exploit the phenomena emerging upon extreme deformation (collapse under ultrafast compression, yielding) for new processes and for adding new functionality to formulated products.

These experimental results, complemented by discrete particle simulations and continuum-scale modeling, will provide new insights that will lay the foundations of the new field of ultrafast soft matter. Ultimately the results of this research program will guide the development of predictive tools that can tackle the time scales of realistic flow conditions for applications to virtual screening of new formulations.","1499186","2015-05-01","2020-04-30"
"Extreme","An Exascale aware and Un-crashable Space-Time-Adaptive Discontinuous Spectral Element Solver for Non-Linear Conservation Laws","Gregor GASSNER","UNIVERSITAET ZU KOELN","""The dynamics of fluids and plasma is described by non-linear conservation laws. Transient behaviour on multiple scales involving turbulence and shocks is intrinsic to these problems. Due to their low dispersion and dissipation errors, adaptive high order numerical methods currently receive growing attention in academia and industry and form an emerging key technology. The potential benefits are massively improved computational efficiency and drastic reduction in memory consumption. Both benefits can be easily justified theoretically, in particular for a space-time adaptive high order method. However, due to high algorithmic complexity, the theoretical performance is difficult to sustain on massively parallel supercomputers. The first challenge that we will address in this project is to design novel, exascale aware, space-time adaptive algorithms and implement them in an open source solver that will scale on over 10^6 computing cores. Another indispensable property for the successful industrialisation of space-time adaptive high order methods is robustness. Robustness, i.e. an """"un-crashable"""" solver, which still retains all the positive benefits of the high order scheme is the """"holy grail"""" of the current research on these methods. This requires new mathematical concepts. The second challenge we will address here is to construct a provable un-crashable, space-time adaptive, high order solver without excessive artificial dissipation. Our mathematical key to achieve robustness is not intuitive at first sight: skew-symmetry. We will show that a specific skew-symmetric formulation guided by careful mathematics will allow us to design methods that are consistent with the second law of thermodynamics. This physical consistency is important as it will enable us to construct a new class of un-crashable space-time adaptive high order methods. We will demonstrate the supremacy of this efficient and robust solver in complex large scale science and engineering applications.""","1495875","2017-04-01","2022-03-31"
"EXTREME BIOPHYSICS","Extreme biophysics: single molecule characterisation of extremophilic protein folding","Lorna Dougan","UNIVERSITY OF LEEDS","Extremophilic (extreme-loving) organisms have evolved unique features to enable them to function in extreme environmental conditions. Despite much progress in understanding extremophilic protein structure, there is a lack of quantitative information on the conformational dynamics and flexibility of proteins in extreme environments, information which is crucial to develop an understanding of their functional capabilities. Understanding the physical mechanisms of extremophilic organisms and their remarkable preservation capability is not only of fundamental interest, but also pivotal to our abilities to rationally engineer or re-engineer biological materials for exploitation. This proposal aims to develop quantitative biophysical approaches to characterise the physical mechanisms of protein folding and stability in extreme environments. This is an ambitious program of work with great potential to lead to ground-breaking scientific breakthroughs in the fields of water and aqueous solutions, protein folding and protein adaption in extremophilic environments. A state-of-the-art, custom built force spectroscopy instrument will be used to examine the conformational dynamics of single extremophilic proteins. A newly built, state-of-the-art diffractometer at the ISIS pulsed neutron facility at the Rutherford Appleton Laboratories in the UK will be exploited to uncover details of the structural architecture of extremophilic proteins and their surrounding solvent environment. The development of these methods will deliver fundamental insights into the mechanisms of extreme organisms, in addition to developing research tools that will be exploited in synthetic biology, industry and bionanotechnology. A unique collection of skills, together with a world class team of collaborators from across Europe, gives this proposal unrivalled ability to transform extremophilic protein folding research in Europe.","1499664","2011-01-01","2016-09-30"
"EXTREMEPHYSICS","The slowest accreting neutron stars and black holes: New ways to probe fundamental physics","Rudi Wijnands","UNIVERSITEIT VAN AMSTERDAM","Very recently, a new class of sub-luminous accreting neutron stars and black holes has been identified. I propose to use these objects to probe the extreme physical processes which are associated with such compact stars. Just as with their better known brighter cousins, studying them when they are actively accreting and when they are in their quiescent states will give us clues about the behavior of ultra-dense matter in neutron stars and the way neutron-star magnetic fields decay due to the accretion of matter. However, given that these new systems behave differently, I expect to derive from their study a novel perspective which will gain in value even further when contrasted with our current knowledge. I further believe their study will allow me to significantly strengthen the observational proof for the presence of event horizons in black holes. The uncommon nature of these systems suggests that they are very unusual outcomes of binary evolution, and I expect this will also provide us with a different set of clues than we have had until now about the formation of binaries which harbor compact stars. These objects have only recently been discovered, both because we did not have the sensitivity to see them, and because we did not know how to optimize our searches to find them. Current instruments finally have reached the necessary sensitivity. I propose new approaches to find and study these sub-luminous systems using these X-ray and radio instruments in combination with multi-wavelength studies. I expect to find these systems in greater numbers than before, allowing systematic studies of their properties which in turn will provide the ingredients needed to investigate the fundamental physics associated with neutron stars and black holes and serve as input for my proposed theoretical study into binary evolution.","500000","2008-10-01","2013-09-30"
"EyeRegen","Engineering a scaffold based therapy for corneal regeneration","Mark Joseph Ahearne","THE PROVOST, FELLOWS, FOUNDATION SCHOLARS & THE OTHER MEMBERS OF BOARD OF THE COLLEGE OF THE HOLY & UNDIVIDED TRINITY OF QUEEN ELIZABETH NEAR DUBLIN","Corneal blindness resulting from disease, physical injury or chemical burns affects millions worldwide and has a considerable economic and social impact on the lives of people across Europe. In many cases corneal transplants can restore vision however the shortage of donor corneas suitable for transplantation has necessitated the development of alternative treatments. The aim of this project is to develop a new approach to corneal tissue regeneration. Previous approaches at engineering corneal tissue have required access to donor cells and lengthy culture periods in an attempt to grow tissue in vitro prior to implantation with only limited success and at great expense. Our approach will differ fundamentally from these in that we will design artificial corneal scaffolds that do not require donated cells or in vitro culture but instead will recruit the patient’s own cells to regenerate the cornea post-implantation. These biomaterial scaffolds will incorporate specific chemical and physical cues with the deliberate aim of attracting cells and inducing tissue formation. Studies will be undertaken to examine how different chemical, biochemical, physical and mechanical cues can be used to control the behaviour of corneal epithelial, stromal and endothelial cells. Once the optimal combination of these cues has been determined, this information will be incorporated into the design of the scaffold. Recent advances in manufacturing and material processing technology will enable us to develop scaffolds with organized nanometric architectures and that incorporate controlled growth factor release mechanisms. Techniques such as 3D bio-printing and nanofiber electrospinning will be used to fabricate scaffolds. The ability of the scaffold to attract cells and promote matrix remodelling will be examined by developing an in vitro bioreactor system capable of mimicking the ocular environment and by performing in vivo tests using a live animal model.","1498734","2015-07-01","2020-06-30"
"FADER","Flight Algorithms for Disaggregated Space Architectures","Pinchas Pini Gurfil","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","Standard spacecraft designs comprise modules assembled in a single monolithic structure. When unexpected situations occur, the spacecraft are unable to adequately respond, and significant functional and financial losses are unavoidable. For instance, if the payload of a spacecraft fails, the whole system becomes unserviceable and substitution of the entire spacecraft is required. It would be much easier to replace the payload module only than launch a completely new satellite. This idea gives rise to an emerging concept in space engineering termed disaggregated spacecraft. Disaggregated space architectures (DSA) consist of several physically-separated modules, interacting through wireless communication links to form a single virtual platform. Each module has one or more pre-determined functions: Navigation, attitude control, power generation and payload operation. The free-flying modules, capable of resource sharing, do not have to operate in a tightly-controlled formation, but are rather required to remain in bounded relative position and attitude, termed cluster flying. DSA enables novel space system architectures, which are expected to be much more efficient, adaptable, robust and responsive. The main goal of the proposed research is to develop beyond the state-of-the-art technologies in order to enable operational flight of DSA, by (i) developing algorithms for semi-autonomous long-duration maintenance of a cluster and cluster network, capable of adding and removing spacecraft modules to/from the cluster and cluster network; (ii) finding methods so as to autonomously reconfigure the cluster to retain safety- and mission-critical functionality in the face of network degradation or component failures; (iii) designing semi-autonomous cluster scatter and re-gather maneuvesr to rapidly evade a debris-like threat; and (iv) validating the said algorithms and methods in the Distributed Space Systems Laboratory in which the PI serves as a Principal Investigator.","1500000","2011-10-01","2016-09-30"
"FAFC","Foundations and Applications of Functional Cryptography","Gil SEGEV","THE HEBREW UNIVERSITY OF JERUSALEM","""Modern cryptography has successfully followed an """"all-or-nothing"""" design paradigm over the years. For example, the most fundamental task of data encryption requires that encrypted data be fully recoverable using the encryption key, but be completely useless without it. Nowadays, however, this paradigm is insufficient for a wide variety of evolving applications, and a more subtle approach is urgently needed. This has recently motivated the cryptography community to put forward a vision of """"functional cryptography'': Designing cryptographic primitives that allow fine-grained access to sensitive data.

This proposal aims at making substantial progress towards realizing the premise of functional cryptography. By tackling challenging key problems in both the foundations and the applications of functional cryptography, I plan to direct the majority of our effort towards addressing the following three fundamental objectives, which span a broad and interdisciplinary flavor of research directions: (1) Obtain a better understanding of functional cryptography's building blocks, (2) develop functional cryptographic tools and schemes based on well-studied assumptions, and (3) increase the usability of functional cryptographic systems via algorithmic techniques.

Realizing the premise of functional cryptography is of utmost importance not only to the development of modern cryptography, but in fact to our entire technological development, where fine-grained access to sensitive data plays an instrumental role. Moreover, our objectives are tightly related to two of the most fundamental open problems in cryptography: Basing cryptography on widely-believed worst-case complexity assumptions, and basing public-key cryptography on private-key primitives. I strongly believe that meaningful progress towards achieving our objectives will shed new light on these key problems, and thus have a significant impact on our understanding of modern cryptography.""","1307188","2017-02-01","2022-01-31"
"FALCONER","Forging Advanced Liquid-Crystal Coronagraphs Optimized for Novel Exoplanet Research","Frans Snik","UNIVERSITEIT LEIDEN","The 39-m European Extremely Large Telescope (E-ELT) has the potential to directly observe and characterize habitable exoplanets, but current technologies are unable to sufficiently suppress the starlight very close to the star. I propose to develop a novel instrumental approach with breakthrough contrast performance by combining coronagraphs based on brand-new liquid crystal technology, and sensitive imaging polarimetry. The novel coronagraphs will provide an achromatic rejection of starlight even right next to the star such that exoplanets can be imaged efficiently in broadband light and characterized through spectropolarimetry. The coronagraphs will incorporate focal-plane wavefront sensing and polarimetry to achieve an ultimate contrast of 1E-9, which will enable the E-ELT to observe habitable exoplanets.

We will prototype coronagraph designs of increasing contrast performance, validate them in the lab, and apply them on-sky using 6-8 meter class telescopes. With our coronagraphs that offer a contrast improvement by a factor of 10 as compared to current systems in 360-degree dark holes, we will search for self-luminous exoplanets very close to stars at thermal infrared wavelengths, and characterize known targets with multi-wavelength observations. Through accurate photometry and polarimetry, we will study their atmospheric hazes. By combining liquid-crystal coronagraphy with sensitive polarimetry, we will study the inner regions of protoplanetary disks to find signs of planet formation.

By manipulating both phase and amplitude in pupil and focal planes, we will establish hybrid coronagraph systems that combine the strengths of individual concepts, and that can be adapted to the telescope mirror segmentation and the observational strategy. The proposed research will demonstrate the technologies necessary for building an instrument for the E-ELT that can successfully study rocky exoplanets in the habitable zones of nearby stars.","1499522","2016-04-01","2021-03-31"
"FAnFArE","Fourier Analysis For/And Partial Differential Equations","Frederic, Jérôme, Louis Bernicot","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""This project aims to develop the field of Harmonic Analysis, and more precisely to study problems at the interface between Fourier Analysis and PDEs (and also some Geometry). 
We are interested in two aspects of the Fourier Analysis:
 
(1) The Euclidean Fourier Analysis, where a deep analysis can be performed using specificities as the notion of ""frequencies"" (involving the Fourier transform) or the geometry of the Euclidean balls. By taking advantage of them, this proposal aims to pursue the study and bring novelties in three fashionable topics: the study of bilinear/multilinear Fourier multipliers, the development of the ""space-time resonances"" method in a systematic way and for some specific PDEs, and the study of nonlinear transport equations in BMO-type spaces (as Euler and Navier-Stokes equations).

(2) A Functional Fourier Analysis, which can be performed in a more general situation using the notion of ""oscillation"" adapted to a heat semigroup (or semigroup of operators). This second Challenge is (at the same time) independent of the first one and also very close. It is very close, due to the same point of view of Fourier Analysis involving a space decomposition and simultaneously some frequency decomposition. However they are quite independent because the main goal is to extend/develop an analysis in the more general framework given by a semigroup of operators (so without using the previous Euclidean specificities). By this way, we aim to transfer some results known in the Euclidean situation to some Riemannian manifolds, Fractals sets, bounded open set setting, ... Still having in mind some applications to the study of PDEs, such questions make also a connexion with the geometry of the ambient spaces (by its Riesz transform, Poincaré inequality, ...). I propose here to attack different problems as dispersive estimates, """"L^p""""-version of De Giorgi inequalities and the study of paraproducts, all of them with a heat semigroup point of view.""","940540","2015-06-01","2020-05-31"
"FANTAST","Frontiers of Analytic Number Theory And Selected Topics","Timothy Daniel Browning","UNIVERSITY OF BRISTOL","""This proposal sits at the interface of analytic number theory and selected topics, viewed through the prism of Diophantine equations defining higher-dimensional algebraic varieties.  A core part of the proposal involves using analytic methods (such as complex analysis, Fourier analysis and additive combinatorics) to tackle a range of problems about Diophantine equations. These include such
basic questions as precisely when  families of equations admit integer or rational solutions and, furthermore, how ``dense'' these solutions are when they exist. In the reverse direction, a significant component of the proposal is dedicated to established problems in number theory (such as stable cohomology of moduli spaces and uniform spectral gaps for arithmetic lattices) which can be tackled via the successful analysis of intermediary Diophantine equations.""","801187","2012-12-01","2017-11-30"
"FarCatCH","Innovative Strategies for Unprecedented Remote C-H bond Functionalization by Catalysis","Tatiana BESSET","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Over the last years, the landscape of the organic chemistry has been reshaped with impressive advances made in the transition metal-catalyzed carbon-hydrogen (C-H) bond functionalization field. Indeed, the functionalization of building blocks that do not display a reactive functional group but only a simple C-H bond is attractive as it avoids time-consuming and expensive prefunctionalization steps and limits the generation of waste. However, as energies required to break C-H bonds are similar, the differentiation between two C-H bonds and the selective functionalization of only one of them remain a key challenge. Therefore, the available approaches are still unsatisfactory due to important limitations: low reactivity, limited scopes and selectivity issues. In this proposal, a general approach to functionalize a CH bond located at a Far position (from a functional group) by Catalysis (FarCatCH) will be implemented with a special focus on underexplored transformations, affording important sulfur-and fluorine-containing compounds. Herein, I will develop new synthetic approaches for the remote functionalization of molecules based on i) a substrate-selectivity control and ii) the design of new catalysts using supramolecular tools. I will then iii) address a longstanding reactivity issue in organic synthesis: the trifluoromethylation of aliphatic compounds and apply the supramolecular catalysts for a remote enantioselective transformation.

Designing a full set of tools as Swiss army knife for the selective functionalization at unconventional positions inaccessible so far, can considerably change the way organic molecules are made. These original technologies will offer new synthetic routes to access original sulfur- and fluorine-containing molecules, compounds of interest in drugs discovery, material sciences, pharmaceutical and agrochemical industry.","1497996","2018-01-01","2022-12-31"
"FAST FILTERING","Fast Filtering for Computer Graphics, Vision and Computational Sciences","Raanan Fattal","THE HEBREW UNIVERSITY OF JERUSALEM","The world of digital signal processing, in particular computer graphics, vision and image processing, use linear and non-linear, explicit and implicit filtering extensively to analyze, process and synthesize images. Given nowadays high-resolution sensors, these operations are often very time consuming and are limited to devices with high-CPU power.

Traditional linear translation-invariant (LTI) transformations, executed using convolution, requires O(N^2) operations. This can be lowered to O(N \log N) via FFT over suitable domains. There are very few sets of filters to which optimal, linear-time, procedures are known. This situation is more complicated in the newly-emerging domain of non-linear spatially-varying filters. Exact application of such filter requires O(N^2) operations and acceleration methods involve higher space dimension introducing severe memory cost and truncation errors.

In this research proposal we intend to derive fast, linear-time, procedures for different types of LTI filters by exploiting a deep connection between convolution, spatially-homogeneous elliptic equations and the multigrid method for solving such equations. Based on this circular connection we draw novel prospects for deriving new multiscale filtering procedures.

A second part of this research proposal is devoted to deriving efficient explicit and implicit non-linear spatially-varying edge-aware filters. One front consists of the derivation of novel multi-level image decomposition that mimics the action of inhomogeneous diffusion operators. The idea here is, once again, to bridge the gap with numerical analysis and use ideas from multiscale matrix preconditioning for the design of new biorthogonal second-generation wavelets.

Moreover, this proposal outlines a new multiscale preconditioning paradigm combining ideas from algebraic multigrid and combinatorial matrix preconditioning. This intermediate approach offers new ways for overcoming fundamental shortcomings in this domain.","1320200","2013-08-01","2018-07-31"
"FastCode","The Next 100 Optimizing Compilers","Greta Yorsh","QUEEN MARY UNIVERSITY OF LONDON","Ideally, advances in hardware design would directly translate to performance or energy improvements in software. In reality, this involves a manual process of tuning a sophisticated production compiler or hardware-specific rewriting of code. This process is challenging even for the few experts who possess the required range of skills.  Moreover, any errors introduced in this process affect the entire software stack and likely compromise its reliability and security.

The aim of this project is to enable software to take full advantage of the capabilities of emerging microprocessor designs without modifying the compiler. 

Towards this end, we propose a new approach to code generation and optimization.  Our approach uses constraint solving in a novel way to generate efficient code for modern architectures and guarantee that the generated code correctly implements the source code.

Unlike existing superoptimization and synthesis methods, our approach shifts the entire search problem into the solver.  Tight integration with the solver provides a way to reuse reasoning steps and guide the solver using domain specific
information about the input program and the target architecture.  

This approach paves the way to employing recent advances in SMT solvers and has the potential to advance SMT solvers
further by providing a new category of challenging benchmarks that come from an industrial application domain.

I expect this project to revolutionize the way compilers perform hardware-specific optimizations.  It will eliminate an entire class of software errors and unrealized potential performance gains caused by modern optimizing compilers.  It will also aid hardware designers by providing greater flexibility for design explorations and faster deployment of new hardware.  Thus, this project will lead to significant improvement in performance and stability of software systems, as well as a fundamental impact on several scientific fields.","1500000","2019-12-01","2024-11-30"
"FASTO-CAT","Fundamentals of ASymmeTric Organo-CATalysis","Johannes Matthias Hunger","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","For most biologically relevant molecules their chirality is decisive for their function. Within the last two decades asymmetric organo-catalysis has emerged as an environmental benign, metal-free alternative for conventional asymmetric transition metal catalysis. The organo-catalysts, which employ catalyst-substrate interaction motifs commonly found for enzymes, yield unprecedented enantiomeric excesses. Despite the success of these organo-chemical routes, remarkably little is known about the molecular details of the interaction between the catalyst and the substrate. Consequently, there is virtually no rationale method to optimize reaction conditions particularly as related to structure-function relationships. Also the exact nature of the intermediates that induce chirality has remained elusive. The aim of this proposal is to experimentally quantify the formation of reaction intermediates and the nature of intermediate induced chirality that lie at the heart of asymmetric control. This will be achieved by using a combination of advanced spectroscopic techniques. With advanced vibrational spectroscopies (ultrafast two-color and two-dimensional infrared spectroscopy), dielectric spectroscopy, and NMR spectroscopy together with quantum chemical calculations we will quantify structure-dependent interactions: binding geometry, strength of attraction, lifetime of binding, reaction intermediates, and the role of steric repulsion, probed on all timescales relevant to catalytic processes ranging from femtoseconds to seconds. Correlation of such information with the enantiomeric excess obtained in catalytic processes will allow isolating the essential ingredients for stereocontrol. Such molecular-level insights will provide fundamental parameters for optimization of reaction conditions and will initiate the transition from a trial and error approach towards a rational design of new catalytic processes.","1892500","2017-04-01","2022-03-31"
"FattyCyanos","Fatty acid incorporation and modification in cyanobacterial natural products","Pedro LEÃO","CIIMAR - Centro Interdisciplinar de Investigação Marinha e Ambiental","Known, but mostly novel natural products (NPs) are in high demand – these are used in drugs, cosmetics and agrochemicals and serve also as research tools to probe biological systems. NP structures inspire chemists to develop new syntheses, and NP biosynthetic enzymes add to the metabolic engineer’s toolbox. The advent of next generation DNA-sequencing has revealed a vastly rich pool of NP biosynthetic gene clusters (BGCs) among bacterial genomes, most of which with no corresponding NP. Hence, opportunities abound for the discovery of new chemistry and enzymology that has the potential to push the boundaries of chemical space and enzymatic reactivity. Still, we cannot reliably predict chemistry from BGCs with unusual organization or encoding unknown functionalities, and, for molecules of unorthodox architecture, it is difficult to anticipate how their BGCs are organized. It is the valuable, truly novel chemistry and biochemistry that lies on these unexplored connections, that we aim to reveal with this proposal. To achieve it, we will work with a chemically-talented group of organisms – cyanobacteria, and with a specific structural class – fatty acids (FAs) – that is metabolized in a quite peculiar fashion by these organisms, paving the way for NP and enzyme discovery. On one hand, we will exploit the unique FA metabolism of cyanobacteria to develop a feeding strategy that will quickly reveal unprecedented FA-incorporating NPs. On the other, we will scrutinize the intriguing biosynthesis of three unique classes of metabolites that we have isolated recently and that incorporate and modify FA-moieties. We will find the BGCs for these compounds and dissect the functionality involved in such puzzling modifications to uncover important underlying enzymatic chemistry. This proposal is a blend of discovery- and hypothesis-driven research at the NP chemistry/biosynthesis interface that draws on the experience of the PI’s work on different aspects of cyanobacterial NPs.","1462938","2018-01-01","2022-12-31"
"FBRAIN","Computational Anatomy of Fetal Brain","François Rousseau","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Studies about brain maturation aim at providing a better understanding of brain development and links between brain changes and cognitive development. Such studies are of great interest for diagnosis help and clinical course of development and treatment of illnesses. Several teams have begun to make 3D maps of developing brain structures from children to young adults. However, working out the development of fetal and neonatal brain remains an open issue. This project aims at jumping over several theoretical and practical barriers and at going beyond the formal description of the brain maturation thanks to the development of a realistic numerical model of brain aging. In this context, Magnetic Resonance (MR) imaging is a fundamental tool to study structural brain development across age group. We will rely on new image processing tools combining morphological information provided by T2-weighted MR images and diffusion information (degree of myelination and fiber orientation) given by diffusion tensor imaging (DTI). The joint analysis of these anatomical features will stress the generic maturation of normal fetal brain. We will first rely on mathematical models to allow reconstruction of high resolution 3D MR images in order to extract relevant features of brain maturation. The results issued from this first step will be used to build statistical atlases and to characterize the neuroanatomical differences between a reference group and the population under investigation. From a methodological point of view, our approach relies on an interdisciplinary research framework aiming at combining medical research to neuroimaging, image processing, statistical modelling and computer science. The robust characterization of the anatomical features of fetal brain and the development of a realistic model of brain maturation from biological concepts will come out from the strong interactions between these different research fields.","753393","2008-09-01","2013-08-31"
"FDML-RAMAN","Stimulated Raman analysis and Raman microscopy with Fourier Domain Mode Locked (FDML) laser sources","Robert Alexander Huber","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","Raman spectroscopy is one of the most specific non-destructive optical techniques to identify the chemical composition of a sample. Further, there is great hope that in the future it may be a powerful biomedical imaging technique for in vitro or in vivo microscopy, providing molecular contrast without exogenous contrast agents.

However, due to the small Raman cross-section, for many applications the acquisition is prohibitively slow. Techniques to solve this problem and to increase the Raman signal levels are coherent anti-Stokes Raman spectroscopy (CARS), surface enhanced Raman spectroscopy (SERS) and stimulated Raman spectroscopy (SRS). However, in many cases, they are currently not able to provide rapid, highly sensitive detection of an undistorted signal with a broad spectral coverage.

The aim of the project is to investigate Fourier domain mode locked (FDML) lasers for the application to stimulated Raman detection. A variety of physical effects, unique to FDML lasers, enables strategies to substantially increase the Raman signal level. This can provide access to highly sensitive Raman spectroscopy and high speed Raman microscopy. The techniques to increase the detection sensitivity include concepts like single- and double-resonant enhancement cavities, high power fibre amplification, dynamic spectral zooming, advanced modulation schemes and parallel designs.

The first part of the project addresses a comprehensive understanding of the underlying physical effects and how to increase the Raman signal by several orders of magnitude using these various strategies. The aim of the second part is to investigate, in how far these improved FDML based Raman systems can be applied to transient real time spectroscopy, analytical sensing, and Raman microscopy.","1168058","2011-01-01","2015-12-31"
"FDP-MBH","Fundamental dynamical processes near massive black holes in galactic nuclei","Tal Alexander","WEIZMANN INSTITUTE OF SCIENCE LTD","""I propose to combine analytical studies and simulations to explore fundamental open questions in the dynamics and statistical mechanics of stars near massive black holes. These directly affect key issues such as the rate of supply of single and binary stars to the black hole, the growth and evolution of single and binary massive black holes and the connections to the evolution of the host galaxy, capture of stars around the black hole, the rate and modes of gravitational wave emission from captured compact objects, stellar tidal heating and destruction, and the emergence of """"exotic"""" stellar populations around massive black holes. These processes have immediate observational implications and relevance in view of the huge amounts of data on massive black holes and galactic nuclei coming from earth-bound and space-borne telescopes, from across the electromagnetic spectrum, from cosmic rays, and in the near future also from neutrinos and gravitational waves.""","880000","2008-09-01","2013-08-31"
"FEASIBLe","Finding how Earthquakes And Storms Impact the Building of Landscapes","Philippe STEER","UNIVERSITE DE RENNES I","Unravelling how tectonics, climate and surface processes act and interact to shape the Earth’s surface is one of the most challenging unresolved issue in Earth Sciences. The foundations of modern quantitative geomorphology have been built within the paradigm of steady-state landscapes responding to slow changes in climatic or tectonic conditions, mainly rainfall or uplift rate. Yet, recent results demonstrate that landscapes are rhythmed by (potentially extreme) storms and earthquakes. These perturbations catalyse geomorphological processes by triggering numerous landslides and lead to a prolonged and transient evolution of the landscape that dominate records of modern erosion. The FEASIBLe project therefore calls for a complete re-assessment of the role of short-term climatic and tectonic perturbations in shaping mountain landscapes and for a paradigm shift from steady-state to constantly perturbed landscapes. My ambition is to push forward our understanding of the short- to long-term dynamics of perturbed landscapes and in turn to unlock our ability to read landscapes in terms of earthquake and storm activity. To succeed in this endeavour, the FEASIBLe project will rely on the development of a new generation of landscape evolution model and of novel approaches to intimately monitor landscape heterogeneities and evolution in Taiwan, New-Zealand and Himalayas at high-resolution. The first work packages (WP1-2) will combine field-data analysis and numerical modelling to investigate landslide triggering and the post-perturbation sediment evacuation and landscape dynamics. I will then blend these elementary processes with a statistical description of climatic and tectonic perturbations in a new generation of landscape evolution model (WP3). This new model will be then applied to diagnose the geomorphological signature of fault “seismogenic” rheology (WP4) and to explore the role of post-glacial hot-moments of landscape dynamics on Quaternary landscape evolution (WP5).","1498829","2019-06-01","2024-05-31"
"FEEDGALAXIES","A new vantage point on how gas flows regulate the build-up of galaxies in the early universe","Michele FUMAGALLI","UNIVERSITY OF DURHAM","Galaxies reside within a web of gas that feeds the formation of new stars. Following star formation, galaxies eject some of their gas reservoir back into this cosmic web. This proposal addresses the fundamental questions of how these inflows and outflows regulate the evolution of galaxies. My research team will tackle two key problems: 1) how gas accretion regulates the build-up of galaxies; 2) how efficiently outflows are in removing gas from star-forming regions. To characterise these flows across five billion years of cosmic history, we will pursue cutting-edge research on the halo gas, which is the material around the central galaxies, within dark matter halos. We will focus on scales ranging from a few kiloparsecs, where outflows originate, up to hundreds of kiloparsecs from galaxies, where inflows and outflows have visible impacts on halos. We will attack this problem using both simulations and observations with the largest telescopes on the ground and in space. With novel applications of absorption spectroscopy, we will gain a new vantage point on the astrophysics of these gas flows. Exploiting unprecedented datasets that I am currently assembling thanks to ground-breaking developments in instrumentation, we will directly connect the properties of halo gas to those of the central galaxies, investigating the impact that the baryonic processes probed in absorption have on galaxies seen in emission. In parallel, using new hydrodynamic simulations and radiative transfer calculations, we will go beyond present state-of-the-art methodologies to unveil the theory behind the origin of these gas flows, a crucial aspect to decode the physics probed by our observations. As a result of this powerful synergy between observations and simulations, this programme will provide the most advanced analysis of the impact that inflows and outflows have on galaxy evolution, shaping the direction of future work at 40m telescopes and the next generation of cosmological simulations.","1499557","2018-02-01","2023-01-31"
"FeelAgain","Restoring natural feelings from missing or damaged peripheral nervous system by model-driven neuroprosthesis","Stanisa RASPOPOVIC","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Due to sensory loss diabetic patients are prone to falls and to foot ulcers, which consequently increase the risk of amputations. Because of the lack of sensory feedback amputees experience falls, perceive the prosthesis as a foreign body and therefore do not rely on it during walking. This causes counterbalancing movements that increase fatigue. Both types of patients suffer neuropathic pain, associable to aberrant sensory inputs. Neural pathways between the periphery and the brain are still functional above the damage or the amputation. Targeting these structures with peripheral neural interfaces could allow the restoration of natural sensory functionalities. The aim of project is to develop the first neuroprosthesis restoring natural foot sensations, through sciatic nerve stimulation, to patients with diabetic neuropathy or leg amputation. To that aim we will develop a detailed computational model of the sensory loop of the sciatic nerve. It will merge the electrical stimulation effects on sensory fibers and transduction of mechanical deformations of the skin into action potentials. Modelling results will be validated. Applying this modeling framework we will optimize the geometry of a peripheral neural interface, its surgical placement and define stimulation protocols that mimic natural sensory feedback responses. Effective device for feedback restoration will be constructed, able to translate the signals recorded by sensorized sole placed under the prosthetic or diabetic foot into the natural foot sensations perceived by subject. The interventional tools for embodiment boosting and pain relief will be developed. Clinical tests on amputee and diabetic subjects will assess the efficacy of the FeelAgain conceptual and technological framework by examination of pain, embodiment, ulcer prevention, falls avoidance and walking ability.","1499637","2018-04-01","2023-03-31"
"FELICITY","Foundations of Efficient Lattice Cryptography","Vadim Lyubashevsky","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","Public key cryptography is the backbone of internet security. Yet it is very likely that within the next few decades some government or corporate entity will succeed in building a general-purpose quantum computer that is capable of breaking all of today's public key protocols. Lattice cryptography, which appears to be resilient to quantum attacks, is currently viewed as the most promising candidate to take over as the basis for cryptography in the future.  Recent theoretical breakthroughs have additionally shown that lattice cryptography may even allow for constructions of primitives with novel capabilities.  But even though the progress in this latter area has been considerable, the resulting schemes are still extremely impractical. 

The central objective of the FELICITY project is to substantially expand the boundaries of efficient lattice-based cryptography.  This includes improving on the most crucial cryptographic protocols, some of which are already considered practical, as well as pushing towards efficiency in areas that currently seem out of reach.  The methodology that we propose to use differs from the bulk of the research being done today.   Rather than directly working on advanced primitives in which practical considerations are ignored, the focus of the project will be on finding novel ways in which to break the most fundamental barriers that are standing in the way of practicality.  For this, I believe it is productive to concentrate on building schemes that stand at the frontier of what is considered efficient -- because it is there that the most critical barriers are most apparent.  And since cryptographic techniques usually propagate themselves from simple to advanced primitives, improved solutions for the fundamental ones will eventually serve as building blocks for practical constructions of schemes having advanced capabilities.","1311688","2015-10-01","2020-09-30"
"FEMTOELEC","Innovative Femtosecond laser-plasma based electron source for studying ultrafast structural dynamics","Jerome Faure","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","How do atoms move in a solid? How long does it take for a phase transition to occur or for a molecule to change its configuration? These are some of the fundamental questions that the field of ultrafast science asks and attempts to answer. Understanding these ultrafast processes in complex matter at the atomic scale requires advanced sources of radiation: X-rays or electrons with sub-angstrom wavelength and femtosecond duration.
In the past decade, such sources have become available, allowing scientists to obtain a first glimpse into the ultrafast world, with the direct observation of atomic motion or structural changes in matter. Until now however, the time resolution has not allowed us to study the fastest processes and has limited our window of observation to processes slower than 100 femtoseconds.
To overcome this limitation, this proposal introduces a new method based on laser-plasma interaction for producing an electron source with shorter duration. The project will explore laser-plasma interaction in a new regime: low energy, high-repetition, few-cycle laser pulses interacting with a plasma for producing femtosecond electron bunches with parameters relevant for probing matter with electron diffraction. It will take advantage of the very high accelerating gradients that plasmas can sustain for accelerating electrons to relativistic energies in micrometer lengths.
This novel electron source will be implemented in diffraction experiments for probing structural dynamics in condensed matter with angstrom spatial resolution and unprecedented time resolution. This table-top innovative electron source has the potential to overcome the limitations of current ultrafast electron diffraction and could offer new insights for transdisciplinary applications in condensed matter physics, chemistry and biology.","1491350","2013-01-01","2017-12-31"
"FEMTOMAGNETISM","Femtosecond Laser Control of Spins in Magnetic Materials: from fundamentals to nanoscale dynamics","Alexey Voldemarovitsj Kimel","STICHTING KATHOLIEKE UNIVERSITEIT","The aim of the project is to develop femtosecond optical control of magnetism: a new area at the junction of coherent nonlinear optics, near-field optics and magnetism. In particular, I am aiming to investigate nonthermal effects of light on magnetic order and to apply this knowledge for highly efficient ultrafast (10-12 seconds and faster) optical control of magnetism at the nanoscale.
The ever increasing demand for faster information processing has triggered an intense search for ways to manipulate magnetically stored bits at the ultimately short time-scale. Although efficient, ultrafast and nonthermal laser control of magnetism may open new prospect of magnetic data storage and manipulation, many fundamental questions concerning the mechanisms that are responsible for the nonthermal effect of photons on spins and ultrafast laser induced changes of magnetic order are poorly understood. This is mainly because an ultrashort laser pulse brings a medium into a strongly non-equilibrium state where conventional description of magnetic phenomena in terms of thermodynamics is no longer valid. In this proposal I am planning to address these fundamental questions using novel experimental approaches for both the excitation and observation of magnetism on an ultrafast timescale. In particular, the proposal involves: a) development of polarization pulse shaping, where specially shaped laser pulses yield control over coherent optical excitations in a medium; b) exploring the ultrafast response of magnetic order with advanced optical and X-ray techniques.
The ultimate goal is to combine the fundamental knowledge of femtosecond opto-magnetism obtained in this project with the methods of near-field optics to achieve ultrafast control of spins in magnetic nanostructures.","1500000","2010-12-01","2015-11-30"
"FEMTOSCOPY","Femtosecond Raman Spectroscopy: ultrafast transformations in physics, chemistry and biology","Tullio Scopigno","UNIVERSITA DEGLI STUDI DI ROMA LA SAPIENZA","We propose the construction and development of a femtosecond broadband stimulated Raman setup to tackle ultra fast chemical, physical and biological processes taking advantage of the top-notch structural sensitivity inherent to the Raman process. The use of a pump-probe stimulated scheme will allow to overcome time-energy restrictions dictated by the uncertainty principle, enabling to reach the femtosecond timescale with energy resolutions which would pertain to the picosecond time domain in the Heisenberg sense. Protein dynamics span several orders of magnitude extending up to macroscopic timescales, the recipes to tailor properties of rubbers and polymers relevant for human timescales are covered by more than 500000 patents, rust reaction occurs over several days, and lethal brain strokes often lead to death within 24 hours on average. The lowest hierarchical level of such processes, however, is hidden in the very act of atomic motion and chemical binding such as the single bond dynamics in a peptide backbone, the monomer cross-linking elemental reactions, the energy flow and re-distribution in a hydrogen bond network, or the oxygen binding to heme proteins, all performing on the femtosecond stage. Mastering these processes is the essence of femtochemistry, born around the backbone of the femtosecond laser technology and boosted by scientific activity which led to the Nobel prize of Prof. A. Zewail in 1999. The new capabilities offered by femtosecond sources have often left behind in the race traditional spectroscopies, which hardly follow the growing emergence of new challenging problems in which the traditional distinction between biology, chemistry and physics is smeared out by the common ultra short timescale. The set up of a non conventional femtosecond Raman technique will be the initiating event for the establishment of a research group of interdisciplinary nature toiling over unsolved problems in which the ultrafast facet plays a key role.","1544400","2008-09-01","2013-08-31"
"FERMILATT","Single-atom-resolved detection and manipulation of strongly correlated fermions in an optical lattice","Stefan Kuhr","UNIVERSITY OF STRATHCLYDE","I propose to realize single-atom- and spin-resolved in-situ imaging of strongly correlated fermions in an optical lattice. Whereas very recently strongly correlated bosonic systems could be imaged in an optical lattice at the single atom level, an experimental proof of single-site-resolved detection of fermions is still lacking. My project will allow to fully exploit the potential of ultracold atoms as a quantum simulator, especially for the Fermi-Hubbard model, which is a key model in condensed matter physics.
Gaining access to the in-trap atom distribution of the fermionic 40-potassium with single-atom and single-site resolution will allow for a new generation of experiments in the field. Direct observation of individual atoms and analysis of their quantum states and their spatial order in the lattice, including individual defects, are then possible. I will use this novel detection method to characterize, e.g., temperature or entropy distribution of the quantum phases such as fermionic  Mott insulators, Band insulators or  metallic phases.
Together with the possibility of local spin manipulations, I will investigate the effect of local perturbations on the system by spatially resolving the ensuing dynamical in-trap evolution. In this way, propagation and healing of artificially created defects can be studied. Local scale density modulations such as Friedel and Wigner oscillations of one-dimensional systems with hard boundaries will become observable. The local manipulation of the trapped atoms will be the key to implement novel cooling schemes that can remove regions of high entropy from the system. In this way much colder temperatures can be realized, where antiferromagnetic ordering is setting in. In a harmonic trap, these magnetically ordered phases are predicted to form ring-like structures, which can be ideally characterized by my novel spin-sensitive in-situ imaging techniques.","1392800","2011-10-01","2016-09-30"
"FEVER","Forecasting the recurrence rate of volcanic eruptions","Luca Caricchi","UNIVERSITE DE GENEVE","Volcanic eruptions occur with a frequency that is inversely proportional to their magnitude. Datasets of natural volcanic events, currently used to determine the recurrence rate of volcanic eruptions are intrinsically biased. Combining physical modelling of processes with detailed statistical analysis has been demonstrated essential for assessing reliably the recurrence rate of natural hazards, such as floods and earthquakes. This would be the first attempt to apply a similar, integrated approach to explosive volcanic eruptions.

 The high-gain final target of FEVER is to produce a physically based statistical model able to ForEcast the recurrence rate of Volcanic Eruptions both at regional and global scale. This is the first project of this kind and consequently involves a significant risk. Because 500 million people live in proximity of volcanoes and eruptions have a significant social and economical impact, forecasting the recurrence rate of volcanic eruption remains a great challenge in Science.

 This project builds on two main directions of my research: a) Thermo-mechanical and statistical modelling targeting the identification of the main physical factors controlling the recurrence rate of volcanic eruptions. We showed that the flux of magma from depth directly controls the magnitude of the largest possible eruptions. Thus, b) we developed a novel method to determine such magma fluxes. These two lines of research combine perfectly in FEVER and will be integrated to answer questions such as: What is the probability of an eruption similar to the Tambora 1815 to occur in the next 100 years on Earth or in Europe? What is the largest physically possible eruption that can occur in Europe?
 
 The high-gain target of FEVER is to mitigate the impact of volcanic eruptions on our society, by producing research of interest for governmental agencies dealing with location of strategic infrastructures, and for businesses such as aviation.","1458192","2016-04-01","2021-03-31"
"FibreRemodel","Frontier research in arterial fibre remodelling for vascular disease diagnosis and tissue engineering","Caitriona Lally","THE PROVOST, FELLOWS, FOUNDATION SCHOLARS & THE OTHER MEMBERS OF BOARD OF THE COLLEGE OF THE HOLY & UNDIVIDED TRINITY OF QUEEN ELIZABETH NEAR DUBLIN","Each year cardiovascular diseases such as atherosclerosis and aneurysms cause 48% of all deaths in Europe. Arteries may be regarded as fibre-reinforced materials, with the stiffer collagen fibres present in the arterial wall bearing most of the load during pressurisation. Degenerative vascular diseases such as atherosclerosis and aneurysms alter the macroscopic mechanical properties of arterial tissue and therefore change the arterial wall composition and the quality and orientation of the underlying fibrous architecture. Information on the complex fibre architecture of arterial tissues is therefore at the core of understanding the aetiology of vascular diseases. The current proposal aims to use a combination of in vivo Diffusion Tensor Magnetic Resonance Imaging, with parallel in silico modelling, to non-invasively identify differences in the fibre architecture of human carotid arteries which can be directly linked with carotid artery disease and hence used to diagnose vulnerable plaque rupture risk.
Knowledge of arterial fibre patterns, and how these fibres alter in response to their mechanical environment, also provides a means of understanding remodelling of tissue engineered vessels. Therefore, in the second phase of this project, this novel imaging framework will be used to determine fibre patterns of decellularised arterial constructs in vitro with a view to directing mesenchymal stem cell growth and differentiation and creating a biologically and mechanically compatible tissue engineered vessel. In silico mechanobiological models will also be used to help identify the optimum loading environment for the vessels to encourage cell repopulation but prevent excessive intimal hyperplasia.
This combination of novel in vivo, in vitro and in silico work has the potential to revolutionise approaches to early diagnosis of vascular diseases and vascular tissue engineering strategies.","1521875","2015-09-01","2020-08-31"
"FIELDGRADIENTS","Phase Transitions and Chemical Reactions in Electric Field Gradients","Yoav Tsori","BEN-GURION UNIVERSITY OF THE NEGEV","We will study phase transitions and chemical and biological reactions in liquid mixtures
in electric field gradients. These new phase transitions are essential in statistical
physics and thermodynamics. We will examine theoretically the complex and yet unexplored
phase ordering dynamics in which droplets nucleate and move under the external nonuniform
force. We will look in detail at the interfacial instabilities which develop when the
field is increased. We will investigate how time-varying potentials produce
electromagnetic waves and how their spatial decay in the bistable liquid leads to phase
changes.
These transitions open a new and general way to control the spatio-temporal behaviour of
chemical reactions by directly manipulating the solvents' concentrations. When two or more
reagents are preferentially soluble in one of the mixture's components, field-induced
phase separation leads to acceleration of the reaction. When the reagents are soluble in
different solvents, field-induced demixing will lead to the reaction taking place at a
slow rate and at a two-dimensional surface. Additionally, the electric field allows us to
turn the reaction on or off. The numerical study and simulations will be complemented by
experiments. We will study theoretically and experimentally biochemical reactions. We will
find how actin-related structures are affected by field gradients. Using an electric field
as a tool we will control the rate of actin polymerisation. We will investigate if an
external field can damage cancer cells by disrupting their actin-related activity. The above
phenomena will be studied in a microfluidics environment. We will elucidate the separation
hydrodynamics occurring when thermodynamically miscible liquids flow in a channel and how
electric fields can reversibly create and destroy optical interfaces, as is relevant in
optofluidics. Chemical and biological reactions will be examined in the context of
lab-on-a-chip.","1482200","2010-08-01","2015-07-31"
"FIELDS-KNOTS","Quantum fields and knot homologies","Piotr Sulkowski","UNIWERSYTET WARSZAWSKI","This project is concerned with fundamental problems arising at the interface of quantum field theory, knot theory, and the theory of random matrices. The main aim of the project is to understand two of the most profound phenomena in physics and mathematics, namely quantization and categorification, and to establish an explicit and rigorous framework where they come into play in an interrelated fashion. The project and its aims focus on the following areas:

- Knot homologies and superpolynomials. The aim of the project in this area is to determine homological knot invariants and to derive an explicit form of colored superpolynomials for a large class of knots and links.

- Super-A-polynomial. The aim of the project in this area is to develop a theory of the super-A-polynomial, to find an explicit form of the super-A-polynomial for a large class of knots, and to understand its properties.

- Three-dimensional supersymmetric N=2 theories. This project aims to find and understand dualities between theories in this class, in particular theories related to knots by 3d-3d duality, and to generalize this duality to the level of homological knot invariants.

- Topological recursion and quantization. The project aims to develop a quantization procedure based on the topological recursion, to demonstrate its consistency with knot-theoretic quantization of A-polynomials, and to generalize this quantization scheme to super-A-polynomials.

All these research areas are connected via remarkable dualities unraveled very recently by physicists and mathematicians. The project is interdisciplinary and aims to reach the above goals by taking advantage of these dualities, and through simultaneous and complementary development in quantum field theory, knot theory, and random matrix theory, in collaboration with renowned experts in each of those fields.","1345080","2013-12-01","2018-11-30"
"FIN","Theory of Fundamental Interactions at the Nanoscale","","THE UNIVERSITY OF NOTTINGHAM","At the heart of this multi-disciplinary research project lie two emerging prominent theoretical models developed by the applicant in the past 12 months, which underpin the fundamental interactions taking place at the nanometre scale. In 2010, the applicant proposed a general solution to the fundamental problem of the attraction between like-charged dielectric nanoparticles. This is the first time a comprehensive solution to this problem has been presented, and it has the potential to transform our understanding of how charged nanoparticles interact in the gas phase and solutions.

Studies of nanoparticles have opened new avenues for exploration of the principles that underpin the transition from the gas phase to the solid state. The capability of nanoparticles to modify their shape in order to minimize the free energy leads to structure modifications that can be observed on a time scale accessible by electron microscopy techniques. A unique computational methodology has been developed by the applicant, which has an advantage over the state-of-the-art image simulation techniques in its ability to simulate the dynamics of structural transformations under the influence of the electron beam.

The proposed core theoretical frameworks are central tools of the project. Their fundamental nature offers solutions to problems across wide-ranging disciplines. The models will be advanced during the project and introduced to the experts in the application areas in order to find solutions to a number of common problems, which to date remain un-solved. The application areas, which will be addressed, include the electrostatic charging of pharmaceutical powders during manufacture and handling; the charge scavenging in the formation of solar systems; self-assembly of charged nanoparticles in solutions; proton transfer in biological molecules; structure-property correlations of nanomaterials; and design of innovative oxidation catalysts using inorganic polyoxometalates.","1400341","2013-01-01","2018-06-30"
"FIRST","The first stars and galaxies","Raffaella Schneider","ISTITUTO NAZIONALE DI ASTROFISICA","The FIRST proposal has the goal of investigating the nature and properties of the first stars and galaxies. The project will exploit synergies between observational cosmology, galaxy formation and stellar evolution.
Observations made using large ground-based and space-borne telescopes have probed cosmic history all the way from the present-day to ~ 700 million years after the Big Bang. Earlier on lies the remaining frontier, where the first stars and galaxies formed. Data collected in the last decade have revealed that the Universe at redshift ~ 7-8 is already mature, with galaxies and quasars already formed, many with metal-rich and dust-rich signatures of even earlier generations of stars. Despite these remarkable progresses, the nature of the first stars and black holes, and the impact they had on their environment and on the properties of the first galaxies and quasars remain largely unknown.
The proposed research program aims to address two main scientific objectives:
(1) Understand the evolution of the first metals and dust and their role in setting the characteristic masses of stars and seed black holes;
(2) Assess the properties of the first galaxies and quasars.
These goals will be addressed following an interdisciplinary approach which involves theoretical models of stellar evolution and nucleosynthesis, semi-analytical and numerical models of galaxy evolution, and detailed comparison with observational data from surveys at low and high redshifts. New numerical techniques will be developed, with potential applications that go beyond the primary goals of FIRST. The results will provide new insights on the nature of the first stars and galaxies and will be of important guidance for interpreting data from ongoing surveys (HST, Spitzer, IRAM, ALMA) and for the preparation of key programs with future large telescopes (ELT, JWST).","882808","2012-10-01","2017-09-30"
"FirstDawn","Imaging the cosmic dawn and the first galaxies with 21cm and atomic line intensity mapping","Jonathan Robin Pritchard","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","""Modern astrophysics has pushed the observational frontier to a time a billion years after the Big Bang.  Lying beyond this frontier is the period when the first stars and galaxies formed, whose light heated and ionized the Universe in the process known as reionization.  Understanding this """"epoch of reionization"""" would fill in a key missing period in our picture of the history of the Universe.  Existing observational techniques have scratched the surface, but new observational techniques are required to truly understand this early period of galaxy formation.  My work will lay the theoretical foundations for three novel probes of this period - 21 cm tomography, the 21 cm global signal,  and line intensity mapping - that would enable three dimensional maps of the epoch of reionization.  If realized through challenging radio-frequency observations, these techniques would transform our understanding of the first galaxies.

Through this ERC starting grant, I will build the theoretical framework needed to predict and interpret observations of line emission from gas in and surrounding the first generation of galaxies.  My team will aim to develop models of the interplay between radiation from the first galaxies and the heating, ionization, and illumination of hydrogen gas that lies in the space between galaxies.  At the same time, we will build models of the formation and properties of the atomic and molecular gas that fills the space inside galaxies.  By combining probes of this """"inner"""" and """"outer"""" space a complete nature of galaxy formation during the first billion years might be achieved. Analysis of sky averaged 21 cm observations will complement this with a broad overview of galaxies back to a few hundred million years after the big bang. This work will provide a clear theoretical road map to guide the design of next generation radio telescopes, such as the Square Kilometer Array, to achieve this ambitious goal.""","1495220","2015-04-01","2020-03-31"
"FIRSTLIGHT","Unveiling first light from the infant Universe","Luitje Vincent Ewoud Koopmans","RIJKSUNIVERSITEIT GRONINGEN","I request ERC funding to set up a dedicated science team to detect, for the first time, the redshifted 21-cm radio line emission of neutral hydrogen (HI) with LOFAR coming from the first billion years of the age of the Universe (the  Epoch of Reionization  and the  Dark Ages ).

The study of this pristine neutral hydrogen gas is a rapidly emerging field of astrophysics, both theoretically and observationally. A number of expert international groups in the US/Australia (MWA), China (21CMA), India (GMRT) and the Netherlands (LOFAR) are contending to be the first to detect this hydrogen gas. My proposed ERC project is high-risk and high-gain; however, all risks are controlled and the scientific rewards of detection of neutral hydrogen at these early times would have a tremendous impact and open a new frontier in astronomy. The study of neutral hydrogen, as in the nearby Universe, will revolutionize our knowledge of astrophysical processes in the first phases of the Universe, just after recombination.

The LOFAR Epoch-of-Reionization Key-Science Project (LOFAR EoR-KSP), of which I am a PI, aims to be the first, and if being the first fails, to provide the best detection of this neutral HI gas. Indeed, we are in a very good starting position to reach both goals. Our team has access to the most sensitive telescope available for these studies (LOFAR) and leads a Key Science Project with guaranteed observing time. Our KSP is rapidly ramping up to the observational phase of the project (2010), and now more than ever requires dedicated scientists that together in a small team maximize the scientific return of the project (i.e. detect and study HI). If successful, our research team would be in a position to start leading similar projects with the Square Kilometer Array (SKA). It is crucial that we gear up for the use of that future instrument and retain Europe s position at the forefront of astrophysics and radio astronomy.","1500000","2010-10-01","2016-09-30"
"FISNT","Frontiers of Integrated Silicon Nanophotonics in Telecommunications","Jeremy Witzens","RHEINISCH-WESTFAELISCHE TECHNISCHE HOCHSCHULE AACHEN","In the last decade, Silicon Photonics has been a rapidly growing field fueled by the promise of highly scalable, ultra-low power, high bandwidth and low cost silicon based optical communication systems. The last few years have seen the emergence of several dedicated world-class research groups, a dedicated international conference, heavy investments by the semiconductor industry giants, multiple private equity funded start-ups, as well as dedicated multi-user foundry services. Nevertheless, several critical roadblocks remain that have so far prevented the field from displacing older optical technologies, the resolution of which presents extremely challenging scientific challenges, requiring highly innovative devices and system architectures as well as bleeding edge process development. In a nutshell, state-of-the-art Silicon Photonics remains marginally too expensive for ultra-short distance links, too low performance for long haul communications, and still has too high a power consumption to displace electrical interconnects at the circuit board level. It is the goal of this proposal to reach three key milestones that in the applicant’s opinion are critical enablers for the field on its path towards becoming a truly disruptive technology.","1917080","2011-10-01","2017-09-30"
"FLAT SURFACES","SL(2,R)-action on flat surfaces and geometry of extremal subvarieties of moduli spaces","Martin Moeller","JOHANN WOLFGANG GOETHE-UNIVERSITATFRANKFURT AM MAIN","Dynamics on polygonal billiard tables is best understood by unfolding the table and studying the resulting flat surface. The moduli space of flat surfaces carries a natural action of SL(2,R) and all the questions about Lie group actions on homogeneous spaces reappear in this
non-homogeneous setting in an even more interesting way.
Closed SL(2,R)-orbits give rise to totally geodesic
subvarieties of the moduli space of curves, called
Teichmueller curves. Their classifcation is a major goal over the coming years. The applicant's algebraic characterization of Teichmueller curves plus the comprehension of the Deligne-Mumford compactification of Hilbert modular varities
make this goal feasible.
on polygonal billiard tables is best understood
unfolding the table and studying the resulting
surface. The moduli space of flat surfaces carries
action of SL(2,R) and all the questions about
group actions on homogeneous spaces reappear in this homogeneous setting in an even more interesting way.
SL(2,R)-orbits give rise to totally geodesic
of the moduli space of curves, called
curves. Their classifcation is a major goal
the coming years. The applicant's algebraic characterization Teichmueller curves plus the comprehension of the Mumford compactification of Hilbert modular varities this goal feasible.","1005600","2010-10-01","2015-09-30"
"FLATLIGHT","Functional 2D metamaterials at visible wavelengths","Patrice Genevet","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","For the last 15 years, optics has undergone a remarkable evolution towards ever decreasing sizes, better integration in complex systems, and more compact devices readily available to mass markets. Whereas traditional optics is at the centimeter scale, newly developed techniques use nanoscale objects to control, guide, and focus light. From the capability to shape metallic and dielectric nanostructures has emerged the field of nanophotonics. 
Advances in nanophotonics offer the possibility to control the material’s optical properties to create artificial materials with electromagnetic properties not found in nature. Man-made 3D metamaterials have interesting fundamental aspects and present many advantages with respect to conventional devices. Unexpected effects have led to the development of interesting applications like high resolution lenses and cloaking devices. 
Inspired by this new technology, we have developed new 2D metamaterials. Our flat metamaterials (metasurfaces) are much simpler to manufacture than their 3D counterparts. By depositing a set of nanostructures at an interface, we can immediately control the light properties; unlike refractive optical components, the wavefront is modified without propagation. As of today, these interfaces are created using metallic nanostructures and work in the infrared. In this ERC, we plan to extend the concept of optical metasurfaces in the visible which is the most important wavelength range for applications. By combining with optically active semiconductors such as InGaAlN, we will add optical gain and modulation capability to the system to create new, efficient optoelectronic devices. The response of the metasurfaces is tunable by changing the environment surrounding the nanostructures. We will use this property to create ultrathin reconfigurable flat devices. Metasurfaces will be integrated with AlN/GaN to modulate light at high frequencies and further exploited to control polariton gases in solid state metasystems.","2000000","2015-09-01","2020-10-31"
"FLATOUT","From Flat to Chiral: A unified approach to converting achiral aromatic compounds to optically active valuable building blocks","Nuno Xavier Dias Maulide","UNIVERSITAT WIEN","""The stereoselective preparation of enantioenriched organic compounds of  high structural complexity and synthetic value, in an economically viable and expeditious manner, is one of the most important goals in contemporary Organic Synthesis. In this proposal, I present a unified and conceptually novel approach for the conversion of flat, aromatic heterocycles into highly valuable compounds for a variety of applications. This approach hinges upon a synergistic combination of the dramatic power of organic photochemical transformations combined with the exceedingly high selectivity and atom-economy of efficient catalytic processes. Indeed, the use of probably the cheapest reagent (light) combined with a catalytic transformation ensures near perfect atom-economy in this journey from flat and inexpensive substructures to chiral added-value products. Conceptually, the photochemical operation is envisaged as a energy-loading step whereas the catalytic transformation functions as an energy-release where asymmetric information is inscribed into the products.
The chemistry proposed herein will open up new vistas in enantioselective synthesis. Furthermore, groundbreaking and unprecedented methodology in the field of catalytic allylic alkylation is proposed that significantly expands (and goes beyond) the currently accepted “dogmas” for these textbook reactions. These include (but are not limited to) systematic violations of well-established rules “by design”, new contexts for application, new activation modes and innovative leaving groups. Finally, the comprehensive body of synthetic technology presented will be applied to pressing target-oriented problems in Organic Synthesis. It shall result in a landmark, highly efficient total synthesis of Tamiflu, as well as in application to an environmentally important target (Fomannosin), allowing the easy production of analogues for biological testing.""","1487000","2012-01-01","2016-12-31"
"FLATRONICS","Electronic devices based on nanolayers","Andras Kis","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The main objective of this research proposal is to explore the electrical properties of nanoscale devices and circuits based on nanolayers.  Nanolayers cover a wide span of possible electronic properties, ranging from semiconducting to superconducting. The possibility to form electrical circuits by varying their geometry offers rich research and practical opportunities. Together with graphene, nanolayers could form the material library for future nanoelectronics where different materials could be mixed and matched to different functionalities.","1799996","2009-09-01","2014-08-31"
"FLAVE","Energetics of natural turbulent flows: the impact of waves and radiation.","Basile GALLET","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","Turbulence in natural flows is an outstanding challenge with key implications for the energetics of planets, stars, oceans, and the Earth’s climate system. Such natural flows interact with waves, radiation or a combination thereof: surface waves and solar radiation on oceans and lakes, bulk waves and radiation inside the rapidly rotating and electrically conducting solar interior, etc. Standard simplified models often discard waves, radiation, or both, with dramatic consequences for the energy budget of natural flows: geostrophic models neglect waves, and Rayleigh-Bénard thermal convection considers heat diffusively injected through a solid boundary, in strong contrast with radiative heating. The purpose of the present multidisciplinary project is to develop a consistent and coupled description of natural flows interacting with waves and radiation, to properly assess their energy budget:

• Because resolving surface waves in global ocean models will remain out-of-reach for decades, I will derive and investigate reduced equations describing their two-way coupling to the ocean currents, with timely implications for the upwelling of nutrients, the strength of the global ocean circulation and ultimately CO2 sequestration and the climate system.

• Building on my recent advances in the field of rotating and magnetohydrodynamic turbulence, I will derive a set of reduced equations to simulate such turbulent flows in the vicinity of the transition where bulk 3D waves appear on a 2D turbulent flow. This approach will allow me to reach unprecedented parameter regimes, orders of magnitude beyond state-of-the-art 3D direct numerical simulations (DNS).

• Finally, I will combine state-of-the-art DNS with a versatile experimental platform to determine the structure, kinetic energy and heat transport of turbulent radiative convection in various geometries. I will extrapolate the resulting scaling-laws to the ocean circulation, the mixing in lakes and the solar tachocline.","1499094","2018-03-01","2023-02-28"
"FLEXABLE","Deformable Multiple-View Geometry and 3D Reconstruction, with Application to Minimally Invasive Surgery","Adrien Bartoli","UNIVERSITE CLERMONT AUVERGNE","Project FLEXABLE lies in the field of 3D Computer Vision, which seeks to recover depth or the 3D shape of the observed environment from images. One of the most successful and mature techniques in 3D Computer Vision is Shape-from-Motion which is based on the well-established theory of Multiple-View Geometry. This uses multiple images and assumes that the environment is rigid.

The world is however made of objects which move and undergo deformations. Researchers have tried to extend Shape-from-Motion to a deformable environment for about a decade, yet with only very limited success to date. We believe that there are two main reasons for this. Firstly there is still a lack of a solid theory for Deformable Shape-from-Motion.  Fundamental questions, such as what kinds of deformation can facilitate unambiguous 3D reconstruction, are not yet answered.  Secondly practical solutions have not yet come about: for accurate and dense 3D shape results, the Motion cue must be combined with other visual cues, since it is certainly weaker in the deformable case. It may require strong object-specific priors, needing one to bridge the gap with object recognition.

This project develops these two key areas. It includes three main lines of research: theory, its computational implementation, and its real-world application. Deformable Multiple-View Geometry will generalize the existing rigid theory and will provide researchers with a rigorous mathematical framework that underpins the use of Motion as a proper visual cue for Deformable 3D Reconstruction. Our theory will require us to introduce new mathematical tools from differentiable projective manifolds. Our implementation will study and develop new computational means for solving the difficult inverse problems formulated in our theory. Finally, we will develop cutting-edge applications of our framework specific to Minimally Invasive Surgery, for which there is a very high need for 3D computer vision.","1481294","2013-01-01","2018-12-31"
"FlexAnalytics","Advanced Analytics to Empower the Small Flexible Consumers of Electricity","Juan Miguel MORALES","UNIVERSIDAD DE MALAGA","David against Goliath: Could small consumers of electricity compete in the wholesale markets on equal footing with the other market agents? Yes, they can and FlexAnalytics will show how.

Activating the demand response, although a major challenge, may also bring tremendous benefits to society, with potential cost savings in the billions of euros. This project will exploit methods of inverse problems, multi-level programming and machine learning to develop a pioneering system that enables the active participation of a group of price-responsive consumers of electricity in the wholesale electricity markets. Through this, they will be able to make the most out of their flexible consumption. FlexAnalytics proposes a generalized scheme for so-called inverse optimization that materializes into a novel data-driven approach to the market bidding problem that, unlike existing approaches, combines the tasks of forecasting, model formulation and estimation, and decision-making in an original unified theoretical framework. The project will also address big-data challenges, as the proposed system will leverage weather, market, and demand information to capture the many factors that may affect the price-response of a pool of flexible consumers. On a fundamental level, FlexAnalytics will produce a novel mathematical framework for data-driven decision making. On a practical level, FlexAnalytics will show that this framework can facilitate the best use of a large amount and a wide variety of data to efficiently operate the sustainable energy systems of the future.","1203125","2018-02-01","2023-01-31"
"FLEXBOT","Flexible object manipulation based on statistical learning and topological representations","Danica Kragic Jensfelt","KUNGLIGA TEKNISKA HOEGSKOLAN","A vision for the future are autonomous and semi-autonomous systems that perform complex tasks safely and robustly in interaction with humans and the environment.  The action of such a system needs to be carefully planned and executed, taking into account the available sensory feedback and knowledge about the environment. Many of the existing approaches view motion planning as a geometrical problem, not taking the uncertainty into account.  Our goal is to study how different type of representations and algorithms from the area of machine learning and classical mathematics can be used to solve some of the open problems in the area of action recognition and action generation.

FLEXBOT will explore how how topological representations can be used for an integrated approach toward i) vision based understanding of complex human hand motion, ii) mapping and control of robotics hands and iii) integrating the topological representations with models for high-level task encoding and planning.

Our research opens for new and important areas scientifically and technologically. Scientifically, we push for new way of thinking in an area that has traditionally been born from mechanical  modeling of bodies. Technologically, we will provide methods plausible for evaluation of new designs of robotic and prosthetic hands.  Further development of machine learning and computer vision methods will allow for scene understanding that goes beyond the assumption of worlds of rigid bodies, including complex objects such as hands.","1398720","2012-01-01","2017-12-31"
"FLEXILOG","Formal lexically informed logics for searching the web","Steven Schockaert","CARDIFF UNIVERSITY","Semantic search engines use structured knowledge to improve traditional web search, e.g. by directly answering questions from users. Current approaches to semantic search rely on the unrealistic assumption that all true facts about a given domain are explicitly stated in their knowledge base or on the web. To reach their full potential, semantic search engines need the ability to reason about known facts. However, existing logics cannot adequately deal with the imperfect nature of knowledge from the web. One problem is that relevant information tends to be distributed over several heterogeneous knowledge bases that are inconsistent with each other. Moreover, domain theories are seldom complete, which means that a form of so-called plausible reasoning is needed. Finally, as relevant logical theories do not exist for many domains, reasoning may need to rely on imperfect probabilistic theories that have been learned from the web. 

To overcome these challenges, FLEXILOG will introduce a family of logics for robust reasoning with messy real-world knowledge, based on vector-space representations of natural language terms (i.e. of lexical knowledge). In particular, we will use lexical knowledge to estimate the plausibility of logical models, using conceptual simplicity as a proxy for plausibility (i.e. Occam’s razor). This will enable us to implement various forms of commonsense reasoning, equipping classical logic with the ability to draw plausible conclusions based on regularities that are observed in a knowledge base. We will then generalise our approach to probabilistic logics, and show how we can use the resulting lexically informed probabilistic logics to learn accurate and comprehensive domain theories from the web. This project will enable a robust data-driven approach to logic-based semantic search, and more generally lead to fundamental progress in a variety of knowledge-intensive applications for which logical inference has traditionally been too brittle.","1451656","2015-05-01","2020-04-30"
"FlexNanoFlow","Ultra-flexible nanostructures in flow: controlling folding, fracture and orientation in large-scale liquid processing of 2D nanomaterials","Lorenzo BOTTO","QUEEN MARY UNIVERSITY OF LONDON","2D nanomaterials hold immense technological promise thanks to extraordinary intrinsic properties such as ultra-high conductivity, strength and unusual semiconducting properties. Our understanding of how these extremely thin and flexible objects are processed in flow is however inadequate, and this is hindering progress towards true market applications. When processed in liquid environments to make nanocomposites, conductive coatings and energy storage devices, 2D nanomaterials tend to fold and break owing to strong shear forces produced by the mechanical agitation of the liquid. This can lead to poorly-oriented, crumpled sheets of small lateral size and therefore of low intrinsic value. Orientation is also a major issue, as ultra-flexible materials are difficult to extend and align.  In this project, I will develop nanoscale fluid-structure simulation techniques to capture with unprecedented resolution the unsteady deformation and fracture dynamics of single and multiple sheets in response to the complex hydrodynamic load produced by shearing flows. In addition, I will demonstrate via simulations new strategies to exploit capillary forces to structure 2D nanomaterials into 3D constructs of desired morphology. To guide the simulations and explore a wider parameter space than allowed in computations, I will develop conceptually new experiments on “scaled-up 2D nanomaterials”, macroscopic particles having the same dynamics as the nanoscopic ones. The simulations will include continuum treatments and atomistic details, and will be analysed within the theoretical framework of microhydrodynamics and non-linear solid mechanics. By uncovering the physical principles governing flow-induced deformation of 2D nanomaterials, this project will have a profound impact on our ability to produce and process 2D nanomaterials on large scales.","1453779","2017-04-01","2022-03-31"
"FLEXOCOMP","Enabling flexoelectric engineering through modeling and computation","Irene Arias Vicente","UNIVERSITAT POLITECNICA DE CATALUNYA","Piezoelectric materials transduce electrical voltage into mechanical strain and vice-versa, which makes them ubiquitous in sensors, actuators, and energy harvesting systems. Flexoelectricity is a related but different effect, by which electric polarization is coupled to strain gradients, i.e. it requires inhomogeneous deformation. Flexoelectricity is present in a much wider variety of materials, including non-polar dielectrics and polymers, but is only significant at small length-scales. Flexoelectricity has demonstrated its potential in information technologies, by flexoelectric-mediated mechanical writing in ferroelectric thin films at the nanoscale, or in flexoelectric electromechanical transducers. It has been suggested that flexoelectricity could enable piezoelectric composites made out of non-piezoelectric components, including soft materials, which could be used in biocompatible and self-powered small-scale devices. Flexoelectricity is a nascent field with major open questions. Furthermore, experimental devices and material designs are limited by what we can understand and analyze, and unfortunately, we lack general engineering analysis tools for flexoelectricity. As a result, current flexoelectric devices are only minimal variations of configurations conceived within the uniform-strain mindset of piezoelectricity. Our main objective in this proposal is to develop an advanced computational infrastructure to quantify flexoelectricity in solids, focusing on continuum models but also exploring multiscale aspects. We plan to use it to (1) analyze accurately flexoelectricity accounting for general geometries, electrode configurations, and material behavior, (2) identify new physics emerging  flexoelectricity, and (3) propose, build and test a new generation of thin-film devices, composites and metamaterials for electromechanical transduction, genuinely designed to exploit small-scale flexoelectricity and make it available at macroscopic scales.","1500000","2016-09-01","2021-08-31"
"FLEXOELECTRICITY","Flexoelectricity","Gustavo Catalan Bernabe","FUNDACIO INSTITUT CATALA DE NANOCIENCIA I NANOTECNOLOGIA","""Flexoelectricity is a general property of all insulators whereby they generate a voltage when subjected to an inhomogeneous deformation such as bending. Research on this property has taken off with the observation that, due to the large gradients they can accommodate, devices operating in the nanoscale display colossal flexoelectric effects. The present proposal aims to set up Euroe’s first laboratory specialized on the exploration and exploitation of flexoelectricity. It shall focus on three areas with specific targets:

1) Flexoelectricity for energy harvesting: the inverse relationship between flexoelectricity and device size means that, at the nanoscale, flexoelectric energy harvesting can deliver electromechanical performances superior to the current state of the art. We aim to demonstrate record-high effective piezoelectric coefficients through the use of flexoelectricity.

2) Flexoelectricity for information technologies: Flexoelectricity affords mechanical control of polarity. This opens the door to novel memory device concepts where polarization (and magnetization) can be controlled by pushing with the tip of a scanning probe. We aim to achieve flexoelectric writing of domains under electrodes, and also to demonstrate the indirect coupling between flexoelectricity and magnetization (“flexomagnetism”).

3) Bioflexoelectricity: Flexoelectricity participates in human hearing, and is expected to participate in other bioelectric phenomena. In particular, bones are known to generate electricity in response to stress, and it has been hypothesised that this is due to strain gradients; if demonstrated, this would represent a significant step towards osteogenetic implants. Determining the role of flexoelectricity in in bone piezoelectricty will be the third aim of this project.""","1478400","2013-01-01","2017-12-31"
"FLICs","Enabling flexible integrated circuits and applications","Kris Jef Ria Myny","INTERUNIVERSITAIR MICRO-ELECTRONICA CENTRUM","Thin-film transistor technologies are present in many products today that require an active transistor backplane e.g. flat-panel displays and flat-panel photodetector arrays. Unipolar n-type transistors based on amorphous Indium-Gallium-Zinc-Oxide (a-IGZO) as semiconductor is currently the most promising technology for next generation products demanding a  high-performant, low power transistor, manufacturable on flexible substrates enabling curved, bendable and even rollable displays. a-IGZO is a wide bandgap material characterized by extremely low off-state leakage currents and electron mobility of ~20 cm2/Vs. IGZO transistors fabricated on flexible substrates will also find their use in applications that require flexible integrated circuits. 

The goal of this FLICs proposal is to develop disruptive technology and ground-breaking design innovations with amorphous oxide TFTs on plastic substrates, targeting large scale or very large scale flexible integrated circuits with unprecedented characteristics in terms of power consumption, supply voltage and operating speed, for applications in IoT and wearable healthcare sensor patches.

We introduce a new logic style, “quasi-CMOS”, which is based on unipolar, oxide dual-gate thin-film transistors. This logic style will drastically decrease the power consumption of unipolar logic gates in a novel way by taking advantage of dynamic backgate driving and of the transistor’s unique low off-state leakage current, without compromising on switching speed. In addition, we also introduce downscaling of the transistor’s dimensions, while remaining compatible with upscaling to large-area manufacturing platforms. Finally, we will investigate novel ultralow-power design techniques on system-level, while exploiting the quasi-CMOS logic gates.

We will demonstrate the power of this innovation with circuits for item-level Internet-of-Things, UHF RFID, and wearable health sensor patches.","1499155","2017-01-01","2021-12-31"
"FLINT","Finite-Length Information Theory","Albert Guillen I Fabregas","UNIVERSIDAD POMPEU FABRA","Shannon's Information Theory establishes the fundamental limits of information processing systems. A concept that is hidden in the mathematical proofs most of the Information Theory literature, is that in order to achieve the fundamental limits we need sequences of infinite duration. Practical information processing systems have strict limitations in terms of length, induced by system constraints on delay and complexity. The vast majority of the Information Theory literature ignores these constraints and theoretical studies that provide a finite-length treatment of information processing are hence urgently needed. When finite-lengths are employed, asymptotic techniques (laws of large numbers, large deviations) cannot be invoked and new techniques must be sought. A fundamental understanding of the impact of finite-lengths is crucial to harvesting the potential gains in practice. This project is aimed at contributing towards the ambitious goal of providing a unified framework for the study of finite-length Information Theory. The approach in this project will be based on information-spectrum combined with tight bounding techniques. A comprehensive study of finite-length information theory will represent a major step forward in Information Theory, with the potential to provide new tools and techniques to solve open problems in multiple disciplines. This unconventional and challenging treatment of Information Theory will advance the area and will contribute to disciplines where Information Theory is relevant. Therefore, the results of this project will be of benefit to areas such as communication theory, probability theory, statistics, physics, computer science, mathematics, economics, bioinformatics and computational neuroscience.","1303606","2011-08-01","2017-07-31"
"FLIRT","Fluid Flows and Irregular Transport","Gianluca Crippa","UNIVERSITAT BASEL","""Several important partial differential equations (PDEs) arising in the mathematical description of physical phenomena exhibit transport features: physical quantities are advected by velocity fields that drive the dynamics of the system. This is the case for instance for the Euler equation of fluid dynamics, for conservation laws, and for kinetic equations.
An ubiquitous feature of these phenomena is their intrinsic lack of regularity. From the mathematical point of view this stems from the nonlinearity and/or nonlocality of the PDEs. Moreover, the lack of regularity also encodes actual properties of the underlying physical systems: conservation laws develop shocks (discontinuities that propagate in time), solutions to the Euler equation exhibit rough and """"disordered"""" behaviors. This irregularity is the major difficulty in the mathematical analysis of such problems, since it prevents the use of many standard methods, foremost the classical (and powerful) theory of characteristics.
For these reasons, the study in a non smooth setting of transport and continuity equations, and of flows of ordinary differential equations, is a fundamental tool to approach challenging important questions concerning these PDEs.
This project aims at establishing:
(1) deep insight into the structure of solutions of nonlinear PDEs, in particular the Euler equation and multidimensional systems of conservation laws,
(2) rigorous bounds for mixing phenomena in fluid flows, phenomena for which giving a precise mathematical formulation is extremely challenging.
The unifying factor of this proposal is that the analysis will rely on major advances in the theory of flows of ordinary differential equations in a non smooth setting, thus providing a robust formulation via characteristics for the PDEs under consideration. The guiding thread is the crucial role of geometric measure theory techniques, which are extremely efficient to describe and investigate irregular phenomena.""","1009351","2016-06-01","2021-05-31"
"FLOVIST","Flow visualization inspired aero-acoustics with time-resolved Tomographic Particle Image Velocimetry","Fulvio Scarano","TECHNISCHE UNIVERSITEIT DELFT","""The recent developments of the Tomographic Particle Image Velocimetry technique and of the non-intrusive pressure field characterization method, by the applicant at TU Delft Aerospace Engineering, now opens unforeseen perspectives in the area of unsteady flow diagnostics and experimental aero-acoustics. As a result of this work it is now possible not only to quantify complex flows in their three-dimensional structure, but also to extract quantities such as pressure. The current research proposal aims at the development of an innovative approach to experimental aero-acoustics and flow control making use of the recently developed Tomographic-PIV technique. The objective is to fully describe and quantify the flow pattern and the related acoustic source term at its origin, which is of paramount importance to understand and control the processes like acoustic noise production and flow separation dominating aerodynamic drag. This is relevant for the improvement of aircrafts design as far as drag reduction and noise emission is related and should enable the development of """"greener"""" aircrafts for a sustainable growth of aviation in populated areas, in harmony with the technology innovation policy in Europe (7th Framework Programme) and TU Delft sustainable development focus (CleanEra, Cost-Effective Low emission And Noise Efficient regional Aircraft) at Aerospace Engineering. To achieve this step it is required that such new-generation diagnostic approach by the Tomo-PIV technique is further developed into a quadri-dimensional measurement tool (4D-PIV), enabling to extract the relevant acoustic information from the experimental observation invoking the aeroacoustic analogies. A wide industrial and academic network (DLR, AIRBUS, DNW, NLR, LaVision, EWA, JMBC Burgerscentrum) developed in recent years is available to exploit the results of the proposed activity.""","1498000","2008-08-01","2013-07-31"
"FLOWTONICS","Solid-state flow as a novel approach for the fabrication of photonic devices","Fabien Sorin","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The development of advanced photon-based technologies offers exciting promises in fields of crucial importance for the development of sustainable societies such as energy and food management, security and health care. Innovative photonic devices will however reveal their true potential if we can deploy their functionalities not only on rigid wafers, but also over large-area, flexible and stretchable substrates. Indeed, providing energy harvesting, sensing, or stimulating abilities over windows, screens, food packages, wearable textiles, or even biological tissues will be invaluable technological breakthroughs. Today, however, conventional fabrication approaches remain difficult to scale to large area, and are not well adapted to the mechanical and topological requirements of non-rigid and curved substrates. In FLOWTONICS, we propose innovative materials processing approaches and device architectures to enable the simple and scalable fabrication of nano-structured photonic systems compatible with flexible and stretchable substrates. Our strategy is to direct the flow of optical materials through an innovative and thus far unexplored exploitation of the solid-state dewetting and thermal drawing processes. Our objectives are three-fold: (1) Study and demonstrate, for the first time, the strong potential of the dewetting of chalcogenide glasses layers for the fabrication of large area photonic devices; (2) Show that dewetting can also be exploited to realize photonic architectures onto engineered, nano-imprinted flexible and stretchable polymer substrates; (3) Demonstrate, for the first time, the use of the thermal drawing process as a novel tool to realize advanced flexible and stretchable photonic ribbons and fibers. These novel approaches can contribute to game-changing scientific and technological advances for the sustainable management of our resources and to meet our growing health care needs, putting Europe at the forefront of innovation in these crucial areas.","1499585","2016-02-01","2021-01-31"
"FLUID-INTERFACE","Analysis of moving incompressible fluid interfaces","Francisco de Asís Gancedo García","UNIVERSIDAD DE SEVILLA","The research of this proposal is focused on solving problems that involve the evolution of fluid interfaces. The project will investigate the dynamics of free boundaries arising between incompressible fluids of different nature. The main concern is well-posed scenarios which include the possible formation of singularities in finite time or existence of solutions for all time. These contour dynamics issues are governed by fundamental fluid mechanics equations such as the Euler, Navier-Stokes, Darcy and quasi-geostrophic systems. They model important problems such as water waves, viscous waves, Muskat, interface Hele-Shaw and SQG sharp front evolution. All these contour dynamics frameworks will be studied with emphasis on singularity formation and global existence results, not only for their importance in mathematical physics, but also for their mathematical interest. This presents huge challenges which will in particular require the use of different tools and methods from several areas of mathematics. A new technique, introduced to the field by the Principal Investigator, has already enabled the analysis of several singularity formations for the water waves and Muskat problems, as well as to obtain global existence results for Muskat. The main goal of this proposal is to develop upon this work, going far beyond the state of the art in these contour dynamics problems for incompressible fluids.","1106936","2015-09-01","2020-08-31"
"FLUINEMS","Suspended Fluidic nanochannels as optomechanical sensors for single molecules","Irene FERNANDEZ-CUESTA","UNIVERSITAET HAMBURG","Early detection of cancer saves lives: the survival rate increases from 10% to 90%. Today, there is  a lack of methods for the early detection of different types of cancer. The main challenge is that a large variety of specific biomarkers have to be counted, and usually at extremely low concentrations. 
Here we will use smart nanoengineering to develop a versatile sensing platform that detects, identifies and counts single molecules, one by one, in a non-purified solution. The main novelty of the proposal is that the platform will integrate optical sensing beyond diffraction (for biochemical recognition) and on-chip molecular mass sensing (to discard non-specific binding events). For this, we will suspend state-of-the-art nanochannels and integrate them with plasmonic nano-antennas. This will make for a versatile, universal test, cheap enough to be used for preventive screening. It will help diagnosing different types of cancers earlier.
We will detect and identify oncoviruses capsides, different protein biomarkers and microRNA in plasma. These are very high impact proof of concepts with clinical relevance.
Dr. Fernandez-Cuesta has expertise in single molecule sensing and has worked in multiple international, interdisciplinary environments. She is expert in nanofabrication and plasmonics for (bio)sensing. She benefited from an IOF Marie Curie. She became CEO and co-founder of a start-up, dedicated to on-chip Raman spectroscopy for environmental monitoring of biotoxins concentration in fresh water. 
Currently, she is at the department for Applied Physics at the University of Hamburg. There, there is state-of-the-art instrumentation for nanofabrication. The collaborators at Hamburg University Klinikum Eppendorf have expertise in applying research to medicine. This interdisciplinary environment is ideal for starting a new field in combined plasmonics and mass sensing with a focus on nanotechnology for bioapplications of clinical relevance.","1500000","2017-06-01","2022-11-30"
"FluoroFix","Catalytic C–F Bond Functionalization for the Fixation of Environmentally Persistent Fluorocarbons","Mark Richard Crimmin","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","The objective of my project is to develop new methods to transform environmentally persistent hydrofluorocarbons (HFCs) and hydrofluoroolefins (HFOs), into reactive chemical building blocks that can be used in chemical manufacture. My team will achieve this by developing a series of new chemical reactions that transform inert carbon–fluorine bonds in HFCs or HFOs into reactive carbon–boron or carbon–aluminum bonds. My team will show that these reactive species can be used to construct carbon–carbon bonds. The idea is to develop a net series of reactions that results in the fixation of fluorine from low-value, volatile, and environmentally damaging organic molecules to high-value, non-volatile organic molecules. When successful, this approach should open up new routes that allow the chemical industry to repurpose HFCs and HFOs as chemical intermediates, and not end products, that complements European Union legislation. By the end of this project, my team will have pioneered methods for the production of agrochemicals and pharmaceuticals from environmentally persistent fluorocarbons.","1496925","2016-04-01","2021-03-31"
"FOC","Foundations of Cryptographic Hardness","Iftach Ilan Haitner","TEL AVIV UNIVERSITY","A fundamental research challenge in modern cryptography is understanding the necessary hardness assumptions required to build different cryptographic primitives. Attempts to answer this question have gained tremendous success in the last 20-30 years. Most notably, it was shown that many highly complicated primitives can be based on the mere existence of one-way functions (i.e., easy to compute and hard to invert), while other primitives cannot be based on such functions. This research has yielded fundamental tools and concepts such as randomness extractors and computational notions of entropy. Yet many of the most fundamental questions remain unanswered. 
Our first goal is to answer the fundamental question of whether cryptography can be based on the assumption that P not equal NP. Our second and third goals are to build a more efficient symmetric-key cryptographic primitives from one-way functions, and to establish effective methods for security amplification of cryptographic primitives. Succeeding in the second and last goals is likely to have great bearing on the way that we construct the very basic cryptographic primitives. A positive answer for the first question will be considered a dramatic result in the cryptography and computational complexity communities.
To address these goals, it is very useful to understand the relationship between different types and quantities of cryptographic hardness. Such understanding typically involves defining and manipulating different types of computational entropy, and comprehending the power of security reductions. We believe that this research will yield new concepts and techniques, with ramification beyond the realm of foundational cryptography.","1239838","2015-03-01","2020-02-29"
"FODEX","Tropical Forest Degradation Experiment","Edward MITCHARD","THE UNIVERSITY OF EDINBURGH","We know how to map tropical forest biomass using an array of satellite and aircraft sensors with reasonable accuracy (±15-40 %). However, we do not know how to map biomass change. Simply differencing existing biomass maps produces noisy and biased results, with confidence intervals unknowable using existing static field plots. Thus the potential for using plentiful free satellite data for biomass change mapping is being wasted.

To solve this I propose setting up the first experimental arrays of biomass change plots. In total 52 large plots will be located in logging concessions in Gabon and Peru, where biomass will be assessed before and after logging, and during recovery. In addition to traditional field inventory, terrestrial laser scanning (TLS) data will give the precise 3D shape of thousands of trees before and after disturbance, allowing biomass change to be estimated without bias. The project’s unmanned aerial vehicle (UAV) will collect LiDAR data 4 times over each concession over 4 years, scaling up the field data to give thousands of hectares of biomass change data. In tandem, data from all potentially useful satellites (17+) flying over the field sites over the study period will be ordered and processed.  

These data will enable the development of new methods for mapping carbon stock changes, with known uncertainty, which I will scale up across the Amazon basin and west/central Africa. For the first time we will have the methods to assess the balance of regrowth and anthropogenic disturbance across tropical forests, informing us about the status and resilience of the land surface carbon sink. As well as of scientific interest, these results are urgently needed for forest conservation: the Paris Agreement relies on paying countries to reduce losses and enhance gains in forest carbon stocks, but we do not currently have the tools to map forest carbon stock changes. Without accurate monitoring it is not possible to target resources nor assess success.","1942471","2018-01-01","2022-12-31"
"FOLDHALO","Folding with Halogen Bonding","Pierangelo Metrangolo","POLITECNICO DI MILANO","""The focus of this research project will be on halogen bonding, namely any noncovalent interactions involving halogen atoms as electrophilic species (electron density acceptor sites, Lewis acids, halogen bonding-donors). In particular, the overall goal of the project will be to fully elucidate the capabilities and properties of halogen atoms as recognition “sticky” sites in the context of biomolecules.
The general objective of this research project will be achieved through the application of a multi-dimensional approach to the understanding of the intermolecular interactions involving halogenated molecules in chemistry and biology. The programme of work will centre around three closely integrated and synergistic strands. The common theme is to exploit halogen bonding for the design of “smart” peptides and foldamers (Strand 1), the obtainment of complexes of polyhalogenated organic pollutants with serum proteins (Strand 2), and to assemble biomimetic sensors for polyhalogenated organic pollutants (Strand 3).
For the first time a multidisciplinary team composed by synthetic chemists, small molecule crystallographers, biologists, physicists, and protein crystallographers will join forces around the fundamental issues of: a) contributing to the establishment of the nature and properties of halogen bonding in ligand/biomolecule systems; b) improving our understanding of long-distance intermolecular interactions and their role on the energy profiles of biochemical transformations; c) facilitating preparation of more rationally designed new halogenated drugs; d) allowing for the mechanistic understanding of reactivity of halogen-containing molecules for the development of efficient and """"green"""" synthetic and bioremediation methods.
The overall aim of this project is, therefore, to enlighten to the scientific community the potential that halogen bonding has to become a very powerful tool in the manipulation of molecular recognition phenomena in chemistry and biology.""","1393000","2013-03-01","2018-02-28"
"FOPS-water","Fundamentals Of Photocatalytic Splitting of Water","Eleonora Hendrika Gertruda Mezger-Backus","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Hydrogen produced by sunlight is a very promising, environmentally-friendly energy source as an alternative for increasingly scarce and polluting fossil fuels. Since the discovery of hydrogen production by photocatalytic water dissociation on a titanium dioxide (TiO2) electrode 40 years ago, much research has been aimed at increasing the process efficiency. Remarkably, insights into how water is bound to the catalyst and into the dynamics of the photodissociation reaction, have been scarce up to now, due to the lack of suitable techniques to interrogate water at the interface. The aim of this proposal is to provide these insights by looking at specifically the molecules at the interface, before, during and after their photo-reaction. With the surface sensitive spectroscopic technique sum-frequency generation (SFG) we can determine binding motifs of the ~monolayer of water at the interface, quantify the heterogeneity of the water molecules at the interface and follow changes in water molecular structure and dynamics at the interface during the reaction. The structure of interfacial water will be studied using steady-state SFG; the dynamics of the water photodissociation will be investigated using pump-SFG probe spectroscopy. At variable delay times after the pump pulse the probe pulses will interrogate the interface and detect the reaction intermediates and products. Thanks to recent developments of SFG it should now be possible to determine the structure of water at the TiO2 interface and to unravel the dynamics of the photodissocation process. These insights will allow us to relate the interfacial TiO2-water structure and dynamics to reactivity of the photocatalyst, and to bridge the gap between the fundamentals of the process at the molecular level to the efficiency of the photocatalys. The results will be essential for developing cheaper and more efficient photocatalysts for the production of hydrogen.","1498800","2014-03-01","2019-02-28"
"FoQAL","Frontiers of Quantum Atom-Light Interactions","Darrick Chang","FUNDACIO INSTITUT DE CIENCIES FOTONIQUES","FoQAL aims to completely re-define our ability to control light-matter interactions at the quantum level. This potential revolution will make use of cold atoms interfaced with nanophotonic systems, exploiting unique features such as control over the dimensionality and dispersion of light, the engineering of quantum vacuum forces, and strong optical fields and forces associated with light confined to the nanoscale. We will develop powerful and fundamentally new paradigms for atomic trapping, tailoring atomic interactions, and quantum nonlinear optics, which cannot be duplicated in macroscopic systems even in principle. Targeted breakthroughs include:
1) Nanoscale traps using quantum vacuum forces. Nanophotonic structures enable strong quantum vacuum forces acting on atoms near dielectric surfaces to be harnessed for novel “vacuum traps.” Their figures of merit (e.g., trap depth and spatial confinement) will exceed what is possible with conventional trapping techniques by 1-2 orders of magnitude.

2) Strong long-range spin-photon-phonon interactions. We will show that nanophotonic systems enable the formation of new “quasi-particles” consisting of atoms dressed by localized photonic clouds. These clouds produce strong multi-physics coupling between photons and atomic spins and motion, facilitating novel long-range interactions and the generation of exotic quantum states of light and matter.

3) New routes to single-photon nonlinear optics. We will develop novel techniques to attain strong interactions between individual photons, which are not based upon the saturation of atomic transitions. These approaches will take advantage of engineered long-range interactions between atoms, and “atom-optomechanics” in which the optical response of atoms and their motion strongly couple. Significantly, our protocols will enable a growth in nonlinearities for moderate atom number N, in contrast to conventional cavity QED where the optimal operating point is N=1.","1340873","2015-03-01","2020-02-29"
"FORCE","Fine Observations of the Rate of Cosmic Expansion: Combining the powers of Weak Gravitational Lensing and Baryon Acoustic Oscillations as Probes of Dark Energy","Catherine Elizabeth Cox Heymans","THE UNIVERSITY OF EDINBURGH","I propose to combine state-of-the-art observations of weak gravitational lensing and baryon acoustic oscillations to answer one fundamental question; is the accelerating expansion of our Universe caused by dark energy, or is it a manifestation of beyond-Einstein gravity theories, as might arise if the Universe has more dimensions? This frontier research will have a wide ranging impact as is it believed that understanding the dark energy phenomenon will revolutionize our understanding of Physics today. The observational task of detecting and analysing probes of dark energy is technically very challenging and may be subject to systematic limits. I detail how I will exploit synergies between the weak lensing and baryon acoustic oscillations techniques, showing that the physical systematics that effect each technique can be neatly resolved using complementary information from the alternative technique. With support from the ERC I will create an inter-disciplinary team well positioned to first solve many of the systematic problems associated with dark energy research and then apply those novel solutions to the dark energy analysis of three world-leading wide-field surveys that I currently co-investigate; CFHTLS, a recently completed 170 square degree ugriz survey, PanSTARRS-1, a soon to be started all-sky grizy survey and ADEPT, a space-based infra-red telescope for baryon acoustic oscillation studies proposed for NASA s Joint Dark Energy Mission. Using innovative 3D statistical analyses, optimised photometric redshifts and new combined lensing and galaxy clustering statistics, my ERC team will aim to control systematic errors to place joint constraints on the evolving nature of dark energy and test directly beyond-Einstein gravity.","1258797","2010-04-01","2015-10-31"
"FORECASToneMONTH","Forecasting Surface Weather and Climate at One-Month Leads through Stratosphere-Troposphere Coupling","Chaim Israel Garfinkel","THE HEBREW UNIVERSITY OF JERUSALEM","Anomalies in surface temperatures, winds, and precipitation can significantly alter energy supply and demand, cause flooding, and cripple transportation networks.  Better management of these impacts can be achieved by extending the duration of reliable predictions of the atmospheric circulation. 

Polar stratospheric variability can impact surface weather for well over a month, and this proposed research presents a novel approach towards understanding the fundamentals of how this coupling occurs. Specifically, we are interested in: 1) how predictable are anomalies in the stratospheric circulation? 2) why do only some stratospheric events modify surface weather? and 3) what is the mechanism whereby stratospheric anomalies reach the surface? While this last question may appear academic, several studies indicate that stratosphere-troposphere coupling drives the midlatitude tropospheric response to climate change; therefore, a clearer understanding of the mechanisms will aid in the interpretation of the upcoming changes in the surface climate. 

I propose a multi-pronged effort aimed at addressing these questions and improving monthly forecasting. First, carefully designed modelling experiments using a novel modelling framework will be used to clarify how, and under what conditions, stratospheric variability couples to tropospheric variability. Second, novel linkages between variability external to the stratospheric polar vortex and the stratospheric polar vortex will be pursued, thus improving our ability to forecast polar vortex variability itself. To these ends, my group will develop 1) an analytic model for Rossby wave propagation on the sphere, and 2) a simplified general circulation model, which captures the essential processes underlying stratosphere-troposphere coupling.  By combining output from the new models, observational data, and output from comprehensive climate models, the connections between the stratosphere and surface climate will be elucidated.","1808000","2016-05-01","2021-04-30"
"FORESTPRIME","Predicting carbon release from forest soils through priming effects: a new approach to reconcile results across multiple scales","Emma Jane Sayer","LANCASTER UNIVERSITY","Feedbacks between plants and soil under environmental change are likely to have a significant impact on ecosystem carbon cycling. Recent work has shown that increased atmospheric carbon dioxide concentrations have enhanced tree growth in forests. However these increases in growth can also cause ‘priming effects’ whereby microbial degradation of soil organic matter is stimulated by fresh carbon inputs, such as plant litter, releasing additional carbon from the soil. Given that forest soils represent the largest terrestrial carbon pool, priming effects could cause a major release of carbon dioxide to the atmosphere. Despite their potential importance in ecosystem carbon dynamics under environmental change, the processes and mechanisms underlying priming effects are still poorly understood. This is in part due to the enormous disparities in the experimental scales and methods required to study microbial processes vs. ecosystem carbon dynamics and the difficulties in extrapolating the results of laboratory studies to the ecosystem level. This project will significantly advance our understanding of the role of priming effects in forest carbon dynamics in different forest types and reconcile the experimental problems of scale using multidisciplinary nested studies across multiple scales. The nested design will explicitly test the validity of extrapolations made at one scale to predict effects at another.  The ultimate aim is to allow the extrapolation of results from small-scale studies of priming to the ecosystem level for a wide range of forests. The results will establish this fundamentally new approach as a widely applicable method in the study of plant-soil feedbacks. This research will provide the first comprehensive comparative dataset on priming effects across forests worldwide and form the solid basis for their inclusion in model predictions of forest carbon cycling under future global change.","1694796","2012-12-01","2018-05-31"
"Fornax","Galaxy evolution in dense environments","Paolo Serra","ISTITUTO NAZIONALE DI ASTROFISICA","The Universe around us is populated with galaxies, each containing from millions to tens of billions of individual stars. Far from being immutable, galaxies undergo profound changes as they age. Their evolution depends on their position in the cosmic web, a network of sheets and filaments of matter that stretches across the entire Universe. The goal of FORNAX is to study the evolution of galaxies in the densest regions of the cosmic web, galaxy clusters. In these regions, a number of physical processes are thought to make galaxies lose their cold gas – the material from which new stars are born – and change their appearance dramatically. I will study these processes in action by observing the flow of cold gas in and out of galaxies living inside an important, nearby cluster of galaxies: Fornax.

I will observe Fornax for 2,450 hours with MeerKAT, a new, state-of-the-art radio telescope precursor of the Square Kilometre Array. Thanks to the unprecedented combination of sensitivity, resolution and sky-coverage of my survey, I will reveal the most subtle signs of the removal of gas from galaxies, I will detect the smallest gas-bearing galaxies in the cluster, and I will hunt the elusive cold gas which, according to cosmological theories, floats in the space between galaxies along the filaments of the cosmic web.","1500000","2016-10-01","2021-09-30"
"FOVEDIS","Formal specification and verification of distributed data structures","Constantin Enea","UNIVERSITE PARIS DIDEROT - PARIS 7","The future of the computing technology relies on fast access, transformation, and exchange of data across large-scale networks such as the Internet. The design of software systems that support high-frequency parallel accesses to high-quantity data is a fundamental challenge. As more scalable alternatives to traditional relational databases, distributed data structures (DDSs) are at the basis of a wide range of automated services, for now, and for the foreseeable future. 

This proposal aims to improve our understanding of the theoretical foundations of DDSs. The design and the usage of DDSs are based on new principles, for which we currently lack rigorous engineering methodologies. Specifically, we lack design procedures based on precise specifications, and automated reasoning techniques for enhancing the reliability of the engineering process.

The targeted breakthrough of this proposal is developing automated formal methods for rigorous engineering of DDSs. A first objective is to define coherent formal specifications that provide precise requirements at design time and explicit guarantees during their usage. Then, we will investigate practical programming principles, compatible with these specifications, for building applications that use DDSs. Finally, we will develop efficient automated reasoning techniques for debugging or validating DDS implementations against their specifications. The principles underlying automated reasoning are also important for identifying best practices in the design of these complex systems to increase confidence in their correctness. The developed methodologies based on formal specifications will thus benefit both the conception and automated validation of DDS implementations and the applications that use them.","1300000","2016-05-01","2021-04-30"
"FOXON","Functionality of Oxide based devices under Electric-field: Towards Atomic-resolution Operando Nanoscopy","Leopoldo MOLINA-LUNA","TECHNISCHE UNIVERSITAT DARMSTADT","Understanding oxygen dynamics is a key to superior device performance in emergent oxide electronics. So far it is an unrealized dream to correlate electrical behavior and atomic structure during device operation. Here, I envision bridging the gap between theoretical models and experimental reality. Recent advances in microelectromechanical systems (MEMS) chips for in situ transmission electron microscopy (TEM) are opening exciting new avenues in nanoscale research. The capability to perform current-voltage measurements while simultaneously analyzing the corresponding structural, chemical or even electronic structure changes during the operation of an electronic device would be a major breakthrough for nanoelectronics. Controlled electric field studies would enable an unprecedented way to investigate metal-oxide functional devices by using a lab-on-a-chip approach. I propose this project based upon own groundbreaking work on (i) how to electrically contact and operate an electron transparent lamella device fabricated from a metal-insulator-metal (MIM) structure (Ultramicroscopy 181 (2017) 144-149) and (ii) the design of a novel MEMS-based chip for in situ electrical biasing. FOXON will provide a platform for atomic scale operando investigations of oxide thin film and interface switching phenomena of MIM devices under electrical bias inside a microscope. My scientific endeavor will establish a group to develop beyond state-of-the-art operando TEM of MIM structured devices and tackle open questions in the field of oxide electronics. My scientific mission incorporates (a) studies of switching processes in oxide devices and (b) a comprehensive understanding of the atomic-level mechanisms that lead to tunable physical properties including dynamics of oxygen vacancies and stability of quantized conductance states in RRAM devices (Adv. Funct. Mater. (2017) 1700432). The results from this ERC Starting Grant could pave the way for novel quantum and information technologies.","1758600","2018-11-01","2023-10-31"
"FPCMB","Fundamental Physics from the Cosmic Microwave Background","Joanna Dunkley","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Much of the foundational evidence for our current model of cosmology, describing the origins and evolution of the Universe, has come from observations of the Cosmic Microwave Background (CMB). This is relic light that has been travelling for almost 14 billion years since the Big Bang, carrying a picture of the Universe in its infancy. So far it has told us what the Universe is made of today, as well as its average density and its age. We find that it is only 5% normal matter, with the remainder composed of unknown components: 72% Dark Energy and 23% Dark Matter. We do not yet know their nature. We have also seen signatures that support the idea that structure in the Universe was seeded by tiny ripples in the otherwise smooth space, created during a rapid expansion of the Universe in the first trillionth of a second, called  inflation'.

In Oxford I now propose to target additional information encoded in the CMB, by looking at measurements with higher resolution and sensitivity than ever before. The main goals of this proposal are to uncover convincing evidence for the inflationary scenario, and to better determine the nature of the Dark Energy component, particularly at early cosmic times. My team will be using data from the Atacama Cosmology Telescope, a 6m telescope in Chile, and from ESA's Planck Satellite mission, which is observing the CMB over the whole sky and launched in 2009. We will have to deal with contamination both from our own Galaxy and from many other distant galaxies in order to convincingly extract the underlying signals from the high energy Universe.","1500000","2011-01-01","2016-06-30"
"FPTOPT","First-passage times and optimization of target search strategies","Olivier, Jacques Benichou","UNIVERSITE PIERRE ET MARIE CURIE - PARIS 6","How long does it take a random walker to reach a given target? This quantity, known as a first-passage time (FPT), has been the subject of a growing number of theoretical studies over the past decade. The importance of FPTs originates from the crucial role played by properties related to first encounters in various real situations, including transport in disordered media, diffusion limited reactions, or more generally target search processes. First-passage times in confinement, their optimization and their relationship to biophysical experiments are at the heart of this project. The following two issues will be investigated.
1) We will determine key first-passage observables of general scale-invariant random walks in confinement, which up to now have remained inaccessible: FPT distribution in the presence of several targets and/or several searchers, statistical properties of the explored territory, FPT distribution of a non-Markovian random walker. Beyond their theoretical interest, these developments will allow us to address in close connection with single-molecule experiments the importance of transport and spatial organization for gene transcription kinetics and stochastic gene expression.
2) We will address the question of the optimization of the search time. We have recently introduced a new type of search strategies, the intermittent strategies, which minimize the search time under general conditions. Here, the objectives are: (i) to determine new first-passage observables of these intermittent processes (eg the full FPT distribution) to allow the comparison of optimal strategies to experimental situations; (ii) to understand the physical mechanisms underlying real intermittent pathways and assess their optimality at the molecular (homologous recombination kinetics), cellular (search for infection markers by dendritic cells) and macroscopic scales (individual search behavior of ants); (iii) to use intermittent strategies to design efficient searches.","1242800","2011-10-01","2017-09-30"
"FRACTALSANDMETRICNT","Fractals, algebraic dynamics and metric number theory","Michael Hochman","THE HEBREW UNIVERSITY OF JERUSALEM","We propose to study the fractal geometry of invariant sets for endomorphisms of compact abelian groups, specifically a family of conjectures by Furstenberg on the dimensions of orbit closures under such dynamics, and on the size of sums and intersections of invariant sets. These conjectures are related to problems on expansion in integer bases, in Diophantine approximation, measure rigidity, analysis and equidistribution. The project focuses on the conjectures themselves and some related problems, e.g. Bernoulli convolutions, and on applications to equidistribution on tori. Our approach combines tools from ergodic theory, geometric measure theory and additive combinatorics, building on recent progress in these fields and recent partial results towards the main conjectures.","1107000","2012-10-01","2018-09-30"
"FRECQUAM","Frequency Combs Quantum Metrology","Nicolas Treps","UNIVERSITE PIERRE ET MARIE CURIE - PARIS 6","Optical frequency combs are extraordinary tools for metrology which have been recently crowned by a Nobel prize: they have replaced complicated frequency chains to perform direct frequency and time measurements with much higher accuracy, which is now getting close to the quantum limit. However, quantum aspects of measurements performed with these sources have not yet been studied. This is the subject of this proposal. Based on model experiments such as space-time positioning, dispersion, velocity or frequency measurements, we propose to assess and reach experimentally ultimate limits derived from information theory in presence of quantum noise. We also propose to go beyond these limits using non-classical states. More specifically, we propose to fulfil the following objectives : &quot; Objective 1 : achieve the best absolute space-time positioning sensitivity ever using quantum optics techniques applied to frequency combs. &quot; Objective 2 : apply those techniques to other high sensitivity measurement such as dispersion, velocity or frequency metrology. &quot; Objective 3 : explore fundamental quantum physics effects in the lab with quantum frequency combs. These tasks will be performed by developing a quantum frequency comb factory, based on mode locked laser sources and parametric oscillators, whose conception is a research line in itself, and that would also be used for new quantum states generation such as macroscopic entanglement and multimode states.","1126000","2010-02-01","2016-01-31"
"FREECO","Freezing Colloids","Sylvain Stephane Francois Deville","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The freezing of colloids is an amazingly common phenomenon encountered in many natural and engineering processes such as the freezing of soils, food engineering or cryobiology. It can also be used as a bioinspired, versatile and environmentally-friendly processing route for bioinspired porous materials and composites exhibiting breakthroughs in functional properties. Yet, it is still a puzzling phenomenon with many unexplained features, due to the complexity of the system, the space and time scales at which the process should be investigated and the multidisciplinary approach required to completely apprehend it.
The objective is to progress towards a deep understanding of the freezing of colloids through novel in situ observations approaches and mathematical modelling, to exert a better control on the processing route and achieve the full potential of this novel class of bioinspired materials. Materials will be processed and their structure/properties relationships investigated and optimized.
This project offers a unique integration of approaches, competences and resources in materials science, chemistry, physics, mathematics and technological developments of observation techniques. For materials science only, the versatility of the process and its control could yield potential breakthroughs in numerous key applications of tremendous human, technological, environmental and economical importance such as catalysis, biomaterials or energy production, and open a whole new field of research. Far-reaching implications beyond materials science are expected, both from the developments in mathematics and physics, and from the implications of colloids freezing in many situations and fields of research.","1469034","2012-01-01","2017-12-31"
"FREENERGY","Lead-free halide perovskites for the highest efficient solar energy conversion","Antonio ABATE","UNIVERSITA DEGLI STUDI DI NAPOLI FEDERICO II","Achieving zero net carbon emissions by the end of the century is the challenge for capping global warming. The largest share of carbon emissions belongs to the production of electric energy from fossil fuels, which renewable energies are progressively replacing. Sunlight is an ideal renewable energy source since it is most abundant and available worldwide. Photovoltaic solar cells can directly convert the sunlight into electric energy by making use of the photovoltaic effect in semiconductors. Halide perovskites are emerging crystalline semiconducting materials with among the strongest light absorption and the most effective electric charge generation needed to design the highest efficient photovoltaic solar cells. The PI has the ambition to reinvent halide perovskites as environmentally friendly photovoltaic material, aiming at:
(i) Removing lead: state-of-the-art perovskite solar cells are based on lead, which is in the list of hazardous substances of the European Union. The PI will prepare new tin-based perovskites and prove them in the highest efficient solar cells.
(ii) Solvent-free crystallisation: organic solvents drive the crystallisation of the perovskite in the most efficient solar cells. However, crystallising the perovskite without using solvents is more environmentally friendly. The PI will establish physical vapour deposition as a solvent-free method for preparing the perovskite and the other materials comprising the solar cell.
(iii) Durable power output: the long-term power output defines the solar energy yield and thus the return on investment. The PI aims to make stable tin-based perovskites addressing the oxidative instability of tin directly.
The quantified target of FREENERGY is demonstrating a tin-based perovskite solar cell with power conversion efficiency over 20% and stability for 25 years. The research strategy to enable this disruptive outcome comprises innovative perovskites formulations and unconventional supramolecular interactions","1500000","2019-02-01","2024-01-31"
"FREQUJOC","Frequency-to-current conversion with coherent Josephson crystals","Wiebke Guichard","UNIVERSITE GRENOBLE ALPES","This project aims at exploring the coherence of Josephson crystals (JC) and to apply this coherence for frequency-to-current conversion. A Josephson crystal can be realized by a Josephson junction chain, formed by repeating a single junction or SQUID in space to form a one-dimensional ladder structure. Such a crystal can show a macroscopic coherent behavior due to the coherent superposition of quantum phase-slips (CQPS), ie the winding of 2 of the superconducting phase-difference occurring on single junctions. This project aims to perform a major breakthrough by addressing the coherence of circuits containing a large number of Josephson junctions. In particular this proposal aims, by novel experiments on Josephson junction chains, to understand the crucial questions of external charge dynamics and dissipation that originates from the many-body effects present in these chains. In order to fight against internal dissipation, I propose novel designs of Josephson junction chains with a disordered or fractal pattern. In addition, I propose to do a first systematic study on the external charge dynamics occurring in Josephson junction chains, in particular noise correlations.  Finally, I aim to use CQPS in a Josephson crystal to realize a frequency-to-current converter. This coherent JC should, under microwave irradiation of frequency f, exhibit exact current quantization I=2nef in multiples n of the electron charge e.","1466110","2013-01-01","2018-04-30"
"FRUSTRATED HYDROGEN","Activation of Hydrogen by Organic Frustrated Lewis Pairs. Applications in catalysis","Manuel Alcarazo Velasco","GEORG-AUGUST-UNIVERSITAT GOTTINGENSTIFTUNG OFFENTLICHEN RECHTS","Activation of H2 is typically a domain of transition metal catalysis -even nature uses metal-centred reactions for this process. However, very recently Stephan and Erker have described metal free systems able to activate dihydrogen by the use of Frustrated Lewis Pairs (FLP), a combination of sterically hindered Lewis acids and Lewis bases that for this geometric reason are not able to quench each other. Moreover they were able to apply these systems for the reduction of some imines using H2 directly instead of any organic surrogate.
Although this process is quite interesting, its applicability is nevertheless quite limited. Only silylenol ethers and very bulky imines can be satisfactorily reduced, non-encumbered ones simply react directly with B(C6F5)3, the ubiquitous  Lewis acid used to form FLPs.  To circumvent this obvious drawback I propose a new approach consisting in the use of completely organic molecules such as electron-poor allenes, alkenes, iminium salts and ketones as attractive alternatives to B(C6F5)3. The development of a totally organic FLP able to activate H2 reversibly goes far beyond the state-of-the-art as it is expected to allow the reduction of carbon-carbon and carbon-heteroatom double bonds without the use of any metal and employing directly H2. This wide-ranging extension of the FLP reactivity will clearly surpass classical metal catalysis in environmental issues as no noble metal will be necessary. Also, organocatalytic reductions will be outstripped in atom economy criteria as H2 and no heavier surrogate will be used as reducing agent.","1499000","2011-10-01","2017-05-31"
"FSA","Fluid Spectrum Acess","Alexandre Proutiere","KUNGLIGA TEKNISKA HOEGSKOLAN","Spectrum is a key and scarce resource in wireless communication networks, and it remains tightly controlled by regulation authorities. Most of the frequency bands are exclusively allocated to a single system licensed to use it everywhere and for long periods of time. This rigid spectrum management model inevitably leads to significant inefficiencies in spectrum use. The explosion of demand for broadband wireless services also calls for more flexible models where much larger spectrum parts could be dynamically shared among users in a fluid manner. In such models, Dynamic Spectrum Access (DSA) techniques will play a major role. These techniques make it possible for radio devices to become frequency-agile, i.e. able to rapidly and dynamically access bands of a wide spectrum part.

The success and spread of dynamic spectrum access strongly rely on the ability for many frequency-agile devices (or systems) to coexist peacefully and efficiently. With multiple interacting devices, the research agenda shifts from spectrum access problems to spectrum sharing problems, which raises original and challenging questions. There may be limited or no communication between the different devices or systems sharing spectrum. We further expect systems to be heterogeneous in their transmission capabilities, but also in the type of service they support. In that context, the design of spectrum access strategies resulting in an efficient and fair spectrum resource use constitutes a challenging puzzle. The broad objective of the proposed research is to develop original analytical and simulation tools to tackle dynamic spectrum sharing issues. The project leverages and marries techniques from distributed optimization and machine learning to design decentralized, efficient, and fair spectrum sharing algorithms. We believe that such algorithms are critical for the birth and rapid expansion of DSA technologies and hence for the development of future wireless broadband systems.","1197040","2012-11-01","2017-10-31"
"FSC","Fast and Sound Cryptography: From Theoretical Foundations to Practical Constructions","Alon Rosen","HERZLIYA","""Much currently deployed cryptography is designed using more “art'” than “science,” and most of the schemes used in practice lack rigorous justification for their security. While theoretically sound designs do exist, they tend to be quite a bit slower to run and hence are not realistic from a practical point of view. This gap is especially evident in “low-level” cryptographic primitives, which are the building blocks that ultimately process the largest quantities of data.

Recent years have witnessed dramatic progress in the understanding of highly-parallelizable (local) cryptography, and in the construction of schemes based on the mathematics of geometric objects called lattices. Besides being based on firm theoretical foundations, these schemes also allow for very efficient implementations, especially on modern microprocessors. Yet despite all this recent progress, there has not yet been a major effort specifically focused on bringing the efficiency of such constructions as close as possible to practicality; this project will do exactly that.

The main goal of the Fast and Sound Cryptography project is to develop new tools and techniques that would lead to practical and theoretically sound implementations of cryptographic primitives. We plan to draw ideas from both theory and practice, and expect their combination to generate new questions, conjectures, and insights. A considerable fraction of our efforts will be devoted to demonstrating the efficiency of our constructions. This will be achieved by a concrete setting of parameters, allowing for cryptanalysis and direct performance comparison to popular designs.

While our initial focus will be on low-level primitives, we expect our research to also have direct impact on the practical efficiency of higher-level cryptographic tasks. Indeed, many of the recent improvements in the efficiency of lattice-based public-key cryptography can be traced back to research on the efficiency of lattice-based hash functions.""","1498214","2012-10-01","2017-09-30"
"FTMEMS","Fiber-top micromachined devices: ideas on the tip of a fiber","Davide Iannuzzi","STICHTING VU","Fiber-top sensors (D. Iannuzzi et al., patent application number PCT/NL2005/000816) are a new generation of miniaturized devices obtained by carving tiny movable structures directly on the cleaved edge of an optical fiber. The light coupled into the fiber allows measurements of the position of the micromechanical parts with sub-nanometer accuracy. The monolithic structure of the device, the absence of electronic contacts on the sensing head, and the simplicity of the working principle offer unprecedented opportunities for the development of scientific instruments for applications in and outside research laboratories. For example, a fiber-top scanning probe microscope (also in the form of a PenFM, where a fiber-top atomic force microscope would be incorporated in a pen-like stylus) could be routinely used in harsh environments and could be easily handled by untrained personnel or through remote control systems – a fascinating perspective for utilization, among others, in surgery rooms and space missions. Similarly, the development of fiber-top biochemical sensors could be exploited for the implementation of portable equipment for in vivo and Point of Care medical testing. Fiber-top sensors could be used for the measurement of parameters of medical relevance in interstitial fluid or in blood – an interesting opportunity for intensive care monitoring and early detection of life-threatening diseases. This scenario calls for a coordinated research program dedicated to this novel generation of devices. It is my intention to forge a laboratory gravitating around fiber-top technology. My group will have the opportunity to pioneer this research area and to become the reference point in the field, on the forefront of an emerging subject that might represent a major breakthrough in the future development of micromachined sensors.","1799915","2008-06-01","2013-05-31"
"FUN-NOTCH","Fundamentals of the Nonlinear Optical Channel","Alex ALVARADO","TECHNISCHE UNIVERSITEIT EINDHOVEN","""Fibre optics are critical infrastructure for society because they carry nearly all the global Internet traffic. For a long time, optical fibre systems were thought to have infinite information-carrying capabilities. With current traffic demands growing by a factor between 10 and 100 every decade, however, this is no longer the case. In fact, it is currently unknown if the installed optical infrastructure will manage to cope with these demands in the future, or if we will face the so-called """"capacity crunch"""".

To satisfy traffic demands, transceivers are being operated near the nonlinear regime of the fibres. In this regime, a power-dependent nonlinear phenomenon known as the Kerr effect becomes the key impairment that limits the information-carrying capability of optical fibres. The intrinsic nonlinear nature of these fibres makes the analysis very difficult and has led to a series of unanswered fundamental questions about data transmission in nonlinear optical fibres, and nonlinear media in general. For example, the maximum amount of information that optical fibres can carry in the highly nonlinear regime is still unknown, and the design of transceivers well-suited for this regime is also completely unexplored.

In this project, the PI will answer these fundamental questions by studying the simplest nontrivial building blocks underlying optical fibres, and will give a definitive answer to the capacity crunch question. The PI will use a systematic methodology that aims at embracing nonlinear effects, consider the continuous-time channel as the correct starting point for analysis, and redesign optical transceivers from scratch, lifting all linear assumptions. The proposed methodology is in sharp contrast with current research trends, which aim at mitigating nonlinearities, and consider discrete-time models in the linear regime. Due to the central role of information transmission in modern society, the results in this project will have broad societal impact.""","1497982","2018-01-01","2022-12-31"
"FUN-PM","Fundamental Understanding of Nanoparticle chemistry: towards the prediction of Particulate emissions and Material synthesis","Andrea COMANDINI","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","While modern societies are facing urgent challenges related to reduction of particulate matter emissions from transportation engines, recent discoveries on the extraordinary properties of carbonaceous functional nanomaterials have revealed opportunities associated with large-scale, flame-based synthesis of these otherwise unwanted combustion products. In both cases, our ability to study new, optimized solutions based on the specific industrial end-user needs is limited by the absence of theoretical tools able to accurately predict the fluid dynamics and the chemistry involved in nanoparticle formation. Indeed, current knowledge on this fascinating but complex process is still rather incomplete. The proposed research program, FUN-PM, will apply an innovative multi-disciplinary, multi-step approach in order to finally answer many unresolved kinetic questions concerning in particular: 1) formation and growth of molecular PAH precursors; 2) particle inception; 3) subsequent particle growth and oxidation. Each single step will be experimentally isolated taking full advantage of complementary conventional shock tube techniques and up-to-date synchrotron-based detection technologies coupled to a newly constructed high-rate repetition shock tube. If successful, the novel synchrotron-shock tube techniques will be utilized for the first time to obtain unique information on unknown key processes. The experimental results, with extensive theoretical ab-initio calculations on relevant PAH reaction pathways, will constitute the base for the development of a comprehensive, detailed chemical kinetic model for particle chemistry applied to Real Fuels. Such model will improve the prediction capabilities of current CFD codes for use in engine design, fuel reformulation, or industrial process optimization, with considerable benefits to the standards of living of European citizens, the environment, and the EU economy, towards the future of clean transportations and novel nanomaterials.","1493839","2018-02-01","2023-01-31"
"FunCatDesign","Fundamental Studies in Catalysis – Reactivity Design with Experimental and Computational Tools","Franziska Schoenebeck","RHEINISCH-WESTFAELISCHE TECHNISCHE HOCHSCHULE AACHEN","Catalysis is ubiquitous in modern academic and industrial chemistry as well as an integral and indispensable discipline that may contribute to solutions of current global challenges. While the field has grown significantly over the past few decades, with numerous transformations that were previously unthinkable now being possible, progress has frequently relied on serendipitous discoveries or elaborate screening efforts. Although it is indisputable that high-throughput screening is an extremely lucrative approach to generate a wealth of chemical information, the next frontier in the development of innovative approaches to meet the high demand for predictable, selective and sustainable processes will likely arise from fundamental insight. However, owing to the complexity of catalytic processes, the fleeting and frequently highly sensitive nature of intermediates and the associated challenges in gaining fundamental mechanistic understanding, insight-driven developments and especially reactivity designs have so far been extremely rare. The objective of this proposal is to capitalize on the tools of experimental and computational chemistry as a powerful means to gain access to the fundamental mechanistic details of key catalytic steps that are required to allow reactivity design. The specific subject for study will focus on the most significant challenges in nickel-catalysis - a highly promising area in the context of sustainability and synthetic diversity owing to nickel’s relatively large abundance and also high reactivities towards relatively inert bonds. The proposed studies will address challenges in relation to Ni-catalyzed C-H activation, cross-coupling and trifluoromethylation reactions, as well as the exploration of novel avenues in catalysis at multinuclear sites.","1491875","2015-05-01","2020-04-30"
"FunCBonds","Chasing a Fundamental Challenge in Catalysis: A Combined Cleavage of Carbon-Carbon Bonds and Carbon Dioxide for Preparing Functionalized Molecules","Ruben Francisco Martin Romo","FUNDACIO PRIVADA INSTITUT CATALA D'INVESTIGACIO QUIMICA","FunCBonds offers a novel perspective to relevant scientific synthetic problems via a synergistic dual catalytic activation of carbon-carbon bonds and CO2, a topic of major interest not only for basic research science but also from an industrial and social point of view. As the use of alternative feedstocks such as CO2 is still one of the most fundamental gaps in catalytic technologies, I believe that FunCBonds project provides an alternative vision and strategy for the preparation of pharmaceutically relevant carboxylic acid derivatives using inexpensive raw materials in a catalytic fashion. In contrast to the well-established methodology based on carbon-carbon bond formation using either ruthenium or palladium catalysts (recently awarded with the Nobel Prize in Chemistry 2005 and 2010, respectively), the main challenge of this project is the discovery of a non-expensive and non-toxic catalyst that allows the cleavage of C-C bonds and CO2 following the principles of the atom economy. FunCBonds will meet these challenges by offering an innovative approach that will unlock the potential of a combined functionalization of inert C-C and C-O bonds. The project will provide the necessary understanding behind the factors influencing both C-C bond cleavage and the subsequent CO2 insertion event, thus opening up new horizons in preparative organic chemistry as well as offering solutions to a social and industrial problem such as the use of CO2 as chemical feedstock.","1423800","2011-12-01","2016-11-30"
"FUNCMOLQIP","Design and Preparation of Functional Molecules for Quantum Computing and Information Processing","Guillem Aromi","UNIVERSITAT DE BARCELONA","The future of Nanotechnology depends inevitably on the creation of molecular devices capable of performing crucial functions. We propose new strategies for the design and synthesis of molecular functional materials based on coordination chemistry, as well as the study of their physico-chemical properties in order to evaluate their relevance in the context of molecular spintronics and electronics. The main rationale underlying these strategies stems from the conviction that the unlimited potential of coordination compounds may be greatly exploited if the processes of self assembly leading to these systems are controlled and manipulated through the careful design of the ligands that will shape their structure and properties. We have designed the synthesis of new families of multinucleating ligands intended to form polynuclear coordination molecules with predetermined structures. Preliminary analysis of their performance has served to identify entries into novel categories of Single Molecule Magnets, SMMs, and Molecular Cluster Pairs, MCPs. The latter are stable molecules that exhibit two quasi independent metallic clusters, which fulfil many of the requirements necessary to act as 2qbit quantum gates for processors in quantum computing. We propose a full synthetic programme aimed at exploiting and expanding this promising avenue toward the fabrication of molecular systems that will be exploited in the context of Quantum Information Processing, QIP. In particular, we have identified from our previous work three classes of MCPs with promising features towards that end. We aim at exploiting the tools that we have created and develop new synthetic resources for the synthesis of robust molecules with the ability to act as 2qbits in QIP based on magnetic nanoclusters.","1500000","2011-07-01","2016-06-30"
"FUNCTIONALDYNA","Investigating Functional Dynamics in Proteins by Novel Multidimensional Optical Spectroscopies in the Ultraviolet","Andrea Cannizzo","UNIVERSITAET BERN","Proteins perform their biological function following specific sequences of events. During these dynamical paths, highly non-trivial cooperative interactions occur. Ultimately, this is the origin of the emerging collective behavior that makes proteins the most sophisticated existing molecular machines. This complex network of processes covers a wide range of timescales, from few fs to ms, and distances, from atoms to large protein domains.
Even the most recent experimental techniques generally provide ns-to-us averaged structural and dynamical information, often in non-physiological conditions. To access simultaneously atomic time and length scales would unveil the elementary conformational steps constituting a functional event and their temporal evolution.
I propose to extend emerging multidimensional ultrafast optical spectroscopic techniques to the deep ultraviolet. These techniques are the analogue of multidimensional Nuclear Magnetic Resonance methods and are able to provide structural information exploiting electric dipole couplings but with fs temporal resolution. The novel extension to ultraviolet, that I shall implement, will open the possibility to exploit the optical absorption of aromatic amino-acid residues with the great advantage of studying wild type proteins. In this way, all drawbacks due to artificial labeling will be ruled out. I will use this new technique to study dynamic-assisted long range electron transfer in copper proteins and enzyme regulation in hemoglobin. These two proteins of great importance from a biological point of view have been chosen because their functions are a clear manifestation of cooperative phenomena. On a long term prospective this methodology will be a universal tool applicable to any wild type protein containing aromatic amino acids.","1473600","2012-01-01","2016-12-31"
"FUNMANIA","Functional nano Materials for Neuronal Interfacing Applications","Yael Hanein","TEL AVIV UNIVERSITY","Recent advances in nano technologies provide an exciting new tool-box best suited for stimulating and monitoring neurons at a very high accuracy and with improved bio-compatibility. In this project we propose the development of an innovative nano-material based platform to interface with neurons in-vivo, with unprecedented resolution. In particular we aim to form the building blocks for future sight restoration devices. By doing so we will address one of the most challenging and important applications in the realm of in-vivo neuronal stimulation: high-acuity artificial retina.
Existing technologies in the field of artificial retinas offer only very limited acuity and a radically new approach is needed to make the needed leap to achieve high-resolution stimulation. In this project we propose the development of flexible, electrically conducting, optically addressable and vertically aligned carbon nanotube based electrodes as a novel platform for targeting neurons at high fidelity. The morphology and density of the aligned tubes will mimic that of the retina photo-receptors to achieve record-high resolution.
The most challenging element of the project is the transduction from an optical signal to electrical activations at high resolution placing this effort at the forefront of nano-science and nano-technology research. To deal with this difficult challenge, vertically aligned carbon nanotubes will be conjugated with additional engineered materials, such as conducting polymers and quantum dots to build a supreme platform allowing unprecedented resolution and bio-compatibility.  Ultimately, in this project we will focus on devising materials and processes that will become the building blocks of future devices so high density retinal implants and consequent sight restoration will become a reality in the conceivable future.","1499560","2012-10-01","2018-09-30"
"Future Proof","Theoretical and Algorithmic Foundations for Future Proof Information and Inference Systems","Volkan Cevher","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","A critical technological challenge for emerging information systems is to acquire, analyze and learn from the ever-increasing high-dimensional data produced by natural and man-made phenomena. Sampling, streaming, and recoding of even the most basic applications now produce a data deluge that severely stresses the available analog-to-digital converter, digital communication and storage resources, and easily swamps the back-end processing and learning systems.

Surprisingly, while the ambient data dimension is large in many problems, the relevant information therein typically resides in a much lower dimensional space. Viewed combinatorially and geometrically, natural constraints often cause data to cluster along low-dimensional structures, such as unions-of-subspaces or manifolds, having a few degrees of freedom relative to their size. This powerful notion suggests the potential for developing highly efficient methods for processing and learning by capturing and exploiting the inherent model, or data’s “information level.”

To this end, we seek to revolutionize scientific and practical modi operandi of data acquisition and learning by developing a new optimization and analysis framework based on the nascent low-dimensional models with broad applications—from inverse problems to analog-to-information conversion, and from automated representation learning to statistical regression. We attack the curse of dimensionality in specific ways, not only by relying on the blessing of dimensionality via concentration-of-measures, but also by exploiting geometric topologies and the diminishing returns (i.e.,  submodularity) within learning objectives. We believe only an approach such as ours can provide the theoretical scaffold for a future proof processing and learning framework that scales its operation to the problem’s information level, promising substantial reductions in hardware complexity, communication, storage, and computational resources.","1824220","2012-01-01","2016-12-31"
"G-SHTUKAS","Moduli spaces of local G-shtukas","Eva Viehmann","TECHNISCHE UNIVERSITAET MUENCHEN","This project provides a novel approach to the local Langlands programme via a comprehensive investigation of local G-shtukas and their moduli spaces and the exploitation of their relations to Shimura varieties.

Local G-shtukas are generalisations to arbitrary reductive groups of the local analogue of Drinfeld shtukas. They also are the function field counterpart of p-divisible groups. Hence moduli spaces of local G-shtukas are of great interest, in particular for the geometric realisation of local Langlands correspondences. Compared to p-divisible groups local G-shtukas have several advantages. They can be defined and studied for any reductive group, enabling a systematic use of group theoretic methods and promising unified results. Furthermore, their local description by elements of loop groups makes them more accessible than the description of p-divisible groups by Cartier theory or displays. Comparison theorems to p-divisible groups then provide a novel way to insight into their moduli spaces.

The research plan of this project is subdivided into three strands which mutually benefit from each other: Firstly we want to understand the representations realised in the cohomology of moduli spaces of local G-shtukas in connection with the geometric local Langlands programme. Secondly, we study the geometry of the moduli spaces and investigate several natural stratifications.  Finally, we build the bridge to Shimura varieties. On the one hand we explore the source of new results obtained by transferring methods developed for one of the two sides (Shimura varieties resp. moduli spaces of local G-shtukas) to prove similar assertions for the other. On the other hand we establish closer ties by proving direct comparison theorems.","900000","2011-09-01","2017-02-28"
"GADA","Group Actions: Interactions between Dynamical Systems and Arithmetic","Emmanuel Breuillard","UNIVERSITE PARIS-SUD","""Our main goal is to apply the powerful analytical tools that are now emerging from areas of more """"applicable"""" parts of mathematics such as ergodic theory, random walks, harmonic analysis and additive combinatorics to some longstanding open problems in more theoretical parts of mathematics such as group theory and number theory. The recent work of Green and Tao about arithmetic progressions of prime numbers, or Margulis' celebrated solution of the Oppenheim Conjecture about integer values of quadratic forms are examples of the growing interpenetration of such seemingly unrelated fields. We have in mind an explicit set of problems: a uniform Tits alternative, the equidistribution of dense subgroups, the Andre-Oort conjecture, the spectral gap conjecture, the Lehmer problem. All these questions involve group theory in various forms (discrete subgroups of Lie groups, representation theory and spectral theory, locally symmetric spaces and Shimura varieties, dynamics on homogeneous spaces of arithmetic origin, Cayley graphs of large finite groups, etc) and have also a number theoretic flavor. Their striking common feature is that each of them enjoys some intimate relationship, whether by the foreseen methods to tackle it or by its consequences, with ergodic theory on the one hand and harmonic analysis and combinatorics on the other. We believe that the new methods being currently developed in those fields will bring crucial insights to the problems at hand. This proposed research builds on previous results obtained by the author and addresses some of the most challenging open problems in the field.""","750000","2008-12-01","2013-11-30"
"GADGET","Geometry and Anomalous Dynamic Growth of Elastic instabiliTies","Dominic Vella","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Elastic instabilities are ubiquitous, from the wrinkles that form on skin to the ‘snap-through’ of an umbrella on a windy day. The complex patterns such instabilities make, and the great speed with which they develop, have led to a host of technological and scientific applications. However, recent experiments have revealed significant gaps in our theoretical understanding of such instabilities, particularly in the roles played by geometry and dynamics. I will establish a group to develop and validate a theoretical framework within which these results can be understood. Central to my approach is an appreciation of the crucial role of geometry in the pattern formation and dynamics of elastic instabilities.

As a starting point, I will consider the model problem of a pressurized elastic shell subject to a geometrically large deformation. This system develops either wrinkles or a stress-focusing instability depending on the internal pressure. As such, this is a natural paradigm with which to understand geometrical features of deformation relevant across length scales from deformed viruses to the subduction zones in Earth’s tectonic plates. My team will combine theoretical and computational approaches with tabletop experiments to determine a new set of shell deformations that are generically observed in contradiction of the classic ‘mirror buckling’. Understanding why these new shapes emerge will transform our perception of shell instabilities and provide new fundamental building blocks with which to model them. These ideas will also be used to transform our understanding of a number of other, previously mysterious, elastic instabilities of practical interest. Turning our focus to the dynamics of instabilities such as the snap-through of shells, we will show that accounting for geometry is again crucial. The new insight gained through this project will increase our ability to control elastic instabilities, benefitting a range of technological and scientific applications.","1361077","2015-04-01","2020-03-31"
"GALACTICA","Dynamical imprints of the evolutionary history of the Milky Way","Amina Helmi","RIJKSUNIVERSITEIT GRONINGEN","Galactic Astronomy is entering a new era, driven by state-of-the-art instrumentation and large surveys, and by the dramatic leaps in our understanding of galaxy formation provided by the cosmological LCDM framework. These surveys have shown that the Galaxy is up for discoveries every single month, and have revealed the first footprints of past mergers. This Era will reach its summit when the Gaia mission, scheduled for launch in 2011, provides the much-awaited survey of Galactic phase-space for a billion stars. This motivates us to propose a program that will provide a comprehensive view of the dynamical imprints leftover from the Galaxy s evolutionary history. This program will address the following key questions: How much memory does a galaxy like the Milky Way retain of its past? What is the relative importance of internally driven (secular processes) and externally acquired (mergers) phase-space substructure? What was the merging history of the Galaxy? Is the Galaxy consistent with LCDM? This ambitious program will advance the field of Galactic archaeology beyond the state-of-the-art thanks to two developments: the Aquarius Project simulations and the RAVE spectroscopic survey. The Aquarius are the largest ever cosmological simulations of a Milky Way dark matter halo. When complemented with a recently built phenomenological galaxy formation model, these superb simulations will serve for comparisons to the latest observational datasets, and in particular to the RAVE survey that is providing a fantastic dynamical map of the Solar vicinity. This will enable us to be in prime position to exploit the first Gaia data release in 2013, and before the end of this Research Program, to harvest its key scientific goal, namely to unravel the assembly history of the Milky Way.","1613680","2010-01-01","2015-12-31"
"GALATEA","Tailoring Material Properties Using Femtosecond Lasers: A New Paradigm for Highly Integrated Micro-/Nano- Scale Systems","Yves, Jérôme Bellouard","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Using recent progress in laser technology and in particular in the field of ultra-fast lasers, we are getting close to accomplish the alchemist dream of transforming materials. Compact lasers can generate pulses with ultra-high peak powers in the Tera-Watt or even Peta-Watt ranges. These high-power pulses lead to a radically different laser-matter interaction than the one obtained with conventional lasers. Non-linear multi-photons processes are observed; they open new and exciting opportunities to tailor the matter in its intimate structure with sub-wavelength spatial resolutions and in the three dimensions.

This project is aiming at exploring the use of these ultrafast lasers to locally tailor the physical properties of glass materials. More specifically, our objective is to create polymorphs embedded in bulk structures and to demonstrate their use as means to introduce new functionalities in the material.

The long-term objective is to develop the scientific understanding and technological know-how to create three-dimensional objects with nanoscale features where optics, fluidics and micromechanical elements as well as active functions are integrated in a single monolithic piece of glass and to do so using a single process.

This is a multidisciplinary research that pushes the frontier of our current knowledge of femtosecond laser interaction with glass to demonstrate a novel design platform for future micro-/nano- systems.","1757396","2012-12-01","2017-11-30"
"GALE","Games and Automata for Logic Extensions","Thomas Colcombet","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","This proposal aims at generalising the central decidability results of Büchi and Rabin to more general settings, and understanding the consequences of those extensions both at theoretical and applicative levels.

The original results of Büchi and Rabin state the decidability of monadic second-order logic (monadic logic for short) over infinite words and trees respectively. Those results are of such importance that Rabin's theorem is also called the `Mother of all decidability results'.

The primary goal of this project is to demonstrate that it is possible to go significantly beyond the expressiveness of monadic logic, while retaining similar decidability results. We are considering extensions in two distinct directions. The first consists of enriching the logic with the ability to speak in a weak form about set cardinality. The second direction is an extension of monadic logic by topological capabilities. Those two branches form the core of the proposal.

The second aspect of this proposal is the study of the `applicability' of this theory. Three tasks are devoted to this. The first task is to precisely determine the cost of using the more complex techniques we will be developing rather than using the classical theory. The second task will be devoted to the description and the study of weaker formalisms allowing better complexities, namely corresponding temporal logics. Finally, the last task will be devoted to the study of related model checking problems.

The result of the completion of this program would be twofold. At a theoretical level, new deep results would be obtained, and new techniques in automata, logic and games would be developed for solving them. At a more applicative level, this program would define and validate new directions of research in the domain of the verification of open systems and related problems.","931760","2011-01-01","2015-12-31"
"GALFOR","The formation of the Galaxy: constraints from globular clusters","Antonino MILONE","UNIVERSITA DEGLI STUDI DI PADOVA","For half a century, globular clusters (GCs) have been considered as prototype of simple stellar populations and their color-magnitude diagrams (CMDs) were believed to be the proxy of an isochrone.  
My research activity has resulted in one of the most exciting and unexpected developments in stellar-population studies in recent years: the discovery that the CMDs of many GCs are made of two or more intertwined sequences that can be followed continuously from the hydrogen-burning limit to the last stages of the stellar evolution. These findings, together with the discovery that multiple sequences correspond to distinct stellar populations with different helium abundance, have dramatically changed the traditional picture of these seemingly simple stellar systems.
Among the open issues are the still-eluding second-parameter problem of the horizontal-branch morphology and the formation mechanisms that build the Galactic halo. Multiple populations may correspond to different generations of stars and prove that GCs, similarly to dwarf galaxies, have experienced a very complex star-formation history. As an alternative, they can be the  product of exotic  phenomena that have taken place in the proto-GC environment only.
The Hubble-Space-Telescope archive is a golden mine to extend the innovative studies that I have  introduced on a few objects to a large sample of hundreds clusters. The proposed research is based on this huge dataset, together with the data that Hubble is in process of collecting as part of my surveys of GCs. I will use the last-generation image analysis and spectroscopic techniques to derive the first atlas of multiple  populations in the Milky Way and in Magellanic Cloud clusters, derive their helium abundance, investigate their chemical composition, spatial distribution and internal dynamics. In summary, I will  trace the series of events that led from massive clouds in the early Universe to the GCs we see today, with their multiple populations.","717246","2017-10-01","2022-09-30"
"GalNUC","Astrophysical Dynamics and Statistical Physics of Galactic Nuclei","Bence Kocsis","EOTVOS LORAND TUDOMANYEGYETEM","We address some of the major unsolved questions of galactic nuclei using methods of condensed matter physics. Galactic nuclei host a central supermassive black hole, a dense population of stars and compact objects, and in many cases a bright gaseous disk feeding the supermassive black hole. The observed stellar distribution exhibits both spherical and counterrotating disk-like structures. Existing theoretical models cannot convincingly explain the origin of the stellar disks. Is there also a “dark cusp” or “dark disk” of stellar mass black holes? Are there intermediate mass black holes in the Galactic center? We examine the statistical physics of galactic nuclei and their long term dynamical evolution. A star orbiting a supermassive black hole on an eccentric precessing orbit covers an axisymmetric annulus. The long-term gravitational interaction between such annuli is similar to the Coulomb interaction between axisymmetric molecules constituting a liquid crystal. We apply standard methods of condensed matter physics to examine these astrophysical systems. The observed disk and spherical structures represent isotropic-nematic phase transitions. We derive the phase space distribution and time-evolution of different stellar components including a population of black holes. Further, we investigate the interaction of a stellar cluster with a gaseous disk, if present. This leads to the formation of gaps, warps, and spiral waves in the disk, the redistribution of stellar objects, and possibly the formation of intermediate mass black holes. We explore the implications for electromagnetic and gravitational wave observatories. Dark disks of black holes could provide the most frequent source of gravitational waves for LIGO and VIRGO. These detectors will open a new window on the Universe; the proposed project will open a new field in gravitational wave astrophysics to interpret the sources. We also explore implications for electromagnetic observations.","1511436","2015-08-01","2020-07-31"
"GALSICO","Resolving Galaxy formation: Small-scale Internal physics in the Cosmological context","Frederic Bournaud","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","The formation of dark matter structures in our Universe can be explained by the standard cosmological model, but the populations of galaxies observed in the distant and nearby Universe pose major challenges to our understanding of galaxy formation. There is increasing recognition that the visible, baryonic part of galaxies does not passively follow the hierarchical build-up of dark halos. A large part of the baryons can be accreted from cold gas flows along the cosmic web. The evolution of galaxies could then be mostly driven by their internal evolution, in addition to interactions and mergers. Many scall-scale processes with major effects on galaxy evolution have been unveiled. They have, however, been studied mostly one by one, ignoring the large-scale cosmological environment. Conversely, cosmological models do not resolve the small-scale internal processes properly yet. This dramatically limits our understanding of galaxy formation. The project is to develop an multi-scale understanding of galaxy formation. We will build  comprehensive numerical models of the small-scale gas physics and star formation processes in, and incorporate them in large-scale cosmological simulations. Taking benefit from the best forthcoming computing facilities, this will develop a new understanding of the role of internal physics and external processes in structuring galaxies. Theoretical predictions will be confronted to observations, preparing and using the next generation of instruments along the whole duration of the project. Owing to a uniquely comprehensive approach including physical processes at different scales and an original combination of theory, simulation and observation, a new understanding of the evolution of the baryons through cosmic times can emerge from the project.","988400","2011-02-01","2016-01-31"
"GAMMARAYBINARIES","Exploring the gamma-ray sky: binaries, microquasars and their impact on understanding particle acceleration, relativistic winds and accretion/ejection phenomena in cosmic sources","Guillaume Dubus","UNIVERSITE JOSEPH FOURIER GRENOBLE 1","The most energetic photons in the universe are produced by poorly known processes, typically in the vicinity of neutron stars or black holes. The past couple of years have seen an increase in the number of known sources of very high energy gamma-ray radiation from a handful to almost 50, thanks to the European collaborations HESS and MAGIC. Many of those sources are pulsar wind nebulae, supernova remnants or active galactic nuclei. HESS and MAGIC have also discovered gamma-ray emission from binary systems, finding that some emit most of their radiation at the highest energies. Expectations are running high with the December launch of the GLAST space telescope which will provide daily all-sky information in high energy gamma-rays with a sensitivity comparable to that achieved in years by its predecessor. I propose to explore the exciting observational opportunities in high energy gamma-ray astronomy with an emphasis on non-thermal emission from compact binary sources. Binary systems are intriguing new laboratories to understand how particle acceleration works in cosmic sources. The physics of gamma-ray emitting binary systems is related to that in pulsar wind nebulae or in active galactic nuclei. High energy gamma-ray emission is the result of non-thermal, out-of-equilibrium processes that challenge our intuitions built upon everyday phenomena. The particles are billions of times more energetic than X-rays and can reach energies greater than those in particle accelerators. Binary systems offer a novel, constrained environment to study how the cosmic rays that pervade our Galaxy are accelerated and how non-thermal emission is related to the formation of relativistic jets from black holes (accretion/ejection). The study requires a combination of skills in multiwavelength observations, interdisciplinary experience with gamma-ray observational techniques originating from particle physics, and theoretical know-how in accretion and high energy phenomena.","794752","2008-07-01","2013-06-30"
"GAN","Groups, Actions and von Neumann algebras","Cyril Houdayer","UNIVERSITE PARIS-SUD","This research project focuses on the structure, classification and rigidity of three closely related objects: group actions on measure spaces, orbit equivalence relations and von Neumann algebras. Over the last 15 years, the study of interactions between these three topics has led to a process of mutual enrichment, providing both striking theorems and outstanding conjectures.

Some fundamental questions such as Connes' rigidity conjecture, the structure of von Neumann algebras associated with higher rank lattices, or the fine classification of factors of type III still remain untouched. The general aim of the project is to tackle these problems and other related questions by developing a further analysis and understanding of the interplay between von Neumann algebra theory on the one hand, as well as ergodic and group theory on the other hand. To do so, I will use and combine several tools and develop new ones arising from Popa's Deformation/Rigidity theory, Lie group theory (lattices, boundaries), topological and geometric group theory and representation group theory (amenability, property (T)). More specifically, the main directions of my research project are:

1) The structure of the von Neumann algebras arising from Voiculescu's Free Probability theory: Shlyakhtenko's free Araki-Woods factors, amalgamated free product von Neumann algebras and the free group factors.

2) The structure and the classification of the von Neumann algebras and the measured equivalence relations arising from lattices in higher rank semisimple connected Lie groups.

3) The measure equivalence rigidity of the Baumslag-Solitar groups and several other classes of discrete groups acting on trees.","876750","2015-04-01","2020-03-31"
"GANOMS","GaAs Nano-OptoMechanical Systems","Ivan Favero","UNIVERSITE PARIS DIDEROT - PARIS 7","""A Nano-OptoMechanical System (NOMS) is an ideal interface between nanomechanical motion and photons. The merits of such a system depend crucially on the level of optical/mechanical coupling. For sufficient coupling, the nanomechanical motion is efficiently imprinted on photons and read-out with the assets of optical detection: broadband, fast, ultra sensitive (ultimately quantum limited). Moreover, in a NOMS, the very dynamics of the motion (its frequency, damping, noise spectrum) can be controlled by optical forces. This opens novel roads for nanomechanical sensing experiments, both classical or quantum, that need now to be experimentally investigated and brought in compliance with future on-chip applications.
This project relies on Gallium-Arsenide (GaAs) disk optomechanical resonators, where photons are stored in high quality factor optical whispering gallery cavities and interact with high frequency (GHz) nanomechanical modes. We have recently shown that these resonators possess a record level of optomechanical coupling and are compatible with on-chip optical integration. The first aim of the project is to investigate in depth the mechanisms leading to optical and mechanical dissipation in GaAs nanoresonators, and obtain GaAs NOMS with ultra-low dissipation. The second aim is to realize prototype nano-optomechanical force measurements with a GaAs disk resonator set in optomechanical self-oscillation, to establish the potential of this novel approach for sensing. This will be done both under vacuum and in a liquid. The behavior of two NOMS integrated on the same chip will also be studied, as first archetype of parallel architectures. A third aim is to operate GaAs NOMS at their quantum limit, using cryogenics, optomechanical cooling and novel concepts where an active optical material like a Quantum dot or Quantum well is inserted in the GaAs NOMS to enhance optomechanical interactions. Transfer of quantum states within a QD-NOMS coupled system will be explored.""","1495800","2013-02-01","2018-01-31"
"GasAroundGalaxies","Studying the gas around galaxies with the Multi Unit Spectroscopic Explorer and hydrodynamical simulations","Joop Schaye","UNIVERSITEIT LEIDEN","""Gas accretion and galactic winds are two of the most important and poorly understood ingredients of models for the formation and evolution of galaxies. We propose to take advantage of two unique opportunities to embark on a multi-disciplinary program to advance our understanding of the circumgalactic medium (CGM).

We will use MUSE, a massive optical integral field spectrograph that we helped to develop and that will be commissioned on the VLT in 2012, to study the CGM in both absorption and emission. We will use 200 hours of guaranteed time to carry out deep redshift surveys of fields centred on bright z≈3.5 and z≈5 QSOs. This will yield hundreds of faint galaxies (mainly Lyα emitters) within 250 kpc of the lines of sight to the background QSOs, an order of magnitude increase compared to the best existing sample (bright, z≈2.3 galaxies). This will allow us to map the CGM in absorption in 3-D using HI and metal lines and to identify, for the first time, the counterparts to most metal absorbers.  MUSE will also enable us to detect Lyα emission from the denser CGM (also using another 300 hours of guaranteed time targeting deep HST fields) and thus to directly explore its connection with galaxies and QSO absorbers.

We will use the new supercomputer of the Virgo consortium to carry out cosmological hydro simulations that contain 1-2 orders of magnitude more resolution elements than the largest existing (spatially adaptive) runs. We will use the results of our previous work to guide our choice of parameters in order to obtain a better match to the observed mass function of galaxies. In parallel, we will carry out a complementary program of zoomed simulations of individual galaxies. These will have the physics and resolution to include a cold gas phase and hence to bypass much of the """"subgrid"""" physics used in the cosmological runs. Both types of simulations will be used to study the physics of gas flows around galaxies and to guide the interpretation of our observations.""","1496400","2012-09-01","2017-08-31"
"GASPARCON","Molecular steps of gas-to-particle conversion: From oxidation to precursors, clusters and secondary aerosol particles.","Mikko SIPILÄ","HELSINGIN YLIOPISTO","Atmospheric aerosol particles impact Earth’s climate, by directly scattering sunlight and indirectly by affecting cloud properties. The largest uncertainties in climate change projections are associated with the atmospheric aerosol system that has been altered by anthropogenic activities. A major source of that uncertainty involves the formation of secondary particles and cloud condensation nuclei from natural and anthropogenic emissions of volatile compounds. This research challenge persists despite significant efforts within recent decades.

I will build a research group that aims to resolve the atmospheric oxidation processes that convert volatile trace gases to particle precursor vapours, clusters and new aerosol particles. We will create novel measurement techniques and utilize the tremendous potential of mass spectrometry for detection of i) particle precursor vapours ii) oxidants, both conventional but also recently discovered stabilized Criegee intermediates, and, most importantly, iii) newly formed clusters. These methods and instrumentation will be applied for resolving the initial steps of new particle formation on molecular level from oxidation to clusters and stable aerosol particles. To reach these goals, targeted laboratory and field experiments together with long term field measurements will be performed employing the state-of-the-art instrumentation developed.

Principal outcomes of this project include i) new experimental methods and techniques vital for atmospheric research and a deep understanding of ii) oxidation pathways producing aerosol particle precursors, iii) the initial molecular steps of new particle formation and iv) mechanisms of growth of freshly formed clusters toward larger sizes, particularly in the crucial size range of a few nanometers. The conceptual understanding obtained during this project will open multiple new research horizons from oxidation chemistry to Earth system modeling.","1953790","2017-02-01","2022-01-31"
"Gauge-string duality","GAUGE-STRING DUALITY AND NON-EQUILIBRIUM PHYSICS","Andrei Starinets","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The proposal is to study non-equilibrium states of strongly correlated quantum systems relevant for heavy ion and condensed matter physics by using existing and developing new methods of gauge-string duality (also know as holography or AdS/CFT correspondence). The gauge-string duality is a set of non-perturbative tools developed within string theory over the last fourteen years. These methods can be used independently of the final status of the string theory itself. Strongly coupled model systems at finite temperature and density are of great interest for they appear in many areas of physics including physics of heavy ion collisions and physics of trapped cold atoms. Gauge-string duality methods already proved very useful in supplying information about transport properties such as viscosity and spectral functions of thermal quantum field theories at strong coupling.

Specific goals of the proposal are divided into two sets, one including open problems in non-equilibrium systems accessible for study by the existing gauge/string duality techniques, and another involving more challenging problems requiring new holographic approaches. Problems of the first set include generalizing existing models of thermalization and isotropization, constructing simple model(s) describing the initial state of the quark-gluon plasma, exploring gravity backgrounds obtained by self-consistent top-down approach, studying theories with dual gravity backgrounds including full back-reaction. Problems of the second set involve holographic approach to turbulence and plasma instabilities, building holographic formalism for highly nonequilibrium processes and  studying possible connection between holography and emergent gravity.","1461074","2012-10-01","2017-09-30"
"GAUGE/GRAVITY","The Gauge/Gravity Duality and Geometry in String Theory","Dario Martelli","KING'S COLLEGE LONDON","While the three sub-atomic forces are described by quantum mechanics, the fourth known force, gravity, is described by Einstein's theory of general relativity. These two very successful theories are incompatible, and understanding how to unify them in a single framework is an outstanding problem. String theory is the most prominent candidate for a unified theory of all forces of Nature. The most important conceptual breakthrough that emerged from string theory is Maldacena's conjectured duality between quantum field theory and gravity, known as AdS/CFT correspondence. This states that strings moving in anti-de Sitter (AdS) space-time, may equivalently be described by a type of quantum theory, called conformal field theory (CFT). More generally, it is a remarkable duality between quantum gauge theories in d dimensions and gravitational theories in (d+1)-dimensional space-times, implying that quantum theory and gravity, instead of being conflicting, are in fact equivalent. In this project I will aim at extending the gauge/gravity duality in multiple directions, which go beyond the current state of the art. In order to achieve a deeper understanding of the gauge/gravity duality I plan to develop novel mathematical approaches, that are likely to lead to new research directions in different areas of physics and mathematics. More specifically, the objectives of this project include: a systematic study of AdS backgrounds arising in string theory as a method for exploring CFTs; the development of geometric structures, such as generalised Sasaki-Einstein geometry, relevant for the AdS/CFT correspondence; a study of supersymmetric gauge theories on curved manifolds and of their gravity duals; a study of dualities between pairs of gauge theories and of related matrix models arising from localisation techniques; exploring the gauge/gravity duality as a tool for studying strongly interacting quantum critical phenomena, such as those that are of interest to real-world physics.","1253098","2013-01-01","2018-12-31"
"GECOMETHODS","Geometric control methods for heat and Schroedinger equations","Ugo Boscain","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""The aim of this project of 5 years is to create a research group on geometric control methods in PDEs with the arrival of the PI at the CNRS Laboratoire CMAP (Centre de Mathematiques Appliquees) of the Ecole Polytechnique in Paris (in January 09). With the ERC-Starting Grant, the PI plans to hire 4 post-doc fellows, 2 PhD students and also to organize advanced research schools and workshops. One of the main purpose of this project is to facilitate the collaboration with my research group which is quite spread across France and Italy. The PI plans to develop a research group studying certain PDEs for which geometric control techniques open new horizons. More precisely the PI plans to exploit the relation between the sub-Riemannian distance and the properties of the kernel of the corresponding hypoelliptic heat equation and to study controllability properties of the Schroedinger equation. In the last years the PI has developed a net of high level international collaborations and, together with his collaborators and PhD students, has obtained many important results via a mixed combination of geometric methods in control (Hamiltonian methods, Lie group techniques, conjugate point theory, singularity theory etc.) and noncommutative Fourier analysis. This has allowed to solve open problems in the field, e.g., the definition of an intrinsic hypoelliptic Laplacian, the explicit construction of the hypoelliptic heat kernel for the most important 3D Lie groups, and the proof of the controllability of the bilinear Schroedinger equation with discrete spectrum, under some """"generic"""" assumptions. Many more related questions are still open and the scope of this project is to tackle them. All subjects studied in this project have real applications: the problem of controllability of the Schroedinger equation has direct applications in Nuclear Magnetic Resonance; the problem of nonisotropic diffusion has applications in models of human vision.""","785000","2010-05-01","2016-04-30"
"GEDENTQOPT","Generation and detection of many-particle entanglement in quantum optical systems","Geza Toth","UNIVERSIDAD DEL PAIS VASCO/ EUSKAL HERRIKO UNIBERTSITATEA","During the last decade, quantum entanglement has been intensively studied within quantum information science and has also appeared as a natural goal of recent quantum experiments. Because of that the theoretical background of detecting entanglement has been rapidly developing. However, most of this development concentrated on bipartite or few-party entanglement, while today's experiments typically involve many particles. Thus, as one of the most interesting part of quantum optics and quantum information, I chose to study multi-partite entanglement theory, with a stress on creation and generation of many-particle entanglement. There are two main system types in today's experiments. In some systems all particles are individually accessible, such as trapped ions or photons. In such systems entanglement detection is still a challenge as the number of local measurements is limited. I propose to study efficient methods for detecting entanglement in such systems. In other physical systems, such as cold ensembles of a million atoms, particles are not accessible individually and only collective measurements are possible. To obtain useful information about the quantum state is a challenge. I propose to study entanglement creation and detection also in such systems. The latter topic is naturally connected to the  efficient modeling of large quantum systems, since exact modeling is not possible for such system sizes.","1294350","2011-03-01","2017-02-28"
"GEL-SYS","Smart HydroGEL SYStems – From Bioinspired Design to Soft Electronics and Machines","Martin KALTENBRUNNER","UNIVERSITAT LINZ","Hydrogels evolved as versatile building blocks of life – we all are in essence gel-embodied soft machines. Drawing inspiration from the diversity found in living creatures, GEL-SYS will develop a set of concepts, materials approaches and design rules for wide ranging classes of soft, hydrogel-based electronic, ionic and photonic devices in three core aims.
Aim (A) will pursue a high level of complexity in soft, yet tough biomimetic devices and machines by introducing nature-inspired instant strong bonds between hydrogels and antagonistic materials – from soft and elastic to hard and brittle. Building on these newly developed interfaces, aim (B) will pursue biocompatible hydrogel electronics with iontronic transducers and large area multimodal sensor arrays for a new class of medical tools and health monitors. Aim (C) will foster the current soft revolution of robotics with self-sensing, transparent grippers not occluding objects and workspace. A soft robotic visual system with hydrogel-based adaptive optical elements and ultraflexible photosensor arrays will allow robots to see while grasping. Autonomous operation will be a central question in soft systems, tackled with tough stretchable batteries and energy harvesting from mechanical motion on small and large scales with soft membranes. GEL-SYS will use our experience on soft, “imperceptible” electronics and devices. By fusing this technology platform with tough hydrogels - nature’s most pluripotent ingredient of soft machines - we aim to create the next generation of bionic systems. The envisioned hybrids promise new discoveries in the nonlinear mechanical responses of soft systems, and may allow exploiting triggered elastic instabilities for unconventional locomotion. Exploring soft matter, intimately united with solid materials, will trigger novel concepts for medical equipment, healthcare, consumer electronics, energy harvesting from renewable sources and in robotics, with imminent impact on our society.","1499975","2018-01-01","2022-12-31"
"GELANDERINDGEOMRGD","Independence of Group Elements and Geometric Rigidity","Tsachik Gelander","THE HEBREW UNIVERSITY OF JERUSALEM","The proposed research contains two main directions in group theory and geometry: Independence of Group Elements and Geometric Rigidity. The first consists of problems related to the existence of free subgroups, uniform and effective ways of producing such, and analogous questions for finite groups where the analog of independent elements are elements for which the Cayley graph has a large girth, or non-small expanding constant. This line of research began almost a century ago and contains many important works including works of Hausdorff, Banach and Tarski on paradoxical decompositions, works of Margulis, Sullivan and Drinfeld on the Banach-Ruziewicz problem, the classical Tits Alternative, Margulis-Soifer result on maximal subgroups, the recent works of Eskin-Mozes-Oh and Bourgain-Gamburd, etc. Among the famous questions is Milnor's problem on the exponential verses polynomial growth for f.p. groups, originally stated for f.g. groups but reformulated after Grigorchuk's counterexample. Related works of the PI includes a joint work with Breuillard on the topological Tits alternative, where several well known conjectures were solved, e.g. the foliated version of Milnor's problem conjectured by Carriere, and on the uniform Tits alternative which significantly improved Tits' and EMO theorems. A joint work with Glasner on primitive groups where in particular a conjecture of Higman and Neumann was solved. A paper on the deformation varieties where a conjecture of Margulis and Soifer and a conjecture of Goldman were proved. The second involves extensions of Margulis' and Mostow's rigidity theorems to actions of lattices in general topological groups on metric spaces, and extensions of Kazhdan's property (T) for group actions on Banach and metric spaces. This area is very active today. Related work of the PI includes his joint work with Karlsson and Margulis on generalized harmonic maps, and his joint work with Bader, Furman and Monod on actions on Banach spaces.","750000","2008-07-01","2013-12-31"
"GEM","Gain-endowed metallic meta-structures and devices: 
towards a unification of photonics and electronics","Raffaele Colombelli","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","This research project aims at building a common framework for photonic and meta-material /electronic devices. As specific examples, we will focus on making key advances for THz and mid-infrared emitters and detectors. Our goal is to demonstrate device functionalities which cannot be achieved within the current scientific/technological framework.
We will implement these concepts on quantum cascade lasers, a family of devices whose potential has seen an explosion in the last few years. They cover the THz (1-10THz) and mid-infrared (10-100THz) ranges of the spectrum, of great importance for medical and environmental applications, and security screening. Their spectral location - between near-infrared and microwaves - makes them ideal candidates to profit from both worlds.

We will first address a fundamental physical issue, then we’ll turn to device applications. While optical resonators are constrained to a minimum dimension set by the wavelength, a peculiar signature of an electronic oscillator is its independence from it. Our first goal is the demonstration of an optical oscillator/laser with the functionalities of an electronic oscillator, i.e. with fundamentally no lower size limit in the three dimensions of space. It will behave as a point-source of radiation.

We will then focus on ground-breaking demonstrations in the THz and mid-infrared ranges of the spectrum: THz lasers which are frequency tuneable acting on a disentangled (electronic) section; electronically beam-steerable devices; antenna-coupled THz quantum detectors. The integration with MEMS (micro-electro-mechanical systems) is also developed in the proposal. This project will also open up horizons and research opportunities on longer-term topics. We will develop strategies to compensate the ohmic losses of mid-IR meta-materials, in order to extend the developed concepts to mid-IR devices.","1497248","2012-10-01","2018-06-30"
"GEM","From Geometry to Motion: inverse modeling of complex mechanical structures","Florence Bertails-Descoubes","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","With the considerable advance of automatic image-based capture in Computer Vision and Computer Graphics these latest years, it becomes now affordable to acquire quickly and precisely the full 3D geometry of many mechanical objects featuring intricate shapes. Yet, while more and more geometrical data get collected and shared among the communities, there is currently very little study about how to infer the underlying mechanical properties of the captured objects merely from their geometrical configurations.

The GEM challenge consists in developing a non-invasive method for inferring the mechanical properties of complex objects from a minimal set of geometrical poses, in order to predict their dynamics. In contrast to classical inverse reconstruction methods, my proposal is built upon the claim that 1/ the mere geometrical shape of physical objects reveals a lot about their underlying mechanical properties and 2/ this property can be fully leveraged for a wide range of objects featuring rich geometrical configurations, such as slender structures subject to frictional contact (e.g., folded cloth or twined filaments).

To achieve this goal, we shall develop an original inverse modeling strategy based upon a/ the design of reduced and high-order discrete models for slender mechanical structures including rods, plates and shells, b/ a compact and well-posed mathematical formulation of our nonsmooth inverse problems, both in the static and dynamic cases, c/ the design of robust and efficient numerical tools for solving such complex problems, and d/ a thorough experimental validation of our methods relying on the most recent capturing tools.

In addition to significant advances in fast image-based measurement of diverse mechanical materials stemming from physics, biology, or manufacturing, this research is expected in the long run to ease considerably the design of physically realistic virtual worlds, as well as to boost the creation of dynamic human doubles.","1498570","2015-09-01","2021-08-31"
"GENEPHYSCHEM","Spatio-temporal control of gene expression by physico-chemical means: from in vitro photocontrol to smart drug delivery","Damien Baigl","UNIVERSITE PIERRE ET MARIE CURIE - PARIS 6","We propose to undertake a new challenge: the control of gene expression systems by physico-chemical means to achieve the following objectives: i) developing robust tools for spatio-temporal control of protein expression; ii) understanding the role of micro-environmental factors in gene regulation; and iii) constructing and implementing in vivo smart nanomachines able to express active molecules in response to a stimulus and deliver them to a targeted cell. First, various biochemical processes (transcription, translation) will be controlled by light in vitro, based on photo-induced conformational changes of nucleic acids (DNA, RNA) and chromatin. Based on conformational changes rather than specific template-protein interaction, and combined with microfluidic methodologies, this novel approach will provide a ubiquitous tool to address gene expression using light regardless of the sequence, with unique control and spatio-temporal resolution. Second, by reconstituting photo-responsive gene expression systems in well-defined giant liposomes, we will study the dynamics of gene expression in response to light stimulation. This will allow us to establish the respective roles of the membrane (surface charge, permeability) and of the inner micro-environment composition (viscosity, molecular crowding). Third, we will develop stable, long-circulating polymer nanocapsules (polymersomes) encapsulating a gene expression material that can be triggered by light and/or molecules of biological interest. In response to the signal, an exogenous, potentially immunogenic enzyme will be expressed inside the protecting nanocapsule to locally and catalytically convert a non toxic precursor present in the medium into a cytotoxic drug that will be delivered to a cell (e.g., a cancer cell). This new concept of triggerable gene-carrying nanomachines with unique amplification capacity of drug secretion shall open new horizons for the development of smart biological probes and future therapeutics.","1450320","2011-01-01","2015-12-31"
"GeneREFORM","Genetically Encoded Multicolor Reporter Systems For Multiplexed MRI","Amnon Bar-Shir","WEIZMANN INSTITUTE OF SCIENCE LTD","In order to fully understand the complexity of biological processes that are reflected by simultaneous occurrences of intra and inter-cellular events, multiplexed imaging platforms are needed. Fluorescent reporter genes, with their “multicolor” imaging capabilities, have revolutionized science and their founders have been awarded the Nobel Prize. Nevertheless, the light signal source of these reporters, which restricts their use in deep tissues and in large animals (and potentially in humans), calls for alternatives. 
Reporter genes for MRI, although in their infancy, showed several exceptionalities, including the ability to longitudinal study the same subject with unlimited tissue penetration and to coregister information from reporter gene expression with high-resolution anatomical images. Inspired by the multicolor capabilities of optical reporter genes, this proposal aims to develop, optimize, and implement genetically engineered reporter systems for MRI with artificial “multicolor” characteristics. Capitalizing on (i) the Chemical Exchange Saturation Transfer (CEST)-MRI contrast mechanism that allows the use of small bioorganic molecules as MRI sensors, (ii) the frequency encoding, color-like features of CEST, and on (iii) enzyme engineering procedures that allow the optimization of enzymatic activity for a desired substrate, a “multicolor” genetically encoded MRI reporter system is proposed. 
By (a) synthesizing libraries of non-natural nucleosides (“reporter probes”) to generate artificially “colored” CEST contrast, and (b) performing directed evolution of deoxyribonucleoside kinase (dNK) enzymes (“reporter genes”) to phosphorylate those nucleosides, the “multicolor” genetically encoded MRI “reporter system” will be created. The orthogonally of the obtained pairs of substrate (CEST sensor)/ enzyme (mutant dNK) will allow their simultaneous use as a genetically encoded reporter system for in vivo “multicolor” monitoring of reporter gene expression with MRI.","1478284","2016-05-01","2021-04-30"
"GenGeoHol","Non AdS holography and generalized geometric structures","Diego HOFMAN","UNIVERSITEIT VAN AMSTERDAM","Holography is by now a fundamental tool in the understanding of both strongly coupled conformal field theories (CFTs) and quantum theories of gravity. While holography in Anti de Sitter (AdS) space-times is rather well understood, we currently lack a basic picture of what it means in non-AdS space-times. Considering non-AdS space-times is an essential and urgent next step in the study of quantum gravity as we seem to live in a universe with a positive cosmological constant that is approaching de Sitter (dS) in the far future. Also, the near-horizon geometries of black holes are typically described by more exotic geometries that need to be understood on their own right.

I propose to address this and study the physics of holographic systems on non-AdS space-times and their connection to generalized geometric structures that naturally arise in these setups. In order do this I will use both conventional field theory techniques and new holographic tools, some of which I have developed recently. 

The relevance of GenGeoHol is illustrated by universal properties of black holes, e.g. their area-law entropy. These are independent of AdS, pointing towards the existence of a more general holographic principle that generalizes the usual symmetries and geometric notions. A great deal of evidence has accumulated recently indicating that this is indeed the case. The physics of extremal black holes and non relativistic systems are clear examples. 

GenGeoHol will impact a wide range of fields. As one moves away from AdS Einstein gravity, the dual quantum-field theories present different symmetries from that of usual relativistic systems. These systems couple naturally to generalized background geometries which are of intrinsic interest and key to a range of concepts extending from Newton-Cartan geometry in non-relativistic systems to higher-spin geometries for so-called W_N CFTs.
Given my experience and track record, I am uniquely positioned to attack this problem successfully.","1300775","2017-09-01","2022-08-31"
"GEO-4D","Geodetic data assimilation: Forecasting Deformation with InSAR","Romain JOLIVET","ECOLE NORMALE SUPERIEURE","Recent space-based geodetic measurements of ground deformation suggest a paradigm shift is required in our understanding of the behaviour of active tectonic faults. The classic view of faults classified in two groups – the locked faults prone to generate earthquakes and the creeping faults releasing stress through continuous aseismic slip – is now obscured by more and more studies shedding light on a wide variety of seismic and aseismic slip events of variable duration and size. What physical mechanism controls whether a tectonic fault will generate a dynamic, catastrophic rupture or gently release energy aseismically? Answering such a fundamental question requires a tool for systematic and global detection of all modes of slip along active faults.
The launch of the Sentinel 1 constellation is a game changer as it provides, from now on, systematic Radar mapping of all actively deforming regions in the world with a 6-day return period. Such wealth of data represents an opportunity as well as a challenge we need to meet today. In order to expand the detection and characterization of all slip events to a global scale, I will develop a tool based on machine learning procedures merging the detection capabilities of all data types, including Sentinel 1 data, to build time series of ground motion.
The first step is the development of a geodetic data assimilation method with forecasting ability toward the first re-analysis of active fault motion and tectonic phenomena. The second step is a validation of the method on three faults, including the well-instrumented San Andreas (USA) and Longitudinal Valley faults (Taiwan) and the North Anatolian Fault (NAF, Turkey). I will deploy a specifically designed GPS network along the NAF to compare with outputs of our method. The third step is the intensive use of the algorithm on a global scale to detect slip events of all temporal and spatial scales for a better understanding of the slip behaviour along all active continental faults.","1499125","2018-01-01","2022-12-31"
"GeoArchMag","Beyond the Holocene Geomagnetic field resolution","Ron Shaar","THE HEBREW UNIVERSITY OF JERUSALEM","For decades the Holocene has been considered a flat and “boring” epoch from the standpoint of
paleomagnetism, mainly due to insufficient resolution of the available paleomagnetic data. However, recent
archaeomagnetic data have revealed that the Holocene geomagnetic field is anything but stable – presenting
puzzling intervals of extreme decadal-scale fluctuations and unexpected departures from a simple dipolar field
structure. This new information introduced an entirely new paradigm to the study of the geomagnetic field and
to a wide range of research areas relying on paleomagnetic data, such as geochronology, climate research, and
geodynamo exploration.
This proposal aims at breaking the resolution limits in paleomagnetism, and providing a continuous
time series of the geomagnetic field vector throughout the Holocene at decadal resolution and
unprecedented accuracy. To this end I will use an innovative assemblage of data sources, jointly unique to
the Levant, including rare archaeological finds, annual laminated stalagmites, varved sediments, and arid
playa deposits. Together, these sources can provide unprecedented yearly resolution, whereby the “absolute”
archaeomagnetic data can calibrate “relative” terrestrial data.
The geomagnetic data will define an innovative absolute geomagnetic chronology that will be used to
synchronize cosmogenic 10Be data and an extensive body of paleo-climatic indicators. With these in hand, I
will address four ground-breaking problems:
I) Chronology: Developing dating technique for resolving critical controversies in Levantine archaeology and
Quaternary geology.
II) Geophysics: Exploring fine-scale geodynamo features in Earth’s core from new generations of global
geomagnetic models.
III) Cosmogenics: Correlating fast geomagnetic variations with cosmogenic isotope production rate.
IV) Climate: Testing one of the most challenging controversial questions in geomagnetism: “Does the Earth's
magnetic field play a role in climate changes?”","1786381","2018-11-01","2023-10-31"
"GEODESI","Theoretical and observational consequences of the Geometrical Destabilization of Inflation","Sébastien Maurice Marceau RENAUX-PETEL","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The GEODESI project aims at interpreting current and forthcoming cosmological observations in a renewed theoretical framework about cosmological inflation and its ending. The simplest toy models of inflation economically explain all current data, leaving no observational clue to guide theorists towards a finer physical understanding. In this context, I very recently unveiled an hitherto unnoticed instability at play in the primordial universe that potentially affects all inflationary models and drastically modifies the interpretation of cosmological observations in terms of fundamental physics. The so-called Geometrical Destabilization of inflation reshuffles our understanding of the origin of structures in the universe, offers a new mechanism to end inflation, and promises unrivaled constraints on high-energy physics. It is crucial to develop this fresh look before a host of high-quality data from large-scale structure surveys and cosmic microwave background observations become available within the 5 year timescale of the project.

With the ERC grant I plan to build a group at the Institute of Astrophysics of Paris (IAP-CNRS) with the objective of determining the full theoretical and observational consequences of the geometrical destabilization of inflation. We will combine insights from non-standard cosmological perturbation theory and lattice simulations to constrain realistic models of inflation in high-energy physics, producing accurate theoretical predictions for a wide variety of observables, including the spectra and the non-Gaussianities of primordial fluctuations and stochastic backgrounds of gravitational waves.","1476672","2018-02-01","2023-01-31"
"GEODYCON","Geometry and dynamics via contact topology","Vincent Maurice Colin","UNIVERSITE DE NANTES","I intend to cross ressources of holomorphic curves techniques and traditional topological methods to study some fundamental questions in symplectic and contact geometry such as:

- The Weinstein conjecture in dimension greater than 3.
- The construction of new invariants for both smooth manifolds and Legendrian/contact manifolds, in particular, try to define an analogue of Heegaard Floer homology in dimension larger than 3.
- The link, in dimension 3, between the geometry of the ambient manifold (especially hyperbolicity) and the dynamical/topological properties of its Reeb vector fields and contact structures.
- The topological characterization of odd-dimensional manifolds admitting a contact structure.

A crucial ingredient of my program is to understand the key role played by open book decompositions in dimensions larger than three.

This program requires a huge amount of mathematical knowledges. My idea is to organize a team around Ghiggini, Laudenbach, Rollin, Sandon and myself, augmented by two post-docs and one PhD student funded by the project. This will give us the critical size to organize a very active working seminar and to have a worldwide attractivity and recognition.
I also plan to invite one confirmed researcher every year (for 1-2 months), to organize one conference and one summer school, as well as several focused weeks.","887600","2012-01-01","2016-12-31"
"GEOFLUIDS","Geometric problems in PDEs with applications to fluid mechanics","Alberto Enciso Carrasco","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","There are many high-profile problems in PDEs that ultimately boil down to assertions of a strongly geometric or topological nature. One feature that makes these problems both very difficult and extremely appealing is that there is not a standard set of techniques that one can routinely resort to in order to attack them. Indeed, the very nature of these questions makes them strongly interdisciplinary, so successful approaches require finely tailored combinations of ideas and techniques coming from different branches of mathematics (analysis, geometry and topology), often interspersed with some physical intuition. In this project I aim at going significantly beyond the state of the art in a wide class of geometric questions in PDEs, with an emphasis on problems in fluid mechanics and encompassing long-standing questions that can be traced back to leading analysts and geometers such as Arnold, De Giorgi and Yau. The project is divided in three interrelated blocks, respectively devoted to the study of Beltrami fields in steady incompressible fluids, to geometric evolution problems and to global approximation theorems. Key to the proposal is a versatile new approach to a number of geometric problems in PDEs that I have pioneered and applied in several seemingly unrelated contexts. The power of this technique is laid bare by my recent proofs of a well-known conjecture on knotted vortex lines in topological fluid mechanics that was popularized by Arnold and Moffatt in the 1960s and of a long-standing conjecture on the existence of thin vortex tubes in steady solutions to the Euler equation that dates back to Lord Kelvin in 1875. The award of a Starting Grant will enable me to establish a top-level research group on these topics.","1256375","2015-03-01","2020-02-29"
"GEOMANGROUP","Geometry and Analysis of Group Rings","Andreas Thom","TECHNISCHE UNIVERSITAET DRESDEN","Eversince, the study of discrete groups and their group rings has attracted researchers from various
mathematical branches and led to beautiful results with proofs involving fields such as number theory,
combinatorics and analysis. The basic object of study is the structure of the group G itself, i.e. its subgroups, quotients, etc. and properties of the group ring kG with coefficients in a field k.

Recently, techniques such as Randomization and Algebraic Approximation have lead to fruitful insights.
This project is focused on new and groundbreaking applications of these two techniques in the
study of groups and group rings. In order to illustrate this, I am explaining how useful these techniques are by focusing on three interacting topics: (i) new characterizations of amenability related to Dixmier’s Conjecture, (ii) the Atiyah conjecture for discrete groups, and (iii) algebraic approximation in the algebraic K-theory of algebras of functional analytic type. All three problems are presently wide open and progress in any of the three problems would mean a breakthrough in current research.

Using Randomization techniques, I want to achieve important results in the understanding of groups
rings by contributing to a better understanding of conjectures of Dixmier’s and Atiyah’s. The field of
Algebraic Approximation is new, and has already been successfully used by G. Cortinas and myself to
resolve a longstanding conjecture in Algebraic K-theory due to Jonathan Rosenberg.","900000","2011-10-01","2016-09-30"
"GeoMeG","Geometry of Metric groups","Enrico LE DONNE","UNIVERSITA DI PISA","What are the best trajectories to park a truck with several trailers?
How fast can a lattice grow? These are some of the questions studied in this project because both the infinitesimal control structure of movement of a truck and the asymptotic geometry of a (nilpotent) lattice are examples of metric groups: Lie groups with homogeneous distances.

The PI plans to study  geometric properties of metric groups and their implications to control systems and nilpotent groups. In particular, the plan is to exploit the relation between the regularity of distinguished curves, sets, and maps in subRiemannian groups, volume asymptotics in nilpotent groups, and embedding results.
The general goal is to develop an adapted geometric measure theory.

SubRiemannian spaces, and in particular Carnot groups, appear in various areas of mathematics, such as control theory, harmonic and complex analysis, asymptotic geometry, subelliptic PDE's and geometric group theory. The results in this project will provide more links between such areas.

The PI has developed a net of high-level international collaborations and obtained several results via a combination of analysis on metric spaces (differentiation of Lipschitz maps, tangents of measures, and Gromov-Hausdorff limits) and the theory of locally compact groups (Lie group techniques and the solutions of the Hilbert 5th problem). This allowed the PI to solve a number of open problems in the field, such as the analogue of Myers-Steenrod theorem on the smoothness of isometries, the analogue of Nash isometric embedding and the non-minimality of curves with corners.
Some of the next aims are to establish an analogue of the De Giorgi's rectifiability result for finite-perimeter sets and prove the smoothness of geodesics, a 30-year-old open problem.
The goal of this project is to tackle them, together with many more related questions.

The PI received his first degree at SNS Pisa (advisor: M.Abate) and his PhD from Yale University (advisor: B.Kleiner). Before obtaining a permanent position only three years after graduation, he was at ETH, Orsay, and MSRI. He received the prestigious position of research fellow of the Academy of Finland.","1248560","2017-08-01","2022-07-31"
"GEOPARDI","Numerical integration of Geometric Partial Differential Equations","Erwan Faou","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","""The goal of this project is to develop new numerical methods for the approximation of evolution equations possessing strong geometric properties such as Hamiltonian systems or stochastic differential equations. In such situations the exact solutions endow with many physical properties that are consequences of the geometric structure: Preservation of the total energy, momentum conservation or existence of ergodic invariant measures. However the preservation of such qualitative properties of the original system by numerical methods at a reasonable cost is not guaranteed at all, even for very precise (high order) methods.

The principal aim of  geometric numerical integration is the understanding and analysis of such problems: How (and to which extend) reproduce qualitative behavior of differential equations over long time? The extension of this theory to partial differential equations is a fundamental ongoing challenge, which require the invention of a new mathematical framework bridging the most recent techniques used in the theory of nonlinear PDEs and stochastic ordinary and partial differential equations. The development of new efficient numerical schemes for geometric PDEs has to go together with the most recent progress in analysis (stability phenomena, energy transfers, multiscale problems, etc..)

The major challenges of the project are to derive new schemes by bridging the world of numerical simulation and the analysis community, and to  consider deterministic and stochastic equations, with a general aim at deriving hybrid methods. We also aim to create a research platform devoted to extensive numerical simulations of difficult academic PDEs in order to highlight new nonlinear phenomena and test numerical methods.""","971772","2011-09-01","2016-08-31"
"GEOPDES","Innovative compatible discretization techniques for Partial Differential Equations","Annalisa Buffa","CONSIGLIO NAZIONALE DELLE RICERCHE","Partial Differential Equations (PDEs) are one of the most powerful mathematical modeling tool and their use spans from life science to engineering and physics. In abstract terms, PDEs describe the distribution of a field on a physical domain. The Finite Element Method (FEM) is by large the most popular technique for the computer-based simulation of PDEs and hinges on the assumption that the discretized domain and field are represented both by means of piecewise polynomials. Such an isoparametric feature is at the very core of FEM. However, CAD software, used in industry for geometric modeling, typically describes physical domains by means of Non-Uniform Rational B-Splines (NURBS) and the interface of CAD output with FEM calls for expensive re-meshing methods that result in approximate representation of domains. This project aims at developing isoparametric techniques based on NURBS for simulating PDEs arising in electromagnetics, fluid dynamics and elasticity. We will consider discretization schemes that are compatible in the sense that the discretized models embody conservation principles of the underlying physical phenomenon (e.g. charge in electromagnetism, mass and momentum in fluid motion and elasticity). The key benefits of NURBS-based methods are: exact representation of the physical domain, direct use of the CAD output, a substantial increase of the accuracy-to-computational-effort ratio. NURBS schemes start appearing in the Engineering literature and preliminary results show that they hold great promises. However, their understanding is still in infancy and sound mathematical groundings are crucial to quantitatively assess the performance of NURBS techniques and to design new effective computational schemes. Our research will combine competencies in different fields of mathematics besides numerical analysis, such as functional analysis and differential geometry, and will embrace theoretical issues as well as computational testing.","750000","2008-07-01","2013-06-30"
"GeopolyConc","Durability of geopolymers as 21st century concretes","John Lloyd Provis","THE UNIVERSITY OF SHEFFIELD","GeopolyConc will provide the necessary scientific basis for the prediction of the long-term durability performance of alkali-activated ‘geopolymer’ concretes. These materials can be synthesised from industrial by-products and widely-available natural resources, and provide the opportunity for a highly significant reduction in the environmental footprint of the global construction materials industry, as it expands to meet the infrastructure needs of 21st century society. Experimental and modelling approaches will be coupled to provide major advances in the state of the art in the science and engineering of geopolymer concretes. The key scientific focus areas will be: (a) the development of the first ever rigorous mathematical description of the factors influencing the transport properties of alkali-activated concretes, and (b) ground-breaking work in understanding and controlling the factors which lead to the onset of corrosion of steel reinforcing embedded in alkali-activated concretes. This project will generate confidence in geopolymer concrete durability, which is essential to the application of these materials in reducing EU and global CO2 emissions. The GeopolyConc project will also be integrated with leading multinational collaborative test programmes coordinated through a RILEM Technical Committee (TC DTA) which is chaired by the PI, providing a route to direct international utilisation of the project outcomes.","1495458","2013-09-01","2018-08-31"
"GEoREST","predictinG EaRthquakES induced by fluid injecTion","Victor VILARRASA","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","Fluid injection related to underground resources has become widespread, causing numerous cases of induced seismicity. If felt, induced seismicity has a negative effect on public perception and may jeopardise wellbore stability, which has led to the cancellation of several projects. Forecasting injection-induced earthquakes is a big challenge that must be overcome to deploy geo-energies to significantly reduce CO2 emissions and thus mitigate climate change and reduce related health issues. The basic conjecture is that, while initial (micro)seisms are caused by well-known mechanisms that could be predicted, subsequent activity is caused by harder to understand and, at present, unpredictable coupled thermo-hydro-mechanical-seismic (THMS) processes, which is the reason why available models fail to forecast induced seismicity. The objective of this project is to develop a novel methodology to predict and mitigate induced seismicity. We propose an interdisciplinary approach that integrates the THMS processes that occur in the subsurface as a result of fluid injection. The methodology, based on new analytical and numerical solutions, will concentrate on (1) understanding the processes that lead to induced seismicity by model testing of specific conjectures, (2) improving and extending subsurface characterization by using industrial fluid injection operations as a long-term continuous characterization methodology, so as to reduce prediction uncertainty, and (3) using the resulting understanding and site specific knowledge to predict and mitigate induced seismicity. Project developments will be tested and verified against fluid-induced seismicity at field sites that present diverse characteristics. Arguably, the successful development of this project will provide operators with concepts and tools to perform pressure management to reduce the risk of inducing seismicity to acceptable levels and thus, improve safety and reverse public perception on fluid injection activities.","1438201","2019-02-01","2024-01-31"
"GEOWAKI","The analysis of geometric non-linear wave and kinetic equations","Jacques, Alexandre SMULEVICI","UNIVERSITE PARIS-SUD","The present proposal is concerned with the analysis of geometric non-linear wave equations, such as the Einstein equations, as well as coupled systems of wave and kinetic equations such as the Vlasov-Maxwell and Einstein-Vlasov equations. We intend to pursue three main lines of research, each of them concerning major open problems in the field.
I) The dynamics in a neighbourhood of the Anti-de-Sitter space with various boundary conditions.
This is a fundamental open problem of mathematical physics which aims at understanding the stability or instability properties of one of the simplest solutions to the Einstein equations. On top of its intrinsic mathematical interest, this question is also at the heart of an intense research activity in the theoretical physics community.
II) Non-linear systems of wave and kinetic equations. We have recently found out that the so-called vector field method of Klainerman, a fundamental tool in the study of quasilinear wave equations, in fact possesses a complete analogue in the case of kinetic transport equations. This opens the way to many new directions of research, with applications to several fundamental systems of kinetic theory, such as the Einstein-Vlasov or Vlasov-Maxwell systems, and creates a link between two areas of PDEs which have typically been studied via different methods. One of our objectives is to develop other potential links, such as a general analysis of null forms for relativistic kinetic equations.
III) The Einstein equations with data on a compact manifold. The long time dynamics of solutions to the Einstein equations arising from initial data given on a compact manifold is still very poorly understood. In particular, there is still no known stable asymptotic regime for the Einstein equations with data given on a simple manifold such as the torus. We intend to establish the existence of such a stable asymptotic regime.","1071008","2017-02-01","2022-01-31"
"GEQIT","Generalized (quantum) information theory","Renato Renner","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Information theory is a branch of science that studies, from a mathematical perspective, the processing, transmission, and storage of information. The classical theory has been established in 1948 by Claude Shannon and has later been extended to incorporate processes where information is represented by the state of quantum systems.

A major limitation of the present theory of information is that various of its concepts and methods require, as an assumption, that the processes to be studied are iterated many times. For example, Shannon's well-known result that the Shannon entropy equals the data compression rate assumes a source that repeatedly emits data according to the same given distribution. In addition, such results are often only valid asymptotically as the number of iterations tends to infinity.

While this limitation is normally acceptable when studying classical information-processing tasks such as channel coding (since communication channels are typically used repeatedly), it turns out to be a severe obstacle when analyzing new types of applications such as quantum cryptography. For instance, there is generally no sensible way to describe the attack strategy of an adversary against a quantum key distribution scheme as a recurrent process.

The goal of this project is to overcome this limitation and develop a theory of (classical and quantum) information which is completely general.  Among the potential applications are new types of cryptographic schemes providing device-independent security. That is, their security guarantees hold independently of the details (and imperfections) of the actual implementations.","1288792","2010-12-01","2015-11-30"
"GFTIPFD","Geometric function theory, inverse problems and fluid dinamics","Daniel Faraco Hurtado","UNIVERSIDAD AUTONOMA DE MADRID","The project will strike  for conquering frontier results in three capital areas in  partial differential equations and mathematical analysis: Elliptic equations and systems, fluid dynamics and inverse problems.

I propose  to tackle the central problems in these areas with a new perspective based on the theory of differential inclusions. A thorough study of oscillating div-curl couples  in this framework  will lead us to the long expected  higher dimensional version of the Tartar conjecture. The  corresponding analysis of differential inclusions for gradient fields will lead to new results respect to  the existence, uniqueness and regularity theory on the so far intractable theory of higher dimensional Beltrami systems.  Next we will concentrate in weak solutions to the classical non linear equations governing  fluid dynamics. A reformulation of  these equations as differential inclusions enables a much more rich theory of weak solutions than the classical one. With this new tool at hand,we will  close several long standing questions about existence, uniqueness and contour dynamics. The third part of the project is devoted to inverse problems in p.d.e.   The most famous inverse problem is Calderón conductivity problem which asks whether the Dirichlet to Neumann map of an  elliptic equation  determines the coefficients. The problem is still open in three or more dimensions  but a new formulation as a differential inclusion will  allow us to close the 1980 Calderón conjecture by constructing new invisible materials.  In dimension n=2 the recent approach based on quasiconformal theory will lead  to the first regularization scheme valid for discontinuous conductivities and  first results for non linear equations. For the stationary Schrödinger equation I propose to exploit a fascinating connection with  the convergence to initial data of  the  non elliptic time dependent  Schrödinger equation.","1121400","2012-10-01","2018-09-30"
"GLASS","InteGrated Laboratories to investigate the mechanics of ASeismic vs. Seismic faulting","Cristiano Collettini","ISTITUTO NAZIONALE DI GEOFISICA E VULCANOLOGIA","Earthquakes are potentially catastrophic phenomena that have a huge impact on the environment and society. Understanding the physical processes responsible for earthquakes and faulting requires high quality data and direct observations of the underlying phenomena. However, no direct measurements can be made at depth where earthquakes initiate and propagate. Our knowledge of the mechanical properties of fault zones relies on Earth surface observations and experiments conducted in rock deformation laboratories. Despite recent progress, we have much to learn about the mechanics of earthquakes and the complex and inherently scale-dependent processes that govern earthquake faulting.

Central Italy is a unique test site that can serve as a natural laboratory for the integration of high resolution data gathered from different disciplines. I propose to develop my innovative and multidisciplinary research to unravel the physico-chemico processes responsible for faulting phenomena ranging from aseismic creep to seismic slip. GLASS will aim to:
(i) locate and analyze different types of transient seismic signals from the actively deforming crust, such as fast/slow and high/low frequency earthquakes and non volcanic tremors;
(ii) study deformation processes in outcrops of ancient faults that represent exhumed analogues of the active structures today;
(iii) characterize the fluid flow and frictional properties of faults in rock deformation experiments;
(iv) investigate earthquake nucleation and recurrence by developing numerical models that will be constrained by field and experimental data and calibrated by seismological records.
The proposed research will allow to create unprecedented insight into the mechanics of earthquakes and to investigate deformation processes from the crustal to the nano-scale and from a time window ranging from the seismic cycle to entire geologic fault history.","1514400","2010-10-01","2015-09-30"
"GLENCO","Gravitational Lensing as a Cosmological Probe","Robert Benton Metcalf","ALMA MATER STUDIORUM - UNIVERSITA DI BOLOGNA","I propose to develop new observational probes of cosmology based
on gravitational lensing.  The project consists of three themes:

1) Computer tools will be developed for detecting and measuring small-scale structures in
the distribution of dark matter using strong gravitational lenses.  This will resolve one of the
few persistent conflicts between observations and the predictions of the Cold Dark Matter
(CDM) model.  The tools will enable the scientific community to take full advantage of present
imaging data and future data coming from missions like the proposed EUCLID satellite.

2) There are several proposed and ongoing surveys designed to measure the weak
gravitational lensing of galaxies across large regions of the sky and to measure the redshifts
of hundreds of millions of galaxies.  To maximize the use of this data I propose a joint analysis
of redshift and lensing surveys that will increase the sensitivity to the expansion history of the Universe and separate two observational consequences of dark energy which are otherwise degenerate in weak lensing data alone. This would enable us to verify whether general
relativity is valid on cosmological scales.

3) The feasibility of studying cosmology using future radio observations of the 21 cm radiation coming from early epochs of the Universe will be investigated.  My work has shown that the
gravitational lensing of this radiation could provide powerful constraints on the density of the Universe, dark energy's equation of state, and the masses of neutrinos. It should also be
possible to actually map the distribution of dark matter across large sections of sky with high
fidelity.  My goal is to treat the observational noise and systematics realistically and determine
what impact telescope design and survey strategy have on the scientific output.","1500000","2011-03-01","2016-02-29"
"GLOBALVISION","Global Optimization Methods in Computer Vision, Pattern Recognition and Medical Imaging","Fredrik Kahl","LUNDS UNIVERSITET","Computer vision concerns itself with understanding the real world through the analysis of images. Typical problems are object recognition, medical image segmentation, geometric reconstruction problems and navigation of autonomous vehicles. Such problems often lead to complicated optimization problems with a mixture of discrete and continuous variables, or even infinite dimensional variables in terms of curves and surfaces. Today, state-of-the-art in solving these problems generally relies on heuristic methods that generate only local optima of various qualities. During the last few years, work by the applicant, co-workers, and others has opened new possibilities. This research project builds on this. We will in this project focus on developing new global optimization methods for computing high-quality solutions for a broad class of problems. A guiding principle will be to relax the original, complicated problem to an approximate, simpler one to which globally optimal solutions can more easily be computed. Technically, this relaxed problem often is convex. A crucial point in this approach is to estimate the quality of the exact solution of the approximate problem compared to the (unknown) global optimum of the original problem. Preliminary results have been well received by the research community and we now wish to extend this work to more difficult and more general problem settings, resulting in thorough re-examination of algorithms used widely in different and trans-disciplinary fields. This project is to be considered as a basic research project with relevance to industry. The expected outcome is new knowledge spread to a wide community through scientific papers published at international journals and conferences as well as publicly available software.","1440000","2008-07-01","2013-06-30"
"GLOWING","Spatio-temporal measurement and plasma-based control of crossflow instabilities for drag reduction","Marios Kotsonis","TECHNISCHE UNIVERSITEIT DELFT","Delay of laminar-turbulent flow transition on aircraft wings can potentially reduce aerodynamic drag by up to 15%, reducing emissions and fuel consumption considerably. The main cause of laminar-turbulent transition on commonly used swept wings is the development of crossflow (CF) instabilities. Despite their importance, our fundamental understanding of CF instabilities is limited due to inability of current measurement techniques to capture their complex and multi-scale spatio-temporal features. This severely limits our ability to delay CF transition, which is further impeded by the lack of simple, robust and efficient control concepts.

In this proposal I will achieve unprecedented spatio-temporal measurements of CF instabilities and develop a novel active flow control system that can successfully delay transition on swept wings. To achieve these goals, I bring forth a unique combination of cutting-edge technologies, such as tomographic particle image velocimetry, advanced plasma-based actuators and linear/non-linear stability and control theory. 

Spatio-temporal volumetric velocity measurements of CF instabilities will be achieved at three important stages of their life, namely inception, growth and breakdown, providing breakthrough insights into the underlying physics of swept wing transition and turbulence production. The results will be used to postulate and validate linear and non-linear stability and control theory models and provide top benchmarks for high-fidelity CFD. The unprecedented wealth of information, enabled through these advances, will be used to design and demonstrate the first synergetic plasma-based laminar flow control system. This system will feature minimum-thickness plasma actuators, able to suppress the growth of CF instabilities and achieve and sustain considerable transition delay at high Reynolds numbers. These advances will finally enable robust and efficient laminar flow on future air transport.","1499460","2019-02-01","2024-01-31"
"GlycoEdit","New Chemical Tools for Precision Glycotherapy","Thomas BOLTJE","STICHTING KATHOLIEKE UNIVERSITEIT","Glycosylation, the expression of carbohydrate structures on proteins and lipids, is found in all the domains of life. The collection of all glycans found on a cell is called the “glycome” which is information rich and a key player in a plethora of physiological and pathological processes. The information that the glycome holds can be written, read and erased by glycosyltransferases, lectins and glycosidases, respectively. The immense structural complexity and the fact that glycan biosynthesis is not under direct genetic control makes it very difficult to study the glycome.
 
The glycosylation pattern of cancer cells is very different from that of healthy cells. It is still unclear whether aberrant glycosylation of cancer cells is a cause or consequence of tumorigenesis but it is associated with aggressive and invasive forms of cancer and hence poor prognosis. Malignant glycans are directly involved in a number of mechanisms that suppress the immune response, increase migration and extravasation (metastasis), block apoptosis and increase resistance to chemotherapy.
 
The aim of this proposal is develop new glycomimetics that can be used to edit the glycome of cancer cells to target such evasive mechanisms. Using combinations of new glycan based inhibitors, a coordinated attack on the cancer glycome can be carried out which is expected to severely cripple the cancers ability to grow and metastasize. This will make the tumor more susceptible to immune mediated killing which may be further enhanced in combination with other anti-cancer strategies.

To minimize systemic side effects, new methods for the local delivery/activation of glycan inhibitors will be developed. The developed methods are expected to have a much broader than just cancer alone since the studied mechanisms are also associated with autoimmune and neurodegenerative disease.","1500000","2017-11-01","2022-10-31"
"GLYCOTRACKER","Tracking Glycosylations with Targeted, Molecule-Sized “Noses”","David Margulies","WEIZMANN INSTITUTE OF SCIENCE LTD","Glycobiology is poised to be the next revolution in biology and medicine; however, technical difficulties in detecting and characterizing glycans prevent many biologists from entering this field, thus hampering new discoveries and innovations. Herein, we propose developing a conceptually novel technology that will allow straightforward identification of specific glycosylation patterns in biofluids and in live cells. Distinct glycosylation states will be differentiated by developing “artificial noses” in the size of a single molecule, whereas selectivity toward particular glycoproteins will be obtained by attaching them to specific protein binders. To achieve high sensitivity and accuracy, several innovations in molecular recognition and fluorescence signalling are integrated into the design of these unconventional molecular analytical devices.
One of the most important motivations for developing these sensors lies in their potential to diagnose a variety of diseases in their early stages. For example, we describe ways by which prostate cancer could be rapidly and accurately detected by a simple blood test that analyzes the glycosylation profile of the prostate-specific antigen (PSA). Another exceptional feature of these molecular analytical devices is their ability to differentiate between glycosylation patterns of specific proteins in live cells. This will solve an immense challenge in analytical glycobiology and will allow one to study how glycosylation contributes to diverse cell-signalling pathways. Finally, in the context of molecular-scale analytical devices, the proposed methodology is exceptional. We will show how “artificial noses” can be designed to target nanometric objects (e.g. protein surfaces) and operate in confined microscopoic spaces (e.g. cells), which macroscopic arrays cannot address. Taken together, we expect that the proposed technology will break new ground in medical diagnosis, cell biology, and biosensing technologies.","1398429","2013-10-01","2018-09-30"
"GMLP","Global Methods in the Langlands Program","Jack THORNE","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The Langlands program is a conjectural framework for understanding the deep relations between automorphic forms and arithmetic. It implies a parameterization of representations of Galois groups of (local or global) fields in terms of representations of (p-adic or adelic) reductive groups. While making progress in the Langlands program often means overcoming significant technical obstacles, new results can have concrete applications to number theory, the proof of Fermat's Last Theorem by Wiles being a key example.

Recently, V. Lafforgue has made a striking breakthrough in the Langlands program over function fields, by constructing an `automorphic-to-Galois' Langlands correspondence. As a consequence, this should imply the existence of a local Langlands correspondence over equicharacteristic non-archimedean local fields.

The goal of this proposal is to show the surjectivity of this local Langlands correspondence. My strategy will be global, and will involve solving global problems of strong independent interest. I intend to establish a research group to carry out the following objectives, in the setting of global function fields:

I. Establish automorphy lifting theorems for Galois representations valued in the (Langlands) dual group of an arbitrary split reductive group.
II. Establish cases of automorphic induction for arbitrary reductive groups.
III. Prove potential automorphy theorems for Galois representations valued in the dual group of an arbitrary reductive group.
IV. Establish cases of soluble base change and descent for automorphic representations of arbitrary reductive groups.
I will then combine these results to obtain the desired surjectivity. This will be a milestone in our understanding of the Langlands correspondence for function fields.","1094610","2017-01-01","2021-12-31"
"GNOC","Towards a Gaussian Network-on-Chip","Isaac Keslassy","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","As chip multi-processor architectures are replacing single-processor architectures and reshaping the semiconductor industry, chip designers can hardly use their old models and benchmarks anymore. While designers were used to deterministic and reliable performance in the chips, they now face networks with unreliable traffic patterns, unreliable throughput and unreliable delays, hence making it hard to provide any guaranteed Quality-of-Service (QoS). In this proposal, we argue that chip designers should focus on the possible set of traffic patterns in their Network-on-Chip (NoC) architectures. We first show how to provide deterministic QoS guarantees by exploiting these patterns. Then, we explain why the cost of providing deterministic guarantees might become prohibitive, and defend an alternative statistical approach that can significantly lower the area and power. To do so, we introduce Gaussian-based NoC models, and show how they can be used to evaluate link loads, delays and throughputs, as well as redesign the routing and capacity allocation algorithms. Finally, we argue that these models could effectively complement current benchmarks, and should be a central component in the toolbox of the future NoC designer.","582500","2008-08-01","2012-07-31"
"GOODSHAPE","numerical geometric abstraction : from bits to equations","Bruno Eric Emmanuel Levy","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","""3D geometric objects play a central role in many industrial processes (modeling, scientific visualisation, numerical simulation). However, since the raw output of acquisition mechanisms cannot be used directly in these processes, converting a real object into its numerical counterpart still involves a great deal of user intervention. Geometry Processing is a recently emerged, highly competitive scientific domain that studies this type of problems. The author of this proposal contributed to this domain at its origin, and developped several parameterization algorithms, that construct a """"geometric coordinate system"""" attached to the object. This facilitates converting from one representation to another. For instance, it is possible to convert a mesh model into a piecewise bi-cubic surface (much easier to manipulate in Computer Aided Design packages). In a certain sense, this retreives an """"equation"""" of the geometry. One can also say that this constructs an *abstraction* of the geometry. Once the geometry is abstracted, re-instancing it into alternative representations is made easier. In this project, we propose to attack the problem from a new angle, and climb one more level of abstraction. In more general terms, a geometric coordinates system corresponds to a *function basis*. Thus, we consider the more general problem of constructing a *dynamic function basis* attached to the object. This abstract forms makes the meaningful parameters appear, and provides the user with new """"knobs"""" to interact with the geometry. The formalism that we use combines aspects from finite element modeling, differential geometry, spectral geometry, topology and numerical optimization. We plan to develop applications for processing and optimimizing the representation of both static 3D objets, animated 3D objets, images and videos.""","1100000","2008-08-01","2013-07-31"
"GOSSPLE","GOSSPLE: A Radically New Approach to Navigating the Digital Information Universe","Anne-Marie Kermarrec","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","Over the past decade, distributed computing has experienced a dramatic scale shift, with respect to size, geographical spread and volume of data. Meanwhile, Internet has moved into homes, creating tremendous opportunities to exploit the huge amount of resources at the edge of the network. Search engines that navigate this universe are astonishingly powerful and rely on sophisticated tools to scan and index the network. However, the network contains far more than just the pages such systems can index. There is a tremendous potential in leveraging these new kinds of information to empower individuals in ways that Internet search will never be able to offer. This reveals striking evidence that navigating the Internet goes beyond traditional search engines. Complementary and different means to navigate the digital world are now required. The objective of GOSSPLE is to provide an innovative and fully decentralized approach to navigate the digital information universe by placing users affinities and preferences at the heart of the search process. GOSSPLE will turn the network into a self-organizing federation of overlapping sub-networks, capturing on the fly the interactions and affinities observed in real life and fully leveraging the huge resource potential available on edge nodes. GOSSPLE will provide a set of fully decentralized algorithms to efficiently search, dynamically index and asynchronously disseminate information to interested users based on their preferences and (implicit) recommendations. Building up upon the peer to peer communication paradigm and harnessing the power of gossip-based algorithms, GOSSPLE will yield a disruptive way of programming distributed collaborative applications. Our goal is ambitious: impose the GOSSPLE approach as a fully decentralized, collaborative and scalable, yet complementary, alternative to traditional search engines to fully exploit the capabilities of the digital universe.","1250000","2008-09-01","2013-08-31"
"GQCOP","Genuine Quantumness in Cooperative Phenomena","Gerardo Adesso","THE UNIVERSITY OF NOTTINGHAM","The proposed research programme addresses issues of fundamental and technological importance in quantum information science and its interplay with complexity.  The main aim of this project is to provide a new paradigmatic foundation for the characterisation of quantumness in cooperative phenomena and to develop novel platforms for its practical utilisation in quantum technology applications.

To reach its main goal, this programme will target five specific objectives:
O1. Constructing a quantitative theory of quantumness in composite systems;
O2. Benchmarking genuine quantumness in information and communication protocols;
O3. Devising practical solutions for quantum-enhanced metrology in noisy conditions;
O4. Developing quantum thermal engineering for refrigerators and heat engines;
O5. Establishing a cybernetics framework for regulative phenomena in the quantum domain.

This project is deeply driven by the scientific curiosity to explore the ultimate range of applicability of quantum mechanics.  Along the route to satisfying such curiosity, this project will fulfill a crucial two-fold mission.  On the fundamental side, it will lead to a radically new level of understanding of quantumness, in its various manifestations, and the functional role it plays for natural and artificial complex systems traditionally confined to a classical domain of investigation.  On the practical side, it will deliver novel concrete recipes for communication, sensing and cooling technologies in realistic conditions, rigorously assessing  in which ways and to which extent these can be enhanced by engineering and harnessing quantumness.

Along with a skillful team which this grant will allow to assemble, benefitting from the vivid research environment at Nottingham, and mainly thanks to his creativity, broad mathematical and physical preparation and relevant inter-disciplinary expertise, the applicant is in a unique position to accomplish this timely and ambitious mission.","1351461","2015-05-01","2020-04-30"
"GQEMS","Graphene Quantum Electromechanical Systems","Christoph Stampfer","RHEINISCH-WESTFAELISCHE TECHNISCHE HOCHSCHULE AACHEN","""The aim of this project is to develop a new class of mechanically tunable quantum devices based on graphene. Adopting an innovative and interdisciplinary approach grounded on both engineering-based microsystem technology and low-temperature solid-state physics, we aim at gaining control over the mechanical and electromechanical properties of graphene nano-membranes and suspended graphene nanostructures, in the low and high strain regime.
The main motivation for going in this direction is the expectation that being able to access both the electronic and the mechanical degrees of freedom of graphene will allow to explore new regimes of quantum physics, and lead to potentially important technological applications. Graphene is in fact a unique platform for the development of a new generation of quantum electromechanical systems, not only because of its high carrier mobility, high elasticity and unrivaled material strength, but also because its electronic properties depend sensitively on local strain and mechanical deformations, allowing to envision revolutionary device concepts.
This is a timely and highly explorative high-gain/high-risk research project. Its successful accomplishment will set the basis of a novel graphene-based microsystem technology. The project is expected to have an important and far-reaching impact in the fields of nanosystems and graphene physics, not only in terms of potential applications, but also giving an important contribution to the investigation of the fundamental properties of this unique material.""","1797200","2012-01-01","2016-12-31"
"GRACE","Genetic Record of Atmospheric Carbon dioxidE (GRACE)","Rosalind Rickaby","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Two key variables, temperature and atmospheric carbon dioxide (pCO2), define the sensitivity of the Earth’s climate system. The geological record provides our only evidence of the past climate sensitivity of the Earth system, but there is no direct quantitative measure of pCO2 or temperature beyond the 650 kyr extent of the Antarctic ice cores. The reconstruction of past climate, on timescales of millions of years, relies on the analysis of chemical or isotopic proxies in preserved shells or organic matter. Such indirect approaches depend upon empirical calibration in modern species, without understanding the biological mechanisms that underpin the incorporation of the climate signal. The intention of this ERC grant proposal is to establish a research team to investigate the “living geological record” to address this major gap in climate research. I hypothesise that direct climate signals of the past are harboured within, and can ultimately be deciphered from, the genetic make up of extant organisms. Specifically, I propose an innovative approach to the constraint of the evolution of atmospheric pCO2 during the Cenozoic. The approach is based on the statistical signal of positive selection of adaptation within the genetic sequences of marine algal Rubisco, the notoriously inefficient enzyme responsible for photosynthetic carbon fixation, but supplemented by analysis of allied carbon concentrating mechanisms. As a calibration, I will characterise the biochemical properties of Rubisco in terms of specificity for pCO2, isotopic fractionation and kinetics, from a range of marine phytoplankton. The prime motivation is a history of pCO2, but the project will yield additional insight into the feedback between phytoplankton and climate, the carbon isotopic signatures of the geological record and the mechanistic link between genetic encoding and specific","1652907","2008-09-01","2015-08-31"
"GRAPH GAMES","Quantitative Graph Games: Theory and Applications","Krishnendu Chatterjee","INSTITUTE OF SCIENCE AND TECHNOLOGYAUSTRIA","The theory of games played on graphs provides the mathematical foundations to study numerous important problems in branches of mathematics, economics, computer science, biology, and other fields. One key application area in computer science is the formal verification of reactive systems. The system is modeled as a graph, in which vertices of the graph represent states of the system, edges represent transitions, and paths represent behavior of the system. The verification of the system in an arbitrary environment is then studied as a problem of game played on the graph, where the players represent the different interacting agents. Traditionally, these games have been studied either with Boolean objectives, or single quantitative objectives. However, for the problem of verification of systems that must behave correctly in resource-constrained environments (such as an embedded system) both Boolean and quantitative objectives are necessary: the Boolean objective for correctness specification and quantitative objective for resource-constraints. Thus we need to generalize the theory of graph games such that the objectives can express combinations of quantitative and Boolean objectives. In this project, we will focus on the following research objectives for the study of graph games with quantitative objectives:
(1) develop the mathematical theory and algorithms for the new class of games on graphs obtained by combining quantitative and Boolean objectives;
(2) develop practical techniques (such as compositional and abstraction techniques) that allow our algorithmic solutions be implemented efficiently to handle large game graphs;
(3) explore new application areas to demonstrate the application of quantitative graph games in diverse disciplines; and
(4) develop the theory of games on graphs with infinite state space and with quantitative objectives.
since the theory of graph games is foundational in several disciplines, new algorithmic solutions are expected.","1163111","2011-12-01","2016-11-30"
"GRAPHCPX","A graph complex valued field theory","Thomas Hans Willwacher","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","The goal of the proposed project is to create a universal (AKSZ type) topological field theory with values in graph complexes, capturing the rational homotopy types of manifolds, configuration and embedding spaces.
If successful, such a theory will unite certain areas of mathematical physics, topology, homological algebra and algebraic geometry. More concretely, from the physical viewpoint it would give a precise topological interpretation of a class of well studied topological field theories, as opposed to the current state of the art, in which these theories are defined by giving formulae without guarantees on the non-triviality of the produced invariants. 
 
From the topological viewpoint such a theory will provide new tools to study much sought after objects like configuration and embedding spaces, and tentatively also diffeomorphism groups, through small combinatorial models given by Feynman diagrams. In particular, this will unite and extend existing graphical models of configuration and embedding spaces due to Kontsevich, Lambrechts, Volic, Arone, Turchin and others.

From the homological algebra viewpoint a field theory as above provides a wealth of additional algebraic structures on the graph complexes, which are some of the most central and most mysterious objects in the field.
Such algebraic structures are expected to yield constraints on the graph cohomology, as well as ways to construct series of previously unknown classes.","1162500","2016-07-01","2021-06-30"
"GRAPHENE","Physics and Applications of Graphene","Konstantin Novoselov","THE UNIVERSITY OF MANCHESTER","This proposal is based on the PI’s recent work in which a conceptually new class of materials – two dimensional atomic crystals – was discovered. Such crystals can be seen as individual atomic planes “pulled out” of bulk crystals and were previously presumed not to exist in the free state. Despite being only one atom thick and unprotected from the immediate environment, these materials can be extremely stable. The PI’s work has focused on graphene, a freestanding monolayer of graphite where carbon atoms are densely packed in a honeycomb lattice. Due to its high quality and unique electronic spectrum (electrons in graphene mimic relativistic quantum particles called Dirac fermions), graphene has become a gold mine for searching for new phenomena. Graphene also offers numerous applications from smart materials to future electronics. The general objective of the proposal is to exploit the PI’s current lead in the emerging research area, so that no opportunity is missed to find new effects that are expected to be abundant in graphene, and to exploit possible applications. The project will cover three main directions, exploring most exciting features about graphene. First, the PI is planning to concentrate on graphene membranes and investigate properties induced by the unique dimensionality of these one atom thick objects. Second, charge carriers in graphene mimic massless relativistic particles, and this exceptional property allows access to the rich and subtle physics of quantum electrodynamics in a bench-top condensed matter experiment. To this end, interaction and many-body effects will be investigated. Third, graphene is considered to be a realistic candidate for electronics beyond the Si age, and one of the priorities of this project will be studies of graphene-based transistor applications. All these research directions combined should create a solid basis for a new internationally-leading research laboratory led by the PI.","1775044","2008-12-01","2013-10-31"
"GRB-SN","The Gamma Ray Burst – Supernova Connection
and Shock Breakout Physics","Ehud Nakar","TEL AVIV UNIVERSITY","Long gamma ray bursts (long GRBs) and core-collapse supernovae (CCSNe) are two of the most spectacular explosions in the Universe. They are a focal point of research for many reasons. Nevertheless, despite considerable effort during the last several decades, there are still many fundamental open questions regarding their physics.
Long GRBs and CCSNe are related. We know that they are both an outcome of a massive star collapse, where in some cases, such collapse produces simultaneously a GRB and a SN. However, we do not know how a single stellar collapse can produce these two apparently very different explosions.  The GRB-SN connection raises many questions, but it also offers new opportunities to learn on the two types of explosions.
The focus of the proposed research is on the connection between CCSNe and GRBs, and on the physics of shock breakout.  As I explain in this proposal, shock breakouts play an important role in this connection and therefore, I will develop a comprehensive theory of relativistic and Newtonian shock breakout. In addition, I will study the propagation of relativistic jets inside stars, including the effects of jet propagation and GRB engine on the emerging SN. This will be done by a set of interrelated projects that carefully combine analytic calculations and numerical simulations. Together, these projects will be the first to model a GRB and a SN that are simultaneously produced in a single star. This in turn will be used to gain new insights into long GRBs and CCSNe in general.
This research will also make a direct contribution to cosmic explosions research in general. Any observable cosmic explosion must go through a shock breakout and a considerable effort is invested these days in large field of view surveys in search for these breakouts. This program will provide a new theoretical base for the interpretation of the upcoming observations.","1468180","2012-01-01","2017-12-31"
"GreatMoves","General Relativistic Moving-Mesh Simulations of Neutron-Star Mergers","Andreas BAUSWEIN","GSI HELMHOLTZZENTRUM FUER SCHWERIONENFORSCHUNG GMBH","In the arising era of gravitational-wave (GW) astronomy the demand for the next-generation of neutron-star (NS) merger models has never been so great. By developing the first relativistic moving-mesh simulations of NS mergers, we will be able to reliably link observables of these spectacular events to fundamental questions of physics. Our approach will allow us to maximize the information that can be obtained from different GW oscillations of the postmerger remnant. In this way we will demonstrate the scientific potential of future postmerger GW detections to unravel unknown properties of NSs and high-density matter. Based on our models we will work out the optimal GW data analysis strategy towards this goal.
Employing a revolutionary numerical technique we will be able to achieve an unprecedented resolution of the merger outflow. High-resolution simulations of these ejecta are critical to uncover the detailed conditions for nucleosynthesis, specifically, for the rapid-neutron capture process (r-process). The r-process forges the heaviest elements such as gold and uranium, but its astrophysical production site still has to be clarified. Moreover, the nuclear decays in the expanding outflow power electromagnetic counterparts, which are targets of optical survey telescopes (iPTF, ZTF, BlackGEM, LSST). Our multi-disciplinary approach combines hydrodynamical models, nuclear network calculations and light-curve computations to facilitate the interpretation of future electromagnetic observations within a multi-messenger picture. Linking these observables to the underlying outflow properties is pivotal to unravel the still mysterious origin of heavy elements created by the r-process.","1499485","2018-07-01","2023-06-30"
"GreenOnWaterCat","Unravelling the Nature of Green Organic “On-Water” Catalysis via Novel Quantum Chemical Methods","Thomas Dae-Song KÜHNE","UNIVERSITAET PADERBORN","The target of the research program, GreenOnWaterCat, is to revolutionize the understanding of green “on-water” catalysis and to unravel its microscopic origin. To enable these goals to be reached, several novel theoretical methods will be developed and implemented that will enable for unprecedented large-scale quantum molecular dynamics simulations, where both the electronic and nuclear Schrödinger equations are solved simultaneously. In addition, these methods will also allow the efficient computation of various state-of-the-art vibrational spectroscopies “on-the-fly”, at essentially no additional computational cost. Furthermore, new analysis techniques permit to assign the spectra and explain their correlation with the atomic structure in order to gain invaluable insights and eventually grasp the relationships between the dynamics and structure of “on-water” catalysis and vibrational spectroscopies. Since the latter offers a convenient connection to experiment, the unique results are of utmost value in order to explain the experimental findings. In consequence, new synthetic processes based on the “on-water” phenomenon will be proposed and investigated. The expected results will be most helpful so that water will soon become not only a viable, but also very attractive solvent in the design of novel synthetic processes and to make it even more useful for industrial applications.
Beside the development and implementation of novel computational methods, which will be made publicly available, the additional outcomes expected are as follows: 

• To conclusively explain the underlying mechanism of the “on-water” rate phenomenon for the first time
• To elucidate the experimental measurements and characterize the corresponding atomic structure
• To propose novel synthetic processes which exploit the “on-water” concept, such as catalysis at the organic/metal oxide interface
• To investigate the possibility of “on-water” catalysis using two water-insoluble solid reactants","1499875","2017-01-01","2021-12-31"
"gRESONANT","Resonant Nuclear Gamma Decay and the Heavy-Element Nucleosynthesis","Ann-Cecilie Larsen","UNIVERSITETET I OSLO","THE GRAND CHALLENGE: The “Holy Grail” of nuclear astrophysics is to understand the astrophysical processes responsible for the formation of the elements. A particularly challenging part is the description of the heavy-element nucleosynthesis. The only way to build the majority of these heavy nuclides is via neutron-capture processes. Unaccounted-for nuclear structure effects may drastically change these rates. 
MAIN HYPOTHESIS: Nuclear low-energy gamma-decay resonances at high excitation energies will enhance the astrophysical neutron-capture reaction rates. 
NOVEL APPROACH: This proposal is, for the first time, addressing the M1 scissors resonance in deformed, neutron-rich nuclei and superheavy elements. A new experimental technique will be developed to determine the electromagnetic nature of the unexpected upbend enhancement. Further, s-process branch points for the Re-Os cosmochronology will be studied for the first time with the Oslo method.
OBJECTIVES: 
1) Measure s-process branch point nuclei with the Oslo method
2) Radioactive-beam experiments for neutron-rich nuclei searching for the low-energy upbend and the M1 scissors resonance
3) Develop new experimental technique to identify the upbend’s electromagnetic nature
4) Superheavy-element experiments looking for the M1 scissors resonance
POTENTIAL IMPACT IN THE RESEARCH FIELD: This proposal will trigger a new direction of research, as there are no data on the low-energy gamma resonances neither on neutron-rich nor superheavy nuclei. Their presence may have profound implications for the astrophysical neutron-capture rates. Developing a new experimental technique to determine the electromagnetic character of the upbend is crucial to distinguish between two competing explanations of this phenomenon. Unknown neutron-capture cross sections will be estimated with a much better precision than prior to this project, and lead to a major leap forward in the field of nuclear astrophysics.","1443472","2015-03-01","2020-02-29"
"GrInflaGal","Gravity, Inflation, and Galaxies: Fundamental Physics with Large-Scale Structure","Fabian Schmidt","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Over the past two decades, a data-driven revolution has occurred in our understanding of the origin and evolution of our Universe and the structure within it. During this period, cosmology has evolved from a speculative branch of theoretical physics into precision science at the intersection of gravity, particle- and astrophysics. Despite all we have learned, we still do not understand why the Universe accelerates, and how the structure in the Universe originated. Recent breakthrough research, with leading contributions by the PI of this proposal, has shown that we can make progress on these questions using observations of the large-scale structure and its tracers, galaxies. This opens up a fascinating, new interdisciplinary research field: probing Gravity and Inflation with Galaxies. The goal of the proposed research is to first, probe our theory of gravity, General Relativity, on cosmological scales. Second, it aims to shed light on the origin of the initial seed fluctuations out of which all structure in the Universe formed, by constraining the physics and energy scale of inflation. While seemingly unrelated, the main challenge in both research directions consists in understanding the nonlinear physics of structure formation, which is dominated by gravity on scales larger than a few Mpc. By making progress in this understanding, we can unlock a rich trove of information on fundamental physics from large-scale structure. The research goals will be pursued on all three fronts of analytical theory, numerical simulations, and confrontation with data. With space missions, such as Planck and Euclid, as well as ground-based surveys delivering data sets of unprecedented size and quality at this very moment, the proposed research is especially timely. It will make key contributions towards maximizing the science output of these experiments, deepen our understanding of the laws of physics, and uncover our cosmological origins.","1330625","2016-09-01","2021-08-31"
"GSF","Two-body dynamics in general relativity: the self-force approach","Leor Barack","UNIVERSITY OF SOUTHAMPTON","""The gravitational two-body problem is a longstanding open problem in General Relativity, dating back to work by Einstein himself in the 1930s. Unlike in Newtonian theory, bound binary orbits in relativity are never periodic: the system loses energy via emission of gravitational waves (GWs), and the two masses gradually inspiral until they merge. The description of this radiative dynamics is extremely challenging, not least due to the non-linearity of Einstein's field equations. The exciting prospects for observing GWs from inspiralling and merging compact binaries using detectors like VIRGO (in Europe) and LIGO (in the US) has renewed interest in this old problem, and provides a modern context to it.

The radiative inspiral of compact stars into massive black holes is a key source for low-frequency GW astronomy.  The intricate GW signature of such inspirals will allow precision tests of Relativity in its most extreme regime.  The inspiral can be modelled within Relativity using semi-analytic perturbation methods: the small object is seen as moving on the background of the large hole, and the problem reduces to computing the back-reaction force, aka """"self force"""", acting on the small object as it interacts with its own gravitational field.

My team has been involved in breakthrough research into the nature of the self force in curved spacetime, establishing international leadership in the field. Our main goals in this project are (1) to compute accurate self-forced inspiral trajectories for realistic (spinning) black hole binaries together with theoretical waveforms for GW searches; (2) by means of synergy with post-Newtonian theory and numerical relativity, to inform a universal model of binary inspirals at any mass ratio; and (3) to explore several exotic aspects of the post-geodesic dynamics, including transient resonances in generic inspirals, critical behavior near the capture threshold, and the possible role of the self-force as a """"cosmic censor"""".""","1459268","2012-10-01","2017-09-30"
"GTMT","Group Theory and Model Theory","Eric Herve Jaligot","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The project is located between logic and mathematics, more precisely between model theory and group theory. There are extremely difficult questions arising about the model theory of groups, notably the question of the construction of new groups with prescribed algebraic properties and at the same time good model-theoretic properties. In particular, it is an important question, both in model theory and in group theory, to build new stable groups and eventually new nonalgebraic groups with a good dimension notion.

The present project aims at filling these gaps. It is divided into three main directions. Firstly, it consists in the continuation of the classification of groups with a good dimension notion, notably groups of finite Morley rank or related notions. Secondly, it consists in a systematic inspection of the combinatorial and geometric group theory which can be applied to build new groups, keeping a control on their first order theory. Thirdly, and in connection to the previous difficult problem, it consists in a very systematic and general study of infinite permutation groups.","366598","2011-10-01","2013-12-31"
"GUTPEPTIDES","Novel therapeutic approaches to improve gastrointestinal wound healing","Markus MUTTENTHALER","UNIVERSITAT WIEN","The gastrointestinal epithelium is a major physical barrier that protects us from diverse and potentially immunogenic or toxic content. A compromised epithelium results in increased permeability to such content, thus leading to inflammation, immune response, pain, and diseases, such as irritable bowel syndrome and inflammatory bowel disease. A therapeutic strategy that controls inflammation and restores the barrier represents an innovative approach for the prevention and treatment of such diseases. This proposal focuses on how gut peptides regulate epithelial protection and repair, and explores novel therapeutic opportunities by targeting gut receptors that become accessible once the epithelium is compromised. We propose to tackle the overall aim of improving gastrointestinal wound healing via three complementary objectives: (I) to investigate the therapeutic potential of the oxytocin receptor during gastrointestinal inflammation, (II) to elucidate the mechanism of trefoil factor peptide-induced gastrointestinal wound healing, and (III) to discover and characterise novel ligands suitable for epithelial repair. To achieve these objectives, we employ a multidisciplinary approach that includes state-of-the-art peptide synthesis, scaffold grafting, pharmacology, gut stability and wound healing assays, and inflammatory mouse models. We will develop probes to study the mechanisms of action at a molecular level that is not possible with current tools, and explore the biological diversity of venoms for novel therapeutic leads. This project will significantly advance our understanding of epithelial protection/repair and reveal drug targets that treat the source of the problems rather than the symptoms. This project has the potential to change the way we think about treating gut disorders and how to develop peptide therapeutics, and it will pave the way towards the intriguing and longer-term goal of modulating the central nervous system via the gut-brain axis.","1487396","2017-09-01","2022-08-31"
"GWT","Gromov-Witten Theory: Mirror Symmetry, Modular Forms, and Integrable Systems","Tom Coates","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","The Gromov-Witten invariants of a space X record the number of curves in X of a given genus and degree which meet a given collection of cycles in X. They have important applications in algebraic geometry, symplectic geometry, and theoretical physics. The program proposed here will allow us to compute Gromov-Witten invariants, and particularly higher-genus Gromov-Witten invariants, for a very broad class of spaces. Recent progress, partly due to the Principal Investigator, has led to a greatly-improved mathematical understanding of the string-theoretic duality known as Mirror Symmetry. This allows us to compute genus-zero Gromov-Witten invariants (those where the curves involved are spheres) for a wide range of target spaces. But at the moment there are very few effective tools for computing higher-genus Gromov-Witten invariants (those where the curves involved are tori, or n-holed tori for n&gt;1). We will solve this problem by extending mathematical Mirror Symmetry to cover this case. In doing so we will draw on and make rigorous recent insights from topological string theory. These insights have revealed close and surprising connections between Gromov-Witten theory, modular forms, and the theory of integrable systems.","620000","2009-11-01","2015-10-31"
"HABITABLEPLANET","Creating a habitable planet: the roles of accretion, core formation and plate tectonics","Helen Williams","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The Earth formed ~ 4.5 billion years ago, from accreting particles of dust and primitive meteorites. It is the only habitable planet in our solar system and has a unique history of extended accretion and core formation coupled with active plate tectonics. Accretion and core formation would have defined the initial elemental composition of the Earth’s interior whereas plate tectonic processes controlled chemical exchange between the Earth’s surface and interior and the distribution of elements between major geochemical reservoirs. The overarching goal of this proposal is to define the roles of these processes in the chemical evolution of the Earth and hence in the creation of a habitable planet.

In order to achieve this goal I propose to investigate the partitioning of new stable isotope systems such as Ge and Se in high-pressure experiments that simulate core formation. This novel, multidisciplinary approach will provide some of the first direct constraints on the extent to which these volatile elements  were partitioned into the core. We will use this information to address the fundamental issue of whether the Earth acquired its volatile elements inventory early, during core formation, or subsequently, as part of a “late veneer”. The second major theme of the proposed research uses Fe, Zn, Mo and Se stable isotopes to trace the cycling of Fe and S during subduction, the tectonic process where one plate sinks beneath another and is recycled into the Earth’s deep interior. The goal of this project is to understand the impact of subduction on the chemical and redox evolution of the Earth’s interior and the relationship between tectonic recycling and the rise of oxygen in the Earth’s atmosphere ~ 2.5 billion years ago. This theme will focus on samples of relict subducted plate material and of the Earth’s interior, obtained as fragments sampled by lavas or as ancient minerals trapped within diamonds.","1999975","2013-01-01","2018-12-31"
"HALODRUGSYN","Innovative Strategies towards Halogenated Organic Molecules: From Reaction Design to Application in Drug Synthesis","Thomas MAGAUER","UNIVERSITAET INNSBRUCK","Halogenated arenes and heteroarenes have become essential structural motifs of the pharmaceutical industry to create novel drugs against bacterial infections and cancer, and constitute highly valuable functional units in chemistry. Current methods for the installation of carbon-halogen bonds lack efficiency, selectivity, and practicability within the complex molecular setting of drug development processes. These restrictions prevent many potential drugs from being synthesized in a time- and cost-efficient manner.
In this project, I aim to address these challenges by engineering a highly elaborated synthetic toolbox that is equipped with novel transformations of unprecedented efficiency, selectivity and practicability. I will apply these transformations to the construction of novel antibiotics against resistant strains and more efficient chemotherapeutics to combat cancer.
The first objective is to establish innovative transformations that enable for the first time an efficient access to halogenated arenes. I will accomplish this goal by developing novel ring-expansion reactions and apply them to the first synthesis of the antibiotic salimabromide in order to address the acute problem of antibiotic resistance. Within the second part of this project, I will extend this unique synthetic platform to heteroarenes and establish a groundbreaking method based on carbon-fluorine bond activation. This will represent the first broadly applicable strategy to produce novel fluorinated heteroarene based anti-cancer drugs with unparalleled precision, efficiency and selectivity. Taken together, the realization of these strategies, all of which are unprecedented, provides for the first time a solution for the limitations associated with current methods. With my expertise in synthetic chemistry, which I have gained from my achievements in natural product synthesis, and an outstanding publication record in this research field, I am confident to accomplish these ambitious goals.","1496664","2017-02-01","2022-01-31"
"HALOGEN","Understanding Halogen Bonding in Solution: Investigation of Yet Unexplored Interactions with Applications in Medicinal Chemistry","Mate Erdelyi","GOETEBORGS UNIVERSITET","Halogen bonding is an electron density donation-based weak interaction that has so far almost exclusively been investigated in computational and crystallographic studies. It shows high similarities to hydrogen bonding; however, its applicability for molecular recognition processes long remained unappreciated and has not been thoroughly explored.

The main goals of this project are (1) to take the major leap from solid state/computational to /solution/ investigations of halogen bonding by developing novel NMR methods, using these (2) perform the first ever systematic physicochemical study of halogen bonding in solutions, and (3) to apply the gained knowledge in structural biology through elucidation of the anaesthetic binding site of native proteins. This in turn is of direct clinical relevance by providing a long-sought understanding of the disease malignant hyperthermia.

Model compounds will be prepared using solution-phase and solid-supported organic synthesis; NMR methods will be developed for physicochemical studies of molecular recognition processes and applied in structural biology through the study of the interaction of anaesthetics with proteins involved in cellular calcium regulation.

Using a peptidomimetic model system and an outstandingly sensitive NMR technique I will systematically study the impact of halogen bond donor and acceptor sites, and of electronic and solvent effects on the strength of the interaction. The proposed method will quantify relative stability of a strategically-designed, cooperatively folding model system.

A second NMR technique will utilize paramagnetic effects and permit simultaneous characterization of bond strength and geometry of weak intermolecular complexes in solution. The technique will first be validated on small, organic model compounds and subsequently be transferred to weak, protein-ligand interactions. It will be exploited to gain an atomic level understanding of anaesthesia.","1495630","2010-09-01","2015-08-31"
"HAMILTONIANPDES","Hamiltonian Partial Differential Equations: new connections between dynamical systems and PDEs with small divisors phenomena","Massimiliano Berti","UNIVERSITA DEGLI STUDI DI NAPOLI FEDERICO II","""The aim of this project of 4 years is to create a research group on Hamiltonian Partial Differential Equations (PDEs) after my new arrival in the University Federico II of Naples as Associate Professor in november 2005. I plan to hire 2 post doc fellows and also to organize advanced research Schools and Workshops. I plan to develop a research group on Hamiltonian PDEs mainly studied by the point of view of """"Dynamical Systems Philosophy"""" and of """"Calculus of Variations"""". Indeed the analysis of the main structures of an infinite dimensional phase space such as periodic orbits, embedded invariant tori, center manifolds, etc., is an essential change of paradigm in the study of hyperbolic equations which has been recently very fruitful. In the last years the principal investigator has developed a net of high level international collaborations and, also with some of his PhD and Post doc students, has obtained many important results via a mixed combination of Critical Point Theory, Nash-Moser Implicit Function Theorems, Number Theory, Dynamical Systems techniques and Bifurcation Theory. This has allowed to solve open problems in the fields, opening new perspectives. With the ERC-Starting Grant we plan to hire first class experts in the above fields, and to collaborate for long periods of joint research with leading experts in the world. Keywords: Hamiltonian Partial Differential Equations, Small divisors problem, Nash-Moser Implicit function theory Variational methods.""","400000","2008-07-01","2012-06-30"
"HamInstab","Instabilities and homoclinic phenomena in Hamiltonian systems","Marcel GUARDIA","UNIVERSITAT POLITECNICA DE CATALUNYA","A fundamental problem in the study of  dynamical systems is to ascertain whether the effect of a perturbation  on an integrable Hamiltonian system accumulates over time and leads to a large effect (instability) or it averages out (stability). Instabilities in nearly integrable systems, usually called Arnold diffusion, take place along resonances and by means of a
framework of partially hyperbolic invariant objects and their homoclinic and heteroclinic connections.

The goal of this project is to develop new techniques, relying on the role of invariant manifolds in the global dynamics, to prove the existence of physically relevant instabilities and homoclinic phenomena in several problems in celestial mechanics and Hamiltonian Partial Differential Equations.

The N body problem models the interaction of N puntual masses under gravitational force. Astronomers have deeply analyzed the role of resonances in this model. Nevertheless, mathematical results showing instabilities along them are rather scarce. I plan to develop a new theory to analyze the transversal intersection between invariant manifolds along mean motion and secular resonances to prove the existence of Arnold diffusion. I will also apply this theory to construct oscillatory motions.

Several Partial Differential Equations such as the nonlinear Schrödinger, the Klein-Gordon and the wave equations can be seen as infinite dimensional Hamiltonian systems. Using dynamical systems techniques and understanding the role of invariant manifolds in these Hamiltonian PDEs, I will study two type of solutions: transfer of energy solutions, namely solutions that push energy to arbitrarily high modes as time evolves by drifting along resonances; and breathers, spatially 
localized and periodic in time solutions, which in a proper setting can be seen as homoclinic orbits to a stationary solution.","1100348","2018-01-01","2022-12-31"
"HAMPDES","Hamiltonian PDE's and small divisor problems: a dynamical systems approach","Michela Procesi","UNIVERSITA DEGLI STUDI ROMA TRE","""A large number of partial differential equations of Physics have the structure of an infinite-dimensional Hamiltonian dynamical system. In this class of equations appear, among others, the Schrödinger equation (NLS), the wave equation (NLW), the Euler equations of hydrodynamics and the numerous models that derive from it. The study of these equations poses some fundamental questions that have inspired an entire research field in the last twenty years: the investigation of the main invariant structures of the phase space of a Hamiltonian system, starting from its stationary, periodic and quasi-periodic orbits. As in the case of finite-dimensional dynamical systems, one of the main problems in this field is linked to the well-known """"small divisors problem''. A further difficulty is due to the fact  that  ''physically'' interesting equations, without outer parameters,  are typically resonant and/or contain derivatives in the non-linearity. There are many fundamental open questions in this field. Our main goals are 1) the study of quasi-periodic solutions, in particular for semi-linear and quasi-linear equations. 2)Study of normal forms, both in integrable and non-integrable cases. 3)  Applications to hydrodynamics and search of quasi-periodic solutions in water wave problems.4) Study of almost periodic solutions for nonlinear PDEs. 5) quasi-periodic solutions for resonant systems  with minimal restrictions on the non-linearity. Together with my group  in Naples we   already have developed several techniques to approach these problems and we have several ideas of possible innovative approaches, combining Nash-Moser and KAM methods, Normal Form Theory, Para-differential calculus, combinatorial methods.""","678000","2012-11-01","2018-10-31"
"HANDY-Q","Quantum Degeneracy at Hand","Maxime Etienne Marie Richard","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Microcavity polaritons are half-light, half-matter composite bosons, which are formed in monolithic semiconductor microcavities of the proper design. Recently, Bose-Einstein condensation of polaritons has been reported, that constitutes a new class of quantum fluid out of equilibrium. Unlike cold atoms, superfluid Helium or superconductors, polaritons are in a driven-dissipative situation, and their mass amounts only to a negligible fraction of an electrons’. This unusual situation has already revealed very interesting phenomena. Moreover, every observables of the polariton fluid, including momentum, energy spectrum and coherence properties are directly accessed via optical spectroscopy experiments.
In this project, we will fabricate and investigate new wide band-gap semiconductor nanostructures both capable of taking unprecedented control over the polariton environment, and capable of sustaining very hot and very dense quantum degenerate polariton fluids. Various confinement configurations - two, one and zero-dimensional -will be realized as well as advanced nanostructures based on traps and tunnel barriers. In these peculiar situations, the quantum degenerate polariton fluid will display a new and rich phenomenology. Hence, many premieres will be achieved like room temperature 1D quantum degeneracy, 1D quasi-condensate in solid-state systems, Josephson oscillations of polariton superfluids, and the fascinating Tonks-Girardeau state where strongly interacting bosons are expected to behave like fermions.","1488307","2010-11-01","2015-10-31"
"HARMONIC","Studies in Harmonic Analysis and Discrete Geometry: Tilings, Spectra and Quasicrystals","Nir Lev","BAR ILAN UNIVERSITY","This proposal is concerned with several themes which lie in the crossroads of Harmonic Analysis and Discrete Geometry. Harmonic Analysis is fundamental in all areas of science and engineering, and has vast applications in most branches of mathematics. Discrete Geometry deals with some of the most natural and beautiful problems in mathematics, which often turn out to be also very deep and difficult in spite of their apparent simplicity. The proposed project deals with some fundamental problems which involve an interplay between these two important disciplines.

One theme of the project deals with tilings of the Euclidean space by translations, and the interaction of this subject with questions in orthogonal harmonic analysis. The PI has recently developed an approach to attack some problems in connection with the famous conjecture due to Fuglede (1974), concerning the characterization of domains which admit orthogonal Fourier bases in terms of their possibility to tile the space by translations, and in relation with the theory of multiple tiling by translates of a convex polytope, or by a function. A main goal of this project is to further develop new methods and extend some promising intermediate results obtained by the PI in these directions.

Another theme of the proposed research lies in the mathematical theory of quasicrystals. This area has received a lot of attention since the experimental discovery in the 1980's of the physical quasicrystals, namely, of non-periodic atomic structures with diffraction patterns consisting of spots. Recently, by a combination of harmonic analytic and discrete combinatorial methods, the PI was able to answer some long-standing questions of Lagarias (2000) concerning the geometry and structure of these rigid point configurations. In the present project, the PI intends to continue the investigation in the mathematical theory of quasicrystals, and to analyze some basic problems which are still open in this field.","1260625","2016-12-01","2021-11-30"
"HARMONIC","Discrete harmonic analysis for computer science","Yuval FILMUS","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","Boolean function analysis is a topic of research at the heart of theoretical computer science. It studies functions on n input bits (for example, functions computed by Boolean circuits) from a spectral perspective, by treating them as real-valued functions on the group Z_2^n, and using techniques from Fourier and functional analysis. Boolean function analysis has been applied to a wide variety of areas within theoretical computer science, including hardness of approximation, learning theory, coding theory, and quantum complexity theory.

Despite its immense usefulness, Boolean function analysis has limited scope, since it is only appropriate for studying functions on {0,1}^n (a domain known as the Boolean hypercube). Discrete harmonic analysis is the study of functions on domains possessing richer algebraic structure such as the symmetric group (the group of all permutations), using techniques from representation theory and Sperner theory. The considerable success of Boolean function analysis suggests that discrete harmonic analysis could likewise play a central role in theoretical computer science.

The goal of this proposal is to systematically develop discrete harmonic analysis on a broad variety of domains, with an eye toward applications in several areas of theoretical computer science. We will generalize classical results of Boolean function analysis beyond the Boolean hypercube, to domains such as finite groups, association schemes (a generalization of finite groups), the quantum analog of the Boolean hypercube, and high-dimensional expanders (high-dimensional analogs of expander graphs). Potential applications include a quantum PCP theorem and two outstanding open questions in hardness of approximation: the Unique Games Conjecture and the Sliding Scale Conjecture. Beyond these concrete applications, we expect that the fundamental results we prove will have many other applications that are hard to predict in advance.","1473750","2019-03-01","2024-02-29"
"HBMAP","Decoding, Mapping and Designing the Structural Complexity of Hydrogen-Bond Networks: from Water to Proteins to Polymers","Michele Ceriotti","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Hydrogen bonds are ubiquitous and fundamental in nature, underpinning the behavior of systems as different as water, proteins and polymers. Much of this flexibility derives from their propensity to form complex topological networks, which can be strong enough to hold Kevlar together, or sufficiently labile to enable reversible structural transitions in allosteric proteins. 
Simulations must treat the quantum nature of both electrons and protons to describe accurately the microscopic structure of H-bonded materials, but this wealth of data does not necessarily translate into deep physical understanding. Even the structure of a compound as essential as water is still the subject of intense debate, despite extensive investigations. Identifying recurring bonding patterns is essential to comprehend and manipulate the structural and dynamical properties of H-bonded systems. 
Our objective is to develop and apply machine-learning techniques to atomistic simulations, and identify the design principles that govern the structure and properties of H-bonded compounds. Our strategy rests on three efforts: (1) recognition of recurring structural motifs with probabilistic data analysis; (2) coarse-grained mapping of the energetically accessible structural landscape by non-linear dimensionality reduction techniques; (3) acceleration of configuration sampling using these data-driven collective variables.
Identifying motifs and order parameters will be crucial to interpret simulations and experiments of growing complexity, and will enable computational design of H-bond networks. We will focus first on two objectives. (1) Rationalizing the structure of crystalline, amorphous and liquid water across its phase diagram, from ambient to astrophysical conditions, and its response to solutes, interfaces or confinement. (2) Enabling efficient simulation and structural design of polymers and proteins in non-biological contexts, targeting biomimetic materials and organic/inorganic interfaces.","1500000","2016-05-01","2021-04-30"
"HBQFTNCER","Holomorphic Blocks in  Quantum Field Theory: New Constructions of Exact Results","Sara Pasquetti","UNIVERSITA' DEGLI STUDI DI MILANO-BICOCCA","A central challenge in theoretical physics is to develop non-perturbative or exact methods to describe quantitatively the dynamics of strongly coupled quantum fields. This proposal aims to establish new exact methods for the study of  supersymmetric  quantum field theories  thereby  unveiling new integrable structures and fostering new  correspondences and dualities. We will develop a  new  cut-and-sew formalism to compute partition functions and  expectation values of observables  of supersymmetric gauge theories  on  compact manifolds through the gluing of   a fundamental set of building blocks, the holomorphic blocks. The decomposition of partition functions into holomorphic blocks corresponds to 
the geometric decomposition of compact manifolds into standard  simpler pieces. Similarly the gluing rules for the holomorphic blocks correspond to the geometric gluing rules. The key insight required to exploit the holomorphic block formalism is the  deep connection between supersymmetric gauge theories  and  low dimensional  exactly solvable systems such as 2d CFTs, TQFTs and spin chains.  Two and four dimensional holomorphic blocks can be reinterpreted as  conformal blocks in Liouville theory  through an established correspondence between supersymmetric gauge theories and Liouville theory. We will provide a similar realisation of three and five dimensional holomorphic blocks in  a new theory, 
a  q-deformed version of Liouville theory where the Virasoro algebra is replaced by the q-deformed Virasoro algebra.
We will  develop this theory classifying the symmetries of correlation functions. These symmetries will be realised as gauge theory dualities, while  the language of the q-deformed Liouville theory will become a new powerful tool to investigate supersymmetric gauge theories.","1287088","2015-09-01","2020-08-31"
"HD-App","New horizons in homogeneous dynamics and its applications","Uri SHAPIRA","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","We present a large variety of novel lines of research in Homogeneous Dynamics with emphasis on the dynamics of the diagonal group. Both new and classical applications are suggested, most notably to 
• Number Theory
• Geometry of Numbers
• Diophantine approximation. 

Emphasis is given to applications in 

• Diophantine properties of algebraic numbers.

The proposal is built of 4 sections.

(1) In the first section we discuss questions pertaining to topological and distributional aspects of periodic orbits of the diagonal group in the space of lattices in Euclidean space. These objects encode deep information regarding Diophantine properties of algebraic numbers. We demonstrate how these questions are closely related to, and may help solve, some of the central open problems in the geometry of numbers and Diophantine approximation. 

(2) In the second section we discuss Minkowski's conjecture regarding integral values of products of linear forms. For over a century this central conjecture is resisting a general solution and a novel and promising strategy for its resolution is presented.

(3) In the third section, a novel conjecture regarding limiting distribution of infinite-volume-orbits is presented, in analogy with existing results regarding finite-volume-orbits. Then, a variety of applications and special cases are discussed, some of which give new results regarding classical concepts such as continued fraction expansion of rational numbers.

(4) In the last section we suggest a novel strategy to attack one of the most notorious open problems in Diophantine approximation, namely: Do cubic numbers have unbounded continued fraction expansion? This novel strategy leads us to embark on a systematic study of an area in homogeneous dynamics which has not been studied yet. Namely, the dynamics in the space of discrete subgroups of rank k in R^n (identified up to scaling).","1432730","2018-10-01","2023-09-30"
"HDEM","High Definition Electron Microscopy: Greater clarity via multidimensionality","Timothy PENNYCOOK","UNIVERSITEIT ANTWERPEN","Atomic resolution microscopy relies on beams of energetic electrons. These beams quickly destroy fragile materials, making imaging them a major challenge. I have recently developed a new approach that provides the greatest possible resolving power per electron. The method provides both double resolution and excellent noise rejection, via multidimensional data acquisition and analysis. Here I propose to couple the new method with breakthroughs in high speed cameras to achieve unprecedented clarity at low doses, almost guaranteeing major advances for imaging beam sensitive materials. Proof of principle will be achieved for biochemical imaging using the easy to handle, commercially available GroEL chaperone molecule. We will combine our enhanced imaging capabilities with the averaging methods recently recognized by the Nobel prize in chemistry for imaging biomolecules at ultra low doses. After proving our low dose capabilities we will apply them to imaging proteins of current interest at greater resolution. Similar techniques will be used for fragile materials science samples, for instance metal organic framework, Li ion battery, 2D, catalyst and perovskite solar cell materials. Furthermore the same reconstruction algorithms can be applied to simultaneously acquired spectroscopic images, allowing us to not only locate all the atoms, but identify them. The properties of all materials are determined by the arrangement and identity of their atoms, and therefore our work will impact all major areas of science, from biology to chemistry and physics.","1500000","2019-03-01","2024-02-29"
"HDSPCONTR","High-Dimensional Sparse Optimal Control","Massimo Fornasier","TECHNISCHE UNIVERSITAET MUENCHEN","""We are addressing the analysis and numerical methods for the tractable simulation and the optimal control of dynamical systems which are modeling the behavior of a large number N of complex interacting agents described by a large amount of parameters (high-dimension). We are facing fundamental challenges:
- Random projections and recovery for high-dimensional dynamical systems: we shall explore how concepts of data compression via Johnson-Lindenstrauss random embeddings onto lower-dimensional spaces can be applied for tractable simulation of complex dynamical interactions. As a fundamental subtask for the recovery of high-dimensional trajectories from low-dimensional simulated ones, we will address the efficient recovery of point clouds defined on embedded manifolds from random projections.
-Mean field equations: for the limit of the number N of agents to infinity, we shall further explore how the concepts of compression can be generalized to work for associated mean field equations.
- Approximating functions in high-dimension: differently from purely physical problems, in the real life the ”social forces” which are ruling the dynamics are actually not known. Hence we will address the problem of automatic learning from collected data the fundamental functions governing the dynamics.
- Homogenization of multibody systems: while the emphasis of our modelling is on “social” dynamics, we will also investigate methods to recast multibody systems into our high-dimensional framework in order to achieve nonstandard homogenization by random projections.
- Sparse optimal control in high-dimension and mean field optimal control: while self-organization of such dynamical systems has been so far a mainstream, we will focus on their sparse optimal control in high-dimension. We will investigate L1-minimization to design sparse optimal controls. We will learn high-dimensional (sparse) controls by random projections to lower dimension spaces and their mean field limit.""","1123000","2012-12-01","2017-11-30"
"HEALINSYNERGY","Material-driven Fibronectin Fibrillogenesis to Engineer Synergistic Growth Factor Microenvironments","Manuel Salmerón Sánchez","UNIVERSITY OF GLASGOW","""Cells within tissues are surrounded by fibrillar extracellular matrices (ECM) that support cell adhesion, migration, proliferation and differentiation. Fibronectin (FN) is an ECM protein organized into fibrillar networks by cells through an integrin-mediated process. This assembly allows the unfolding of the molecule, exposing cryptic domains not available in the native globular FN structure and activating intracellular signalling complexes.
This project aims to engineer functional interfaces between living cells and synthetic biomaterials, making use of the fundamental role of fibronectin (FN) to direct cell-material interactions. First, we will engineer material surfaces able to direct the physiological organization of FN into fibrillar networks in absence of cells, so-called material driven fibronectin fibrillogenesis. These surfaces will trigger the organization of FN upon simple adsorption of FN from solutions and will provide a biomimetic interface better recognized by cells, since it resembles the nature ECM environment in tissues. The mechanisms that promote the organization of FN at the material interface will be elucidated making use of different FN fragments and key modifications of the protein. The enhanced cellular activities of the material-driven FN matrices will be used to direct the behavior of human mesenchymal stem cells (hMSCs), seeking to direct either cell lineage or multipotency in combination with the properties of the underlying surface.
Secondly, we will engineer functional – living  biointerphases, on which the intermediate layer of proteins between the material surfaces and the cell population is expressed on the surface of non-pathogenic bacteria. This radical idea will provide the field with a living interphase that consists of genetically modified bacteria with FN fragments in the membrane.
These bacteria will be modified to secrete the desired proteins or factors in response to external stimuli, to direct the cell behavior of hMSCs.""","1410755","2013-04-01","2018-03-31"
"HEATTRONICS","Mesoscopic heattronics: thermal and nonequilibrium effects and fluctuations in nanoelectronics","Tero Tapio Heikkilä","JYVASKYLAN YLIOPISTO","Few systems in nature are entirely in equilibrium. Out of equilibrium, there are heat currents, and different degrees of freedom or parts of studied systems may be described by entirely different temperatures if the concept of temperature is at all well defined. In this project we will study the emergence of the subsystem temperatures in different types of small electronic systems, and the physical phenomena associated with those temperatures. Our emphasis is on the mesoscopic effects, residing between the microscopic world of individual atoms and electrons, and the macroscopic everyday world. In particular, we will research thermometry methods, different types of relaxation, magnitudes of fluctuations and effects at high frequencies. We will explore these effects in a wide variety of systems: normal metals and superconductors, carbon nanostructures, nanoelectromechanical and spintronic systems. Besides contributing to the understanding of the fundamental properties of electronic systems, our studies are directly relevant for the development of thermal sensors and electron refrigerators. The improved understanding of the thermal phenomena will also benefit the study of almost any type of a nonlinear phenomenon in electronics, for example the research of solid-state realizations of quantum computing or the race towards quantum limited mass and force detection.","1322371","2010-01-01","2015-12-31"
"HEMs-DAM","Hybrid Epitaxial Materials for Novel Quantum State Detection and Manipulation","Peter KROGSTRUP","KOBENHAVNS UNIVERSITET","Research in quantum information technologies is receiving increasing attention worldwide, and huge efforts are put into the realization of the first reliable building blocks for quantum computation. The main challenge the field is facing today is unquestionably related to the loss of information due to quantum state decoherence. As indicated by our recent experiments, the perennial problem of decoherence might be solved by fully epitaxial semiconductor-superconductor growth techniques of high quality topological superconducting materials. Moreover, quantum systems based around design principles such as gate-controlled semiconductor-superconductor materials that can hold topologically‐motivated symmetry protection, might enable simpler forms of control and less dependence on available control technology.

While research has made a lot of progress in the growth of semiconductor heterostructures and associated interfaces, the synthesis of semiconductor – metal/superconductor interfaces are comparably both uncontrolled and very poorly understood. As the device performance and potential applicability of nanostructured crystals largely depend on the quality of the involved interfaces, progress in synthesis of high quality interfaces will likely dictate the advancement and development not only of future quantum electronics but also play a key role in nanostructured device applications in general. 

The core of this proposal concerns the material synthesis of epitaxially grown semiconductor - metal/superconductor materials for advanced topological quantum electronics. The ambition will be to build an innovative environment that links between material and quantum sciences - with an overall emphasis on developing disorder-free hybrid semiconductor-superconductor crystals for novel quantum state detection and manipulation. This also includes an emphasis on developing high quality Josephson junctions - the key control point and crucial element in gatable superconductivity.","1339600","2017-08-01","2022-07-31"
"HEROIC","High-frequency printed and direct-written Organic-hybrid Integrated Circuits","Mario Caironi","FONDAZIONE ISTITUTO ITALIANO DI TECNOLOGIA","The HEROIC project aims at filling the gap between the currently low operation frequencies of printed, organic flexible electronics and the high-frequency regime, by demonstrating polymer-based field-effect transistors with maximum operation frequencies of 1 GHz and complementary integrated logic circuits switching in the 10-100 MHz range, fabricated by means of printing and direct-writing scalable processes in order to retain low temperature manufacturability of cost-effective large area electronics on plastic. The recent development of semiconducting polymers with mobilities in the range of 1 to 10 cm^2/Vs, and even higher in the case of aligned films, suggests that suitably downscaled printed polymer transistors with operation frequencies in the GHz regime, at least three orders of magnitude higher than current printed polymer devices, are achievable, by addressing in a holistic approach the specific challenges set in the HEROIC trans-disciplinary research programme: (i)development of scalable high resolution processes for the patterning of functional inks, where printing will be combined with direct-writing techniques such as fs-laser machining, both in an additive and subtractive approach; (ii)development of printable nanoscale hybrid dielectrics with high specific capacitance, where low-k polymer buffer materials will be combined with solution processable high-k dielectrics, such as insulating metal oxides; (iii)improvement of the control of charge injection and transport in printed polymer and hybrid semiconductors, where high-mobility 1-D and 2-D structures are included in polymer films; (iv)development of advanced printed and direct-written transistors architectures with low parasitic capacitances for high-speed operation. HEROIC will radically advance and expand the applicability of polymer-based printed electronics, thus making it suitable for next generation portable and wearable short-range wireless communicating devices with low power consumption.","1608125","2015-04-01","2020-03-31"
"HetScaleNet","Analysis and control of large scale heterogeneous networks: scalability, robustness and fundamental limits","Ioannis Lestas","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The proposed research will make a contribution towards the analysis and synthesis of large scale complex networks: fundamental theory will be developed and important applications will be addressed, by extending tools from control theory. Networks are present throughout the physical and biological world, but nowadays they also pervade our societies and everyday lives. Major challenges that will be addressed are:

I. The engineering of large scale heterogeneous networks that are guaranteed to be robust and scalable. 

II. The reverse engineering of biological networks.

A distinctive feature of the networks we would like to engineer, which falls outside more traditional domains in systems and control, is that of scalability, i.e. the ability to guarantee robust stability for an arbitrary interconnection by conditions on only local interactions. The methodologies that will be developed will have a significant impact in various applications where scalability is important, such as data network protocols, group coordination problems and power distribution networks, as they can lead to network designs with guaranteed robustness, thus avoiding conservative schemes with poor performance. The proposed project will also make a contribution towards the reverse engineering of biological networks at the molecular level. Life in the cell is dictated by chance; noise is ubiquitous with its sources ranging from fluctuating environments to intrinsic fluctuations due to the random births and deaths of molecules. The fact that a substantial part of the noise is intrinsic provides a major challenge in control theoretic methodologies. How can feedback be used to suppress these fluctuations, what are the associated tradeoffs and limitations, and how does nature manage to handle these so efficiently? These are questions that will be addressed by developing tools for analyzing known configurations, but more importantly, by deriving fundamental limitations that hold for arbitrary feedback.","1498531","2016-08-01","2021-07-31"
"HEVO","Holomorphic Evolution Equations","Filippo Bracci","UNIVERSITA DEGLI STUDI DI ROMA TOR VERGATA","The scope of this project is to study  holomorphic evolution equations and the associated dynamical systems, both from the local and the global point of view. In particular we aim to study general Loewner equations (both autonomous and non-autonomous) and applications to dynamical systems, in one and several complex variables. In one variable we plan to develop a general version of SLE's for non-slit evolutions and apply to physical and others problems. In several variables we plan to develop the general theory, together with applications.","700000","2011-11-01","2016-10-31"
"HHNCDMIR","Hochschild cohomology, non-commutative deformations and mirror symmetry","Wendy Lowen","UNIVERSITEIT ANTWERPEN","""Our research programme addresses several interesting current issues in non-commutative algebraic geometry, and important links with symplectic geometry and algebraic topology. Non-commutative algebraic geometry is concerned with the study of algebraic objects in geometric ways. One of the basic philosophies is that, in analogy with (derived) categories of (quasi-)coherent sheaves over schemes and (derived) module categories, non-commutative spaces can be represented by suitable abelian or triangulated categories. This point of view has proven extremely useful in non-commutative algebra, algebraic geometry and more recently in string theory thanks to the Homological Mirror Symmetry conjecture. One of our main aims is to set up a deformation framework for non-commutative spaces represented by """"enhanced"""" triangulated categories, encompassing both the non-commutative schemes represented by derived abelian categories and the derived-affine spaces, represented by dg algebras. This framework should clarify and resolve some of the important problems known to exist in the deformation theory of derived-affine spaces. It should moreover be applicable to Fukaya-type categories, and yield a new way of proving and interpreting instances of """"deformed mirror symmetry"""". This theory will be developed in interaction with concrete applications of the abelian deformation theory developed in our earlier work, and with the development of new decomposition and comparison techniques for Hochschild cohomology. By understanding the links between the different theories and fields of application, we aim to achieve an interdisciplinary understanding of non-commutative spaces using abelian and triangulated structures.""","703080","2010-10-01","2016-09-30"
"HHQM","Hydrodynamics, holography and strongly-coupled quantum matter","Blaise GOUTÉRAUX","ECOLE POLYTECHNIQUE","The dynamics of weakly-coupled quantum matter can be solved by techniques deriving from perturbative quantum field theory. Conventional metals are described by long-lived quasiparticles (Fermi liquids). No such methods are available for strongly-coupled quantum matter where quasiparticles are short-lived, like the Quark-Gluon-Plasma, high Tc superconductors (HTCs) or graphene near the charge neutrality point.

In HTCs, it has been argued the interaction timescale is the fastest scale in the system, which warrants a hydrodynamic description. In a recent series of remarkable theoretical and experimental developments, hyrodynamics signatures have been discovered in several strongly-coupled quantum systems such as graphene, delafossites and HTCs. Further theoretical progress is impeded by the lack of symmetry: momentum is only approximately conserved, which complicates the use of hydrodynamics as an effective low-energy theory; and the strange metallic phenomenology of HTCs, believed to originate from a quantum critical point, is not captured by conventional scaling arguments. New ideas are required to move beyond the current state of the art.

Gauge/Gravity duality is a radically new approach which links a relativistic strongly-coupled quantum field theory to a classical theory of gravity. The hydrodynamic regime of the QGP has been very successfully described by these methods, which predict a shear viscosity very close to experimental values.

Our focus in this proposal is to use holography to consistently model hydrodynamics with momentum relaxation and study its interplay with unconventional quantum criticality. This is crucial for a better understanding of the phenomenology in strongly-coupled quantum matter. As many systems are not relativistic, we will also consider hydrodynamics in non-relativistic holographic theories, thus enhancing our understanding of holographic dualities beyond the original Anti de Sitter/Conformal Field Theory correspondence.","1498028","2018-09-01","2023-08-31"
"Hi-EST","Holistic Integration of Emerging Supercomputing Technologies","David Carrera Perez","BARCELONA SUPERCOMPUTING CENTER - CENTRO NACIONAL DE SUPERCOMPUTACION","Hi-EST aims to address a new class of placement problem, a challenge for computational sciences that consists in mapping workloads on top of hardware resources with the goal to maximise the performance of workloads and the utilization of resources. The objective of the placement problem is to perform a more efficient management of the computing infrastructure by continuously adjusting the number and type of resources allocated to each workload. 

Placement, in this context, is well known for being NP-hard, and resembles the multi-dimensional knapsack problem. Heuristics have been used in the past for different domains, providing vertical solutions that cannot be generalised. When the workload mix is heterogeneous and the infrastructure hybrid, the problem becomes even more challenging. This is the problem that Hi-EST plans to address. The approach followed will build on top of four research pillars: supervised learning of the placement properties, placement algorithms for tasks, placement algorithms for data, and software defined environments for placement enforcement.

Hi-EST plans to advance research frontiers in four different areas: 1) Adaptive Learning Algorithms: by proposing the first known use of Deep Learning techniques for guiding task and data placement decisions; 2) Task Placement: by proposing the first known algorithm to map heterogeneous sets of tasks on top of systems enabled with Active Storage capabilities, and by extending unifying performance models for heterogeneous workloads to cover and unprecedented number of workload types; 3) Data Placement: by proposing the first known algorithm used to map data on top of heterogeneous sets of key/value stores connected to Active Storage technologies; and 4) Software Defined Environments (SDE): by extending SDE description languages with a still inexistent vocabulary to describe Supercomputing workloads that will be leveraged to combine data and task placement into one single decision-making process.","1467783","2015-05-01","2020-04-30"
"HI-ONE","Hybrid Inorganic-Organic NanoElectronics","Wilfred Gerard Van Der Wiel","UNIVERSITEIT TWENTE","This project aims at combining inorganic and organic materials in hybrid nanoelectronic structures for addressing a set of key problems in solid-state physics: (1) the magnetic ordering of 2D spin systems and their interaction with conduction electrons, (2) the coherent transport properties of organic molecules, and (3) reliable electronic characterization of single nanostructures. For all objectives we will integrate top-down and bottom-up (self-assembly) techniques, benefitting from strong collaborations with leading chemistry groups. For Objective 1, we will apply self-assembled monolayers of organic paramagnetic molecules on various substrates. This geometry offers great tunability for the nature, density and ordering of spins, and for their interaction with underlying electrons. We will study (many-body) phenomena that lie at the very heart of solid-state physics: the Kondo effect, RKKY interaction, spin glasses and the 2D Ising/Heisenberg model, addressing open questions concerning the extension of the Kondo cloud, RKKY-Kondo competition, and the relevance for high-Tc superconductivity. For Objective 2, molecular monolayers are inserted in an electron interferometer, allowing a systematic study of molecular charge coherence. We will study how coherence depends on the molecule s characteristics, such as length and chemical composition. For Objective 3 we will attach single nanostructures (quantum dots) by an innovative self-assembly method to highly-conductive, selectively metallized DNA molecules, bridging the gap between nano and micro. A crucial advantage compared to conventional (top-down) nanocontacting schemes is the high control and reproducibility afforded by sequence-specificity of DNA hybridization, enabling a wide range of fascinating experiments.","1750000","2009-12-01","2014-11-30"
"HiCoS","Higher Co-dimension Singularities: Minimal Surfaces and the Thin Obstacle Problem","Emanuele SPADARO","UNIVERSITA DEGLI STUDI DI ROMA LA SAPIENZA","Singular solutions to variational problems and to partial differential equations are naturally ubiquitous in many contexts, and among these minimal surfaces theory and free boundary problems are two prominent examples both for their  analytical content and their physical interest.
A crucial aspect in this regard is the co-dimension of the objects under consideration: indeed, many of the analytical and geometric principles which are valid for minimal hypersurfaces or regular points of the free boundary do not apply to higher co-dimension surfaces or singular free boundary points.

The aim of this project is to investigate some of the most compelling questions about the singularities of two classical problems in the geometric calculus of variations in higher co-dimension:

I. Mass-minimizing integer rectifiable currents, i.e. solutions to the Plateau problem of finding the surfaces of least area, attacking specific conjectures about the structure of the singular set, most prominently the boundedness of its measure.

II. The thin obstacle problem, consisting in minimizing the Dirichlet energy (or a variant of it) among functions constrained above an obstacle that is assigned on a lower dimensional space, with the purpose of answering some of the main open questions on the singular free boundary points.

The main unifying theme of the project is the central role played by geometric measure theory, which underlines various common aspects of these two problems and makes them suited to be treated in an unified framework.
Although these are classical questions with a long tradition, our knowledge about them is still limited and their investigation is among the most challenging issues in regularity theory. This is the central focus of the project, with the final goal to develop suitable analytical techniques that provides valuable insights on the mathematics at the basis of higher co-dimension singularities, eventually fruitful in other geometric and analytical settings.","1341250","2018-02-01","2023-01-31"
"HIDGR","Higher dimensional general relativity: explicit solutions and the classification and stability of black holes","Harvey Stephen Reall","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","""Higher-dimensional General Relativity (GR) is well-motivated by string theory e.g. via the gauge/gravity correspondence or scenarios that predict black hole production at the Large Hadron Collider. In this proposal it is regarded as a self-contained mathematical subject that extends conventional 4d GR. It is known that higher-dimensional GR exhibits qualitative differences from 4d GR, especially for black holes,
e.g. there exist """"black ring"""" solutions describing rotating, donut-shaped black holes. It is likely that there are many other interesting solutions. This project will investigate the following topics in higher-dimensional GR: 1. Methods for obtaining explicit solutions of the Einstein equation, especially those based on algebraic classification of the Weyl tensor; 2. Classical stability of black holes; 3. Classification of black hole
solutions: What data is required to specify uniquely black hole solutions? What are the allowed topologies and symmetries of black holes?""","1337044","2011-11-01","2016-10-31"
"HIENA","Hierarchical Carbon Nanomaterials","Michael Franciscus Lucas De Volder","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","""Over the past years, carbon nanomaterial such as graphene and carbon nanotubes (CNTs) have attracted the interest of scientists, because some of their properties are unlike any other engineering material. Individual graphene sheets and CNTs have shown a Youngs Modulus of 1 TPa and a tensile strength of 100 GPa, hereby exceeding steel at only a fraction of its weight. Further, they offer high currents carrying capacities of 10^9 A/cm², and thermal conductivities up to 3500 W/mK, exceeding diamond. Importantly, these off-the-chart properties are only valid for high quality individualized nanotubes or sheets. However, most engineering applications require the assembly of tens to millions of these nanoparticles into one device. Unfortunately, the mechanical and electronic figures of merit of such assembled materials typically drop by at least an order of magnitude in comparison to the constituent nanoparticles.

In this ERC project, we aim at the development of new techniques to create structured assemblies of carbon nanoparticles. Herein we emphasize the importance of controlling hierarchical arrangement at different length scales in order to engineer the properties of the final device. The project will follow a methodical approach, bringing together different fields of expertise ranging from macro- and microscale manufacturing, to nanoscale material synthesis and mesoscale chemical surface modification. For instance, we will pursue combined top-down microfabrication and bottom-up self-assembly, accompanied with surface modification through hydrothermal processing.

This research will impact scientific understanding of how nanotubes and nanosheets interact, and will create new hierarchical assembly techniques for nanomaterials. Further, this ERC project pursues applications with high societal impact, including energy storage and water filtration. Finally, HIENA will tie relations with EU’s rich CNT industry to disseminate its technologic achievements.""","1496379","2014-01-01","2018-12-31"
"HiggspT","Differential Higgs distributions as a unique window to New Physics at the LHC","Kerstin Tackmann","STIFTUNG DEUTSCHES ELEKTRONEN-SYNCHROTRON DESY","In 2012, the ATLAS and CMS experiments at the Large Hadron Collider at CERN announced a ground-breaking discovery: both experiments observed a new particle. Subsequent measurements confirmed it to be a Higgs boson. In the Standard Model (SM) of particle physics, which describes the known elementary particles and their interactions, as well as in many extensions of the SM, the Higgs boson is fundamentally linked to the question of how elementary particles acquire mass. A thorough program of measurements is necessary to determine if this particle has indeed the properties of a Higgs boson as predicted by the SM or one of its extensions, and to gain a complete understanding of the mass generation for elementary particles. Studies of the Higgs sector now open a unique window to the discovery of New Physics.

The aim of the presented project is to perform a detailed analysis of the Higgs differential distributions measured in Higgs decays to diphotons and to four leptons, using the data collected by the ATLAS experiment between 2015 and 2021. These distributions are sensitive to effects from New Physics and will be confronted with precise theoretical predictions. In this way, the indirect extraction of Higgs couplings and the search for effects from new heavy particles can lead to a discovery of New Physics.

The detailed analysis of differential distributions goes substantially beyond the standard analyses based on measured event counts. A dedicated program is needed to achieve these goals. With an ERC Starting Grant, I will assemble a team to make decisive contributions to these challenging measurements and build a unique research program. As a former leader of the ATLAS Higgs-to-diphoton physics group and current leader of the electron and photon reconstruction group I am in an ideal position to establish a strong research team. This team will build on the important contributions to the Higgs boson discovery and property studies made by my Young Investigators Group.","1317500","2016-05-01","2021-04-30"
"HIGHACCTC","High-accuracy models in theoretical chemistry","Mihály Kállay","BUDAPESTI MUSZAKI ES GAZDASAGTUDOMANYI EGYETEM","Even today, quantum chemical calculations with experimental accuracy are only feasible for small molecules. This statement is especially true if the considered molecule is far from the equilibrium structure, where the overwhelming majority of quantum chemical models break down. The main purpose of this proposal is to develop new quantum chemical methods that are applicable to at least medium-sized molecules and simultaneously provide results sufficiently close to the experimental data and are capable of describing entire potential energy surfaces. The accuracy goal will be achieved through the reduction of the computational cost of high-precision quantum chemical calculations, which are currently practical for molecules of up to 15 atoms. The cost reduction will be accomplished principally by decreasing the number of numerical parameters to be optimized without sacrificing accuracy. To this end, the negligible parameters will be identified and dropped by adopting the corresponding techniques of computer science. The correct behavior of the models for distorted structures will be ensured by developing new approaches that use a linear combination of functions rather than a single function as a starting point for the description of electronic states. Since the programming work associated with the implementation of the proposed schemes is very complex, the project will rely on the automated programming tools previously developed by the proposer. In addition to the outlined challenging tasks, the proposal aims to implement several more straightforward objectives. In particular, the high-accuracy calculations will be extended to molecular properties that are presently not available. Furthermore, the developed methods will be applied to real-life problems, especially in the field of spectroscopy and atmospheric chemistry.","500000","2008-07-01","2013-06-30"
"HIGHWIND","Simulation, Optimization and Control of High-Altitude
Wind Power Generators","Moritz Mathias Diehl","ALBERT-LUDWIGS-UNIVERSITAET FREIBURG","A new class of large scale wind power generators shall be investigated via
mathematical modelling, computer simulation and multidisciplinary optimization methods. The underlying
technical idea is to use fast flying tethered airfoils that fly in altitudes of several hundred
meters above the ground. They will perform specially controlled loops accompanied by line length
and line tension variations, that are used to drive a generator on the ground.
Being a high risk / high gain technology, the applicant believes that the main focus in the first development
years should not be on building large and expensive experimental setups (as some courageous
experimentalists currently do in Europe and the US), but on mathematical modelling, computer
simulation and optimization studies, accompanied by only small scale experiments for model
and control system validation. This will help finding optimal system designs before expensive and
potentially dangerous large scale systems are built. The research requires an interdisciplinary collaboration
of scientists from mathematical, mechanical, aerospace, and control engineering, as well
as from the computational sciences.  At the end of the project, a small scale, automatically flying prototype shall be realized, accompanied
by validated and scalable mathematical models and a toolbox of efficient computational methods
for simulation and multidisciplinary optimization of high altitude wind power systems. If successful,
the project will help to establish this new type of wind power generator that might provide electricity
more cheaply than fossil fuels and is deployable at considerably more sites than conventional windmills.","1499800","2011-03-01","2017-02-28"
"HIP-LAB","High-throughput integrated photonic lab-on-a-DVD platforms","Andreu Llobera","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","The main aim of the proposed research line is to develop high-throughput highly sensitive photonic lab-a-DVD platforms for multiple parallel analysis with an extremely high degree of integration. The already existing high-throughput platforms only use the CD platform as a substrate, without any given functionality, conversely, in this research line, in the DVD platform it is proposed the integration of the following elements: (i) polymeric photonic components (high-sensitivity Mach-Zehnder interferometers, diffraction gratings and hollow prisms). (ii) polymeric microfluidics (hydrophobic valves and mixers). (iii) Chemical modification of the surface with functional groups prone to interact with the specific analyte and (iv) the necessary information in the DVD tracks to allow the usage of the proposed system in modified DVD readers. Additionally, a new set-up will be mounted, in which a second DVD-header will be incorporated, in such a way that simultaneous high-throughput photonic measurements could be easily performed. Clearly, as compared to the existing platforms, the presented research line requires the establishment of a dynamic multidisciplinary group comprising experts of photonics, microfluidics and (bio)chemistry and the results obtained therein will allow the definition of an advanced photonic high-throughput lab-on-a-DVD platform that will definitely have a large number of application fields, ranging from molecular diagnosis to analytical chemistry or proteomics.","1717200","2008-10-01","2014-09-30"
"HiPerBat","Hunting for high performance energy storage in batteries","Marnix Wagemaker","TECHNISCHE UNIVERSITEIT DELFT","One of the great challenges of this century is unquestionably energy storage. Storage is essential to make more efficient use of renewable energy sources and to enable electrical mobility. Recent developments have raised both hopes and fundamental challenges in the next generation Li batteries (including Li-ion and Li-air/Li-sulphur). Despite large research efforts, the improvement of battery performance over the last decades has been relatively small because the full potential of the storage materials is not utilized. Most of the attention has been devoted to the development of new electrode materials; however, marginal understanding has been achieved of the functioning of these materials in electrodes. The key problem is that established micro and macroscopic methods are not sensitive to the relevant time and length scales under the required in-situ conditions. Moreover, up to date calculational models do not represent the full complexity of the electrode systems.
Using novel experimental and calculational approaches this project aims at fundamental understanding and improvement of Li electrodes. This requires a broad multidisciplinary approach, ranging from nuclear magnetic resonance probing nanoscopic charge transfer to in-situ neutron depth profiling exploring the mesoscopic charge transport. Calculations will combine the complex solid state diffusion in storage materials with the mesoscopic charge transport through the electrodes. By systematic variation of the electrode micro and nanostructure, this will lead to deep fundamental understanding. This project will be the first major systematic study on the fundamentals of complete electrodes. By bringing our current understanding from the level of the storage material towards complete electrodes, it will also pave the way to optimal high performance energy storage in batteries. The impact on society cannot be overstated as energy storage is a key enabler for the use of renewable energy and electrical transport.","1497838","2013-01-01","2017-12-31"
"hipQCD","Highest Precision QCD predictions for a new era in Higgs boson phenomenology","Fabrizio CAOLA","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The discovery of the Higgs boson at the CERN Large Hadron Collider (LHC) marked the beginning of a new era for particle physics. For the first time, we may have an experimentally tested and theoretically coherent picture of fundamental interactions, valid up to very high energies. A thorough exploration of the Higgs sector, to ascertain whether or not the new particle behaves as predicted by the Standard Model is now paramount.

Such an investigation is extremely challenging, and it requires absolute control over many complex Higgs signal and background processes. The goal of hipQCD is to develop innovative techniques for highest precision theoretical predictions at colliders, and to apply them for a wide range of high impact Higgs phenomenological studies at the LHC.

hipQCD addresses the major Higgs production and decay channels. Its main objectives are

1. to provide realistic predictions at ultimate accuracy for the main Higgs production and decay channels, by developing cutting-edge fully differential predictions at the third order in QCD perturbation theory for Higgs production in gluon and vector boson fusion and for Higgs decay to b quarks;

2. to allow for precise and reliable Higgs characterization studies at very high energy scales, by developing novel techniques to tackle multi-loop amplitudes in extreme kinematics configurations;

3. to significantly improve our description of Higgs production in association with other Standard Model particles, by performing groundbreaking investigations of key 2 → 3 reactions at higher orders in perturbation theory.

hipQCD involves different areas of particle theory, ranging from multi-loop amplitude computations to the study of soft/collinear structures in QFT to comprehensive Higgs LHC phenomenology. Besides their crucial impact on Higgs physics, its results could also be applied to a broader range of phenomenological studies and will be essential to fully profit from existing and future collider data.","1497016","2019-06-01","2024-05-31"
"HiRISE","High-Resolution Imaging and Spectroscopy of Exoplanets","Arthur Antoine VIGAN","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Atmospheric composition provides essential markers of the most fundamental properties of exoplanets, such as their formation mechanism or internal structure. New-generation exoplanet imagers have been designed to achieve high contrast for the detection of young giant planets in the near-infrared, but they only provide very low spectral resolutions (R<100) for their characterization. For a major breakthrough in the comprehension of young exoplanets and their atmospheres, an increase of a factor 100 to 1000 in spectral resolution is absolutely required.

This proposal ambitions to develop a novel demonstrator that will combine the capabilities of two flagship instruments installed on the ESO Very Large Telescope, the high-contrast exoplanet imager SPHERE and the high-resolution spectrograph CRIRES+, with the goal of answering fundamental questions on the formation, composition and evolution of young planets.

The work will be organized along two axes interconnected with transverse activities: (i) the astrophysics block will investigate signal extraction from high-resolution data and atmospheric modeling, and (ii) the instrumentation block will develop a demonstrator designed to pick up the near-infrared light in SPHERE and feed CRIRES+ via a dedicated injection module and optical fiber relay. We will explore all the key aspects of the project using a combination of instrumental and astrophysical simulations, as well as laboratory validation of components and methods on our high-contrast imaging testbed.

We will use the demonstrator to observe a sample of directly imaged companions and obtain high-resolution spectroscopy of their atmospheres. From the data we will (1) determine their formation mechanism through an accurate determination of the carbon and oxygen abundances in their atmospheres, and (2) map the temporal variability of their photosphere through time-resolved Doppler imaging to study dynamical processes related to the formation and evolution of clouds.","1496730","2017-12-01","2022-11-30"
"HISOL","High Energy Optical Soliton Dynamics for Efficient Sub-Femtosecond and Vacuum-Ultraviolet Pulse Generation","John Travers","HERIOT-WATT UNIVERSITY","I will study a new regime of high-energy temporal optical soliton dynamics in gas and plasma filled large-bore hollow capillaries—something never previously attempted. Soliton dynamics are fundamental to many of the most fascinating and useful nonlinear processes occurring in conventional optical fibres. Currently the peak powers demonstrated are around 100 megawatts, in hollow-core photonic crystal fibres, with energies of tens of microjoules. I aim to achieve terawatt peak power, millijoule energy-scale, soliton dynamics, and thus combine high-field laser science with the physics of solitons.

I will transfer energy from millijoule pump solitons in the near-infrared to the vacuum ultraviolet (100 nm to 200 nm, 6 eV to 12 eV), through resonant dispersive-wave emission. The emitted radiation will be coherent, ultrafast, and tunable through control of the filling gas pressure and capillary bore radius. The predicted conversion efficiencies are up to 20%, leading to VUV energies of over 400 microjoules in pulse durations of just 400 attoseconds (a single-cycle), with corresponding terawatt peak power; making this low-cost and table-top VUV source brighter than synchrotron sources. This will have wide impact: the VUV region, poorly served by current sources, is of great importance to many ultrafast spectroscopy techniques because many materials have electronic resonances there.

Through soliton self-compression I will also compress 10 femtosecond, millijoule-scale, near-infrared, pump pulses to both single-cycle and even sub-cycle waveforms, achieving sub-femtosecond durations and terawatt peak powers. These will be the shortest isolated optical pulses ever generated in the near-infrared spectral region. I will use them to drive high-energy isolated attosecond pulse generation in the XUV through HHG.

Finally, I will combine these VUV and XUV sources, in a single experiment, to perform proof-of-concept attosecond resolved VUV–XUV pump-probe spectroscopy experiments.","1723191","2016-07-01","2021-06-30"
"HOLOBHC","Holography for realistic black holes and cosmologies","Geoffrey Gaston Joseph Jean-Vincent Compère","UNIVERSITE LIBRE DE BRUXELLES","String theory provides with a consistent framework which combines quantum mechanics and gravity. Two grand challenges of fundamental physics - building realistic models of black holes and cosmologies - can be addressed in this framework thanks to novel holographic methods.

Recent astrophysical evidence indicates that some black holes rotate extremely fast, as close as 98% to the extremality bound. No quantum gravity model for such black holes has been formulated so far. My first objective is building the first model in string theory of an extremal black hole. Taking on this challenge is made possible thanks to recent advances in a remarkable duality known as the gauge/gravity correspondence. If successful, this program will pave the way to a description of quantum gravity effects that have been conjectured to occur close to the horizon of very fast rotating black holes.


Supernovae detection has established that our universe is starting a phase of accelerated expansion. This brings a pressing need to better understand still enigmatic features of de Sitter spacetime that models our universe at late times. My second objective is to derive new universal properties of the cosmological horizon of de Sitter spacetime using tools inspired from the gauge/gravity correspondence. These results will contribute to understand its remarkable entropy, which, according to the standard model of cosmology, bounds the entropy of our observable universe.","1020084","2014-02-01","2019-01-31"
"HOLOLHC","Holography for the LHC era","David Julian Mateos Sole","UNIVERSITAT DE BARCELONA","With the advent of the Large Hadron Collider (LHC), particle and nuclear physics have entered a new era. Heavy ion collisions at the LHC have already begun to provide new insights into the quark-gluon plasma (QGP) phase of Quantum Chromodynamics (QCD). The beam energy scan program at the Relativistic Heavy Ion Collider (RHIC) will explore part of the rich phase diagram of QCD at finite baryon density. Proton-proton collisions at the LHC will  uncover the mechanism responsible for electroweak symmetry breaking (EWSB).

Much of the physics involved in these experiments is, or may turn out to be, strongly coupled.  Making contact with experiment thus requires a theoretical understanding of strongly coupled gauge dynamics.  This project aims at using the gauge/string duality to make essential contributions to our understanding of: (i) Out-of-equilbrium dynamics of the QGP, in particular of the  thermalization process; (ii) The QCD phase diagram, in particular of color superconducting phases; (iii) Strongly coupled dynamics potentially relevant for EWSB, in particular of walking dynamics.

These three main objectives are interconnected by two horizontal lines: (i) Identification of universal observables, which hold the best potential to make contact with experiment, and  (ii) Communication with other fields, which is crucial for the success of such an interdisciplinary proposal.","1419424","2012-10-01","2017-09-30"
"HOLOVIEW","Single active dopant detection in semiconductor nanowires using electron holography","David Edward Cooper","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","""The end of """"happy scaling"""" in the semiconductor industry has lead to innovation being a key parameter in nanoelectronics. Doped nanowires of all types of sizes, shapes and composition are now used as components to build nano-electronic devices, light sources, detectors and for photovoltaic applications. As these devices are reduced in size, the location of individual dopant atoms becomes more important and the behavior of only one or two atoms can dominate their properties. More recently, research into devices that contain a single dopant atom has begun to gain momentum. At this time there is no method that can routinely measure the presence of the single dopant atoms that are inside these devices and our experience in the nanotechnology age has taught us that we cannot make what we cannot see. In 2011, several review papers in high-impact journals have highlighted the need for a technique that can see these atoms. Their detection is now within reach, using a technique known as off-axis electron holography.""","1500000","2012-12-01","2018-07-31"
"HOMOVIS","High-level Prior Models for Computer Vision","Thomas Pock","TECHNISCHE UNIVERSITAET GRAZ","Since more than 50 years, computer vision has been a very active research field but it is still far away from the abilities of the human visual system. This stunning performance of the human visual system can be mainly contributed to a highly efficient three-layer architecture: A low-level layer that sparsifies the visual information by detecting important image features such as image gradients, a mid-level layer that implements disocclusion and boundary completion processes and finally a high-level layer that is concerned with the recognition of objects.
Variational methods are certainly one of the most successful methods for low-level vision. However, it is very unlikely that these methods can be further improved without the integration of high-level prior models. Therefore, we propose a unified mathematical framework that allows for a natural integration of high-level priors into low-level variational models. In particular, we propose to represent images in a higher-dimensional space which is inspired by the architecture for the visual cortex. This space performs a decomposition of the image gradients into magnitude and direction and hence performs a lifting of the 2D image to a 3D space. This has several advantages: Firstly, the higher-dimensional embedding allows to implement mid-level tasks such as boundary completion and disocclusion processes in a very natural way. Secondly, the lifted space allows for an explicit access to the orientation and the magnitude of image gradients. In turn, distributions of gradient orientations – known to be highly effective for object detection – can be utilized as high-level priors. This inverts the bottom-up nature of object detectors and hence adds an efficient top-down process to low-level variational models.
The developed mathematical approaches will go significantly beyond traditional variational models for computer vision and hence will define a new state-of-the-art in the field.","1473525","2015-06-01","2020-05-31"
"HONEYPOL","Polariton networks: from honeycomb lattices to artificial gauge fields","Alberto Amo Garcia","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Boson gases confined in lattices present fundamental properties which strongly depart from their 3D counterparts. A notorious example is the honeycomb lattice, whose geometry results in massless Dirac-like states. By engineering the phase picked by the particles when tunneling from site to site, lattices also allow for the generation of artificial gauge fields. They result in very strong effective magnetic fields, opening the way to the observation of new quantum Hall regimes in neutral particles. In this context, polaritons appear as an excellent platform for the study of boson fluid effects in confined geometries. Polaritons are two-dimensional half-light/half-matter quasi-particles arising from the strong coupling between quantum well excitons and photons confined in a semiconductor microcavity. They are fully accessible by optical means and present strong non-linear properties. In this project, I will fabricate polariton microsstructures to study mesoscopic physics in 2D lattics.

I will start by studying the non-linear Josephson dynamics in coupled micropillars, and engineer a double tunneling structure showing single polariton blockade. I will then fabricate a graphene-like honeycomb lattice, where I will study transport phenomena such as anomalous (Klein) tunneling and antilocalisation in the presence of disorder, phenomena originating from the Dirac-cone characteristic of honeycomb lattices. In the high density regime, I will investigate non-linear effects, and address the question of superfluidity of massless Dirac particles.

Finally, I will undertake the realization of artificial gauge fields for polaritons. I will adapt to the polariton case a recent theoretical proposal to create artificial gauges in photons using coupled microdisks. Our results will have strong impact on current studies on the transport properties of graphene, of boson gases in atomic condensates, and also on the design of photonic systems with topological protection from disorder.","1499950","2013-10-01","2018-09-30"
"HOTLHC","Hot and dense QCD in the LHC era","Carlos Alberto Salgado Lopez","UNIVERSIDAD DE SANTIAGO DE COMPOSTELA","QCD, the theory of strong interactions, has been defined as our most perfect physical theory, in part because its compact and apparently simple Lagrangian hides a plethora of emerging phenomena. The aim of the present project is to make the essential contributions to fully exploit the new possibilities of the Large Hadron Collider to characterise unexplored domains of QCD.

Three main working plans are foreseen: i) The partonic structure of the protons and nuclei at LHC energies, where I plan to unravel the structure at small fraction of momentum of the colliding objects, characterising new regimes of QCD at high parton densities; ii) A new theory of jets in a medium, in which a new way of understanding the phenomenon of parton branching of a quark or gluon in a medium, including new evolution equations is proposed; iii) A Monte Carlo for jet quenching, where the solid theoretical framework developed in the previous point will be implemented into a Monte Carlo code for general use. Along these working plans, two horizontal lines - Talking to the experiment: finding the signatures; and Talking to other fields - will ensure the coherence of the project and the communication of the results as well as the collaboration with external researchers, especially experimentalist.

In order to fulfil these ambitious goals, the research team will need of reinforcement in terms of (wo)manpower and travel and computer resources. The total requested contribution from the ERC-StG to the project is 1.499.376 Euros, which is divided as follows: 39% for postdocs; 18% for PhD students; 12% for visits to CERN; 10% for the PI salary; 8% for travel; 8% for computers and 4% for visitors.","1379376","2012-01-01","2017-12-31"
"HP4all","Persistent and Transportable Hyperpolarization for Magnetic Resonance","Sami Antoine Adrien JANNIN","UNIVERSITE LYON 1 CLAUDE BERNARD","Magnetic resonance imaging (MRI) and nuclear magnetic resonance (NMR) and are two well-established powerful and versatile tools that are extensively used in many fields of research, in clinics and in industry.
Despite considerable efforts involving highly sophisticated instrumentation, these techniques suffer from low sensitivity, which keeps many of today’s most interesting problems in modern analytical sciences below the limits of MR detection. 
Hyperpolarization (HP) in principle provides a solution to this limitation. We have recently pioneered breakthrough approaches using dissolution dynamic nuclear polarization (d-DNP) for preparing nuclear spins in highly aligned states, and therefore boosting sensitivity in several proof-of-concept reports on model systems. The proposed project aims to leverage these new advances through a series of new concepts i) to generate the highest possible hyperpolarization that can be transported in a persistent state, and ii) to demonstrate their use in magnetic resonance experiments with > 10’000 fold sensitivity enhancements, with the potential of revolutionizing the fields of MRI and NMR.
By physically separating the source of polarization from the substrate at a microscopic level, we will achieve polarized samples with lifetimes of days that can be stored and transported over long distances to MRI centers, hospitals and NMR laboratories. Notable applications in the fields of drug discovery, metabolomics and real-time metabolic imaging in living animals will be demonstrated.
These goals require a leap forward with respect to today’s protocols, and we propose to achieve this through a combination of innovative sample formulations, new NMR methodology and advanced instrumentation.
This project will yield to a broadly applicable method revolutionizing analytical chemistry, drug discovery and medical diagnostics, and thereby will provide a powerful tool to solve challenges at the forefront of molecular and chemical sciences today.","1995000","2017-01-01","2021-12-31"
"HPAH","Hydrogen interaction with polycyclic aromatic hydrocarbons – from interstellar catalysis to hydrogen storage","Liv Hornekaer","AARHUS UNIVERSITET","In a truly cross-disciplinary research project encompassing surface science, astrophysics and chemistry we aim to address two of the major outstanding questions in the field of astrochemistry, namely i) how molecular hydrogen, the most abundant molecule in the interstellar medium, form, and ii) whether it is possible to identify specific Polycyclic Aromatic Hydrocarbon (PAH) species in interstellar spectra. The insights gained from the experimental investigations may revolutionize our current understanding of astrochemistry and will have impact even beyond the field. Special emphasis will be placed on the impact our findings will have on ascertaining the suitability of PAHs as a hydrogen storage medium. By combining scanning tunneling microscopy, thermal desorption spectroscopy, laser-induced thermal desorption time-of-flight mass spectrometry, fluorescence spectroscopy experiments and density functional theory calculations we will map out the interaction of atomic hydrogen with PAHs. The goal of the investigation is to obtain atomic level understanding of the atomic hydrogen – PAH interaction in order to i) ascertain whether interstellar molecular hydrogen formation, contrary to present belief but in accordance with our recent calculations, could occur predominantly via interaction with PAHs, ii) measure the adsorption/emission spectrum of Hydrogen-PAH complexes and thereby facilitate observational detection of these complexes in the interstellar medium, iii) determine whether PAHs are a promising medium for hydrogen storage and iv) ascertain whether the hydrogen storage properties of PAHs are tunable by electro-magnetic radiation. This ambitious and cross-disciplinary research project will predominantly take place at the newly established Surface Dynamics Laboratory at the University of Aarhus, headed by the applicant, but will also benefit from fruitful collaborations already initiated with local, national and international colleagues.","1499810","2008-07-01","2013-06-30"
"HPCNTW","High performance and ultralight carbon nanotube wires for power transmission","Krzysztof Kazimierz Koziol","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Due to their unique molecular structure carbon nanotubes can offer high electrical conductivity and superior current density. Both of these properties are sought after, especially for overhead power transmission lines where the extremely high axial strength of nanotubes would also be a bonus. In this research proposal single wall carbon nanotubes (nanometer size tubes made of rolled up graphene sheets) with desirable dimensions and controlled way of the graphene sheet rolled up into a tube (referred to as chirality), will be synthesized and spun into fibres using two unique methods, which were developed in Cambridge. These high performance carbon nanotube fibres will be explored as flexible, lightweight, highly efficient materials for use as wires for a variety of power transmission applications.
The project will focus on achieving precise chirality control of carbon nanotubes through crystallographic manipulation of the catalyst particles using a recently-discovered in-house method. Tuning the molecular structure of individual nanotubes will achieve maximum uniformity and desired level of electrical conductivity. Next, carbon nanotube fibres will be spun using a unique process currently available only in Cambridge. The quality of fibres will be assessed, after which the fibres will be assembled into strands and cables. In the final stage, different polymeric coatings will be investigated as insulation for the wires and diverse geometries explored. There will be several fundamental benefits from the outcome of this research proposal. Demonstration of the chirality control of nanotubes, which is the “holy grail” in the field, would be important in itself, while application of the material as useful wires and cables will make it much more immediately useful","1470114","2010-08-01","2015-07-31"
"HPFLUDY","The h-Principle for Fluid Dynamics","László Székelyhidi","UNIVERSITAET LEIPZIG","""A fundamental problem of the theory of turbulence is to find a satisfactory mathematical framework linking the Navier-Stokes equations to the statistical theory of Kolmogorov. A central difficulty in this task is the inherent non-uniqueness and pathological behaviour of weak solutions
of the Euler equations, the inviscid limit of the Navier-Stokes equations. This non-uniqueness, rather than an isolated phenomenon, turns out to be directly linked to the celebrated construction of Nash and Kuiper of rough isometric embeddings and, more generally, to Gromov's h-principle in geometry. The central aim of this project is deepen the understanding of this link, with the following goals:

I. Scaling Laws. Attack specific conjectures concerning weak solutions of the Euler equations that are motivated by the Kolmogorov theory of homogeneous
isotropic turbulence. Most prominently the conjecture of Onsager, which relates the critical regularity requiring energy conservation to the scaling of the energy spectrum in the inertial range.

II. Selection Criteria. Study the initial value problem for weak solutions, with the aim of characterizing the set of initial data for which an entropy condition implies uniqueness, and obtaining information on the maximal possible rate of energy decay and identifying selection criteria that single out a physically relevant solution when uniqueness fails.

III. General Theory. Identify universal features of the construction, in order to be applicable to a large class of problems. This involves an analysis of the geometry induced by the equations in an appropriate state space, a better understanding of how an iteration scheme using only a finite number of """"cell-problems"""" can be developed, and developing versions of convex integration that use higher-dimensional constructions.""","870000","2011-10-01","2016-09-30"
"HPSuper","High-Pressure High-Temperature Superconductivity","Sven FRIEDEMANN","UNIVERSITY OF BRISTOL","Superconductors promote electrical currents without loss and are exploited for applications like magnets in medical imaging. Further applications like large scale usage in electrical power generation and transmission, however, are limited by the need to cool materials below a critical temperature Tc. Thus, novel superconductors with higher Tc are highly desirable.

High Tc has been predicted almost 50 years ago for hydrogen and hydrogen compounds but was only confirmed in 2015 with the discovery of superconductivity at a record temperature of 203K in hydrogen sulphide H3S at high pressures. This long term effort highlights that finding new superconductors remains challenging as theory is very limited in predicting specific compounds for high-temperature superconductivity. The reason for this is that a favourable combination of materials and electronic properties is needed. This project will unravel the mechanism of high-temperature superconductivity in H3S, derive design principles, and find new high-temperature superconductors.

We will measure key parameters of the superconducting state in H3S including the London penetration depth, coherence length, superconducting gap, charge carrier concentration, electron-phonon coupling, and Fermi surface topology as well as the isotope effect on these. This will be achieved through measurements of the critical field, Hall effect, quantum oscillations, and tunnelling spectroscopy.

This insight will be used to derive design principles for new superconductors with increased Tc and at lower pressures. We will work together with theory and materials science to predict, synthesise and test novel superconductors working towards hydrogen based high-temperature superconductivity at ambient pressure. We will focus on two materials classes with high hydrogen content: i) phosphanes with excellent control of complementary elements and ii) hydrogen storage materials alanates and borohydrades with light complementary elements.","1809752","2017-02-01","2022-01-31"
"HQ-NOM","Hybrid Quantum Nano-Optomechanics","Olivier Arcizet","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""The chief endeavor of the project is to develop, investigate and exploit systems associating nanoscale mechanical resonators with single quantum objects. Such combinations belong in the category of so-called “hybrid nanomechanical systems” which constitutes a rapidly expanding field in modern quantum- and nanophysics.
The benefit of exploring hybrid systems is manifold. From a practical point of view, due to their size, nanoresonators are extremely sensitive to external forces.  If associated with a high resolution optical sensor through which the nanoresonator can be non-invasively probed and manipulated, the hybrid system holds promise to act as an ultrasensitive force probe. On a more fundamental level, unexplored quantum regimes become within reach, where the interface between quantum objects and mechanical systems can be thoroughly investigated.  From a conceptual point of view, such experiments are of paramount importance as they could reveal the quantum behavior of macroscopic objects.
To accommodate these ideas, I propose to develop and investigate two types of hybrid systems. The first one consists of a single nitrogen-vacancy (NV) defect hosted in a diamond nanocrystal, positioned at the extremity of a nanowire. My team and I recently demonstrated magnetic coupling of the NV spin to the resonator position and thereby evidenced the feasibility of realizing such a quantum to mechanical interface.  This novel system can readily be improved to meet the severe requirements of the quantum opto-mechanical experiments envisioned in this project. The second approach also exploits a NV centre, but this time as an integrated part of a diamond resonator. This monolithic system potentially offers an unprecedented coupling, a supreme overall stability, and NV centres with improved characteristics, together expanding the scope of conceivable experiments.""","1792140","2012-11-01","2017-10-31"
"HToMS","Homotopy Theory of Moduli Spaces","Oscar RANDAL-WILLIAMS","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Moduli spaces are spaces which describe all mathematical objects of some type. This proposal concerns the study of certain moduli spaces via techniques from homotopy theory, from several different points of view. The main moduli spaces in which we are interested are moduli spaces of manifolds, or equivalently classifying spaces of diffeomorphism groups of manifolds. We are also interested in spaces of positive scalar curvature metrics on smooth manifolds, which we study by relating them to moduli spaces of smooth manifolds.

The study of moduli spaces of manifolds via homotopy theory has seen a great deal of development in the last 20 years, the breakthrough result being Madsen and Weiss' calculation of the stable homology of moduli spaces of surfaces. More recently, Galatius and I have established analogous results for manifolds of higher dimension.

A main goal of this proposal is to study the homology of moduli spaces from a multiplicative point of view. This leads to higher-order forms of the phenomenon of homological stability in which the failure of ordinary homological stability is itself stable. Remarkably, our methods developed to handle moduli spaces of manifolds are sufficiently general to yield deep new results when applied to other moduli spaces in algebra and topology, such as moduli spaces of modules (equivalently, classifying spaces of general linear groups) or moduli spaces of graphs (equivalently, classifying spaces of automorphism groups of free groups). In each case our methods give new information about their homology outside of the traditional stable range.

Other goals of this proposal are to form new connections between spaces of Riemannian metrics of positive scalar curvature and infinite loop spaces, and to investigate the structure of tautological subrings of the cohomology of moduli spaces of manifolds, especially in relation to the tautological rings of moduli spaces of Riemann surfaces studied in algebraic geometry.","974526","2018-10-01","2023-09-30"
"HUMANIS","Human Motion Analysis from Image Sequences","Lourdes De Agapito","UNIVERSITY COLLEGE LONDON","Recent research has uncovered real potential for humans to interact with computers in natural ways by using their body motion, gestures and facial expressions. This has resulted in a huge surge of research within the Computer Vision community to develop algorithms able to understand, model and interpret human motion using visual information. Commercial motion capture solutions exist that can reconstruct the full motion of a human body or the deformations of a face. However these systems are severely restricted by the need to use markers on the subject and multiple calibrated cameras besides being costly and technically complex. Imagine instead the possibility of pointing a camera at a person for a few seconds and obtaining a fully parameterised detailed 3D model in a completely automated way. This 3D model could subsequently be used for animation tasks, to assist physiotherapists in the rehabilitation of patients with injuries or ultimately to guide a robot in a surgical operation. The aim of this project is to bring this scenario closer to reality by conducting the ground-breaking research needed to crack some of the challenging open problems in visual human motion analysis. So far visual human motion tracking systems have typically modelled the human body as a 3D skeleton ignoring the fact that each of its articulated parts is not strictly rigid but can also deform, since they are surrounded by soft tissue, muscles and clothes. Think of a torso performing small twists, a bicep flexing or a face performing different facial expressions. In this grant I are interested in recovering the full detailed 3D shape of the human body, including a model for the supporting 3D skeleton that captures its underlying articulated structure and a collection of deformable models to describe the non-rigid nature of each of its parts. Crucially, I plan to obtain these models without the use of markers, prior models or exemplars --- purely from image measurements.","1478208","2008-11-01","2014-10-31"
"HurdlingOxoWall","Late First-Row Transition Metal-Oxo Complexes for C–H Bond Activation","Aidan McDonald","THE PROVOST, FELLOWS, FOUNDATION SCHOLARS & THE OTHER MEMBERS OF BOARD OF THE COLLEGE OF THE HOLY & UNDIVIDED TRINITY OF QUEEN ELIZABETH NEAR DUBLIN","The chemical, pharmaceutical, and materials industries rely heavily upon chemicals from oil and natural gas feed-stocks (saturated hydrocarbons) that require considerable functionalisation prior to use. Catalytic oxidative functionalisation (e.g. CH4 + [O] + cat. → CH3OH), using first row transition metal catalysts, is potentially a sustainable, cheap, and green route to these high-commodity chemicals. However, catalytic oxidation remains a great modern challenge because such hydrocarbons contain remarkably strong inert C–H bonds that can only be activated with potent catalysts. We will take a Nature-inspired approach to designing and preparing powerful oxidation catalysts: we will interrogate the active oxidant, a metal-oxo (M=O) species, to guide our catalyst design. Specifically, we will prepare unprecedented Late first-row transition Metal-Oxo complexes (LM=O’s, LM = Co, Ni, Cu) that will activate the strongest of C–H bonds (e.g. CH4). 

This will be accomplished using a family of novel low coordinate ligands that will support LM=O’s. Due to their expected potent reactivity we will prepare LM=O’s under unique oxidatively robust, low-temperature conditions to ensure their stabilisation. The poorly understood factors (thermodynamics, metal, d-electron count) that control the reactivity of M=O’s will be thoroughly investigated. Based on these investigations LM=O reactivity will be manipulated and optimised. We expect LM=O’s will be significantly more reactive than any early transition metal-oxo’s (EM=O’s), because they will display a greater thermodynamic driving force for C–H activation. It is thus expected that LM=O’s will be capable of the activation of the strongest of C–H bonds (i.e. CH4). Driven by the knowledge acquired from these investigations, we will design and prepare the next generation of molecular oxidation catalysts - a family of late first-row transition metal compounds capable of catalysing hydrocarbon functionalisation under ambient conditions.","1499865","2016-03-01","2021-02-28"
"HURRICANE","Past hurricane activity reconstructed using cave deposits: Have humans increased storm risk?","James Baldini","UNIVERSITY OF DURHAM","The proposed research would utilise various geochemical proxies (oxygen, carbon, and trace elements) in cave calcite deposits (stalagmites) to develop extraordinarily high-resolution North Atlantic hurricane activity records for the past five hundred years, extending existing historical datasets by hundreds of years. This new stalagmite record would be the first high resolution record to extend beyond 1850, thus permit more statistically robust comparisons of hurricane activity between pre- and post-anthropogenic greenhouse gas climatic states, and help to constrain any natural cyclicities inherent in North Atlantic hurricane activity. Additionally, the three study sites were chosen to test the hypothesis that variations in the North Atlantic Oscillation (NAO) index may influence hurricane track direction. The records will also be used to reconstruct El Nino-Southern Oscillation variability back through time, something that on its own would be an important result. The research would help evaluate the risk of stronger/more frequent future hurricanes associated with global climate change by allowing more rigorous testing of currently conflicting climate models.","1387814","2010-01-01","2015-12-31"
"HY-CAT","Multifunctional Hybrid Platforms based on Colloidal Nanocrystals to Advance CO2 Conversion Studies","Raffaella BUONSANTI","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","In reimagining the world’s energy future, while researchers are seeking alternative ways to produce energy, our current dependence on fossil fuels requires us to capture and store the CO2 to prevent reaching unacceptable CO2 levels in the atmosphere. In this scenario, recycling CO2 by converting it into useful chemicals, such as fuels for transportation, represents an important research area as it will eventually lead to independence from fossil fuels and petroleum. While much progress has been made, this emerging field is challenged by huge technical and scientific questions. The intrinsic thermodynamic stability of the CO2 molecule, combined with slow multi-electron transfer kinetics, makes its reduction exceedingly energetically demanding. Hy-Cat aims to develop novel material platforms to investigate different chemical paths that promote electrochemical CO2 reduction and direct product selectivity. We will synthesize hybrid materials comprising atomically defined CO2 sorbents and nanocrystalline CO2 catalysts intimately bound in a single integrated system. Three different classes of hybrids, each characterized by one specific absorption/pre-activation mechanism, will allow to investigate the effect of each mechanism on the catalyst activity. A key component of the research will be to develop synthetic schemes to access these multifunctional systems with an unprecedented level of control across multiple lengthscales. This control and the intrinsic tunability of the chosen building blocks will allow us to methodically compare structure and activity, so to determine the design principles upon which better catalysts can be made. We will argue that this understanding is required to remove the main bottlenecks towards efficient and selective catalysts to convert CO2 into useful products, such hydrocarbons. Hy-Cat is highly multidisciplinary and its scientific outcome will positively impact several other research fields in chemistry, materials science and engineering.","1420648","2017-01-01","2021-12-31"
"HY-NANO","HYbrid NANOstructured multi-functional interfaces for stable, efficient and eco-friendly photovoltaic devices","Giulia GRANCINI","UNIVERSITA DEGLI STUDI DI PAVIA","HY-NANO focuses on one of the current major challenges in Europe: a global transition to a low-carbon society and green economy by 2050. Solar energy can lead a “paradigm shift” in the energy sector with a new low-cost, efficient, and stable technology (3-pillars strategy). Nowadays, low-cost three dimensional (3D) Hybrid Perovskites (HP) solar cells are revolutionizing the photovoltaic scene, with stunning power conversion efficiency beyond 22%. However, poor device stability (due to degradation in contact with water) and dependence on toxic components (lead) substantially hamper their commercialization. 
HY-NANO aims to realize a new low-cost and efficient hybrid solar technology combining long-term stability with a reduced environmental impact. Design and engineering innovative multi-dimensional hybrid interfaces is the core idea. This will be achieved by: 1. design and characterization of new stable and eco-friendly perovskites structures, with tunable composition and dimensionality ranging from 3D to 2D; 2. exploiting new synergistic functions by combining 3D and 2D perovskites together into novel stable and efficient multi-dimensional interfaces while addressing the interface physics therein; 3. integrating the hybrid interfaces into high efficient and stable device architectures engineered “ad hoc”. In addition, I propose the development of new solar cell encapsulant using metal-organic frameworks (MOFs) functionalized as selective lead receptors to minimize the environmental risks associated with the potential release of lead. 
My multidisciplinary expertise in advanced material design, cutting-edge photophysical experimental investigations, and solar cell engineering will enable me to successfully target the ambitious goals. HY-NANO is timely and it will generate the new fundamental knowledge that is urgently needed for a scientific and technological breakthrough in materials and devices for near future photovoltaics.","1499084","2019-07-01","2024-06-30"
"HYBRIDNANO","Engineering electronic quantum coherence 
and correlations in hybrid nanostructures","Silvano De Franceschi","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","Nanoelectronic devices can provide versatile and relatively simple systems to study complex quantum phenomena under well-controlled, adjustable conditions. Existing technologies enable the fabrication of low-dimensional nanostructures, such as quantum dots (QDs), in which it is possible to add or remove individual electrons, turn on and off interactions, and tune the properties of the confined electronic states, simply by acting on a gate voltage or by applying a magnetic field. The hybrid combination of such nanostructures, having microscopic (atomic-like) quantum properties, with metallic elements, embedding different types of macroscopic electronic properties (due, e.g., to ferromagnetism or superconductivity), can open the door to unprecedented research opportunities. Hybrid nanostructures can serve to explore new device concepts with so far unexploited functionalities and, simultaneously, provide powerful tools to study fundamental aspects of general relevance to condensed-matter physics. Only recently, following progress in nanotechnology, have hybrid nanostructures become accessible to experiments.
Here we propose an original approach that takes advantage of recently developed self-assembled QDs grown on Si-based substrates. These QDs have many attractive properties (well-established growth, ease of contacting, etc.). We will integrate single and multiple QDs with normal-metal, superconducting, and ferromagnetic electrodes and explore device concepts such as spin valves, spin pumps, and spin transistors (a long standing challenge). Using these hybrid devices we will study spin-related phenomena such as the dynamics of confined and propagating spin states in different solid-state environments (including superconducting boxes), long-distance spin correlations and entanglement. The new knowledge expected from these experiments is likely to have a broad impact extending from quantum spintronics to other areas of nanoelectronics (e.g. superconducting electronics).","1780442","2012-01-01","2016-12-31"
"HybridNet","Hybrid Quantum Networks","Julien Laurat","UNIVERSITE PIERRE ET MARIE CURIE - PARIS 6","The development of correlated quantum networks based on interconnected material nodes and quantum channels is a major challenge for the field of quantum information science, including quantum communication, computing, and metrology. Two main encodings of quantum information are generally used: a ‘discrete-variable’ encoding based for instance on single-photons and a ‘continuous-variable’ approach which relies on continuous degrees of freedom, such as the quadrature components of light modes. A mostly unexplored area is the mixing of these two approaches leading to ‘hybrid schemes’ where the advantages of both paradigms can be merged. This is the subject of the present proposal.

Stated succinctly, we aim at developing the scientific and technical foundations for the realization of hybrid quantum networks with applications to the distribution and processing of quantum information. The new research activities that we propose to undertake are as follows:

•	The implementation of storage and subsequent rotation of a hybrid qubit
•	The laboratory demonstration of storage, readout and subsequent purification of continuous-variable entanglement
•	The experimental realization of a segment of a hybrid quantum repeater

We will reach these objectives by developing compatible quantum light source (pulsed optical parametric oscillator) and light-matter interface (cold atoms trapped in the vicinity of elongated nanofibers) and by demonstrating novel capabilities for hybrid protocols, such as non-Gaussian state storage and quantum gates. These activities will be accompanied by a strong theoretical effort focused on the development of resource-efficient hybrid protocols for improved scaling.","1500000","2012-10-01","2017-09-30"
"HYBRIDQED","Hybrid Cavity Quantum Electrodynamics with Atoms and Circuits","Andreas Joachim Wallraff","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","We plan to investigate the strong coherent interaction of light and matter on the level of individual photons and atoms or atom-like systems. In particular, we will explore large dipole moment superconducting artificial atoms and natural Rydberg atoms interacting with radiation fields contained in quasi-one-dimensional on-chip microwave frequency resonators. In these resonators photons generate field strengths that exceed those in conventional mirror based resonators by orders of magnitude and they can also be stored for long times. This allows us to reach the strong coupling limit of cavity quantum electrodynamics (QED) using superconducting circuits, an approach known as circuit QED. In this project we will explore novel approaches to perform quantum optics experiments in circuits. We will develop techniques to generate and detect non-classical radiation fields using nonlinear resonators and chip-based interferometers. We will also further advance the circuit QED approach to quantum information processing. Our main goal is to develop an interface between circuit and atom based realizations of cavity QED. In particular, we will couple Rydberg atoms to on-chip resonators. To achieve this goal we will first investigate the interaction of ensembles of atoms in a beam with the coherent fields in a transmission line or a resonator. We will perform spectroscopy and we will investigate on-chip dispersive detection schemes for Rydberg atoms. We will also explore the interaction of Rydberg atoms with chip surfaces in dependence on materials, temperature and geometry. Experiments will be performed from 300 K down to millikelvin temperatures. We will realize and characterize on-chip traps for Rydberg atoms. Using trapped atoms we will explore their coherent dynamics. Finally, we aim at investigating the single atom and single photon limit. When realized, this system will be used to explore the first quantum coherent interface between atomic and solid state qubits.","1954464","2009-09-01","2014-08-31"
"Hybrids","Hybrid Semiconductors: Design Principles and Material Applications","Aron Walsh","UNIVERSITY OF BATH","Materials chemistry is generally focused on inorganic or organic systems, and their combination is an emerging area with many exploratory experimental studies. Self-assembling hybrid organic-inorganic networks offer immense potential for functionalising material properties for a wide scope of applications including solar cells, solid-state lighting, gas sensors and transparent conductors. The flexibility of combining two distinct material classes into a single system provides an almost infinite number of chemical and structural possibilities, but there is currently no systematic approach established for designing new compositions and configurations that match the criteria required for technological applications, e.g. high chemical stability and low electrical resistivity.

Modern computational chemistry approaches enable the accurate prediction of the structural and electronic properties of materials at an atomistic scale. This project will apply state-of-the-art simulation techniques to: (i) Develop design principles for forming hybrid solids and tuning their physicochemical properties; (ii) Construct and characterise prototypal material systems tailored for technological applications.

The project will develop fundamental design rules for hybrid systems: the effects of functional groups and network dimensionality will be assessed in relation to the pertinent material properties. The rules can then be applied to construct prototypes for optoelectronic applications, with the candidates being tested through an established experimental collaboration. These challenging goals will require a combination of bulk, surface and excited-state calculations, using both classical and electronic structure simulation techniques, which draw directly from my previous experiences, and will utilise the existing high-performance computing infrastructure in the UK. The principal outcome of the project will be to enhance our understanding of this new field of materials science.","996374","2012-01-01","2017-09-30"
"HybridSolarFuels","Efficient Photoelectrochemical Transformation of CO2 to Useful Fuels on Nanostructured Hybrid Electrodes","Csaba JANAKY","SZEGEDI TUDOMANYEGYETEM","Given that CO2 is a greenhouse gas, using the energy of sunlight to convert CO2 to transportation fuels (such as methanol) represents a value-added approach to the simultaneous generation of alternative fuels and environmental remediation of carbon emissions. Photoelectrochemistry has been proven to be a useful avenue for solar water splitting. CO2 reduction, however, is multi-electron in nature (e.g., 6 e- to methanol) with considerable kinetic barriers to electron transfer. It therefore requires the use of carefully designed electrode surfaces to accelerate e- transfer rates to levels that make practical sense. In addition, novel flow-cell configurations have to be designed to overcome mass transport limitations of this reaction.
We are going to design and assemble nanostructured hybrid materials to be simultaneously applied as both adsorber and cathode-material to photoelectrochemically convert CO2 to valuable liquid fuels. The three main goals of this project are to (i) gain fundamental understanding of morphological-, size-, and surface functional group effects on the photoelectrochemical (PEC) behavior at the nanoscale (ii) design and synthesize new functional hybrid materials for PEC CO2 reduction, (iii) develop flow-reactors for PEC CO2 reduction. Rationally designed hybrid nanostructures of large surface area p-type semiconductors (e.g., SiC, CuMO2, or CuPbI3) and N-containing conducting polymers (e.g., polyaniline-based custom designed polymers) will be responsible for: (i) higher photocurrents due to facile charge transfer and better light absorption (ii) higher selectivity towards the formation of liquid fuels due to the adsorption of CO2 on the photocathode (iii) better stability of the photocathode. The challenges are great, but the possible rewards are enormous: performing CO2 adsorption and reduction on the same system may lead to PEC cells which can be deployed directly at the source point of CO2, which would go well beyond the state-of-the-art.","1498750","2017-01-01","2021-12-31"
"HydMet","Fundamentals of Hydrogen in Structural Metals at the Atomic Scale","Peter FELFER","FRIEDRICH-ALEXANDER-UNIVERSITAET ERLANGEN NUERNBERG","H is an element that plays an important role in the production and efficient usage of energy as it significantly influences the way we produce and consume energy: In high-strength materials, the usability and service life is limited by H induced failure. These materials are key in transport systems, wind power and H storage. Despite the enormous economic significance, little is known fundamentally about the underlying damage mechanisms, which are inherently playing out on the atomic scale.

The PI’s team will use atom probe tomography, an atomic scale 3D microscopy method to systematically analyse the location and pathways of H in the microstructure and shed light on damage mechanisms in Fe and Ni based materials. This will include vacancies/clusters (0D), dislocations (1D), interfaces (2D) and second phased (3D). The approach will be combined with micro-mechanics to investigate the involvement of H in fracture behaviour. We will measure the amount of H at dislocations required for enhanced plasticity, in the plastic wake of a crack and at the crack tip. In production materials, we will determine the amount of H at identified traps after processing as well as penetration pathways into the material. Finally, we will clarify the contribution of H to a important problem for wind power generation: white-etching cracks.

These experiments are now made possible in a commercial atom probe by using 2H (D) charging combined with cryo specimen transfers to avoid H loss. In the project, the team will go a step further and build an atom probe with ultra-low H background to enable the direct detection of 1H, enabling analysis without tracers.

The resulting knowledge will greatly enhance our knowledge on the fundamentals of H in metals at the atomic scale. This will lead to increased predictability of failures, the rational design of H resistant high strength materials and protection measures and with it great cost savings especially in renewable energy generation and electromobility.","1497959","2018-12-01","2023-11-30"
"HYDROCARB","Towards a new understanding of carbon processing in freshwaters: methane emission hot spots and carbon burial","Sebastian Sobek","UPPSALA UNIVERSITET","In spite of their small areal extent, inland waters play a vital role in the carbon cycle of the continents, as they emit significant amounts of the greenhouse gases (GHG) carbon dioxide (CO2) and methane (CH4) to the atmosphere, and simultaneously bury more organic carbon (OC) in their sediments than the entire ocean. Particularly in tropical hydropower reservoirs, GHG emissions can be large, mainly owing to high CH4 emission. Moreover, the number of tropical hydropower reservoirs will continue to increase dramatically, due to an urgent need for economic growth and a vast unused hydropower potential in many tropical countries. However, the current understanding of the magnitude of GHG emission, and of the processes regulating it, is insufficient. Here I propose a research program on tropical reservoirs in Brazil that takes advantage of recent developments in both concepts and methodologies to provide unique evaluations of GHG emission and OC burial in tropical reservoirs. In particular, I will test the following hypotheses: 1) Current estimates of reservoir CH4 emission are at least one order of magnitude too low, since they have completely missed the recently discovered existence of gas bubble emission hot spots; 2) The burial of land-derived OC in reservoir sediments offsets a significant share of the GHG emissions; and 3) The sustained, long-term CH4 emission from reservoirs is to a large degree fuelled by primary production of new OC within the reservoir, and may therefore be reduced by management of nutrient supply. The new understanding and the cross-disciplinary methodological approach will constitute a major advance to aquatic science in general, and have strong impacts on the understanding of other aquatic systems at other latitudes as well. In addition, the results will be merged into an existing reservoir GHG risk assessment tool to improve planning, design, management and judgment of hydropower reservoirs.","1798227","2013-09-01","2019-08-31"
"HYDROFAKIR","Roughness design towards reversible non- / full-wetting surfaces: From Fakir Droplets to Liquid Films","Athanasios Papathanasiou","NATIONAL TECHNICAL UNIVERSITY OF ATHENS - NTUA","Creating tunable surfaces that are able to undergo reversible transitions between superhydrophobic and superhydrophilic behaviour is a challenging and vital issue due to their potential use in applications involving self cleaning, very low flow resistance and liquid handling without moving mechanical parts. Superhydrophobic surfaces arising from micro-scale roughened hydrophobic materials spontaneously exhibit transitions to become superhydrophilic when their material wetting properties are suitably modified by external stimuli. The reverse transition, however, requires external actuation/ perturbation which can be strong as to deteriorate the liquids handled and therefore limit the use such techniques in applications. Here we plan to combine continuum and mesoscale computational analysis of wetting phenomena in solid surfaces to create designer roughness that will minimize, or even eliminate, the strength of the actuation required to achieve full- to non-wetting reversibility. The modelling will be done in a continuous dialogue with surface fabrication and wetting tests. Wetting experiments will be performed along with novel microactuation techniques for liquid interfaces.","1131840","2010-02-01","2015-09-30"
"HyGate","Hydrophobic Gating in nanochannels: understanding single channel mechanisms for designing better nanoscale sensors","Alberto GIACOMELLO","UNIVERSITA DEGLI STUDI DI ROMA LA SAPIENZA","Hydrophobic gating is the phenomenon by which the flux of ions or other molecules through biological ion channels or synthetic nanopores is hindered by the formation of nanoscale bubbles. Recent studies suggest that this is a generic mechanism for the inactivation of a plethora of ion channels, which are all characterized by a strongly hydrophobic interior. The conformation, compliance, and hydrophobicity of the nanochannels – in addition to external parameters such as electric potential, pressure, presence of gases – have a dramatic influence on the probability of opening and closing of the gate. This largely unexplored confined phase transition is known to cause low frequency noise in solid-state nanopores used for DNA sequencing and sensing, limiting their applicability. In biological channels, hydrophobic gating might conspire in determining the high selectivity towards a specific ions or molecules, a characteristic which is sought for in biosensors. 
The objective of HyGate is to unravel the fundamental mechanisms of hydrophobic gating in model nanopores and biological ion channels and exploit their understanding in order to design biosensors with lower noise and higher selectivity. In order to achieve this ambitious goal, I will deploy the one-of-a-kind simulation and theoretical tools I developed to study vapor nucleation in extreme confinement, which comprises rare-event molecular dynamics and confined nucleation theory. These quantitative tools will be instrumental in designing better biosensors and nanodevices which avoid the formation of nanobubbles or exploit them to achieve exquisite species selectivity. The novel physical insights into the behavior of water in complex nanoconfined environments are expected to inspire radically innovative strategies for nanopore sensing and nanofluidic circuits and to promote a stepwise advancement in the fundamental understanding of hydrophobic gating mechanisms and their influence on bio-electrical cell response.","1496250","2019-02-01","2024-01-31"
"HyLEF","Hydrodynamic Limits and Equilibrium Fluctuations: universality from stochastic systems","ANA PATRICIA CARVALHO GONÇALVES","INSTITUTO SUPERIOR TECNICO","A classical problem in the field of interacting particle systems (IPS) is to derive the macroscopic laws of the thermodynamical quantities of a physical system by considering an underlying microscopic dynamics which is composed of particles that move according to some prescribed stochastic, or deterministic, law. The macroscopic laws can be partial differential equations (PDE) or stochastic PDE (SPDE) depending on whether one is looking at the convergence to the mean or to the fluctuations around that mean. One of the purposes of this research project is to give a mathematically rigorous description of the derivation of SPDE from different IPS. We will focus on the derivation of the stochastic Burgers equation (SBE) and its integrated counterpart, namely, the KPZ equation, as well as their fractional versions. The KPZ equation is conjectured to be a universal SPDE describing the fluctuations of randomly growing interfaces of 1d stochastic dynamics close to a stationary state. With this study we want to characterize what is known as the KPZ universality class: the weak and strong conjectures. The latter states that there exists a universal process, namely the KPZ fixed point, which is a fixed point of the renormalization group operator of space-time scaling 1:2:3, for which the KPZ is also invariant. The former states that the fluctuations of a large class of 1d conservative microscopic dynamics are ruled by stationary solutions of the KPZ. Our goal is threefold: first, to derive the KPZ equation from general weakly asymmetric systems, showing its universality; second, to derive new SPDE, which are less studied in the literature, as the fractional KPZ from IPS which allow long jumps, the KPZ with boundary conditions from IPS in contact with reservoirs or with defects, and coupled KPZ from IPS with more than one conserved quantity. Finally, we will analyze the fluctuations of  purely strong asymmetric systems, which are conjectured to be given by the KPZ fixed point.","1179496","2016-12-01","2021-11-30"
"HyMoCo","Hybrid Node Modes for Highly Efficient Light Concentrators","Patrick Görrn","BERGISCHE UNIVERSITAET  WUPPERTAL","The meaning of solar energy for future decentralized power supply will largely depend on both efficiency and cost of solar to electrical power conversion. All kinds of conversion strategies including photovoltaics, concentrated solar power, solar to fuel and others would benefit from efficiently collecting solar power on large areas. For this reason luminescent solar concentrators have been developed for over thirty years, but due to waveguide losses their maximum size is still limited to a few centimeters.

The proposed project suggests the exploitation of a new type of electromagnetic waveguide in order to realize passive planar concentrators of unsurpassed collection efficiency, size, concentration, lifetime and costs.
 
A dielectric TE1-mode shows a node, a position in the waveguide where no intensity is found. A thin film placed in this node remains largely “invisible” for the propagating mode. Such dielectric node modes (DNMs) have been investigated by the applicant in previous work, but only recently a silver island film (SIF) was for the first time placed in such a node. The resulting extremely low waveguide losses cannot be explained by our current understanding of waveguide modes and hint to a hybridization between the SIF-bound long-range surface plasmon polaritons (LRSPPs) and the DNMs into what we call hybrid node modes (HNMs).
 
The SIFs strongly interact with incident light. An appropriate nanopatterning of SIFs enables efficient excitation of low-loss HNMs modes collecting solar power over square meters and concentrating it. To achieve this goal new technological methods are used that enable patterning on the nanometer scale and low cost roll-to-roll processing at the same time. New measurement techniques and numerical simulation tools will be developed to investigate the HNMs – a novel kind of electromagnetic modes – and their exploitation in the passive solar concentrators.","1485000","2015-03-01","2020-02-29"
"HYPER","Hybrid Photovoltaic Energy Relays","Henry James Snaith","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Photovoltaic (PV) solar cells promise to be a major contributor to our future energy supply, and the current silicon and thin film photovoltaic industry is growing at a fast rate (25 to 80% pa). Despite this however, only 10 to 20 GW of the total 15TW global energy demand is met by PV generated power. The ramping up in production and affordable global uptake of solar energy requires a significant reduction in materials and manufacture costs and furthermore, a solar industry on the TW scale must be based on abundant and preferably non-toxic materials. The challenge facing the photovoltaic industry is cost effectiveness through much lower embodied energy. Plastic electronics and solution-processable inorganic semiconductors can revolutionise this industry due to their ease of processing (low embodied energy), but a significant increase in performance is required. To enable this jump in performance in a timely manner, incremental improvements and optimisations (evolutionary approaches) are unlikely to provide sufficiently rapid advances and a paradigm shift, such as that described in this project, is thus required. HYPER is lead by Henry Snaith, a prominent young scientist developing hybrid and organic based solar cells. The project will create a new series of hybrid solar cells, based on photoactive semiconductor nanocrystals and light absorbing polymer semiconductors. At the core of the research is the synthesis of new semiconductor and metallic nanostructures, combined with device development and advanced spectroscopic characterisation. The central operational principle to be developed is long range energy transfer of photoexcitons from the bulk of the semiconductors to the charge generating material interfaces, maximising charge generation in these thin film composites Combined with this, advanced photonic structuring of the photoactive layers, and the introduction of nano-plasmonic light harvesting components will represent a new paradigm for hybrid solar cells.","1870337","2011-11-01","2016-10-31"
"HYPERION","HYbrid PERovskites for Next GeneratION Solar Cells and Lighting","Samuel David STRANKS","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","An emerging class of materials called hybrid perovskites is poised to revolutionise how power is both produced and consumed by enabling the production of highly-efficient, tunable solar photovoltaics (PV) and light-emitting diodes (LEDs) at exceptionally low cost. Although the efficiencies of perovskite devices are rising fast, both PV and LEDs fall short of out-performing current technology and reaching their theoretical performance limits. To achieve their full potential, parasitic non-radiative losses and bandgap instabilities from ionic segregation must be fundamentally understood and eliminated. HYPERION will address these issues by i) elucidating the origins of non-radiative decay and ion segregation in films and devices, ii) devising means to eliminate these processes, and iii) implementing optimised materials into boundary-pushing PV and LED devices. This will be achieved through a groundbreaking hierarchical analysis of the perovskite structures that not only characterises thin films and interfaces, but also the sub-units that comprise them, including grain-to-grain and sub-granular properties. The optoelectronic behaviour on these scales will be simultaneously correlated with local structural and chemical properties. HYPERION will use this fundamental understanding to eliminate non-radiative losses and ionic segregation on all scales through passivation treatments and compositional control. Addressing these knowledge gaps in the operation of perovskites will produce fundamental semiconductor science discoveries as well as illuminate routes to yield optimised and functional perovskites across the broad bandgap range 1.2–3.0 eV. These will be used to demonstrate all-perovskite tandem PV devices with efficiency exceeding crystalline silicon (26%), and white light LEDs with efficacies surpassing fluorescent light (50 lm/W). The work will realise the promise of perovskite technology as a versatile and scalable energy solution to secure a sustainable future.","1759733","2017-11-01","2022-10-31"
"HYPROTIN","Hyperpolarized Nuclear Magnetic Resonance Spectroscopy for Time-Resolved Monitoring of Interactions of Intrinsically Disordered Breast-Cancer Proteins","Dennis KURZBACH","UNIVERSITAT WIEN","HYPROTIN proposes a pioneering research platform for hyperpolarized magnetic resonance of breast-cancer related proteins that will revolutionize our view on tumorigenesis at the atomic level, through bottom-up reconstitution of medicinal relevant interaction pathways involving the breast cancer susceptibility protein 1 (BRCA1). 
The risk to develop a hereditary breast or ovarian cancer (HBOC) increases to 55-65 % upon mutation of the BRCA1 gene. Yet, little is known about the biochemistry of tumorigenesis, so that drugs directed towards molecular targets are not satisfactory. To date, mastectomy remains the only preventive treatment. This dramatic lack of knowledge is a consequence of BRCA1 being an intrinsically disordered protein (IDP). Recognizing the importance of IDPs has revolutionized structural biology in the last decade, but this also represents a huge experimental challenge. To date, nuclear magnetic resonance (NMR) is the only technique available to study IDPs at high resolution. However, several limits of the technique must be overcome. Its low sensitivity impedes investigations under biologically meaningful conditions, so that new approaches are required.
The HYPROTIN project aims to achieve two methodological goals: 1) Residue-resolved studies of the BRCA1 IDP under physiological conditions; and 2) real-time monitoring of BRCA1-ligand binding, thereby adding a time-resolved dimension to the NMR characterization of IDPs. This systematic approach will provide unprecedented insight into the BRCA1 interactome, provide medically relevant data and residue-resolved protein interaction kinetics. This will open a new knowledge base for rational drug design.
The project will employ cutting-edge equipment that is unique worldwide, and will represent the first facility in Europe suited for these ground-breaking experiments. The PI has unique interdisciplinary experience enabling the demanding hyperpolarization approach to IDPs.","1990728","2019-03-01","2024-02-29"
"HYQS","Hybrid atom-ion Quantum Systems","René Gerritsma","UNIVERSITEIT VAN AMSTERDAM","This project focusses on realizing and studying a new hybrid ultra-cold atom-ion system for studying quantum many-body physics. It combines state-of-the-art technologies in quantum optics and quantum gases. The proposed system of cold (fermionic) atoms interacting with ion crystals has surprising  analogies  with  natural  solid  state  systems  and  molecules,  with  now  by fermionic 6Li atoms in place of electrons and heavy  174Yb+  ions in place of ionic cores. In particular, an atomic band structure may arise with tunable atom-phonon interactions. The proposed experimental approach is inspired by advances in pioneering experiments with hybrid atom-ion systems. By using a new atom-ion combination that has the highest experimentally feasible mass ratio of 29 (Li and Yb+), heating due to the dynamical trapping potential of the ions is suppressed. This eliminates an important road block in existing hybrid atom-ion experiments towards reaching deep into the quantum regime. I will use optical micro-traps in conjunction with segmented ion traps to study the system in a regime with a small number of atoms (1-100) and ions. This offers unprecedented control over the quantum states of atoms and ions. Engineering non-classical states in the ions will allow for quantum enhanced measurements of the combined atom-ion system, with single atom and single collision resolution. State-dependence in the atom-ion interactions can be employed to engineer quantum potentials for the atoms, leading to large scale ion-atomic Schrödinger cat-type entanglement.","1490152","2013-12-01","2018-11-30"
"HYRAX","Rock Hyrax Middens and Climate Change in Southern Africa during the last 50,000 years","Brian Mc Kee Chase","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","In stark contrast to the abundance of high quality palaeoenvironmental records obtained from the temperate regions of the northern hemisphere, terrestrial palaeoenvironmental information from southern Africa's drylands comes from discontinuous deposits with poor absolute age control and ambiguous palaeoclimatic significance. Confronted with the possibility of future environmental and social disruption as a result of climate change, the need for reliable records from southern Africa has never been so acute. This project seeks to develop rock hyrax middens as novel palaeoenvironmental archives to investigate long-term climate change. Hyrax middens (fossilised accumulations of urine and faecal pellets) contain a range of palaeoenvironmental proxies, including fossil pollen and stable isotopes. As part of a pilot study, I have created new collection and sampling methodologies, establishing the proof of principle and showing that middens provide continuous sub-annual to multi-decadal multi-proxy records of environmental change spanning the last 50,000 years. This work has been exceptional in terms of its ability to elucidate long-term climate dynamics at the local scale, and I now intend to apply my techniques to studying environmental change across the whole of southern Africa, a climatically sensitive, but poorly understood region of the globe. Developing new sites, proxies and analytical techniques, HYRAX will provide the first opportunity to study rapid climate change events, the extent and phasing of major climatic phenomena, and the direction and potential impacts of future climate change.","1484046","2010-11-01","2016-10-31"
"HYSCORE","Hybrid quantum networks for spin coherent technologies","Ronald Hanson","TECHNISCHE UNIVERSITEIT DELFT","""Spins in solids are at the heart of fundamental physical phenomena such as magnetism. Today, they are harnessed in a range of technologies such as magnetic resonance imaging and spintronics for recording media. The revolutionary potential of future quantum technologies has fuelled research efforts to gain control of the quantum nature of spins. Thanks to a string of recent breakthroughs, it is now possible to initialize and read out individual spins in a solid, and to manipulate their dynamics using carefully defined control fields. However, all experiments to date have been limited to “open-loop” control on locally interacting few-spin systems, precluding efficient correction of noise and errors and preventing long-distance applications.

My HYSCORE project will realize two critical breakthroughs: “closing the loop” by performing quantum feedback and generating quantum entanglement between remote spins. These goals will allow for entering a new era, in which robustly controlled spin registers are connected to form true quantum networks.

To achieve these ambitious goals, I will exploit and combine the key strengths of different types of quantum information carriers: the robustness of nuclear spins for storage and core computational tasks, the optical interface of electron spins for initializing and reading the nuclear spin registers, and the mobility and coherence of photons for establishing truly long-distance links. Two promising solid-state platforms will be studied: nitrogen-vacancy defects in diamond and fluorine donors in zinc selenide. If successful, HYSCORE will yield novel methods for closed-loop quantum control, fundamental insights into quantum measurement, robust multi-qubit registers in a solid, and the establishment of elementary long-distance quantum networks.""","1500000","2012-12-01","2017-11-30"
"HySPOD","Hybrid Solution Processable Materials for Opto-Electronic Devices","Maria Antonietta Loi","RIJKSUNIVERSITEIT GRONINGEN","This proposal aims at developing, studying the physical properties, and fabricating solar cell devices based on novel hybrid semiconductors. These new hybrids will combine the electronic properties of colloidal semiconducting nanocrystrals (nanorods) with those of semiconducting organic molecules. Semiconducting nanocrystals are confined systems and therefore are usually not good building blocks for electrical devices. The solution proposed by this ERC project to turn them into efficient components for optoelectronics devices is to build a functional interface between nanocrystals using organic molecules. This will allow extracting charge carriers from these confined systems by means of different physical phenomena including multiple exciton generation. The proposal thus aims to carry out fundamental research as an important step towards making solar cells an economically viable alternative source of energy.
The execution of this highly challenging investigation will be based on multidisciplinary expertise in physics, device physics, and physical chemistry and delivered through three well defined, interconnected and targeted key objectives. i) The creation of a fully functional interface for semiconducting nanorods for the extraction of charge carriers; ii) the first fundamental investigation of multiple exciton generation with direct electrical measurement of the photo-excited carriers in these new hybrids based on nanorods; iii) the use of the new hybrid materials for the fabrication of highly efficient low cost solar cells.
The applicant is well-established in this field, and has already achieved major breakthroughs in the design, study and fabrication of organic-inorganic hybrids. With the strong support of the Host Institution, she is in a perfect position to deliver on the ambitious goals of this proposal.","1500000","2013-01-01","2017-12-31"
"IBiDT","Individualized Binaural Diagnostics and Technology","Mathias DIETZ","CARL VON OSSIETZKY UNIVERSITAET OLDENBURG","Humans have two ears – and for good reason: So-called binaural hearing is critical not only for localizing acoustic events but also for selectively focusing on a target sound while suppressing sound from other directions. In order to perform these tasks, neural circuits with the most temporally precise processing within the entire nervous system have evolved.
360 million people have impaired hearing. Although hearing aids and cochlear implants help restore audibility, they provide insufficient benefit in restoring the advantages of true binaural hearing.
IBiDT is designed to fundamentally change this perspective. Appreciating the individual nature of each hearing deficit, it will provide the means of diagnosing pathologies, not just the perceptual symptoms. IBiDT will suggest algorithms specific to the individual detailed patient profile and suggest therapeutic interventions specific to the listening situation. To achieve these aims, a multidisciplinary approach in which both auditory and non-auditory aspects of patient profiles and a computer model simulating the impaired auditory system will, together, transform diagnosis of hearing impairment from one concerned with audibility to one concerned with effective communication in any listening environment. Binaural hearing is an ideal conceptual framework in which to investigate this approach as it increases greatly the number of possible pathologies, compared to unilateral diagnostics.
The binaural hearing system is also ideal to investigate because it allows for large improvements in listening performance. Despite significant R&D expenditure, cochlear-implant performance has plateaued over the last 15 years, at least with respect to unilateral devices. Improvements from Individualized Binaural Diagnosis and Technology will have a large, positive impact on the increasing number of bilateral cochlear implant users (many of them children), as well as on the many tens of millions of people who use hearing aids.","1500000","2018-01-01","2022-12-31"
"ICARO","Colloidal Inorganic Nanostructures for Radiotherapy and Chemotherapy","Teresa Pellegrino","FONDAZIONE ISTITUTO ITALIANO DI TECNOLOGIA","Radio and chemotherapy are the major clinical treatments for cancer. However these treatments lack cell specificity and can have severe side effects against healthy cells, especially when used in combination. My goal is to develop a nanocrystal (NC) platform to merge radio and chemotherapy into a single entity that is more specific towards tumor cells. To achieve ICARO’s goal, three main objectives will be pursued. The first objective is to introduce post-synthesis reactions, namely cation exchange (CE) and intercalation (INT) reactions, as new protocols to replace or intercalate cations that are useful as radionuclides within the crystal lattice of water-soluble NCs. Our goal is to establish protocols for the preparation of radiolabelled-NCs that will be easily translated to the medical practice for radiotherapy. This requires CE/INT reactions that occur in aqueous media, possibly with NCs prefunctionalized with specific recognition molecules to achieve targeted radiotherapy. To minimize the radio exposure of the operator, CE/INT protocols will be carried out as the last step of NC preparation. The second objective of ICARO will be to explore in situ CE/INT reactions with NCs entrapped in a matrix that simulates the tumor mass. By first located NCs at the tumor and then let the CE/INT to occur, enhance therapeutic effect is expected. The third objective of ICARO will be to develop heterostructures to combine radio and chemotherapy. They will include at least one semiconductor NC on which to perform radiolabelling protocols and one portion made of a superparamagnetic (SP) NC for magnetically triggered drug release. With respect to magnetic hyperthermia, which exploits SP-NCs to produce bulk heat (>46°C) at the tumor, the local heat effect generated at the surface of SP-NCs will enable drug release using a lower dose of magnetic material. Finally, new types of heterostructures combining radio and chemotherapy will be tested, for the first time, in preclinical trials.","1160000","2016-03-01","2020-02-29"
"ICARUS","Towards Innovative cost-effective astronomical instrumentation","Emmanuel Hugot","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Enabling disruptive technologies has always been crucial to trigger revolutionary science discoveries. The daring challenges in astronomy and astrophysics are extremely demanding in terms of high angular resolution and high contrast imaging, and require extreme stability and image quality. Instruments based on current classical designs tend to get bigger and more complex, and are faced to ever increasing difficulties to meet science requirements.
This proposal has the clear objective to propose breakthrough compact optical architectures for the next generation of giant observatories. The project focus on the niche of active components and is structured in two main research pillars to (I) enable the use of additive manufacturing (3D-printing) to produce affordable deformable mirrors for VIS or NIR observations, (II) pave the road for a common use of curved and deformable detectors. Extensive finite element analysis will allow to cover the parameter space and broad prototyping will demonstrate and characterize the performance of such devices. 
Both pillars are extremely challenging, the fields of detectors and optical fabrication being driven by the market. We will then orientate the activities towards a mass production method. 
To maximize the impact of this high gain R&D, the pillars are surrounded by two transverse activities: (i) design and optimization of a new zoo of optical systems using active mirrors and flexible detectors, and (ii) build a solid plan of technology transfer to end-user industrial companies, through a patenting and licensing strategy, to maximize the financial return and then perpetuate the activities. 
The pathway proposed here is mandatory to develop affordable components in the near future, and will enable compact and high performance instrumentation. These high potential activities will dramatically reduce the complexity of instruments in the era of giant observatories, simplify the operability of systems and offer increased performance.","1747667","2016-08-01","2021-07-31"
"ICE","Laboratory and modelling studies of ice nucleation and crystallisation in the Earth's atmosphere","Benjamin Murray","UNIVERSITY OF LEEDS","The formation of ice particles in the Earth s atmosphere strongly affects the properties of clouds and their impact on climate. However, our basic understanding of ice nucleation and crystallisation is still in its infancy. Despite the importance of ice formation in determining the properties of clouds, the Intergovernmental Panel on Climate Change (IPCC) was unable to assess the impact of atmospheric ice formation in their most recent report, because our basic knowledge is insufficient. In this proposal plans are described to establish a laboratory dedicated to improving our fundamental understanding of ice nucleation and crystallisation. It is proposed to develop a series of laboratory experiments designed to quantify atmospherically relevant processes at a fundamental level. In work package 1 the role of glassy solids and ultra-viscous liquids in cloud formation will be investigated; in work package 2 the rate at which various mineral dusts nucleate ice in the immersion mode will be quantified; the phase of ice that deposits onto frozen solution droplets or heterogeneous ice nuclei will be determined in work package 3; and in work package 4 the laboratory data from work packages 1-3 will be used to constrain ice nucleation in numerical clouds models in order to assess radiative forcings. The instrumentation and modelling experience gained in this five year project will provide a lasting legacy and open doors to new research areas in the future. As an international hub of atmospheric and climate science, the University of Leeds is a unique and ideal institute in which to bridge the gap between fundamental studies and the cloud/climate modelling community.","1664190","2009-11-01","2015-04-30"
"ICEPROXY","Novel Lipid Biomarkers from Polar Ice: Climatic and Ecological Applications","Guillaume Masse","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","It is widely acknowledged that polar sea ice plays a critical role in global climate change. As such, sea ice reconstructions are of paramount importance in establishing climatic evolution of the geological past. In the current project, some well characterised organic chemicals (biomarkers) from microalgae will be used as proxy indicators of current and past sea ice in the Arctic and Antarctic regions. These biomarkers, so-called highly branched isoprenoids (HBIs), possess a number of characteristics that make them attractive as sea ice proxies. Firstly, some HBIs are unique to sea ice diatoms, so their presence in polar sediments can be directly correlated with the previous occurrence of sea ice. Secondly, they are relatively resistant to degradation, which extends their usefulness in the geological record. Thirdly, their relative abundance makes them straightforward to measure with a high degree of geological resolution. One component of this project will consist of performing regional calibrations of the proxies. Concentrations of selected biomarkers in recent Arctic and Antarctic sediments will be correlated with the sea ice abundances determined using satellite technology over the last 30 years. The successful calibration of the proxies will then enable reconstructions of past sea ice extents to be performed at unprecedented high resolution. Sediment cores will be obtained from key locations across both of the Arctic and Antarctic regions and the data derived from these studies will be used for climate modelling studies. As a complement to these physico-chemical studies on sea ice, a second component of the project will investigate the use of these biomarkers for studying sea ice-biota interactions and, by examining the transfer of these chemicals through food chains, new tools for determining the consequences of future climate change on polar ecosystems will be established.","1888594","2008-10-01","2013-09-30"
"ICY-LAB","Isotope CYcling in the LABrador Sea","Katharine Rosemary Hendry","UNIVERSITY OF BRISTOL","The high-latitude regions are experiencing some of the most rapid changes observed in recent decades: polar temperatures are rising twice as fast as the global mean and there are concerns about the impact of sea-ice and glacier retreat on global oceans and climate. The high-latitude North Atlantic is also a key region for ecologically and economically important natural resources such as fisheries. How these resources will change in the future depends strongly on the response of marine biogeochemical cycling of essential nutrients to increasing anthropogenic stress.

Diatoms are photosynthetic algae that are responsible for nearly half of the export of carbon from the sea surface to the seafloor, and they are a sensitive indication of the state of nutrient cycling. Diatoms are one of many organisms that precipitate biogenic opal, an amorphous glass made of silica (hydrated SiO2), to form protective skeletons, and one of the essential nutrients is therefore dissolved silicon (Si) in the form of silicic acid. The response of the silicon cycle to changing environmental conditions is critical for both carbon and nutrient cycling and it can now be addressed through high precision silicon isotopes, which is the focus of ICY-LAB. 

The approach will be to capture the whole silicon cycle system in areas of marked environmental change using careful field sampling strategies - with research expeditions to coastal Greenland and the open ocean Labrador Sea - coupled with cutting-edge analytical methods. The results will lead to an unprecedented and cross-disciplinary view of nutrient cycling, biomineralisation, and the taxonomy and biogeography of siliceous organisms in an ecologically important region of the North Atlantic. 

ICY-LAB is an exciting and novel project for which I am ideally placed to carry out, allowing me to develop a new method for looking at modern biogeochemical processes, adding to my existing palaeoclimate and biochemical expertise.","1999885","2016-07-01","2021-06-30"
"icyMARS","Cold and wet early Mars: Proposing and testing a new theory to understand the early Martian environments","Alberto Gonzalez Fairen","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","Geologic evidence indicative of flowing and ponding liquid water on the surface of ancient Mars appears abundantly across most of the Martian landscape, indicating that liquid water was present in variable amounts and for long periods of time on and/or near the surface at different moments of Mars’ early history, the Noachian era. Early Mars appears to have been “wet”. However, the presence of liquid water on the surface of early Mars is difficult to reconcile with the reduced solar luminosity at 3.8 Ga. and before, which would have imposed mean temperatures below freezing all over the planet. Atmospheric greenhouse gases and carbon dioxide ice clouds in the upper troposphere are suggested to provide over freezing temperatures, explaining some of this discrepancy, but these solutions have been probed to face numerous problems. So, it is difficult to explain the early Martian hydrology invoking global “warm” conditions. Here I propose to conduct interdisciplinary investigations in order to define and test a new hypothesis to understand the early environmental traits on Mars: that the young Martian surface was characterized by global mean freezing conditions, as predicted by climate models, and at the same time a vigorous hydrogeological cycle was active during hundreds of millions of years, as confirmed by geomorphological and mineralogical analyses. The aim of this investigation is to comprehensively analyze the triggers, traits and consequences of a cold aqueous environment dominating the Noachian, studying the geomorphological, mineralogical and geochemical evidences that such a hydrological cycle would have left behind, and also proposing new paths for the astrobiological exploration of Mars on the basis of geochemical and geomicrobiological studies in cold aqueous environments. Mission-derived datasets will be used to test hypotheses through paleogeomorphological reconstructions, theoretical modeling and experiments in the laboratory.","1411200","2014-01-01","2018-12-31"
"IDEal reSCUE","Integrated DEsign and control of Sustainable CommUnities during Emergencies","Gian Paolo Cimellaro","POLITECNICO DI TORINO","Integrated DEsign and control of Sustainable CommUnities during Emergencies","1271138","2015-11-01","2020-10-31"
"IDIU","Integrated and Detailed Image Understanding","Andrea Vedaldi","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The aim of this project is to create the technology needed to understand the content of images in a detailed, human-like manner, significantly superseding the current limitations of automatic image understanding, and enabling new far reaching human-centric applications. The first goal is to substantially broaden the spectrum of visual information that machines can extract from images. For example, where current technology may discover that there is a ``person'' in an image, we would like to produce a description such as ``person wearing a red uniform, tall, brown haired, with a bayonet, and a long black hat.'' The second goal is to do so efficiently, by developing integrated image representations that can share knowledge and computation in multiple computer vision tasks, from detecting edges to recognising and describing thousands of different object types.

In order to do so, we will investigate, for the fist time in a systematic manner, the breadth of information that humans can extract from images, from abstract patterns to object parts and attributes, and we will incorporate it in the next generation of machine vision systems. Compared to existing technology, the new algorithms will have a significantly richer and more detailed understanding of the content of images. They will be learned from data building on recent breakthroughs in large scale discriminative and deep machine learning, and will be delivered as general-purpose open-source software for the benefit of the research community and businesses. In order to make these systems future-proof, we will develop methods to extend them automatically, by learning from images downloaded from the Internet with very little human supervision. These new advanced capabilities will be demonstrated in breakthrough applications in large scale image search and visual information retrieval.","1497271","2015-08-01","2020-07-31"
"iEXTRACT","Information Extraction for Everyone","Yoav Goldberg","BAR ILAN UNIVERSITY","Staggering amounts of information are stored in natural language documents, rendering them unavailable to data-science techniques. Information Extraction (IE), a subfield of Natural Language Processing (NLP), aims to automate the extraction of structured information from text, yielding datasets that can be queried, analyzed and combined to provide new insights and drive research forward.

Despite tremendous progress in NLP, IE systems remain mostly inaccessible to non-NLP-experts who can greatly benefit from them. This stems from the current methods for creating IE systems: the dominant machine-learning (ML) approach requires technical expertise and large amounts of annotated data, and does not provide the user control over the extraction process. The previously dominant rule-based approach unrealistically requires the user to anticipate and deal with the nuances of natural language.

I aim to remedy this situation by revisiting rule-based IE in light of advances in NLP and ML. The key idea is to cast IE as a collaborative human-computer effort, in which the user provides domain-specific knowledge, and the system is in charge of solving various domain-independent linguistic complexities, ultimately allowing the user to query
unstructured texts via easily structured forms.

More specifically, I aim develop:
(a) a novel structured representation that abstracts much of the complexity of natural language; 
(b) algorithms that derive these representations from texts; 
(c) an accessible rule language to query this representation;
(d) AI components that infer the user extraction intents, and based on them promote relevant examples and highlight extraction cases that require special attention.

The ultimate goal of this project is to democratize NLP and bring advanced IE capabilities directly to the hands of
domain-experts: doctors, lawyers, researchers and scientists, empowering them to process large volumes of data and
advance their profession.","1499354","2019-05-01","2024-04-30"
"IgYPurTech","IgY Technology: A Purification Platform using Ionic-Liquid-Based Aqueous Biphasic Systems","Mara Guadalupe Freire Martins","UNIVERSIDADE DE AVEIRO","With the emergence of antibiotic-resistant pathogens the development of antigen-specific antibodies for use in passive immunotherapy is, nowadays, a major concern in human society. Despite the most focused mammal antibodies, antibodies obtained from egg yolk of immunized hens, immunoglobulin Y (IgY), are an alternative option that can be obtained in higher titres by non-stressful and non-invasive methods. This large amount of available antibodies opens the door for a new kind of cheaper biopharmaceuticals. However, the production cost of high-quality IgY for large-scale applications remains higher than other drug therapies due to the lack of an efficient purification method. The search of new purification platforms is thus a vital demand to which liquid-liquid extraction using aqueous biphasic systems (ABS) could be the answer. Besides the conventional polymer-based systems, highly viscous and with a limited polarity/affinity range, a recent type of ABS composed of ionic liquids (ILs) may be employed. ILs are usually classified as “green solvents” due to their negligible vapour pressure. Yet, the major advantage of IL-based ABS relies on the possibility of tailoring their phases’ polarities aiming at extracting a target biomolecule. A proper manipulation of the system constituents and respective composition allows the pre-concentration, complete extraction, or purification of the most diverse biomolecules.
This research project addresses the development of a new technique for the extraction and purification of IgY from egg yolk using IL-based ABS. The proposed plan contemplates the optimization of purification systems at the laboratory scale and their use in countercurrent chromatography to achieve a simple, cost-effective and scalable process. The success of this project and its scalability to an industrial level certainly will allow the production of cheaper antibodies with a long-term impact in human healthcare.","1386020","2014-02-01","2019-01-31"
"IHEARU","Intelligent systems' Holistic Evolving Analysis of Real-life Universal speaker characteristics","Bjoern Wolfgang Schuller","UNIVERSITAT PASSAU","""Recently, automatic speech and speaker recognition has matured to the degree that it entered the daily lives of thousands of Europe's citizens, e.g., on their smart phones or in call services. During the next years, speech processing technology will move to a new level of social awareness to make interaction more intuitive, speech retrieval more efficient, and lend additional competence to computer-mediated communication and speech-analysis services in the commercial, health, security, and further sectors. To reach this goal, rich speaker traits and states such as age, height, personality and physical and mental state as carried by the tone of the voice and the spoken words must be reliably identified by machines. In the iHEARu project, ground-breaking methodology including novel techniques for multi-task and semi-supervised learning will deliver for the first time intelligent holistic and evolving analysis in real-life condition of universal speaker characteristics which have been considered only in isolation so far. Today's sparseness of annotated realistic speech data will be overcome by large-scale speech and meta-data mining from public sources such as social media, crowd-sourcing for labelling and quality control, and shared semi-automatic annotation. All stages from pre-processing and feature extraction, to the statistical modelling will evolve in """"life-long learning"""" according to new data, by utilising feedback, deep, and evolutionary learning methods. Human-in-the-loop system validation and novel perception studies will analyse the self-organising systems and the relation of automatic signal processing to human interpretation in a previously unseen variety of speaker classification tasks. The project's work plan gives the unique opportunity to transfer current world-leading expertise in this field into a new de-facto standard of speaker characterisation methods and open-source tools ready for tomorrow's challenge of socially aware speech analysis.""","1498200","2014-01-01","2018-12-31"
"ILLUMINATING NERVES","Hybrid imaging agents for the illumination of peripheral nerve structures","Fijs Van Leeuwen","ACADEMISCH ZIEKENHUIS LEIDEN","""The aim of the ILLUMINATING NERVES project is to develop and synthesize new imaging agents that eventually can be used for surgical guidance around delicate nerve structures. The applicants research group already has made a major contribution to the clinical field of surgical guidance by introducing a concept wherein a multimodal/hybrid, imaging agent is used to provide fully integrated preoperative 3D imaging, surgical procedure planning, and real-time surgical fluorescence guidance. Illumination of delicate anatomical structures like nerves will have a direct influence on the quality of life of patients and as such creates a new window of opportunities for surgical guidance and for the chemical development of hybrid imaging agents. To illuminate both somatic and autonomic peripheral nerves, imaging agents will be developed that bind to receptors on myelinating Schwann cells and/or accumulate in neurons. These different targeting concepts dictate the use of different scaffold molecules varying from targeted antibodies to modified neurotoxin proteins that act as bionanoparticles and viral capsids, each scaffold demanding different synthetic routes for functionalization. For this particular application, the visualization of small nerves, especially the fluorescent imaging contrast has to increase compared to conventional imaging agents. The applicant’s multidisciplinary track record the chemical and (bio)medical field, combined with his proven ability to bridge the gap between bench and bedside aids in the successful integration of synthetic chemical strategies with biomedical assays and in vitro/in vivo validation studies. Although possibly providing interesting imaging agent candidates suitable for optimization towards a future clinical translation, the project will mainly generate fundamental insight in the types of (hybrid) imaging agents that have potential for the surgical illumination of peripheral nerves.""","1498800","2012-08-01","2017-07-31"
"IMAGEMS","Exploring applications of spatial-map and velocity-map imaging mass spectrometry","Claire Vallance","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Our aim is to develop a next-generation mass spectrometer with unique imaging capabilities. For each mass, the new instrument will image the complete velocity or spatial distribution of the ions at their point of formation. The velocity distributions of fragment ions are highly sensitive to the detailed dynamics of the fragmentation process, such that in velocity imaging mode the new instrument will provide a powerful alternative to conventional tandem mass spectrometry approaches for fragmentation studies. In addition to the mechanistic and structural information encoded the images, the set of photofragment velocity distributions constitutes a unique ‘fingerprint’ for the parent molecule that may be used in molecular identification. In spatial imaging mode, there are clear applications in the areas of surface analysis and high throughput sampling, both of which will be explored over the course of the project. The spectrometer will utilise the method of velocity/spatial-map imaging, a technique originally developed for studying the photofragmentation dynamics of small molecules. A standard velocity-map imaging measurement yields the detailed speed and angular distributions for a single fragment. However, by employing advanced detector technology, our instrument will be capable of recording such distributions for all fragments simultaneously, opening the way for the study of much larger molecules with complex fragmentation pathways. A working prototype of the spectrometer will be constructed within the first year of the project, with further developments and improvements taking place over the remaining four years. The instrument will be calibrated using results from previous studies, and its capabilities in both spatial and velocity-map imaging modes will then be explored using a number of carefully chosen chemical systems. These include fundamental dynamics studies, ultraviolet photodissociation of peptides, and imaging of biomolecules and single cells on surfaces.","1499969","2008-06-01","2013-05-31"
"ImagePlanetFormDiscs","Imaging the Dynamical Imprints of Planet Formation in Protoplanetary Discs","Stefan Kraus","THE UNIVERSITY OF EXETER","The gas and dust discs around young stars are thought to be the birthplace of planetary systems and are a key area to study if further progress is to be made on understanding the history of our solar system and our own origins. Once planets have formed in these discs, they dynamically sculpt their environment, for instance by opening tidally-cleared gaps or triggering spiral arms and disc warps. The late stages of this process are likely observed in the “transitional” discs, where regions spanning tens of astronomical units (AU) have been cleared. The aim of this project is to image the planet formation signatures both during the transitional disc and the earlier T Tauri or Herbig Ae/Be stars phase, where the protoplanetary bodies are just starting to carve gaps in the optically thick disc. For this purpose, we will employ the latest generation of near-infrared, mid-infrared, and sub-millimeter interferometric instruments that will allow us to trace a wide range of stellocentric radii, disc scale heights, and dust opacities. We will make use of recent revolutionary advancements in infrared detector technology and equip the CHARA/MIRC 6-telescope beam combiner with a low-read noise camera that will significantly increase the sensitivity of this instrument and enable us to image protoplanetary discs with 2.5 times higher resolution and much higher efficiency than ever before. These quick-look imaging capabilities will enable us to trace time-variable structures in the inner few AU and to investigate their relation to the commonly observed photometric and spectroscopic variability. Our interferometric observations in spectral lines aim to detect the accretion signatures of the young protoplanets themselves. Employing sophisticated radiation hydrodynamics simulations we will achieve an unprecedented global view on protoplanetary disc structure and obtain fundamentally new constraints on theoretical models of planet formation, planet-disc interaction, and disc evolution.","1648265","2015-07-01","2020-06-30"
"ImageToSim","Multiscale Imaging-through-analysis Methods for Autonomous Patient-specific Simulation Workflows","Dominik SCHILLINGER","GOTTFRIED WILHELM LEIBNIZ UNIVERSITAET HANNOVER","Due to the intricate process of transferring diagnostic imaging data into patient-specific models, simulation workflows involving complex physiological geometries largely rely on the manual intervention of specially trained analysts. This constitutes a significant roadblock for a wider adoption of predictive simulation in clinical practice, as the associated cost and response times are incompatible with tight budgets and urgent decision-making. Therefore, a new generation of imaging-through-analysis tools is needed that can be run autonomously in hospitals and medical clinics. The overarching goal of ImageToSim is to make substantial progress towards automation by casting image processing, geometry segmentation and physiology-based simulation into a unifying finite element framework that will overcome the dependence of state-of-the-art procedures on manual intervention. In this context, ImageToSim will fill fundamental technology gaps by developing a series of novel comprehensive variational multiscale methodologies that address robust active contour segmentation, upscaling of voxel-scale parameters, transition of micro- to macro-scale failure and flow through vascular networks of largely varying length scales. Focusing on osteoporotic bone fracture and liver perfusion, ImageToSim will integrate the newly developed techniques into an imaging-through-analysis prototype that will come significantly closer to automated operation than any existing framework. Tested and validated in collaboration with clinicians, it will showcase pathways to new simulation-based clinical protocols in osteoporosis prevention and liver surgery planning. Beyond its technical scope, ImageToSim will help establish a new paradigm for patient-specific simulation research that emphasizes full automation as a key objective, accelerating the much-needed transformation of healthcare from reactive and hospital-centered to preventive, proactive, evidence-based, and person-centered.","1555403","2019-01-01","2023-12-31"
"IMAGINE","Imaging magnetic fields at the nanoscale with a single spin microscope","Vincent, Henri Jacques","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Detecting and imaging magnetic fields with high sensitivity and nanoscale resolution is a topic of crucial importance for a wealth of research domains, from material science, to mesoscopic physics, and life sciences. This is obviously also a key requirement for fundamental studies in nanomagnetism and the design of innovative magnetic materials with tailored properties for applications in spintronics. Although a remarkable number of magnetic microscopy techniques have been developed over the last decades, imaging magnetism at the nanoscale remains a challenging task.

It was recently realized that the experimental methods allowing for the detection of single spins in the solid-state, which were initially developed for quantum information science, open new avenues for high sensitivity magnetometry. In that spirit, it was recently proposed to use the electronic spin of a single nitrogen-vacancy (NV) defect in diamond as a nanoscale quantum sensor for scanning probe magnetometry. This approach promises significant advances in magnetic imaging since it provides quantitative and vectorial magnetic field measurements, with an unprecedented combination of spatial resolution and magnetic sensitivity, even under ambient conditions.

The IMAGINE project intend to exploit the unique performances of scanning-NV magnetometry to achieve major breakthroughs in nanomagnetism. We will first explore the structure of domain walls and individual skyrmions in ultrathin magnetic wires, which both promise disruptive applications in spintronics. This will lead  (i) to solve an important academic debate regarding the inner structure of domain walls and (ii) to the first detection of individual skyrmions in ultrathin magnetic wire under ambient conditions. This might result in a new paradigm for spin-based applications in nanoelectronics. We will then explore orbital magnetism in graphene, which has never been observed experimentally and is the purpose of surprising theoretical predictions.","1498810","2015-09-01","2020-08-31"
"IMCOLMAT","Impurities in Colloidal Materials - tuning the properties of crystals, powders and glasses","Roel Petrus Angela Dullens","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","We aim to establish an multidisciplinary research programme that is focussed on the underlying structural and dynamical processes which determine the intimate relation between impurities and material properties. We propose to exploit the advantages of colloidal model systems and study the impact of impurities on the structural and dynamical properties of crystalline, polycrystalline and amorphous colloidal solids using a combination of state-of-the art fast confocal microscopy and three-dimensional holographic optical laser tweezers. We plan to achieve control over the subtle interplay between glass formation and crystallisation by the tuned addition of impurities. We envisage that our approach will not only offer a direct entry into key mechanisms like impurity drag, but will also allow us to directly and quantitatively measure the central forces at play such as the Zener pinning force. We also aim to study the glass transition from a completely new point of view by tuning the structure using impurities and subsequently ‘freezing-in’ part of the system using holographic optical tweezing. This approach could lead to the determination of a thermodynamic signature of the glass transition, which would put the glass transition in a completely new perspective.  In addition, we will investigate the relation between the presence of impurities and the (micro)mechanical properties of doped colloidal materials using (micro)rheological techniques. This ambitious project opens up a huge range of exciting possibilities to gain deep and fundamental understanding of the relation between the (micro)mechanical properties of glasses, polycrystals and crystals and the presence of impurities; a prerequisiute for exploiting these effects in tailoring the properties of materials.","1499979","2012-01-01","2016-12-31"
"IMMOCAP","'If immortality unveil…'– development of the novel types of energy storage systems with excellent long-term performance","Krzysztof FIC","POLITECHNIKA POZNANSKA","The major goal of the project is to develop a novel type of an electrochemical capacitor with high specific power (up to 5 kW/kg) and energy (up to 20 Wh/kg) preserved along at least 50 000 cycles. Thus, completion of the project will result in remarkable enhancement of specific energy, power and life time of modern electrochemical capacitors. Advanced electrochemical testing (galvanostatic cycling with constant power loads, electrochemical impedance spectroscopy, accelerated aging and kinetic tests) will be accompanied by materials design and detailed characterization. Moreover, the project aims at the implementation of novel concepts of the electrolytes and designing of new operando technique for capacitor characterization. All these efforts aim at the development of sustainable and efficient energy conversion and storage system.","1385000","2017-10-01","2022-09-30"
"iModel","Intelligent Shape Modeling","Olga Sorkine","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Digital 3D content creation and modeling has become an indispensable part of our technology-driven society. Any modern design and manufacturing process involves manipulation of digital 3D shapes. Many industries have been long expecting ubiquitous 3D as the next revolution in multimedia. Yet, contrary to “traditional” media such as digital music and video, 3D content creation and editing is not accessible to the general public, and 3D geometric data is not nearly as wide-spread as it has been anticipated. Despite extensive geometric modeling research in the past two decades, 3D modeling is still a restricted domain and demands tedious, time consuming and expensive work effort even from trained professionals, namely engineers, designers, and digital artists. Geometric modeling is reported to constitute one of the lowest-productivity components of product life cycle.

The major reason for 3D shape modeling remaining inaccessible and tedious is that our current geometry representation and modeling algorithms focus on low-level mathematical properties of the shapes, entirely missing structural, contextual or semantic information. As a consequence, current modeling systems are unintuitive, inefficient and difficult for humans to work with. We believe that instead of continuing on the current incremental research path, a concentrated effort is required to fundamentally rethink the shape modeling process and re-align research agendas, putting high-level shape structure and function at the core. We propose a research plan that will lead to intelligent digital 3D modeling tools that integrate semantic knowledge about the objects being modeled and provide the user an intuitive and logical response, fostering creativity and eliminating unnecessary low-level manual modeling tasks. Achieving these goals will represent a fundamental change to our current notion of 3D modeling, and will finally enable us to leverage the true potential of digital 3D content for society.","1497442","2012-09-01","2017-08-31"
"IMPRO","Implicit Programming","Viktor Kuncak","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","""I propose implicit programming, a paradigm for developing reliable software using new programming language specification constructs and tools, supported through the new notion of software synthesis procedures. The paradigm will enable developers to use specifications as executable programming language constructs and will automate some of the program construction tasks to the point where they become feasible for the end users. Implicit programming will increase developer productivity by enabling developers to focus on the desired software functionality instead of worrying about low-level implementation details. Implicit programming will also improve software reliability, because the presence of specifications will make programs easier to analyze.

From the algorithmic perspective, I propose a new agenda for research in algorithms for decidable logical theories. An input to such an algorithm is a logical formula (or a boolean-valued programming language expressions). Whereas a decision procedure for satisfiability merely checks whether there exists a satisfying assignment for the formula, we propose to develop synthesis procedures. A synthesis procedure views the input as a relation between inputs and outputs, and produces a function from input variables to output variables. In other words, it transforms a specification into a computable function. We will design synthesis procedures for important classes of formulas motivated by useful programming language fragments. We will use synthesis procedures as a compilation mechanism for declarative programming language constructs, ensuring correctness by construction. To develop practical synthesis procedures we will combine insights from decision procedure research (including the results on SMT solvers), with the research on compiler construction, program analysis, and program transformation. The experience from the rich model toolkit initiative (http://RichModels.org) will help us address these goals.""","1439240","2012-12-01","2017-11-30"
"In Motion","Investigation and Monitoring of Time-varying Environments on Macro and Nano Scales","Pavel Ginzburg","TEL AVIV UNIVERSITY","The ultimate goal of my research is to develop novel approaches to detect dynamical changes in cluttered time-dependent electromagnetic environments. Theoretical and experimental methods will be applied on a range of highly important problems, including radar tracking and optical imaging of complex processes on micro and nano scales. Nowadays demands, set by increasing complexity of systems under study, challenges applicability of existent solutions, opening a room of opportunities for multidisciplinary rewarding research. Scalability of Maxell’s equations with respect to frequency and classical-quantum correspondence principles suggest developing a broad range of dynamical phenomena by applying multidisciplinary concepts, as my team has recently demonstrated. Radio detection of macroscopic objects (e.g. airborne targets) and optical imaging of conformational changes in colloids (e.g. bio-chemical activities), being representative examples on a very diverse size scales, share similar underlining physics and engineering principles for their analysis. This multidisciplinary research considers the phenomena on macro, micro and nano scales, utilizing classical and quantum properties of electromagnetic radiation and light for achieving superior performances in detection beyond existing capabilities. Radio detection will be performed via mapping internal mechanical properties of a target, enabling attributing a unique signature in a clutter. The novel concept of ‘swimming antennas’, driven by holographic optical tweezers, will be developed for optical mapping of micro and nano scale motion. Slow decaying luminescent tags, conjugated with antennas, will allow monitoring a motion beyond the diffraction limit by considering quantum properties of light. 
Fundamental study and exploration of mechanical motion impact on photonic and electromagnetic applications, including tracking in a clutter, classical and quantum imaging and sensing is the core objective of the Proposal.","1999375","2018-10-01","2023-09-30"
"In-Need","III-Nitrides Nanostructures for Energy-Efficiency Devices","Elison de Nazareth Matioli","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Energy efficiency offers a vast and low-cost resource to address future energy demand while reducing carbon dioxide emissions. The unique properties of III-Nitride semiconductors make them the ideal material for future energy challenges. Their outstanding optical properties are revolutionizing the world with efficient LED light bulbs. Even greater impact is anticipated for power electronics. The much larger Baliga’s figure of merit of GaN compared to SiC and Si enables drastically more efficient power switches, which are at the heart of any energy generation/management system. However, current III-Nitride device performance is far from the fundamental materials capabilities, and severe thermal management and reliability limitations hinder their full potential for energy-efficiency.
The In-Need proposes a unique approach to address concurrently all current challenges based on advanced nanostructures designed to optimally exploit the superior properties of the new bulk GaN materials. Nanostructuring distinct regions of the device will allow a precise control over their intrinsic characteristics. To address reliability issues and yield unprecedented device performance, these nanostructures will be combined to the excellent properties of bulk GaN. This will open opportunities for new vertical devices, enabling smaller structures with larger voltages and higher efficiencies. Efficient thermal management will be achieved with ultra-near junction cooling. Nano/micro-channels filled with high thermal conductivity materials or coolants will be embedded inside the device.
We believe our judicious nano-scale design of new high-performing materials will result in state-of-the-art devices, leading to a large-scale impact in energy efficiency. The miniaturization and large power density enabled by our approach will allow future integration of power devices into single power microchips. This will revolutionize energy use much like Silicon microchips did for information processing.","1750000","2016-02-01","2021-01-31"
"INCEPT","INhomogenieties and fluctuations in quantum CohErent matter Phases by ultrafast optical Tomography","Daniele Fausti","UNIVERSITA DEGLI STUDI DI TRIESTE","Standard time domain experiments measure the time evolution of the reflected/transmitted mean number of photons in the probe pulses. The evolution of the response of a material is typically averaged over the illuminated area as well as over many pump and probe measurements repeated stroboscopically. The aim of this project is to extend time domain optical spectroscopy beyond mean photon number measurements by performing a full Time Resolved Quantum State Reconstruction (TRQSR) of the probe pulses as a function of the pump and probe delay. The nature of the light matter interaction and the transient light-induced states of matter will be imprinted into the probe quantum state after the interaction with the material and can be uncovered with unprecedented detail with this new approach to time domain studies.

TRQSR will be implemented by combining pump and probe experiments resolving single light pulses with balanced homodyne detection quantum tomography in the pulsed regime. We will apply and exploit the unique capabilities of TRQSR to address two different unresolved problems in condensed matter. Firstly, we will investigate the coherent and squeezed nature of low energy photo-induced vibrational states. We will use TRQSR with probe pulses shorter than the phonon timescale to interrogate the time evolution of the vibrational state induced by the pump pulse. Secondly, we will address inhomogeneities in photo-induced phase transformations. With TRQSR we can perform time domain measurements with a very small photon number per pulse which will give information on the interaction between the material (as prepared by the pump pulse) and individual photons. In this limit, TRQSR will allow us to retrieve rich statistics. While the average will deliver the information of a standard pump and probe experiment, higher order moments will give information on the time evolution of spatial inhomogenieties in the transient state.","1500000","2016-06-01","2021-05-31"
"inCITe","Seeing Citrulline: A Molecular Toolbox for Peptidyl Arginine Deiminases","Kimberly BONGER","STICHTING KATHOLIEKE UNIVERSITEIT","Roughly 1% of the world’s population is affected by rheumatoid arthritis (RA); a devastating autoimmune disease causing cartilage destruction and bone erosion. Recent evidences suggest that dysregulation of Peptidyl Arginine Deiminase (PAD) levels are associated with the onset of the disease, leading to the production of antibodies targeting the citrullinated neoepitopes. The exact role of each of the PAD isotypes in these pathological processes is unknown and fundamental questions on the intracellular activation mechanism and substrate specificity remain unanswered. Moreover, isoform specific and high affinity enzyme inhibitors are lacking thereby not only hampering fundamental research towards each PAD isotype, but also excluding PAD as a potential therapeutic target for these diseases.
This proposal is aimed at developing innovative chemical biology- and molecular tools to study PAD functioning and protein citrullination in health and disease. The work reflects my interdisciplinary experiences as well as my interest I have obtained over the last years in chemical immunology as well as my ambition to improve patients wellbeing. More detailed, I aim to 1) find unknown PAD modulators, 2) find PAD substrates, 3) find selective and high affinity PAD inhibitors using enzyme-templated inhibitor evolution as novel lead discovery strategy, 4) explore multifunctional targeted PAD ‘nanosponges’ as advanced avidity-based nanomedicine approach and 5) explore unprecedented citrulline ‘eraser’ enzymes by innovative chemical biology strategies. 
The workpackages described in this ambitious and highly interdisciplinary proposal deliver high-end molecules and methods that can be used to answer fundamental (conflicting) questions on citrullination and PAD biology. Moreover, possible molecular leads and advanced therapeutic insights are provided thereby centring PAD as therapeutic target for citrulline-mediated autoimmune diseases such as RA.","1500000","2019-01-01","2023-12-31"
"inCREASE","Coding for Security and DNA Storage","Antonia Wachter-Zeh","TECHNISCHE UNIVERSITAET MUENCHEN","Communication and data storage systems are indispensable parts of our every-day life. However, these systems deal with severe challenges in security and reliability. Security is important whenever a user communicates or stores sensitive data, e.g., medical information; reliability has to be guaranteed to be able to transmit or store information while noise occurs. Algebraic codes (ACs) are a powerful means to achieve both.

Within inCREASE, I will construct and evaluate special codes for security applications and DNA storage.
The tasks are structured into three work packages: (1) post-quantum secure code-based cryptosystems, (2) secure key regeneration based on ACs, (3) ACs for DNA-based storage systems. The focus of inCREASE
lies on innovative theoretical concepts.

The goal of work package (1) is to investigate and design code-based cryptosystems; one promising idea is to apply insertion/deletion correcting codes. The security of these systems will be analysed from two points of view: structural attacks on the algorithms and hardware implementations with side-channel attacks.

Secure cryptographic key regeneration is the goal of (2) and can be achieved by physical unclonable functions (PUFs). Here, ACs are necessary to reproduce the key reliably. This project will study the error patterns that occur in PUFs, model them theoretically, and design suitable coding schemes.

The investigation on (3) will start with a study of the data of existing DNA storage systems. The outcome will be an error model that will include insertions, deletions, substitutions, and duplications. Therefore, inCREASE will design ACs for these error types. This will be especially challenging regarding the mathematical concepts. These codes will be evaluated by simulations and using data sets of DNA storage systems.

This project is high risk/high gain with impact not only to storage and security, but to the methodology as well as other areas such as communications.","1471750","2019-03-01","2024-02-29"
"INDEX","Rigidity of groups and higher index theory","Piotr Wojciech Nowak","INSTYTUT MATEMATYCZNY POLSKIEJ AKADEMII NAUK","The Atiyah-Singer index theorem was one of the most spectacular achievements of mathematics in the XXth century, connecting the analytic and topological properties of manifolds. The Baum-Connes conjecture is a hugely successful approach to generalizing the index theorem to a much broader setting. It has remarkable applications in topology and analysis. For instance, it implies the Novikov conjecture on the homotopy invariance of higher signatures of a closed manifold and the Kaplansky-Kadison conjecture on the existence of non-trivial idempotents in the reduced group C*-algebra of a torsion-free group. At present, the Baum-Connes conjecture is known to hold for a large class of groups, including groups admitting metrically proper isometric actions on Hilbert spaces and Gromov hyperbolic groups. 

The Baum-Connes conjecture with certain coefficients is known to fail for a class of groups, whose Cayley graphs contain coarsely embedded expander graphs. Nevertheless, the conjecture in full generality remains open and there is a growing need for new examples of groups and group actions, that would be counterexamples to the Baum-Connes conjecture. The main objective of this project is to exhibit such examples. 

Our approach relies on strengthening Kazhdan’s property (T), a prominent cohomological rigidity property, from its original setting of Hilbert spaces to much larger classes of Banach spaces. Such properties are an emerging direction in the study of cohomological rigidity and are not yet well-understood. They lie at the intersection of geometric group theory, non-commutative geometry and index theory. In their study we will implement novel approaches, combining geometric and analytic techniques with variety of new cohomological constructions.","880625","2016-08-01","2021-07-31"
"INDIMEDEA","Inclusions in diamonds: messengers from the deep Earth","Fabrizio Nestola","UNIVERSITA DEGLI STUDI DI PADOVA","Diamonds and their inclusions are the deepest materials originating from the Earth’s interior reaching the surface of our planet. Their study plays a key role in understanding and interpreting the geodynamics, geophysics, petrology, geochemistry and mineralogy of the Earth’s mantle and those processes which governed trough the time the evolution of the Earth. The most abundant mineral inclusions in diamonds are olivines, orthopyroxenes, clinopyroxenes, garnets, spinels, and sulfides. All of these mineral phases have been identified by X-ray diffraction or electron microprobe analysis since the 1950’s almost always after their extraction from the diamonds. However, a non-destructive in-situ investigation of an inclusion in diamond is useful and important because: (a) some mineral inclusions under pressure could have a different crystal structure, and thus different petrologic significance compared to that at ambient pressure; (b) the internal pressure on the inclusion can provide information about the formation pressure of the diamond; (c) the morphology and growth relationships of the inclusion with the host diamond can provide indications about its protogenetic vs. syngenetic and/or epigenetic nature.
In this project a new experimental approach, based on X-ray diffraction technique, will be used in order to determine, for the first time, the crystal structure of the inclusions still trapped in their host diamonds allowing to obtain chemical information capable to provide the inclusion remnant pressure and, from this, the pressure of formation of the diamond-inclusion pair. At the same time, our approach will allow to obtain any possible epitaxial relationship between diamond and its inclusions in order to provide strong constraints about the syngenetic or protogenetic nature of minerals included in diamond solving a 50 years old debate. Several geochemical and geodynamical models are based on the assumption of syngenesis but this has yet to be demonstrated.","1423464","2013-01-01","2018-12-31"
"INFIBRENANOSTRUCTURE","Fabrication and characterization of dielectric encapsulated millions of ordered kilometer-long nanostructures and their applications","Mehmet Bayindir","BILKENT UNIVERSITESI VAKIF","The objective of this project is the realization of a radically new nanowire fabrication technique, and exploration of its potential for nanowire based science and technology. The proposed method involves fabrication of unusually long, ordered nanowire and nanotube arrays in macroscopic fibres by means of an iterative thermal co-drawing process. Starting with a macroscopic rod with an annular hole tightly fitted with another rod of another compatible material, by successive thermal drawing we obtain arrays of nanowires embedded in fibres. With the method, wide range of materials, e.g. semiconductors, polymers, metals, can be turned into ordered nanorods, nanowires, nanotubes in various cross-sectional geometries. Main challenges are the thermal drawing steps that require critical matching of the viscoelastic properties of the protective cover with the encapsulated materials, and the liquid instability problems and phase intermixing with higher temperatures and smaller feature sizes that require high thermal and mechanical precision. Initially, fabrication by drawing will begin with soft amorphous semiconductors, phase change materials, polymers of interest in high temperature polymers, followed by a wider range of materials, low melting temperature metals, metals and common semiconductors (Si, Ge) in silica glass matrices. In this way nanowires that are ordered, easily accessible and hermetically sealed in a dielectric encapsulation will be obtained in high volumes. Potentially, these nanowires are advantages over on-chip nanowires in building flexible out of plane geometries, light weight, wearable and disposable devices. Ultimately, attaining ordered arrays of 1-D nanostructures in an extended flexible fibre with high yields will facilitate sought-after but up-to-now difficult applications such as the large area nanowire electronics and photonics, nanowire based scalable phase-change memory, nanowire photovoltaics, and emerging cell-nanowire interfacing.","1495400","2012-10-01","2017-09-30"
"INFLUENCE","Influence-based Decision-making in Uncertain Environments","Frans OLIEHOEK","TECHNISCHE UNIVERSITEIT DELFT","Decision-theoretic sequential decision making (SDM) is concerned with endowing an intelligent agent with the capability to choose actions that optimize task performance. SDM techniques have the potential to revolutionize many aspects of society and recent successes, e.g., agents that play Atari games and beat a world champion in the game of Go, have sparked renewed interest in this field.

However, despite these successes, fundamental problems of scalability prevents these methods from addressing other problems with hundreds or thousands of state variables. For instance, there is no principled way of computing an optimal or near-optimal traffic light control plan for an intersection that takes into account the current state of traffic in an entire city. I will develop one in this project.

To achieve this, I will develop a new class of influence-based SDM methods that overcome scalability issues for such problems by using novel ways of abstraction. Considered from a decentralized system perspective, the intersection’s local problem is manageable, but the influence that the rest of the network exerts on it is complex. The key idea is that by using (deep) machine learning methods, we can learn sufficiently accurate representations of such influence to facilitate near-optimal decisions.

This project will construct a theoretical framework for such approximate influence representations and SDM methods that use them. Scalability of these methods will be demonstrated by rigorous empirical evaluation on two simulated challenge domains: traffic lights control in an entire city, and robotic order picking in a large-scale autonomous warehouse.

If successful, INFLUENCE will produce a range of influence-based SDM algorithms that can, in a principled manner, deal with a broad range of very large complex problems consisting of hundreds or thousands of variables, thus making an important step towards realizing the promise of autonomous agent technology.","1475560","2018-02-01","2023-01-31"
"InfoInt","An Information Theory of Simple Interaction","Ofer Shayevitz","TEL AVIV UNIVERSITY","Motivated by our recent progress in feedback information theory and its deep relations to stochastic dynamical systems, and inspired by natural phenomena such as bio-molecular interactions and human conversation, this research will explore the fundamental limits of information transfer via simple interaction. In the standard information theoretic framework, the problem of reliable communications is typically studied in an asymptotic unidirectional regime, where optimal performance is attained via complex codes employed over increasingly long time epochs. Here, we will investigate a markedly different paradigm where communicating parties are restricted to use simple finite-state rules to act and react on the fly. We will consider a broad spectrum of models ranging from feedback communications and two-way channels to multiuser setups and large homogeneous networks, and study measures of information transfer and dissipation, their relations to dynamical system contraction factors, and the fundamental tradeoffs between complexity and performance. While prominently theoretic, our investigation is expected to admit important practical applications and a cross-disciplinary impact. In communications, and especially in resource-limited systems such as wireless sensor networks where battery-life is a bottleneck, a breakthrough in the understanding of optimal interaction can lead to a paradigm shift in system design, yielding simpler, cheaper, more robust solutions. In Finance, where market behavior is a cumulative effect of local actions taken by individuals based on limited noisy observations, quantifying interaction and its relation to information propagation can enhance our ability to forecast and explain macro level phenomena. Finally, an information theoretic characterization of interaction in large networks can shed light on the underlying mechanisms governing various biological systems that are empirically amenable to cellular automata modeling.","1323875","2015-03-01","2020-02-29"
"Inhomogeneities","Micro-scale inhomogeneities in compressed systems and their impact onto the PROCESS- functioning-chain and the PRODUCT-characteristics","Andreas Braeuer","TECHNISCHE UNIVERSITAET BERGAKADEMIE FREIBERG","Compressed fluid systems handled in high pressure processes feature diffusivities smaller than the kinematic viscosity. Therefore during mixing the lifetime of micro(µ)-scale(s) inhomogeneities exceeds that one of macro(m)-scale(s) inhomogeneities. Thus m-s homogeneous systems can still exhibit µ-s inhomogeneities. They affect the functioning-chain of processes, e.g. reactions and phase-transitions or –separations, which themselves also take place on a sub-macro-scale.
Therefore it will be analyzed in situ how µ-s inhomogeneities influence the functioning chain of the particle generation (supercritical antisolvent technology), the reaction (high pressure combustion), and the phase-separation or phase-transition mechanisms (surfactant-free CO2-based micro-emulsions and gas hydrates) and to which extend these inhomogeneities are responsible for the characteristics of the product, such as unfavourable size distributions of particulate products and/or pollutant emissions.
On this purpose the here proposed and self-developed non-invasive and in situ Raman spectroscopic technique considers the INTENSITY-ratios of Raman signals to analyze the m-s composition and the SIGNATURE of the OH stretch vibration Raman signal of water (or alcohols) to analyze the µ-s composition of fluid mixtures. The SIGNATURE of the OH stretch vibration Raman signal is influenced by the development of the hydrogen bonds -an intermolecular interaction- and thus provides the µ-s composition, though the probe volume of the Raman sensor is m-s. The signal-INTENSITY-ratio and signal-SIGNATURE are extracted both from one and the same “m-s” Raman spectrum of the mixture. This allows the comparison of the degree of mixing on m-s and µ-s simultaneously, and enables the analysis of whether a system at any instance of mixing (instance of the onset of a reaction or a phase transition or –separation) has reached the favourable µ-s homogeneity, which would result in homogeneous and uniform products.","1943750","2015-05-01","2020-04-30"
"INNODYN","Integrated Analysis & Design in Nonlinear Dynamics","Jakob Søndergaard Jensen","DANMARKS TEKNISKE UNIVERSITET","Imagine lighter and more fuel economic cars with improved crashworthiness that help save lives, aircrafts and wind-turbine blades with significant weight reductions that lead to large savings in material costs and environmental impact, and light but efficient armour that helps to protect against potentially deadly blasts. These are the future perspectives with a new generation of advanced structures and micro-structured materials.

The goal of INNODYN is to bring current design procedures for structures and materials a significant step forward by developing new efficient procedures for integrated analysis and design taking the nonlinear dynamic performance into account. The assessment of nonlinear dynamic effects is essential for fully exploiting the vast potentials of structural and material capabilities, but a focused endeavour is strongly required to develop the methodology required to reach the ambitious goals.

INNODYN will in two interacting work-packages develop the necessary computational analysis and design tools using
1) reduced-order models (WP1) that enable optimization of the overall topology of structures which is today hindered by excessive computational costs when dealing with nonlinear dynamic systems
2) multi-scale models (WP2) that facilitates topological design of the material microstructure including essential nonlinear geometrical effects currently not included in state-of-the-art methods.

The work will be carried out by a research group with two PhD-students and a postdoc, led by a PI with a track-record for original ground-breaking research in analysis and optimization of linear and nonlinear dynamics and hosted by one of the world's leading research groups on topology optimization, the TOPOPT group at the Technical University of Denmark.","823992","2012-02-01","2016-01-31"
"InsideChromatin","Towards Realistic Modelling of Nucleosome Organization Inside Functional Chromatin Domains","Rosana COLLEPARDO GUEVARA","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Reading the genome is one thing – finding out how it functions, is something else altogether. The next big challenge to understand gene behaviour is deciphering (a) how the genome is organized in space and (b) how this organization influences its function. Inside Eukaryotic cells, genomic DNA is packed together with proteins into a remarkable structure known as chromatin. Nucleosomes, the building blocks of chromatin, interact with each other to enable high-density packaging. Our understanding of chromatin structure is limited by the lack of ‘close up views’ and molecular-level mechanistic information of how nucleosome interactions are regulated in vivo by many highly coupled factors.

InsideChromatin aims to develop a groundbreaking multiscale approach that will push the current limits of realistic computational modelling of in vivo chromatin structure. The vision is to achieve the first multiscale simulation study that describes nucleosome organization inside functionally different kilobase-scale domains, while explicitly accounting for the combination of epigenetic marks, the binding of architectural proteins, and nucleosome remodelling activity that distinguishes each domain. InsideChromatin will integrate atomistic simulations with two levels of coarse-graining and experimental data for validation to understand how nucleosome organization at kilobase scales leads to physical properties at megabase scales. The output from InsideChromatin will bring us closer to the ‘holy grail’ of deciphering the connection between genome characteristics, structure, and function.","1490380","2019-04-01","2024-03-31"
"INSILICO-CELL","Predictive modelling and simulation in mechano-chemo-biology: a computer multi-approach","Jose Manuel Garcia-Aznar","UNIVERSIDAD DE ZARAGOZA","Living tissues are regulated by multi-cellular collectives mediated at cellular level through complex interactions between mechanical and biochemical factors. A further understanding of these mechanisms could provide new insights in the development of therapies and diagnosis techniques, reducing animal experiments. I propose a combined and complementary methodology to advance in the knowledge of how cells interact with each other and with the environment to produce the large-scale organization typical of tissues. I will couple in-silico and in-vitro models for investigating the micro-fabrication of tissues in-vitro using a 3D multicellular environment. By computational cell-based modelling of tissue development, I will use a multiscale and multiphysics approach to investigate various key factors: how environmental conditions (mechanical and biochemical) drive cell behaviour, how individual cell behaviour produces multicellular patterns, how cells respond to the multicellular environment, how cells are able to fabricate new tissues and how cell-matrix interactions affect these processes. In-vitro experiments will be developed to validate numerical models, determine their parameters, improve their hypotheses and help designing new experiments. The in-vitro experiments will be performed in a microfluidic platform capable of controlling biochemical and mechanical conditions in a 3D environment. This research will be applied in three applications, where the role of environment conditions is important and the main biological events are cell migration, cell-matrix and cell-cell interactions: bone regeneration, wound healing and angiogenesis.","1299083","2012-11-01","2018-05-31"
"INSITUNANO","In-situ metrology for the controlled growth and interfacing of nanomaterials","Stephan Hofmann","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","This proposal will use novel in-situ metrology to probe the atomic level mechanisms that govern the growth and device behaviour of nanomaterials in realistic process environments. We focus on the catalytic chemical vapour deposition of carbon nanotubes, graphene, Si/Ge nanowires and related heterostructures. The application potential for these nanostructures is large, but currently limited by insufficient control of growth. We propose to use a range of complementary in-situ probes, including environmental transmission electron microscopy, high-pressure X-ray photoelectron spectroscopy (XPS), in-situ X-ray diffraction (XRD)  and in-situ Raman spectroscopy, to significantly advance the understanding of their growth mechanisms. We see these nanomaterials as model systems to advance the fundamental understanding of phase behaviour, nucleation and interface dynamics in nanoscale systems, which is the key to future materials design. Deeper insights into these phenomena are also crucial to understand the behaviour of nanomaterials under device operation conditions. We propose to address critical performance parameters of nano-structured Si-based anodes for Li ion batteries by in-situ nuclear magnetic resonance (NMR) spectroscopy and in-situ XRD methods under repeated Li cycling in an operational battery. We further propose to study the morphological origins of the collective adhesive and mechanical properties of carbon nanotube forests by in-situ scanning electron microscopy as basis for the design of biomimetic, functional dry adhesives and compliant interconnect structures.","1367834","2011-12-01","2016-11-30"
"INSPIRAL","Spin-Delocalization with a Twist: Chiral Open-Shell Helices","Michal JURICEK","UNIVERSITAT ZURICH","Spin-delocalization in molecules containing unpaired electrons gives rise to an unusual intermolecular interaction, named as pancake bond. This bonding interaction couples unpaired electrons between multiple pairs of atoms from each face-to-face oriented spin units, such as phenalenyl radical, and abets formation of assemblies that display large antiferromagnetic interaction in the solid state. This phenomenon governs the potential use of such spin systems as molecular conductors, magneto-optical bistable materials, or molecular spin batteries. Formation of such assemblies during crystallization is spontaneous. It is therefore difficult to control and tune the antiferromagnetic interaction, which dictates the electronic properties of the solid material. Inspired by this challenge, the goal of this project is to develop systems, where the spin-interaction can be tuned within a single molecule to understand principles that govern an intermolecular assembly in the solid state and the bulk properties.

To achieve this goal, I propose to synthesize and study chiral open-shell helices, in which the intramolecular spin-interaction can be tuned by varying (1) the coupling mode, ferromagnetic versus antiferromagnetic, and (2) its strength. The helical character of these systems enables tuning of the coupling strength by control of the degree of overlap and distance between the spin units, which is difficult to achieve by a spontaneous assembly. Additionally, it provides access to both racemic and enantiopure solid-state architectures that can further impact the properties. Two model systems will be investigated: one in which spins communicate simultaneously through backbone and space, and one where spins communicate only through space. Understanding the principles of spin-interactions in these helical systems is of fundamental interest for designing molecules with tailor-made properties, as well as features that arise unexpectedly from the interplay of the spins in a spiral.","1499925","2017-04-01","2022-03-31"
"INsPIRE","Chip-scale INtegrated Photonics for the mid-Infra REd","Delphine, Marie-Line, Cornélie Marris-Morini","UNIVERSITE PARIS-SUD","Mid-infrared (mid-IR) spectroscopy is a nearly universal way to identify chemical and biological substances, as most of the molecules have their vibrational and rotational resonances in the mid-IR wavelength range. Commercially available mid-IR systems are based on bulky and expensive equipment, while lots of efforts are now devoted to the reduction of their size down to chip-scale dimensions. The demonstration of mid-IR photonic circuits on silicon chips will benefit from reliable and high-volume fabrication to offer high performance, low cost, compact, low weight and power consumption photonic circuits, which is particularly interesting for mid-IR spectroscopic sensing systems that need to be portable and low cost. 
   In this context, the INsPIRE project will address a new route towards key advances in the development of chip-scale integrated circuits on silicon for the mid-IR wavelength range. The original idea is to use nonlinear optical properties in Ge/SiGe quantum well (QW) active devices combined with Ge-rich-SiGe waveguides. The objectives of the INsPIRE project are far beyond the state of the art, by targeting the monolithic integration of passive and active devices for operation in the 3 to 15 µm wavelength range. 
   As a main cornerstone we will demonstrate an optical photonic circuit based on Ge/SiGe QWs relying on a mid-IR light emitter combined with a mid-IR spectrometer and a detector array. The integration will be performed using Ge-rich-SiGe waveguides allowing the extension of the wavelength range up to 15 µm.
   Such demonstration, which will constitute a breakthrough for establishing chip-scale circuits for the mid-IR photonics, requires a deep knowledge and understanding of Ge/SiGe optical properties. In particular, second- and third-order nonlinear optical properties of Ge/SiGe QW structures will be investigated in a wide spectral range from 3 to 15 µm.","1498125","2015-04-01","2020-03-31"
"INTEG-CV-SIM","An Integrated Computer Modelling Framework for Subject-Specific Cardiovascular Simulation: Applications to Disease Research, Treatment Planning, and Medical Device Design","Carlos Alberto Figueroa Alvarez","KING'S COLLEGE LONDON","Advances in numerical methods and three-dimensional imaging techniques have enabled the quantification of cardiovascular mechanics in subject-specific anatomic and physiologic models. Research efforts have been focused mainly on three areas: pathogenesis of vascular disease, development of medical devices, and virtual surgical planning. However, despite great initial promise, the actual use of patient-specific computer modelling in the clinic has been very limited. Clinical diagnosis still relies entirely on traditional methods based on imaging and invasive measurements and sampling. The same invasive trial-and-error paradigm is often seen in vascular disease research, where animal models are used profusely to quantify simple metrics that could perhaps be evaluated via non-invasive computer modelling techniques. Lastly, medical device manufacturers rely mostly on in-vitro models to investigate the anatomic variations, arterial deformations, and biomechanical forces needed for the design of stents and stent-grafts. In this project, I aim to develop an integrated image-based computer modelling framework for subject-specific cardiovascular simulation with dynamically adapting boundary conditions capable of representing alterations in the physiologic state of the patient. This computer framework will be directly applied in clinical settings to complement and enhance current diagnostic practices, working towards the goal of personalized cardiovascular medicine.","1491593","2012-12-01","2018-11-30"
"IntegraBrain","Integrated Implant Technology for Multi-modal Brain Interfaces","Ivan MINEV","TECHNISCHE UNIVERSITAET DRESDEN","Bioelectronic medicine may soon replace systemic drugs for treating some chronic conditions. The clinician will implant a miniature laboratory to deliver and coordinate a multi-modal treatment program directly at the affected tissue. The technology to bring this vision to the clinic is not yet available.
 
The IntegraBrain project will contribute by building an implantable network of sensors and actuators. Actuators will deploy electricity, light, drugs and thermal energy as modalities of the therapeutic program, while sensors will monitor its progress. A key technological advance will be a method for direct writing of the sensor-actuator network. To achieve this, we will develop a palette of functional inks where each ink supports one of the therapeutic modalities.

The technology has the potential to be tailored for applications in soft tissue organs, especially in the nervous system, where injury or degeneration can result in chronic disability. We will apply IntegraBrain technology in two niches of the nervous system in rodents. In the central nervous system, we will demonstrate seizure control by multi-modal neuromodulation. In the peripheral nervous system, we will demonstrate reversible block and excitation. For the first time, we will observe if multi-modal neuromodulation leads to synergistic effects on the nervous system.

With the IntegraBrain project, we hope to catalyse pre-clinical development of implantable human-machine interfaces for therapeutic applications.","1496000","2019-01-01","2023-12-31"
"InteGraDe","Integrating Graphene Devices","Max Christian Lemme","UNIVERSITAET SIEGEN","Technology requirements for future IC systems include low power computing and communication, sensing capabilities and energy harvesting. These will unlikely be met with silicon technology alone. The proposed research therefore investigates graphene as a potential alternative technology and contributes to bridging the “valley of death” in innovation that is all too present in Europe. In detail, the proposal focuses on the experimental exploration of novel (opto-) electronic devices and systems based on graphene. Strong emphasis is put on integration, defined as an interdisciplinary approach combining graphene manufacturing, graphene process technology, device engineering and -physics as well as system design. This kind of approach is urgently needed in order to open new horizons for graphene, because it enables a transition from fascinating science to a realistic demonstration of graphene’s application potential in electronics and optoelectronics. The first requirement for the applicability of graphene in ICT is a scalable graphene fabrication technology that can be co-integrated with silicon. The second logical aspect to be investigated is the intricate relationship between process technology and graphene device performance. The third aspect to be considered when discussing integration is how devices can be integrated in existing or future systems, including questions of circuit design. Will graphene systems outperform existing solutions and thus replace them? Will new functionalities emerge and generate novel applications? Hence, the key objectives of this proposal are: 1) a scalable, CMOS compatible large area fabrication technology for graphene and graphene devices, 2) demonstration and assessment of performance advantages and new functionalities of RF graphene devices, 3) high performance graphene-based optoelectronic devices integrated with silicon technology and 4) experimental exploration of the performance potential of graphene-based integrated systems.","1498687","2012-09-01","2017-08-31"
"IntelGlazing","Intelligent functional glazing with self-cleaning properties to improve the energy efficiency of the built environment","Ioannis Papakonstantinou","UNIVERSITY COLLEGE LONDON","The latest forecast by the International Energy Agency predicts that the CO2 emissions from the built environment will reach 15.2Gt in 2050, double their 2007 levels. Buildings consume 40% of the primary energy in developed countries with heating and cooling alone accounting for 63% of the energy spent indoors. These trends are on an ascending trajectory - e.g. the average energy demand for air-conditioning has been growing by ~17% per year in the EU. Counterbalancing actions are urgently required to reverse them.
The objective of this proposal is to develop intelligent window insulation technologies from sustainable materials. The developed technologies will adjust the amount of radiation escaping or entering a window depending upon the ambient environmental conditions and will be capable of delivering unprecedented reductions to the energy needed for regulating the temperature in commercial and residential buildings. 
Recognising the distinct requirements between newly built and existing infrastructure, two parallel concepts will be developed: i) A new class of intelligent glazing for new window installations, and, ii) a flexible, intelligent, polymer film to retrofit existing window installations. Both solutions will be enhanced with unique self-cleaning properties, bringing about additional economic benefits through a substantial reduction in maintenance costs.
Overall, we aim to develop intelligent glazing technologies that combine: i) power savings of >250 W/m2 of glazing capable of delivering >25% of energy savings and efficiency improvements >50% compared with existing static solutions; ii) visible transparency of >60% to comply with the EU standards for windows ,and, iii) self-cleaning properties that introduce a cost balance. 
A number of technological breakthroughs are required to satisfy such ambitious targets which are delivered in this project by the seamless integration of nanotechnology engineering, novel photonics and advanced material synthesis.","1762823","2016-03-01","2021-02-28"
"INTERACT","Interactive Systems Involving Multi-point Surfaces, Haptics and true-3D displays","Sriram Subramanian","THE UNIVERSITY OF SUSSEX","The grand challenge of this project is to 1) develop integrated multi-point surfaces that include multiple touch points, multiple haptic feedback on fingers (and tangible objects on the surface) and reconfigurable ‘true-3D’ content for a ‘walk-up and use’ scenario; 2) identify interaction design principles and visualization techniques to support users around such surfaces and 3) demonstrate the added value of this multi-point surface by integrating this within the workflow of stem-cell researchers  to demonstrate that better visual and mechanical characterization of biological processes is achievable with our system.
The knowledge generated can be applied to a wide range of applications from entertainment and education to medical and life-sciences. For example, with our proposed system students can collaborate around an interactive table to feel plant textures and human organs while visualizing them in 3D while discussing with fellow students to allow for a very rich learning experience.","1419636","2011-11-01","2016-10-31"
"INTERACT","Modelling the neuromusculoskeletal system across spatiotemporal scales for a new paradigm of human-machine motor interaction","Massimo Sartori","UNIVERSITEIT TWENTE","Neurological injuries such as stroke leave millions of people disabled worldwide every year. For these individuals motor recovery is often suboptimal. The impact of current neurorehabilitation machines is hampered by limited knowledge of their physical interaction with the human. As we move, our body adapts positively to optimal stimuli; motor improvement after stoke is promoted via physical training with an appropriate afferent input to the nervous system and mechanical loads to muscles. Loss of appropriate stimuli leads to motor dysfunction.
Motor recovery requires positive neuromuscular adaptations to be steered over time. If neuro-modulative and orthotic machines could be controlled to generate optimal stimuli to the neuromuscular system, a new era in neurorehabilitation would begin.
This project creates multi-scale models of human-machine interaction for radically new closed-loop control paradigms. We will combine biosignal recording and numerical modeling to decode the cellular activity of motor neurons in the spinal cord with resulting musculoskeletal forces at a resolution not considered before. This will enable breakthroughs for tracking the spinal-musculoskeletal system across spatiotemporal scales: short-to-long term adaptation from cellular to organ scales. We will use these concepts to design new machine control schemes. With a focus on spinal cord electrical stimulation and mechatronic exosuits, we will demonstrate how motor dysfunction is repaired by inducing optimal changes in neuromuscular targets. The innovative aspect is that of gaining control of the stimuli that govern neuromuscular function over time. This will enable machines to co-adapt with the body; an achievement that will disrupt the development of man-machine interfaces from neuroprostheses, to robotic limbs, to exosuits.
INTERACT will answer fundamental questions in movement neuromechanics via novel principles of human-machine interaction with broad impact on bioengineering and robotics","1500000","2019-01-01","2023-12-31"
"InterActive","Interacting with Active Particles","Jaakko Vaino Isakki TIMONEN","AALTO KORKEAKOULUSAATIO SR","Active particles refer to out-of-equilibrium self-propulsive objects such as biological microswimmers and engineered colloidal particles that can form various fascinating collective states. Active particles are easy to observe experimentally but notoriously difficult to interact with due to their fast and stochastic dynamics at both single-particle and collective state levels. In this project, I aim at scientific breakthrough in both instrumentation that allows direct interaction with active particles and using the methodology to progress substantially our understanding of dynamics and phase transitions of active particles.
The first part focuses on rendering active particles, including E. coli, C. reinhardtii and Quincke rollers, permanently magnetized and designing suitable hardware for controlling them in real time. These particles are rendered “intelligent” by programming their behavior based on real-time image analysis (long-range vision) and steering with external magnetic field. I will program these particles to reveal the limits of using local dissipative hydrodynamic near-fields to guiding active particles, and demonstrate unambiguously the extent to which a single active particle within a collective state can control the collective behaviour.
The second part aims at realizing tuneable magnetic traps and other conservative potential energy landscapes for non-magnetic active particles by using magnetophoresis in superparamagnetic fluids. I will use the technique to establishing confinement-activity phase diagrams for both biological (C. reinhardtii) and synthetic (Quincke rollers) active particles in quadratic confinements. I will further reveal the role of dimensionality (1D vs 2D vs 3D) in the phase transitions of active particles and carry out the seminal investigation of active particles in periodic potentials.
The results and methodologies will have a major impact, both immediately and in long-term, on experimental physics of active particles.","1499938","2019-01-01","2023-12-31"
"InteractiveSkin","InteractiveSkin: Digital Fabrication of Personalized On-Body User Interfaces","Jürgen STEIMLE","UNIVERSITAT DES SAARLANDES","User interfaces are moving onto the human body. However, today’s rigid and mass-fabricated devices do not conform closely to the body, nor are they customized to fit individual users. This drastically restricts their interactive capabilities. 

This project aims to lay the foundations for a new generation of body-worn UIs: interactive skin. Our approach is unique in proposing computational design and rapid manufacturing of stretchable electronics as a means to customize on-body UIs. Our vision is that laypeople design highly personalized interactive skin devices in a software tool and then print them. Interactive skin has the advantage of being very thin, stretchable, of custom geometry, with embedded sensors and output components. This allows it to be used as highly conformal interactive patches on various body locations, for many mobility tasks, leveraging the many degrees of freedom of body interaction.

This vision requires ground-breaking contributions at the intersection of on-body interaction, stretchable electronics, and digital fabrication: 1) We will contribute an automatic method to generate printable electronic layouts for interactive skin from a high-level design specification. 2) We will contribute multimodal interaction primitives that address the unique challenges of skin interaction. 3) We will develop principles for design tools that allow end-users to easily design a personalized interactive skin device. 4) We will use the newly developed methodology to realize and empirically evaluate interactive skin in unsolved application cases. 

The project will establish digital fabrication as a strong complement to existing mass-manufacturing of interactive devices. We will contribute to a deep and systematic understanding of the on-body interaction space and show how to build UIs with unprecedented body compatibility and interactive capabilities. We expect that our method will act as a key enabler for the next generation of body-UIs.","1496382","2017-01-01","2021-12-31"
"INTERCELLMED","SENSING CELL-CELL INTERACTION HETEROGENEITY IN 3D TUMOR MODELS:TOWARDS PRECISION MEDICINE","Loretta DEL MERCATO","CONSIGLIO NAZIONALE DELLE RICERCHE","This project aims to investigate the role of potassium (K+), protons (H+) and oxygen (O2) gradients in the extracellular space of tumour cells grown in 3D cultures by using a combination of imaging, cell biology and in silico analyses. By embedding ratiometric fluorescent particle-based sensors within 3D scaffolds, the changes in target analyte concentrations can be monitored and used to study the interactions between tumour cells and stromal cells in 3D tumoroids/scaffolds and to monitor response of the cells to drug treatments. I first demonstrated successful fabrication of barcoded capsules for multiplex sensing of H+, K+, and Na+ ions. Next, I demonstrated the use of pH-sensing capsules as valid real time optical reporter tools to sense and monitor intracellular acidification in living cells. Thus, I can fabricate capsule sensors for investigating the role of key analytes that are involved in regulation of crucial physiological mechanisms. In addition, I successfully integrated pH-sensing capsules within 3D nanofibrous matrices and demonstrated their operation under pH switches. INTERCELLMED will engineer 3D scaffolds that do not only sense extracellular pH but are also able to sense K+ and O2 changes. To this aim, a novel set of anisotropic analyte-sensitive ratiometric capsules will be developed and applied for generating robust and flexible capsules-embedded sensing scaffolds. To validate the functions of the 3D sensing platform, cocoltures of tumour cells and stromal cells will be grown and their interaction and response to drug treatments will be studied by mapping the K+/H+/O2 gradients in and around the cell aggregates. Finally, the 3D sensing platform will be adapted for growing tumour tissue-derived cells that will be tested ex-vivo with anticancer dugs. Specific mathematical models of cellular interactions will be developed to represent the biological processes occurring within the 3D sensing platform.","1050000","2018-02-01","2023-01-31"
"INTERDIFFUSION","Unraveling Interdiffusion Effects at Material Interfaces -- Learning from Tensors of Microstructure Evolution Simulations","Nele Marie Moelans","KATHOLIEKE UNIVERSITEIT LEUVEN","Multi-materials, combining various materials with different functionalities, are increasingly desired in engineering applications. Reliable material assembly is a great challenge in the development of innovative technologies. The interdiffusion microstructures formed at material interfaces are critical for the performance of the product. However, as more and more elements are involved, their complexity increases and their variety becomes immense. Furthermore, interdiffusion microstructures evolve during processing and in use of the device. Experimental testing of the long-term evolution in assembled devices is extremely time-consuming. The current level of materials models and simulation techniques does not allow in silico (or computer aided) design of multi-component material assemblies, since the parameter space is much too large. 
With this project, I aim a break-through in computational materials science, using tensor decomposition techniques emerging in data-analysis to guide efficiently high-throughput interdiffusion microstructure simulation studies. The measurable outcomes aimed at, are 
1) a high-performance computing software that allows to compute the effect of a huge number of material and process parameters, sufficiently large for reliable in-silico design of multi-materials, on the interdiffusion microstructure evolution, based on a tractable number of simulations, and 
2) decomposed tensor descriptions for important multi-material systems enabling reliable computation of interdiffusion microstructure characteristics using a single computer.  
If successful, the outcomes of this project will allow to significantly accelerate the design of innovative multi-materials. My expertise in microstructure simulations and multi-component materials, and access to collaborations with the top experts in tensor decomposition techniques and materials characterization are crucial to reach this ambitious aim.","1496875","2017-03-01","2022-02-28"
"INTERFACES","Mechanical modeling of interfaces in advanced materials and structures","Laura De Lorenzis","TECHNISCHE UNIVERSITAET BRAUNSCHWEIG","This proposal aims at providing fundamental research results in computational mechanical modelling of structural interfaces at different scales, encompassing debonding and contact phenomena. Interfaces are present everywhere in the physical reality, and have a deep impact in civil, mechanical and electronic engineering, biomechanics and material science. Interface mechanical problems are thus multi-disciplinary and involve several length scales. This research is then motivated by the need of transferring the knowledge on properties of interfaces at lower scales to a rational interpretation of their macroscopic behavior, which is essential for the development of truly predictive models as opposed to the empirical, phenomenological models currently available. The main objective of this project is twofold: 1) to develop a multi-scale computational setting to handle modeling of interfacial debonding and contact, featuring a seamless coupling between length scales, and 2) to use such a framework for the development of effective macroscopic mechanical models for debonding and contact, which are able to capture the information stemming from the lower-scale mechanics and morphology, and which are consistent with the observed behavior at various scales. This objective will be pursued mostly at the computational level, but also resorting to laboratory testing and analytical modeling. The project will focus on debonding and contact at macroscopic interfaces where two (or more) interfacial length scales are considered significant for the analysis and these are both (or all) much larger than atomic dimensions. Hence modeling will be conducted within the framework of continuum mechanics at all scales and implemented with the finite element method. Appropriate examples will be considered throughout the project.","1399087","2011-12-01","2016-11-30"
"INTERTRAP","Integrated absolute dating approach for terrestrial records of past climate using trapped charge methods","Alida Iulia Gabor","UNIVERSITATEA BABES BOLYAI","The practice of tuning different climate proxies prevents the observation of regional response times of terrestrial archives to global changes. Thus, it is imperative to develop correlation protocols based on absolute chronologies. Loess-palaeosol deposits are continental archives of Quaternary paleoclimates and loess is generally considered an ideal material for the application of luminescence dating. The agreement obtained for 10-20 ka ages using different techniques has given us confidence in using the state of the art measurement protocols for young deposits, as confirmed by comparison with independent age control. INTERTRAP proposes detailed investigations of loess samples from three continents collected in close proximity to the transition to the recent soil, with the purpose of obtaining a temporal quantification of the ending of the Late Tardiglacial and the beginning of the Holocene. However, a series of recent luminescence investigations carried out on quartz of different grain sizes extracted from Romanian and Serbian loess yielded severe age discrepancies for ages >~40 ka. While the cause of this observation is hitherto not fully explained, our ongoing studies on Chinese loess prove that it is a general effect, potentially affecting deposits worldwide, and raising doubts on previous chronologies. Methodological studies within INTERTRAP will develop an integrated approach using optically stimulated luminescence, thermoluminescence and electron spin resonance investigations. This part of the study aims at unravelling the mechanism responsible for the observed discrepancies and developing innovative trapped charge dating measurement protocols based on quartz that will yield reliable ages for and beyond the last interglacial glacial cycle.","1500000","2016-04-01","2021-03-31"
"INTHERM","Design, manufacturing and control of INterfaces in THERMally conductive polymer nanocomposites","Alberto Fina","POLITECNICO DI TORINO","This proposal addresses the design, manufacturing and control of interfaces in thermally conductive polymer/graphene nanocomposites. 
In particular, the strong reduction of thermal resistance associated to the contacts between conductive particles in a percolating network throughout the polymer matrix is targeted, to overcome the present bottleneck for heat transfer in nanocomposites.
The project includes the investigation of novel chemical modifications of nanoparticles to behave as thermal bridges between adjacent particles, advanced characterization methods for particle/particle interfaces and controlled processing methods for the preparations of nanocomposites with superior thermal conductivity.
The results of this project will contribute to the fundamental understanding of heat transfer in complex solids, while success in mastering interfacial properties would open the way to a new generation of advanced materials coupling high thermal conductivity with low density, ease of processing, toughness and corrosion resistance.","1404132","2015-03-01","2020-02-29"
"INTRODUCING SPRITES","Real-Time Observation of Biological Reactions Using Femtosecond 2D-IR Spectroscopy – Introducing SPRITES","Neil Terence Hunt","UNIVERSITY OF STRATHCLYDE","A fundamental question facing physical chemistry and biology is to determine the basic mechanisms by which biological molecules react and change structure. Ultrafast two dimensional infrared (2D-IR) spectroscopy has emerged as an exciting new tool for probing the structure and dynamics of bio-systems. This proposal is the first concerted application of transient 2D-IR spectroscopy to answer pressing questions relating to reactions of biological systems. The unique combination of ultrafast time resolution and structural insight makes 2D-IR the perfect platform to observe real-time structure changes during reactions. We will exploit this ability by developing SPRITES – Structure changes in Protein Reactions via Infrared Time Evolution Spectroscopy. We will trigger reactions in peptide and enzyme systems and use 2D-IR as a time-delayed probe of molecular structure to follow them in unprecedented real-time detail. This ambitious project will be split into three stages for reasons of risk management: In stage one, simple photochemical reactions of model compounds of the hydrogenase enzyme active site will be initiated and the structural evolution of the reactants followed using 2D-IR. In stage two, the novel pH-jump SPRITES technique will be developed to study peptide folding reactions. These experiments will lead to a “molecular movie” of a folding peptide in a natural environment, representing a unique scientific development. The final stage of the project will use pH-jump SPRITES to initiate and observe the reaction of a complete hydrogenase enzyme. This ambitious final stage will yield unprecedented insight into the mechanisms of biological reactions. The purpose of the proposal is to assist the PI in consolidating the independence gained through the award of a prestigious EPSRC Fellowship. The proposal seeks funding to create a research team of two postdoctoral research associates and two PhD students. This team will be managed and directed by the PI.","999745","2008-08-01","2012-07-31"
"INVARIANT","Invariant manifolds in dynamical systems and PDE","Daniel Peralta-Salas","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","""The goal of this project is to develop new techniques combining tools from dynamical systems, analysis and differential geometry to study the existence and properties of invariant manifolds arising from solutions to differential equations. These structures are relevant in the study of the qualitative properties of ODE and PDE and appear very naturally in important questions of mathematical physics. This proposal can be divided in three blocks: the study of periodic orbits and related dynamical structures of vector fields which are solutions to the Euler, Navier-Stokes or Magnetohydrodynamics equations (in the spirit of what is called topological fluid mechanics); the analysis of critical points and level sets of functions which are solutions to some elliptic or parabolic problems (e.g.
eigenfunctions of the Laplacian or Green's functions); a very novel approach based on the nodal sets of a PDE to study the limit cycles of planar vector fields. With the introduction by the Principal Investigator, in collaboration with A. Enciso, of totally new techniques to prove the existence of solutions with prescribed invariant sets for a wide range of PDE, it is now possible to approach these apparently unrelated problems using the same strategy: the construction of local solutions with robust invariant sets and the subsequent uniform approximation by global solutions. Our recent proof of a well known conjecture in topological fluid mechanics, which was popularized by the works of Arnold and Moffatt in the 1960's, illustrates the power of this method. In this project, I intend to delve into and extend the pioneering techniques that we have developed to go significantly beyond the state of the art in some long-standing open problems on invariant manifolds posed by Ulam, Arnold and Yau, among others. This project will allow me to establish an internationally recognized research group in this area at the Instituto de Ciencias Matemáticas (ICMAT) in Madrid.""","1260042","2014-01-01","2018-12-31"
"INVEST","inVEST: Foundations for a Shift from Verification to Synthesis","Jean-Francois Raskin","UNIVERSITE LIBRE DE BRUXELLES","Reactive systems are computer systems that maintain a continuous interaction with the environment in which they execute.  Examples of reactive systems are controllers embedded in cars or planes, system level software, device drivers, communication protocols, etc.  On the one hand, those systems are notoriously difficult to develop correctly (because of characteristics like concurrency, real-time constraints, parallelism, etc).  And on the other hand, their correctness is often critical as they are used in contexts where safety is an issue, or because of economical reasons related to mass production.

To ensure reliability of reactive systems, advanced verification techniques have been developed.  One particularly successful approach is model-checking.  Nevertheless, model-checking is used to find bugs in designs but it does not support the design itself.

In this project, we want to develop new algorithms and tools to support the automatic synthesis of modern reactive systems (instead of their verification a posteriori).  We aim at a shift from verification to synthesis.  To allow this shift, we need new foundations: we propose to generalize transition systems and automata – models of computation in the classical approach to verification – by the more flexible, and mathematically deeper, game-theoretic framework.  Our work will be of fundamental nature but will also aim at the development of algorithms and tools.  Those new foundations will allow for the development of a new generation of computer-aided design tools that will support the automatic synthesis of modern reactive systems and ensure correctness by construction.","1415255","2012-01-01","2017-09-30"
"InvProbGeomPDE","Inverse Problems in Partial Differential Equations and Geometry","Mikko Salo","JYVASKYLAN YLIOPISTO","Inverse problems research concentrates on the mathematical theory and practical interpretation of indirect measurements. Applications are found in virtually every research field involving scientific, medical, or industrial imaging and mathematical modelling. Familiar examples include X-ray Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). Inverse problems methods make it possible to employ important advances in modern mathematics in a vast number of application areas. Also, applications inspire new questions that are both mathematically deep and have a close connection to other sciences. This has made inverse problems research one of the most important and topical fields of modern applied mathematics.

The research team proposes to study fundamental mathematical questions in the theory of inverse problems. Particular emphasis will be placed on questions involving the interplay of mathematical analysis, partial differential equations, and Riemannian geometry. A major topic in the research programme is the famous inverse conductivity problem due to Calderón forming the basis of Electrical Impedance Tomography (EIT), an imaging modality proposed for early breast cancer detection and nondestructive testing of industrial parts. The geometric version of the Calderón problem is among the outstanding unsolved questions in the field. The research team will attack this and other aspects of the problem field, partly based on substantial recent progress due to the PI and collaborators. The team will also work on integral geometry questions arising in Travel Time Tomography in seismic imaging and in differential geometry, building on the solution of the tensor tomography conjecture in two dimensions obtained by the PI and collaborators in 2011. The research will focus on fundamental theoretical issues, but the motivation comes from practical applications and thus there is potential for breakthroughs that may lead to important advances in medical and seismic imaging.","1041240","2012-12-01","2017-11-30"
"IONOSENSE","Exploitation of Organic Electrochemical Transistors for Biological Ionsensing","Roisin Meabh Owens","ASSOCIATION POUR LA RECHERCHE ET LE DEVELOPPEMENT DES METHODES ET PROCESSUS INDUSTRIELS","In biological systems many tissue types have evolved a barrier function to selectively allow the transport of matter from the lumen to tissue beneath. Characterization of these barriers is very important as their disruption or malfunction is often indicative of toxicity/disease. The degree of barrier integrity is also a key indicator of the appropriateness of in vitro models for use in toxicology/drug screening. The advent of organic electronics has created a unique opportunity to interface the worlds of electronics and biology, using devices such as the organic electrochemical transistor (OECT), that provides a very sensitive way to detect minute ionic currents. This proposal aims to integrate the barrier function of biological systems with OECTs to yield devices that can detect minute disruptions in barrier function. Specifically, OECTs will be integrated with cell monolayers that form tight junctions and with membranes that incorporate ion channels. A disruption in tight junctions or a change in permeability of ion channels will be detected by the OECT. These devices will have unprecedented sensitivity, in a format that can be mass produced at low-cost. The potential benefits of this multidisciplinary project are numerous: It will be a vehicle for fundamental research in life sciences and the development of new in vitro models for toxicology screening of disruptive agents and the development of drugs to treat disorders linked with barrier tissue malfunction (e.g. mutations in ion channels). Moreover, through the use of various cell lines and ion channels, this platform will also lead to the engineering of new sensors and biomedical instrumentation, with a host of applications in medical diagnostics, food/water safety, homeland security and environmental protection.","1496539","2011-04-01","2016-03-31"
"IOWAGA","Interdisciplinary Ocean Wave for Geophysical and other applications","Fabrice Ardhuin","INSTITUT FRANCAIS DE RECHERCHE POUR L'EXPLOITATION DE LA MER","Ocean waves are the essential gearbox between the atmosphere and ocean and also consitute a very peculiar prism through which most satellite sensors see the ocean. IOWAGA proposes a systemic investigation of ocean waves for improving the ocean surface wave compartment of Earth system models. The project will integrate in a coherent manner exisiting and new wave-related observations from multiple sources, including remote sensing, seismic records, and in situ measurements, from climate and global scales to coastal scales and single events. Going through several cycles from observations to numerical modelling via theory and parameterizations, a consistent numerical model will be refined. This modelling tool will be exploited for multi-scale hindcasts and analyses of wave related parameters, with applications to geophysics at large, in particular remote sensing, air-sea interactions, coastal hydrodynamics and seismic studies, and practical applications with associated societal benefits (ocean energy planning and management, marine safety, pollution mitigation &amp;). The consistency between the various wave observations and the numerical modelling efforts is essential to constrain and advance better understandings of wave-related processes, to improve the accuracy of the wave-related parameters to be estimated, but also to help instrumental designs and future ocean surface remote sensing space observations. IOWAGA will be a focal point for ocean wave research, with close connection to other efforts in Europe that are focused on other compartments of the Earth system models.","1099040","2010-01-01","2013-12-31"
"IP4EC","Image processing for enhanced cinematography","Marcelo Bertalmío Barate","UNIVERSIDAD POMPEU FABRA","The objective is to develop image processing algorithms for cinema that allow people watching a movie on a screen to see the same details and colors as people at the shooting location can. It is due to camera and display limitations that the shooting location and the images on the screen are perceived very differently.

We want to be able to use common cameras and displays (as opposed to highly expensive hardware systems) and work solely on processing the video so that our perception of the scene and of the images on the screen match, without having to add artifical lights when shooting or to manually correct the colors to adapt to a particular display device.

Given that in terms of sensing capabilities cameras are in most regards better than human photoreceptors, the superiority of human vision over camera systems lies in the better processing which is carried out in the retina and visual cortex. Therefore, rather than working on the hardware, improving lenses and sensors, we will instead use, whenever possible, existing knowledge on visual neuroscience and models on visual perception to develop software methods mimicking neural processes in the human visual system, and apply these methods to images captured with a regular camera.

From a technological standpoint, reaching our goal will be a remarkable achievement which will impact how movies are made (in less time, with less equipment, with smaller crews, with more artistic freedom) but also which movies are made (since good-visual-quality productions will become more affordable.) We also anticipate a considerable technological impact in the realm of consumer video.

From a scientific standpoint, this will imply finding solutions for several challenging open problems in image processing and computer vision, but it also has a strong potential to bring methodological advances to other domains like experimental psychology and visual neuroscience.","1499160","2012-10-01","2018-03-31"
"IPES","Innovative Polymers for Energy Storage","David Mecerreyes Molero","UNIVERSIDAD DEL PAIS VASCO/ EUSKAL HERRIKO UNIBERTSITATEA","iPes project aims to provide adequate support to Dr. David Mecerreyes (DM) who is at the stage of consolidating an independent research team. During his scientific career, DM has demonstrated creative thinking and excellent capacity to carry out research and going beyond the state of the art. His meritorious record of research, scientific publications (128 ISI articles, h index = 33), project conception, private sector experience, networking ability (participated in 10 European collaborative projects) and capacity for supervising and coordinating a research team are presented in detail in the initial part of the proposal.  He recently moved from the private sector to create a new research group at the University of the Basque Country. He is now in an excellent academic position and research environment to commit and be devoted to an ERC frontier research project. DM’s proposal passed to the second stage in the ERC starting grant call of last year. This year the research project has been re-built taking into account his group directions and the detected weak points of last year’s proposal. This is his last opportunity for participating to the ERC starting-grant call.

iPes proposes an innovative research programme at the forefront of polymer chemistry. The proposal goes in depth into the topic of energetic polymers. iPes activities will fully develop the field of polymers for energy storage by using an innovative macromolecular engineering approach generating the ground for future innovations. The main S&T goal is to obtain new polymeric materials, to get an insight into their unique electronic properties, to model the new energetic polymers and to investigate their application in innovative battery prototypes.  These technologies are currently dominated by inorganic electrode materials. iPes aims at bringing polymer chemistry to a next level and developing basic knowledge about innovative polymeric materials which may open up new opportunities for Energy Storage.","1430239","2012-12-01","2018-11-30"
"IQP","Integrated quantum photonics","Jeremy Lloyd O'brien","UNIVERSITY OF BRISTOL","Quantum information science (QIS) promises fundamental insight into the workings of nature, as we gain mastery over single and coupled quantum systems, as well as a paradigm shift in information technologies. It is a pioneering field at the interface of the physical and information sciences of major international interest, with substantial investment in North America, Asia and Europe. The first quantum technology has just arrived: quantum cryptography systems are now being used commercially to provide improved communication security. However, this is just the start of the anticipated quantum revolution that promises communication networks with the ultimate security, high precision measurements and lithography, and processors with unprecedented power. The ability to simulate quantum systems is an important task which could aide the design of new materials and pharmaceuticals, and provide profound insights into the working of complex quantum systems. Low noise, high-speed transmission, and ease of manipulation make single photons model systems for exploring fundamental scientific questions, as well as a leading approach to future quantum technologies. However, current techniques are limited by a lack of high-efficiency components, integration, and single light-matter quantum systems that would allow single photon sources and deterministic non-linearities to be developed. This ERC StG project will establish a major new research direction in Europe for photonic QIS via: 1. Development of waveguide photonic quantum circuits, sources and detectors for on-chip QIS 2. Undertake the next generation of fundamental quantum physics investigations with on-chip QIS 3. Develop `atom&apos;-cavity photonic modules that can be used to generate single photons, entangle multiple photons in arbitrary ways, and detect single photons 4. Integrate waveguide photonics and photonic modules for fundamental QIS and quantum technologies","1532400","2009-10-01","2014-09-30"
"IROCSIM","Integrated high-resolution on-chip structured illumination microscopy","Niels Verellen","INTERUNIVERSITAIR MICRO-ELECTRONICA CENTRUM","Fluorescent microscopy is an indispensable tool in biology and medicine that has fueled many breakthroughs in a wide set of sub-domains.
Recently the world of microscopy has witnessed a true revolution in terms of increased resolution of fluorescent imaging techniques. To break the intrinsic diffraction limit of the conventional microscope, several advanced super-resolution techniques were developed, some of which have even been awarded with the Nobel Prize in 2014. High resolution microscopy is also responsible for the spectacular cost reduction of DNA sequencing during the last decade.
Yet, these techniques remain largely locked-up in specialized laboratories as they require bulky, expensive instrumentation and highly skilled operators.
The next big push in microscopy with a large societal impact will come from extremely compact and robust optical systems that will make high-resolution (fluorescence) microscopy highly accessible, enabling both cellular diagnostics at the point of care and the development of compact, cost-effective DNA sequencing instruments, facilitating early diagnosis of cancer and other genomic disorders.
IROCSIM will facilitate this next breakthrough by introducing a novel high-resolution imaging platform based entirely on an intimate marriage of active on-chip photonics and CMOS image sensors.
This concept will completely eliminate the necessity of standard free-space optical components by integrating specially designed structured optical illumination, illumination modulation, an excitation filter and an image sensor in a single chip.
The resulting platform will enable high resolution, fast, robust, zero-maintenance, and inexpensive microscopy with applications reaching from cellomics to DNA sequencing, proteomics, and highly parallelized optical biosensors.","1499625","2019-01-01","2023-12-31"
"IRON","Robust Geometry Processing","Pierre Alliez","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","Digital Geometry Processing (DGP) started nearly ten years ago on the premise that geometry would soon become the fourth type of digital medium after sounds, images, and video. While recent research efforts have successfully established some theoretical and algorithmic foundations to deal with this very special  signal  that is geometry, DGP has not resulted in the expected societal and technological impacts that Digital Signal Processing has generated, mostly due to the lack of robustness and genericity of the geometry processing pipeline. We propose a research agenda to harness the full potential of Digital Geometry Processing and make it as robust and impactful as Digital Signal Processing. Specifically, we argue that streamlining the DGP pipeline cannot be achieved by direct adaptation of existing machinery: a new and focused research phase is required to address such fundamental issues as the reconstruction and approximation of complex shapes from heterogeneous data, in order to develop ironclad techniques that are robust to defect-laden inputs and offer strong guarantees on the outputs. Only then can DGP will be ready, as promised, to bring forth a technological revolution.","1370198","2011-01-01","2015-12-31"
"ISMAGiC","Ice ages, Sea level, and Magmatism: Coupled oscillations","Richard Foa Katz","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","There is widespread recognition of the connectivity of different components of the Earth system, but many of these connections have not been studied. This is certainly true of connections between climate and the solid Earth. A thorough understanding of the climatic variations recorded in the geologic record cannot be obtained by studying climate in isolation from the solid Earth, and a complete understanding of the volcanic record requires consideration of the effects of climate variation.  This is a proposal to investigate the coupling between climate and the solid Earth, and hence to better understand climate history and its impact on volcanism.  The proposed work will use computational models of two-phase magma/mantle dynamics and petrology to explore links between glacial cycles and mid-ocean ridge volcanism.  Glacial cycles redistribute water between the oceans and continents, changing sea level and hence varying the load on mid-ocean ridges.  Melting beneath ridges responds to pressure changes, and should produce observable variation in crustal thickness and concentration of incompatible elements. Carbon dioxide is one such incompatible element, and pressure-induced variations in out-gassing rate from the mid-ocean ridge system to the climate system may provide the negative feedback that gives rise to glacial oscillations.  The plausibility of this hypothesis depends on details of the response functions of the coupled systems.  The proposed research group will develop a set of independent but synergistic projects that employ computational simulation to assess these responses, make testable geochemical and geophysical predictions, and validate models against observational data. This investigation has the potential to transform our understanding of mid-ocean ridge volcanism and of Quaternary ice ages.","1358793","2012-01-01","2017-12-31"
"ISOBIO","Isogeometric Methods for Biomechanics","Alessandro Reali","UNIVERSITA DEGLI STUDI DI PAVIA","Computational Mechanics and the numerical analysis of the structural behavior are becoming a more and more fundamental tool for the engineering design process in many different fields. This is particularly true in Biomechanics, nowadays widely recognized as a fundamental research field, where reliable analyses of structures and fluids (and of their interactions) are often needed on complex geometries described by tools of Computational Geometry.
Isogeometric Analysis (IGA) is a recent (2005) idea proposed to bridge the gap between Computational Mechanics and Computer Aided Design (CAD). The key feature of IGA is to extend the Finite Element Method (FEM) representing geometry by functions, such as Non-Uniform Rational B-Splines (NURBS), which are used by CAD systems, and then invoking the isoparametric concept to define field variables. Thus, the computational domain exactly reproduces the CAD description of the physical domain. Moreover, numerical testing in different situations shows that IGA holds great promises, with a substantial increase in the accuracy-to-computational-effort ratio with respect to standard FEM, also thanks to the high regularity properties of the employed functions.
The fact that IGA is very accurate and with a great potential for better integrating analysis with geometry makes it particularly suitable for the simulation of Biomechanics systems, where the approximation of complicate morphologies is a key issue to go along with the reliability of the numerical results. Therefore, the objective of ISOBIO is to construct an analysis tool, based on the peculiar features of IGA, to perform simulations of complex biomechanical systems (such as arteries, stents, aortic valves, etc.) which can be successfully used for biomedical device design as well as in clinical decision process.","1195200","2010-11-01","2015-10-31"
"ISOBOREAL","Towards Understanding the Impact of Climate Change on Eurasian Boreal Forests: a Novel Stable Isotope Approach","Katja Teresa RINNE-GARMSTON","LUONNONVARAKESKUS","The vast boreal forests play a critical role in the carbon cycle. As a consequence of increasing temperature and atmospheric CO2, forest growth and subsequently carbon sequestration may be strongly affected. It is thus crucial to understand and predict the consequences of climate change on these ecosystems. Stable isotope analysis of tree rings represents a versatile archive where the effects of environmental changes are recorded. The main goal of the project is to obtain a better understanding of δ13C and δ18O in tree rings that can be used to infer the response of forests to climate change. The goal is achieved by a detailed analysis of the incorporation and fractionation of isotopes in trees using four novel methods: (1) We will measure compound-specific δ13C and δ18O of leaf sugars and (2) combine these with intra-annual δ13C and δ18O analysis of tree rings. The approaches are enabled by methodological developments made by me and ISOBOREAL collaborators (Rinne et al. 2012, Lehmann et al. 2016, Loader et al. in prep.). Our aim is to determine δ13C and δ18O dynamics of individual sugars in response to climatic and physiological factors, and to define how these signals are altered before being stored in tree rings. The improved mechanistic understanding will be applied on tree ring isotope chronologies to infer the response of the studied forests to climate change. (3) The fact that δ18O in tree rings is a mixture of source and leaf water signals is a major problem for its application on climate studies. To solve this we aim to separate the two signals using position-specific δ18O analysis on tree ring cellulose for the first time, which we will achieve by developing novel methods. (4) We will for the first time link the climate signal both in leaf sugars and annual rings with measured ecosystem exchange of greenhouse gases CO2 and H2O using eddy-covariance techniques.","1814610","2018-01-01","2022-12-31"
"isoineqintgeo","Isoperimetric Inequalities and Integral Geometry","Franz Ewald Schuster","TECHNISCHE UNIVERSITAET WIEN","""Among several trends in convex geometric analysis, two have undergone an explosive development in recent years: the theory of affine isoperimetric and analytic inequalities, and the enhanced understanding of fundamental concepts of the subject as a whole lent by the theory of valuations. The proposal concerns both of these trends.

The connections between convex body valued valuations and isoperimetric inequalities (like, the Petty projection inequality or affine Sobolev inequalities and their Lp extensions) have attracted the interest of first-rate research groups in the world. However, the underlying bigger picture behind these strong relations has yet to be discovered. A goal of the proposed research program is to systematically exploit the """"valuations point of view"""" to reshape not only the way (affine) isoperimetric inequalities are thought of and applied but also the way these powerful inequalities are established.

Through the introduction of new algebraic structures on the space of translation invariant scalar valued valuations substantial inroads have been made towards a fuller understanding of the integral geometry of groups acting transitively on the sphere. An aim of the proposed program is to introduce a corresponding algebraic machinery in the theory of convex body valued valuations which would provide the means to attack long standing major open problems in the area of affine isoperimetric inequalities.

It is the PI's strong belief that over the next years it will become clear that many classical inequalities from affine geometry hold in a much more general setting than is currently understood. This will not only lead to the discovery of new inequalities but also should reveal the full strength of affine inequalities compared to their counterparts from Euclidean geometry. The proposed research goals of this ERC grant proposal would therefore represent a huge step towards advancing these developments that will alter two main subjects at the same time.""","982461","2012-11-01","2017-10-31"
"ISORI","Ion Spectroscopy of Reaction Intermediates","Jana Roithova","UNIVERZITA KARLOVA","Modern chemistry experiences a fast development of new reactions with dominance in organometallics and recently also organocatalysis. The massive synthetic progress however greatly foreruns mechanistic studies and the deeper insight is often rather limited. This large unexplored area accordingly challenges pioneering research and formulation of new concepts in chemistry. The present research project uses the most powerful tools of several research disciplines and aims towards the investigation of the elementary steps in organic reactions by means of mass spectrometry (MS) in combination with electrospray ionization (ESI) and quantum chemistry with a particular focus on ion spectroscopy.
The research will concentrate on elementary reactions in catalysis, e.g. the interaction of catalysts with substrates or bimolecular reactions of reactant/catalyst complexes. A major innovative contribution consists in applying ion spectroscopy for the structural characterization of reaction intermediates using a newly proposed tandem mass spectrometer with a cooled linear ion trap, which will allow two-photon experiments with IR and UV tunable lasers. The experiments will provide specific information about various intermediates and will help to disentangle even complicated mixtures or isomeric ions. In addition, an innovative experiment is designed, in which bimolecular reactivity of isobaric ions will be studied individually. Kinetics of selected reactions in solution will also be followed by ESI/MS. The combined efforts of these different approaches will provide a comprehensive understanding of the reaction mechanisms and will lead to the formulation of new general concepts in organic and organometallic reactivity.","1294800","2011-01-01","2015-12-31"
"ISOSYC","Initial Solar System Composition and Early Planetary Differentiation","Vinciane Chantal A Debaille","UNIVERSITE LIBRE DE BRUXELLES","Meteorites are privileged witnesses of solar system accretion processes and early planetary evolution. Short-lived radioactive chronometers are particularly adapted in dating and understanding these early differentiation processes. This proposal is dedicated to two main questions: (1) what is the initial composition of the solar system and terrestrial planets?; (2) having refined these parameters, how and when silicate bodies differentiated?
Among short-lived chronometers, the system 146Sm-142Nd is particularly adapted to solve these questions. While it is generally assumed that the global bulk composition of Earth and other terrestrial planets is chondritic for refractory elements such as Sm and Nd, it has recently been shown that the 142Nd/144Nd values display a systematic and reproducible bias between all the chondrites and the average composition of the Earth, and also possibly of other planets. Several hypotheses have been proposed: (i) there is an enriched reservoir hidden deep in Earth, with a composition balancing the currently observed terrestrial composition in order to get a global chondritic composition for the Earth. (ii) The Earth and other terrestrial planets are non-chondritic for their composition in refractory elements. (iii) Nucleosynthetic anomalies have modified the isotopic composition measured in chondrites. (iv) The starting parameters of the 146Sm-142Nd system are not well defined. However, this last point has never been carefully evaluated.
The main scientific strategy of this proposal is based on reinvestigating with the best precision ever achieved the starting parameters of the 146Sm-142Nd systematic using the oldest objects of the solar system: Ca-Al inclusions and chondrules. The final goal of the present proposal is to determine if Earth and other planets are chondritic or not, and to understand the implications of their refined starting composition on their geological evolution in terms of early planetary differentiation.","1485299","2014-03-01","2019-02-28"
"ITERQCD","Iterative solution of the  QCD perturbative expansion","Charalampos Anastasiou","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Particle physics phenomenology is in a revolutionary phase, due to breakthroughs in understanding
the perturbative expansion of gauge theory amplitudes. Latest research hints to the
existence of an unknown iterative structure. We aim to solve loop amplitudes at an arbitrary
perturbative order in QCD from amplitudes at lower orders and eventually from the leading
order.
We will obtain accurate theoretical predictions for cross-sections of important LHC processes,
leading to a preciser extraction of coupling strengths for new particle interactions that
may be discovered at the LHC.","995300","2010-12-01","2015-11-30"
"iTools4MC","Hypervalent Iodine Reagents: A Tool Kit for Accessing Molecular Complexity","Jérôme Waser","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","""Against the backdrop of an ever-expanding world population and increasingly limited resources, progress in chemistry, and organic chemistry in particular, is essential for the future of humanity. In the last century, transition metal chemistry has completely changed the field of synthesis. Nevertheless, it is often based on rare and toxic metals. Traditional organic chemistry, on the other hand, makes use of cheap and innocuous organic molecules but at the cost of more limited reactivity. Herein, we propose to design new hypervalent iodine reagents, which will combine the high reactivity of metals with the lower toxicity and cost of main group elements while opening new horizons for the synthesis of organic molecules.
The most important impact of the project will be to accelerate the innovative circle of progress, especially for research in medicinal chemistry. An extremely useful toolbox, an """"iKit"""", will become available for medicinal chemists. The optimal outcome would be a """"magic iodine bullet"""", which the chemist can use to install a chemical functional group on an organic molecule of his or her choice.
An added impact of the project will be greater understanding of the reactivity of hypervalent iodine reagents and their interplay with metal catalysts, leading to unforeseen applications. This understanding can lead to the development of reactions catalytic in iodine, which can be useful not only for research, but also for the larger scale production of chemicals.
Based on the successful outcome of this project, an unlimited number of organic transformations will be possible in the future. Applications will not be solely limited to synthetic chemistry, as there exists the possibility for emergent development of other well-defined reagents tailored to meet the needs of scientists in chemical biology and materials science.""","1500000","2014-01-01","2018-12-31"
"ITOP","Integrated Theory and Observations of the Pleistocene","Michel Crucifix","UNIVERSITE CATHOLIQUE DE LOUVAIN","There are essentially two approaches to climate modelling. Over the past decades, efforts to understand climate dynamics have been dominated by computationally-intensive modelling aiming to include all possible processes, essentially by integrating the equations for the relevant physics. This is the bottom-up approach. However, even the largest models include many approximations and the cumulative effect of these approximations make it impossible to predict the evolution of climate over several tens of thousands of years. For this reason a more phenomenological approach is also useful. It consists in identifying coherent spatio-temporal structures in the climate time-series in order to understand how they interact. Theoretically, the two approaches focus on different levels of information and they should be complementary. In practice, they are generally perceived to be in philosophical opposition and there is no unifying methodological framework. Our ambition is to provide this methodological framework with a focus on climate dynamics at the scale of the Pleistocene (last 2 million years). We pursue a triple objective (1) the framework must be rigorous but flexible enough to test competing theories of ice ages (2) it must avoid circular reasonings associated with ``tuning&apos;&apos; (3) it must provide a credible basis to unify our knowledge of climate dynamics and provide a state-of-the-art ``prediction horizon&apos;&apos;. To this end we propose a methodology spanning different but complementary disciplines: physical climatology, empirical palaeoclimatology, dynamical system analysis and applied Bayesian statistics. It is intended to have a wide applicability in climate science where there is an interest in using reduced-order representations of the climate system.","1047600","2009-09-01","2014-08-31"
"iTPX","In-cavity thermophotonic cooling","Jani Erkki Oksanen","AALTO KORKEAKOULUSAATIO SR","Thermophotonic (TPX) coolers and generators based on electroluminescent (EL) cooling have the potential to enable a high efficiency replacement for thermoelectric devices. Highly optimized TPX devices can even outperform modern compressor based household refrigerators and heat pumps, enabling a significant reduction in the global energy consumption of cooling and heating. While the EL cooling phenomenon is theoretically well understood, it was only very recently demonstrated for the first time under very small power conditions. Enabling high power EL cooling, however, will require a breakthrough in reducing the losses present in conventional light emitting diodes (LED).

iTPX aims to enable this breakthrough by developing an alternative approach to enhance the efficiency of light emission. The approach is based on enclosing the emitter-absorber pair used in TPX in a single semiconductor structure forming an optical cavity. This enhances the light emission rate by an order of magnitude and provides a substantial increase in the efficiency as well as several other technical and fundamental benefits. The main goal of iTPX is to demonstrate high power EL cooling for the first time and to provide quantitative insight on the limitations and possibilities of the cavity-based approach.  Recent studies have shown extremely high – over 99 % – internal and external quantum efficiencies of light emission from optically pumped semiconductor structures. This suggests that the material quality of common III-V compound semiconductors is perfectly sufficient for EL cooling if similarly performing electrically injected structures can be fabricated in the single cavity configuration.","1981250","2015-10-01","2020-09-30"
"IWASAWA","Iwasawa theory of p-adic Lie extensions","Otmar Venjakob","RUPRECHT-KARLS-UNIVERSITAET HEIDELBERG","One of the most challenging topics in modern number theory is the mysterious relation between special values of L-functions and Galois cohomology: they are the “shadows” in the two completely different worlds of complex and p-adic analysis of one and the same geometric object, viz the space of solutions for a given diophantine equation over the integral numbers, or more generally a motive M. The main idea of Iwasawa theory is to study manifestations of this principle such as the class number formula or the Birch and Swinnerton Dyer Conjecture simultaneously for whole p-adic families of such motives, which arise e.g. by considering towers of number fields or by (Hida) families of modular forms. The aim of this project is to supply further evidence for I. the existence of p-adic L-functions and for main conjectures in (non-commutative) Iwasawa theory, II. the (equivariant) epsilon-conjecture of Fukaya and Kato as well as III. the 2-variable main conjecture of Hida families. In particular, we hope to construct the first genuine “non-commutative” p-adic L-function as well as to find (non-commutative) examples fulfilling the expectation that the epsilon-constants, which are determined by the functional equations of the corresponding L-functions, build p-adic families themselves. In the third item a systematic study of Lie groups over pro-p-rings and Big Galois representations is planned with applications to the arithmetic of Hida families.","500000","2008-07-01","2013-06-30"
"JELLY","Biomolecular Hydrogels – from Supramolecular Organization and Dynamics to Biological Function","Ralf Peter Richter","ASOCIACION CENTRO DE INVESTIGACION COOPERATIVA EN BIOMATERIALES- CIC biomaGUNE","Certain proteins and glycans self-organize in vivo into soft and strongly hydrated, dynamic and gel-like supramolecular assemblies. Among such biomolecular hydrogels are the jelly-like matrix that is formed around the egg during ovulation, mucosal membranes, slimy coats produced by bacteria in biofilms, and the nuclear pore permeability barrier.
Even though biomolecular hydrogels play crucial roles in many fundamental biological processes, there is still a very limited understanding about how they function. Our goal is to assess and to understand the relation between the organizational and dynamic features of such supramolecular assemblies, their physicochemical properties, and the resulting biological functions. We will investigate these relationships directly on the supramolecular level, a level that - for this type of assemblies - is hardly accessible with conventional approaches.
To this end, we use purpose-designed in vitro model systems that are well-defined in the sense that their composition and supramolecular structure can be controlled and interrogated. These tailor-made models, together with a toolbox of surface-sensitive in situ analysis techniques, permit tightly controlled and quantitative experiments. Combined with polymer physics theory, the experimental data allow us to directly test existing hypotheses and to formulate new hypotheses that can be further tested in complementary molecular and cell-based assays.
This project focuses on two types of biomolecular hydrogels: (i) the nuclear pore permeability barrier, a nanoscopic protein meshwork that regulates all macromolecular transport into and out of the nucleus of eukaryotic cells, and (ii) extracellular hydrogel-like matrices that are scaffolded by the polysaccharide hyaluronan and that are of prime importance in a wide range of physiological and pathological processes including inflammation, fertilization and osteoarthritis.","1497167","2012-12-01","2018-02-28"
"JSPEC","Josephson Junction Spectroscopy of Mesoscopic Systems","Caglar Ozgun Girit","COLLEGE DE FRANCE","Spectroscopy is a powerful tool to probe matter.  By measuring the spectrum of elementary excitations, one reveals the symmetries and interactions inherent in a physical system.  Mesoscopic devices, which preserve quantum coherence over lengths larger than the atomic scale, offer a unique possibility to both engineer and investigate excitations at the single quanta level.  Unfortunately, conventional spectroscopy techniques are inadequate for coupling radiation to mesoscopic systems and detecting their small absorption signals.  I propose an on-chip, Josephson-junction based spectrometer which surpasses state-of-the-art instruments and is ideally suited for probing elementary excitations in mesoscopic systems.  It has an original design providing uniform wideband coupling from 2-2000 GHz, low background noise, high sensitivity, and narrow linewidth.

I describe the operating principle and design of the spectrometer, show preliminary results demonstrating proof-of-concept, and outline three experiments which exploit the spectrometer to address important issues in condensed matter physics.  The experiments are: measuring the lifetime of single quasiparticle and excited Cooper pair states in superconductors, a topic relevant for quantum information processing; determining whether graphene has a bandgap, a fundamental yet unresolved question; and recording a clear spectroscopic signature of Majorana bound states in topological superconductor weak links.

Various applications of the superconducting circuits developed for the spectrometer include a Josephson vector network analyzer, a cryogenic mixer, a THz camera, a detector for radioastronomy, and a scanning microwave impedance microscope.  In itself the proposed JJ spectrometer is a general purpose tool that will benefit researchers studying mesoscopic systems.  Ultimately, Josephson junction spectroscopy should not only be useful to detect existing elementary excitations but also to discover new ones.","1997498","2015-04-01","2020-03-31"
"KAONLEPTON","Precision Lepton Flavour Conservation Tests in Kaon Decays","Evgueni Goudzovski","THE UNIVERSITY OF BIRMINGHAM","""A unique and innovative test of a cornerstone principle of the Standard Model of particle physics, the Lepton Favour (LF) conservation, is proposed in the framework of the NA62 experiment at CERN. The search for nine decay modes of the charged kaon and the neutral pion forbidden in the Standard Model by LF conservation will be carried out at a record sensitivity of one part in a trillion. Such sensitivity will be achieved due to the uniquely intense kaon beam that will become available to the experiment in 2014, as well as a range of novel particle detection technologies employed. The collection of the LF violating decay candidates will take place in """"parasitic"""" mode alongside main NA62 data taking, which guarantees the feasibility, high data quality and cost-effectiveness. The project will bridge a significant research gap that has developed due to the absence of dedicated LF projects in the kaon sector, in sharp contrast with B-meson, lepton and neutrinoless double beta decay experiments. Any observed LF violating process will unambiguously point to physical phenomena beyond the Standard Model description, and will thus represent a major discovery. The Standard Model extensions that will be probed include those involving heavy Majorana neutrinos and R-parity breaking supersymmetry. Entire classes of new physics models will be confirmed, rigorously constrained or eliminated.""","1617546","2014-01-01","2019-06-30"
"KAPIBARA","Homotopy Theory of Algebraic Varieties and Wild Ramification","Piotr ACHINGER","INSTYTUT MATEMATYCZNY POLSKIEJ AKADEMII NAUK","The aim of the proposed research is to study the homotopy theory of algebraic varieties and other algebraically defined geometric objects, especially over fields other than the complex numbers. A noticeable emphasis will be put on fundamental groups and on K(pi, 1) spaces, which serve as building blocks for more complicated objects. The most important source of both motivation and methodology is my recent discovery of the K(pi, 1) property of affine schemes in positive characteristic and its relation to wild ramification phenomena. 

The central goal is the study of etale homotopy types in positive characteristic, where we hope to use the aforementioned discovery to yield new results beyond the affine case and a better understanding of the fundamental group of affine schemes. The latter goal is closely tied to Grothendieck's anabelian geometry program, which we would like to extend beyond its usual scope of hyperbolic curves.

There are two bridges going out of this central point. The first is the analogy between wild ramification and irregular singularities of algebraic integrable connections, which prompts us to translate our results to the latter setting, and to define a wild homotopy type whose fundamental group encodes the category of connections.

The second bridge is the theory of perfectoid spaces, allowing one to pass between characteristic p and p-adic geometry, which we plan to use to shed some new light on the homotopy theory of adic spaces. At the same time, we address the related question: when is the universal cover of a p-adic variety a perfectoid space? We expect a connection between this question and the Shafarevich conjecture and varieties with large fundamental group.

The last part of the project deals with varieties over the field of formal Laurent series over C, where we want to construct a Betti homotopy realization using logarithmic geometry. The need for such a construction is motivated by certain questions in mirror symmetry.","1007500","2019-06-01","2024-05-31"
"KARSD","Ar/Ar and K/Ar geochronology by stepwise dissolution","Pieter Vermeesch","UNIVERSITY COLLEGE LONDON","Isotopic closure is typically considered to be governed by temperature controlled volume diffusion. However, theoretical considerations as well as experimental evidence suggest that fluid-induced metamorphic recrystallisation may be orders of magnitude more important than thermal diffusion in many if not most field settings. This simple concept may explain the irregular release spectra observed in many stepwise heating experiments. The proposed research will develop a radically new approach to argon geochronology, inspired by a technological breakthrough which occurred in U-Pb geochronology in the mid-1990s. At that time, it was found that the discordance of zircons suffering from common Pb or apparent Pb-loss is greatly reduced by stepwise dissolution in hydrofluoric acid. Acid etching may be equally effective at removing compositionally distinct zones in other minerals as well. In fact, several workers successfully removed excess argon from plagioclase and K-feldspar by partially dissolving them in acid during the 1980s. The proposed research will revisit and extend these earlier experiments. The idea is to subject several aliquots of a well-characterised mineral separate to different degrees of dissolution. Plotting the 40Ar/39Ar ages of these aliquots against their respective degrees of dissolution will yield an age spectrum just like those obtained by stepwise heating experiments. Alternatively, by measuring the Ar content in absolute abundance units and determining the K-content of the acid, an age spectrum can be obtained without the need for neutron irradiation. Thus, the stepwise dissolution technique has the potential to revive conventional K-Ar geochronology and solve the problems of excess argon and non-Arrhenian diffusion behaviour that have plagued the 40Ar/39Ar community for decades.","580922","2011-07-01","2016-10-31"
"KINPOR","First principle chemical kinetics in nanoporous materials","Veronique Van Speybroeck","UNIVERSITEIT GENT","The design of an optimal catalyst for a given process is at the heart of what chemists do, but is in many times more an art than a science. The quest for molecular control of any, either existing or new, production process is one of the great challenges nowadays. The need for accurate rate constants is crucial to fulfil this task. Molecular modelling has become a ubiquitous tool in many fields of science and engineering, but still the calculation of reaction rates in nanoporous materials is hardly performed due to major methodological bottlenecks. The aim of this proposal is the accurate prediction of chemical kinetics of catalytic reactions taking place in nanoporous materials from first principles. Two types of industrially important nanoporous materials are considered: zeotype materials including the standard alumino-silicates but also related alumino-phosphates and the fairly new Metal-Organic Frameworks (MOFs). New physical models are proposed to determine: (i) accurate reaction barriers that account for long range host/guest interactions and (ii)the preexponential factor within a harmonic and anharmonic description, using cluster and periodic models and by means of static and dynamic approaches. The applications are carefully selected to benchmark the influence of each of the methodological issues on the final reaction rates. For the zeotype materials, reactions taking place during the Methanol-to-Olefin process (MTO) are chosen. A typical MTO catalyst is composed of an inorganic cage with essential organic compounds interacting as a supramolecular catalyst. For the hybrid materials, firstly accurate interaction energies between xylene based isomers and MOF framework, will be determined. The outcome serves as a step-stone for the study of oxidation reactions. This proposal creates perspectives for the design of tailor made catalyst from the molecular level.","1150000","2010-01-01","2014-12-31"
"KISMOL","Kinetics in Soft Molecular Layers
- from interstellar ices to polymorph control","Hermina Margaretha Cuppen","STICHTING KATHOLIEKE UNIVERSITEIT","This project centers around the investigation of molecular mobility in solid layers by a truly multidisciplinary
approach: combining the expertise from crystal growth, astrophysics, and chemistry. We aim to
answer long standing questions in the context of two cross-disciplinary applications: the formation and
evolution of interstellar ices and the solid state transition from one crystal structure — polymorph —
to another. The first is important for fundamental questions dealing with the origin of life, specifically
concerning the delivery of molecules—like H2O, CO2 and organic molecules—to habitable planets.
The second application is of great interest to the pharmaceutical industry where polymorph control is
crucial. The polymorphic form controls the solubility of the compound and is therefore key in dose
determination.
The goal of the investigation is to obtain an understanding of mobility in molecular layers on the
molecular level in order to (i) understand the processes in interstellar ices leading to the meeting of two
reactive species, (ii) identify the trapping mechanisms in interstellar ices, (iii) predict which molecules can survive in ices in the harsh environment of star and planet forming regions, (iv) determine which processes are fundamental to polymorphic conversion, and (v) design a way to inhibit or promote polymorphic conversion. I propose to study the mobility in molecular layers This project centers around the investigation of molecular mobility in solid layers by a truly multidisciplinary approach: combining the expertise from crystal growth, astrophysics, and chemistry. We aim to answer long standing questions in the context of two cross-disciplinary applications: the formation and evolution of interstellar ices and the solid state transition from one crystal structure - polymorph -to another. The first is important for fundamental questions dealing with the origin of life, specifically concerning the delivery of molecules -like H2O, CO2 and organic molecules - to habitable planets. The second application is of great interest to the pharmaceutical industry where polymorph control is crucial. The polymorphic form controls the solubility of the compound and is therefore key in dose determination.

The goal of the investigation is to obtain an understanding of mobility in molecular layers on the molecular level in order to (i) understand the processes in interstellar ices leading to the meeting of two reactive species, (ii) identify the trapping mechanisms in interstellar ices, (iii) predict which molecules can survive in ices in the harsh environment of star and planet forming regions, (iv) determine which processes are fundamental to polymorphic conversion, and (v) design a way to inhibit or promote polymorphic conversion. I propose to study the mobility in molecular layers using a combination of simulation techniques. The fundamental difficulty is to cover processes that take place over a large range of timescales: from picoseconds to years. Advances in numerical simulations have only recently made this research possible.  Using Molecular Dynamics and Monte Carlo simulations we will study the interactions and processes in molecular layers on different lengthscales and covering a timescale range of roughly 20 orders of magnitude.

This ambitious research project will be carried out in the Institute for Molecules and Materials at the Radboud University in Nijmegen, but will also benefit from existing and new collaborations with local, national and international colleagues.","1500000","2011-02-01","2016-01-31"
"L-SID","Light and sound waves in silicon and nonlinear glass waveguides","Avinoam Zadok","BAR ILAN UNIVERSITY","The interplay of light and sound waves in matter has attracted the attention of researchers for decades and has found many technological applications. Photonic integrated circuits (PICs) provide an exciting playground for such investigations, due to wavelength-scale guiding structures, periodicity in one or two dimensions, and high-quality resonance structures. The objectives of this proposal are to introduce, investigate and employ interactions between guided optical modes and hyper-sonic acoustic waves, within PICs in silicon and in chalcogenide glass media. Both these platforms are extremely important: silicon for its potential for integration of photonics and digital micro-electronics and mature fabrication technology, and chalcogenides for their unique nonlinear-optical and photo-sensitive properties. However, the introduction of hyper-sonic acoustic waves to both materials is highly challenging, due to the absence of piezoelectricity.
To address these challenges, this project is based on developing and validating two alternative methods for the generation of high-frequency acoustic waves. First, photo-acoustic absorption of intense, ultrafast laser pulses by periodic, metallic patterns will be employed. The technique is being used in bulk silicon substrates, and will be carried over and adapted for use in silicon and chalcogenide glass PICs. Second, carefully controlled stimulated Brillouin scattering (SBS) processes will be used to excite acoustic waves along chalcogenide PICs in a highly localized fashion.
Prospective outcomes include new fundamental insights into the opto-mechanical properties of materials, films and periodic structures; novel functionalities of silicon and chalcogenide PICs, such as acousto-optic modulation, dynamic gratings and elasto-optic super-lattices; new types of sensors, such as chip-level distributed measurements of strain, temperature and modal profile; and a first look at non-local behaviour of SBS.","1496944","2016-04-01","2021-03-31"
"L3VISU","Life Long Learning for Visual Scene Understanding (L3ViSU)","Christoph Lampert","INSTITUTE OF SCIENCE AND TECHNOLOGYAUSTRIA","""My goal in the project is to develop and analyze algorithms that use continuous, open-ended machine learning from visual input data (images and videos) in order to interpret visual scenes on a level comparable to humans.

L3ViSU is based on the hypothesis that we can only significantly improve the state of the art in computer vision algorithms by giving them access to background and contextual knowledge about the visual world, and that the most feasible way to obtain such knowledge is by extracting it (semi-) automatically from incoming visual stimuli. Consequently, at the core of L3ViSU lies the idea of life-long visual learning.

Sufficient data for such an effort is readily available, e.g. through digital TV-channels and media-
sharing Internet platforms, but the question of how to use these resources for building better computer vision systems is wide open. In L3ViSU we will rely on modern machine learning concepts, representing task-independent prior knowledge as prior distributions and function regularizers. This functional form allows them to help solving specific tasks by guiding the solution to """"reasonable"""" ones, and to suppress mistakes that violate """"common sense"""". The result will not only be improved prediction quality, but also a reduction in the amount of manual  supervision necessary,  and the possibility to introduce more  semantics into computer vision, which has recently been identified as one of the major tasks for the next decade.

L3ViSU is a project on the interface between computer vision and machine learning. Solving it requires expertise in both areas, as it is represented in my research group at IST Austria. The life-long learning concepts developed within L3ViSU, however, will have impact outside of both areas, let it be as basis of life-long learning system with a different focus, such as in bioinformatics, or as a foundation for projects of commercial value, such as more intelligent driver assistance or video surveillance systems.""","1464712","2013-01-01","2018-06-30"
"LAA-THz-CC","Lens Antenna Arrays for Coherent THz Cameras","Nuria Llombart Juan","TECHNISCHE UNIVERSITEIT DELFT","The THz region was limited to applications in radio astronomy and space science. In recent years, THz systems have expanded into many more areas of science, defence, security, and non-destructive industrial applications. Microwave based THz cameras have demonstrated the highest sensitivity at large distances. However, their current state of the art is comparable to the first analog photographic cameras characterized by long exposition times. Two fundamental problems have to be addressed to change this situation: technologically, there is the lack of integrated coherent arrays with high power and sensitivity; and theoretically, a field representation to characterize analytically these systems is missing. 

I propose to tackle the technological problem by exploiting the coherency between small antenna arrays coupled to actuated lenses to overcome the sensitivity problem and achieve instantaneous refocusing (i.e. zooming). The proposed antenna technology is based on a recent breakthrough that I pioneered: micro-lenses excited by leaky waves with seamless integration in silicon technology. This antenna enables the fabrication of large fly’s eye cameras in just two wafers, and promises one order of magnitude better scanning performances than previous solutions. An analytical model to investigate the electromagnetic response of coherent THz arrays is the enabling tool for optimizing the camera performances. I will develop this tool by combining advance spectral antenna techniques with coherent Fourier Optics. This model will not only be used in new beamforming techniques, but also for the characterization of future THz telecommunication links.
 
This project will make the first significant strides in developing the next generation of coherent THz imaging cameras. The outcome of this project will be instrumental in pushing today's costly THz niche applications into the main stream, and possibly pole vault THz systems into the 21st century communication society.","1497500","2015-10-01","2020-09-30"
"LAB-SMART","""Lewis Acidic Borocations:  improving Suzuki couplings, Material synthesis, Alkylation and Radical Transformations""","Michael Ingleson","THE UNIVERSITY OF MANCHESTER","""Carbon-carbon bond formation is arguably the most important reaction in synthetic chemistry, exemplified by the award of five noble prizes. The most recent Nobel prize was awarded for the development of palladium catalysed cross coupling, of which Suzuki cross coupling is the most widely applied version in industry and academia and utilizes organo-boronates (RB(OR)2) as the nucleophilic component. The aims of this project are; (i) to simplify the synthesis of organo-boron compounds that are utilized in (a) Suzuki cross coupling and (b) as boron containing materials for organic electronic applications. (ii) Reduce the dependency on expensive and toxic palladium by a) extending the Friedel Crafts C-C bond forming reaction to broad scope, electrophilic trifluoromethylation and electrophilic arylation (b) generating and applying efficient iron catalysts in an iron analogue of the Suzuki Reaction.

To achieve each of our aims we will utilise the unique properties of electrophilic borocations. Previously we have used boro-cations that combine a coordinatively unsaturated and electrophilic boron centre with a ‘masked’ form of a strong base to develop fundamentally new reactivity. These borocations enabled the sequential one pot activation of a substrate by a strong Lewis acid (the boro-cation) and then release of the masked Lewis base for a subsequent step (e.g., deprotonation). This concept of a boron reagent enabling sequential reactivity by subsequent dissociation of a group is a continual theme through this proposal. This property of borocations will be combined with appropriate leaving groups on the nucleophile to tackle the important challenges outlined above. Key to expanding the synthetic utility is design of the borocation to enable the release not only of a neutral Lewis base (for direct borylation, including the synthesis of RB(OR)2) but also an anionic group (for arylation/alkenylation) or a cationic moiety (for alkylation).""","1267161","2012-10-01","2017-09-30"
"LABCHIP_MULTIPLEX","Simultaneous Detection of Multiple DNA and Protein Targets on Paramagnetic Beads Packed in Microfluidic Channels using Quantum Dots as Tracers","Martin Pumera","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The detection of DNA hybridization and protein recoginittion event (immunoassay) is very important for the diagnosis and treatment of genetic diseases, for the detection infectious agents and for reliable forensic analysis. Recent activity has focused on the development of hybridization assays that permit simultaneous determination of multiple DNA or protein targets, using optical or electrochemical coding technology, based on unique encoding properties of semiconductor crystal nanoparticle tags (quantum dots). Described multi-target bio assays were performed in batch mode, involving significant amount of steps, connected with the possibility of human error, time and reagents consuming. Lab-on-a-chip technology offers tremendous potential for obtaining desired analytical information in a simpler, faster and cheaper way compared to traditional batch/laboratory-based technology. Particularly attractive for multiple DNA and protein recognition applications (i.e. point-of-care) is the high-throughput, automation, versatility, portability, reagent/sample economy and high-performance of such micromachined devices. Overall objective of the proposed research is to create and characterize a portable microanalyzer, based on a novel advanced Lab-on-a-Chip technology with magnetic separation and end-column quantum dots tracers voltammetric detection of multiple DNA and protein targets for point-of-care , automated, high-throughput, sensitive, selective and simultaneous assays. The new micro-total analytical system will rely on coupling of microfluidic transport of samples, effective flow-through magnetic separation complementary/non-complementary DNA and protein targets and a novel chip-based voltammetric stripping detection of quantum dot tags. To successfully complete such advanced micro-total analytical system, several fundamental and practical issues will be addressed.","1400000","2010-04-01","2015-03-31"
"LaGaTYb","Exploring lattice gauge theories with fermionic Ytterbium atoms","Monika AIDELSBURGER","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","Gauge theories establish a connection between seemingly different physical areas, ranging from high-energy to condensed matter physics and topological quantum computing. Very often gauge theories are difficult to study theoretically in particular in the strongly-interacting regime, where perturbative methods are not reliable. Despite the remarkable progress offered by numerical methods, such as classical Monte Carlo simulations, the sign problem imposes severe limitations, for instance, regarding real-time dynamics. This motivates the search for alternative approaches. Recent progress in the control of engineered quantum systems has revitalized Feynmans's idea of quantum simulation, which naturally does not suffer from the sign problem because its working principle is quantum mechanical. Ultracold atoms in optical lattices have proven powerful in studying important condensed matter models and intriguing results have been achieved in simulating static background gauge fields. This establishes a link to more general gauge theories, yet these are out-of-reach due to complex requirements e.g. regarding the implementation of gauge and matter field degrees of freedom. Achieving significant progress in this direction requires a radically new approach. I propose to develop a novel experimental platform that combines two unique features: precise local control as typical for ion traps and scalability of cold-atom setups to generate advanced optical lattices with locally controllable tunnel couplings. It will facilitate the implementation of a broad class of gauge theories, so-called quantum link models, with fermionic atoms, where matter and gauge fields are interpreted as different lattice sites. The proposed model exhibits paradigmatic phenomena of quantum electrodynamics and doped Mott insulators in connection to high temperature superconductors and provides a roadmap to study more complex non-Abelian models based on the nuclear spin states of Alkaline-earth-like atoms.","1498980","2019-02-01","2024-01-31"
"LanAsCat","Lanthanides as electron Dimmer switch in organometallic catalysis","Gregory NOCTON","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Complexes containing redox non-innocent ligands have been well developed in the last decade with transition metal ions and have led to very important chemical transformations at lowest environmental and economic costs. Nonetheless examples with f-element are very rare and the field is almost empty with lanthanides. This is unfortunate since divalent lanthanides are excellent sources of single electron and would provide a good control of the ligand reduction because of strong electron correlation in these systems. Thus, this proposal aims at developing this field with organolanthanides. The synthesis of original complexes containing lanthanides, redox non-innocent ligands and transitions metals is herein proposed: the first providing reversible electron(s) source(s) (remote control), the second acting as electron(s) reservoir and controlling the electron correlation strength (“dimmer switch”), and the last being the site of the selective catalytic reaction. Because of this regulated electron transfer, the oxidation state of the transition metal will be modified only at specific steps, allowing the establishment of a new paradigm is organometallic catalysis with group 9 and 10 transition metals. These original complexes will be synthetized and deeply characterized by specific spectroscopic and theoretical analyses. The principal goal is to synthetize active catalysts toward C-H bonds activation, and methane activation is regarded as an ultimate achievement as is hydroalkylation of olefins. To increase the chances of success, the proposal is based on preliminary results obtained recently in the group. Several examples of original heterobimetallic and heterotrimetallic complexes containing lanthanides and transition metals of group 10 will be discussed as a good starting point for the feasibility of this challenging project that aims at answering a large societal concern: the reduction of atmospheric pollutants, such as methane, and transformation in valuable products.","1499929","2017-04-01","2022-03-31"
"LARGEMS","The Dynamic Composition of Protein Complexes: A New Perspective in Structural Biology","Michal Sharon","WEIZMANN INSTITUTE OF SCIENCE LTD","80% of the proteome exists in complexes or large macromolecular assemblies. It is accepted that revealing the structure of these protein complexes is a key towards mechanistic understanding of cellular processes. Yet, this might not be sufficient; a higher level of complexity probably exists and protein complexes may not be static and uniform in form and function as thought. A protein complex may actually represent an ensemble of compositionally distinct entities with functional versatility. My main aim is to provide evidence for this conceptual change and to reveal the dynamic architecture of a protein assembly. As a model system, I will investigate the COP9 signalosome (CSN), an evolutionary conserved multisubunit complex, which is involved in a variety of essential functions ranging from cell-cycle progression, DNA-repair and apoptosis. My strategy is based on a comprehensive approach, made up of four main steps; i) Revealing the structural organization of the native complex. ii) Establishing whether the complex has co-existing independent modules that function separately of, or coordinately with the holocomplex. iii) Monitoring in real-time the biogenesis and activation pathway of the complex and developing an approach for shifting its oligomerization equilibrium. iv) Determining the correlation between modularity of the complex and cell cycle progression and comparing its composition in healthy versus cancerous cells. I will integrate genetic, biochemical and structural biology approaches. In particular, I will apply a state of the art mass spectrometry technique, that will enable us to define the stoichiometry, subunit composition, dynamic interactions and structural organization of protein complexes isolated directly from the cellular environment.","1500000","2009-09-01","2014-08-31"
"LASER OPTIMAL","Laser Ablation: SElectivity and monitoRing for OPTImal tuMor removAL","Paola SACCOMANDI","POLITECNICO DI MILANO","Laser Ablation (LA) was extensively investigated for its benefits as minimally invasive thermal therapy for tumor. Despite the LA pros as potential alternative to surgical resection (e.g., use of small fiber optics, echo-endoscope procedures and image-guidance without artifact), the lack of tools for safe and patient-specific treatment restrained its clinical use. LASER OPTIMAL offers a renaissance to LA for the practical management of challenging tumors (e.g., pancreatic cancer): it investigates and develops integrated solutions to achieve an effective and selective LA, that thermally destroys the whole tumor mass, while spearing the normal tissue around. The excellent ambition of LASER OPTIMAL is to achieve and merge: a) biocompatible nanoparticles (BNPs) injected in the tumor, to enhance the selective absorption of laser light; b) patient-specific anatomy of tumor and its surrounding, extracted from clinical images, to retrieve the optimal laser settings; c) accurate, fast and real-time heat-transfer model to simulate laser-tissue-BNPs interaction, predict and visualize the treatment dynamics; d) real-time temperature measurement system to monitor LA effects, account for unpredictable physiological events and tune the settings (closed-loop). The design of ex vivo and in vivo animal tests allows assessing the system performances and driving the possible workflow re-design. Finally, human trials are envisaged to prove the significant impact of the LASER OPTIMAL paradigm. The collaboration of researchers, engineers and clinicians will drive the use of this innovative strategy in clinical routine. The research on the patient-specific system for the mini-invasive tumors removal, and the ground-breaking insights on clinical use of BNPs will strongly impact on EU healthcare system and society, by creating a novel product. This paradigm is also embeddable in existing system of industrial partner, extendable to other procedures, thus able to encourage a dedicated market.","1499575","2018-05-01","2023-04-30"
"LASER-ARPES","Laser based photoemission: revolutionizing the spectroscopy of correlated electrons","Felix Baumberger","UNIVERSITE DE GENEVE","It is proposed to develop a novel instrument for angular resolved photoelectron spectroscopy (ARPES) by combining a laser based ultraviolet light source with a state-of-the-art electron spectrometer. This combination will be unique in Europe and will push this important technique to an entirely new level of resolution, comparable to the thermal broadening at 1 K and nearly an order of magnitude lower than the resolution achievable in practical ARPES experiments with the latest synchrotron light sources. The low photon energy of this new source will also markedly enhance the bulk sensitivity of ARPES and thus enable the investigation of interesting materials that were not accessible so far. These new capabilities will be used to study the subtle quantum many-body states of correlated electrons in transition metal oxides, a frontier topic in condensed-matter physics. Specifically, we will focus on electronic instabilities in perovskites and elucidate how different degrees of freedom play together to determine the often vastly different properties of chemically closely related materials. Moreover, we will apply modern electron spectroscopy to correlated molecular solids with complex phase diagrams that challenge existing theory for satisfactory explanations. This field is largely unexplored but is fundamental for advances in molecular electronics.","1450825","2008-08-01","2013-07-31"
"LAST","Large Scale Privacy-Preserving Technology in the Digital World - Infrastructure and Applications","Yehuda Lindell","BAR ILAN UNIVERSITY","Data mining provides large benefits to the commercial, government and homeland security sectors, but the aggregation and storage of huge amounts of data about citizens inevitably leads to erosion of privacy. To achieve the benefits that data mining has to offer, while at the same time enhancing privacy, we need technological solutions that simultaneously enable data mining while preserving privacy. The current state of the art has focused on providing privacy-preserving solutions for very specific problems, and has thus taken a local perspective. Although this is an important first step in the development of privacy-preserving solutions, it is time for a global perspective on the problem that aims for providing full integrated solutions. Our goal in this research is to study privacy and develop comprehensive solutions for enhancing it in the digital era. Our proposed research project includes foundational research on privacy, an infrastructure level for achieving anonymity over the Internet, key cryptographic tools for constructing privacy-preserving protocols, and development of large-scale applications that are built on top of all of the above. The novelty of our research is in our focus on fundamental issues towards comprehensive solutions that are aimed for large-scale data sources. The project s outcome will allow migration from local solutions for specific problems that are suited for small to medium scale data sources to comprehensive privacy-preserving database and data mining solutions for large scale data warehouses. Achieving this great challenge carries immense scientific, technological and societal rewards.","1921316","2009-10-01","2014-09-30"
"LATENTCAUSES","Modelling latent causes in molecular networks","Fabian Theis","HELMHOLTZ ZENTRUM MUENCHEN DEUTSCHES FORSCHUNGSZENTRUM FUER GESUNDHEIT UND UMWELT GMBH","In systems biology, we aim at deriving gene-regulatory or signaling models based on multivariate readouts, thereby generating predictions for novel experiments. However any model only approximates reality, leaving out details or other types of regulation. Here I ask why a given model fails to predict a set of observations with acceptable accuracy and how to refine the model using this experimental knowledge. This resembles a question from signal processing, namely the blind identification of hidden (latent) variables in a mixing model. Many, powerful methods have been proposed to answer it. However, they have not been extended to dynamical systems due to the involved strong nonlinearities.
I propose to infer additional upstream species in a given model, denoted as latent causes, that improve the prediction and at the same time are subject to the model dynamics. Multiple causes are estimated using statistical assumptions such as minimum mutual information. The model estimation will be performed within a Bayesian framework. This will allow for the efficient but crucial inclusion of prior biological information. The method will be applied to infer a differentiation model describing lineage segregation of embryonic stem (ES) cells to endo- and mesoderm. Here, latent causes are known to be transcription factors and microRNAs, but also small molecules/drugs. Identified off-target effects of these causes will be validated in collaboration with experimental partners.
This study will establish links between information-theoretic signal processing and dynamical systems. Its application to a detailed ES cell model will foster our understanding of differentiation and may ultimately contribute to the development of more efficient differentiation protocols for cell replacement therapy.","1238590","2011-01-01","2016-03-31"
"LattAC","Lattices: algorithms and cryptography","Damien, Noel Stehle","ECOLE NORMALE SUPERIEURE DE LYON","Contemporary cryptography, with security relying on the factorisation and discrete logarithm problems, is ill-prepared for the future: It will collapse with the rise of quantum computers, its costly algorithms  require growing resources, and it is utterly ill-fitted for the fast-developing trend of externalising computations to the cloud. The emerging field of *lattice-based cryptography* (LBC) addresses these concerns: it resists would-be quantum computers, trades memory for drastic run-time savings, and enables computations on encrypted data, leading to the prospect of a privacy-preserving cloud economy. LBC could supersede contemporary cryptography within a decade. A major goal of this project is to enable this technology switch. I will strengthen the security foundations, improve its performance, and extend the range of its functionalities.

A lattice is the set of integer linear combinations of linearly independent real vectors, called lattice basis. The core computational problem on lattices is the Shortest Vector Problem (SVP): Given a basis, find a shortest non-zero point in the spanned lattice. The hardness of SVP is the security foundation of LBC. In fact, SVP and its variants arise in a great variety of areas, including computer algebra, communications (coding and cryptography), computer arithmetic and algorithmic number theory, further motivating the study of lattice algorithms. In the matter of *algorithm design*, the community is quickly nearing the limits of the classical paradigms. The usual approach, lattice reduction, consists in representing a lattice by a basis and steadily improving its quality. I will assess the full potential of this framework and, in the longer term, develop alternative approaches to go beyond the current limitations.

This project aims at studying all computational aspects of lattices, with cryptography as the driving motive. The strength of LattAC lies in its theory-to-practice and interdisciplinary methodological approach","1414402","2014-01-01","2018-12-31"
"LATTICE","Lattices in Computer Science","Oded Regev","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","A lattice is defined as the set of all integer combinations of $n$ linearly independent vectors in $\R^n$. These geometrical objects possess a rich combinatorial structure that has attracted the attention of great mathematicians over the last two centuries. Lattices have an impressive number of applications in mathematics and computer science, from number theory and Diophantine approximation to complexity theory and cryptography. Over the last two decades, the computational study of lattices has witnessed several remarkable discoveries. Most notable are the development of the LLL algorithm by Lenstra, Lenstra and Lovasz and Ajtai&apos;s discovery of lattice-based cryptographic constructions. I propose to pursue these research directions and attempt to discover new connections between lattices and computer science. A particular focus will be put on applications in cryptography, as these can lead to many advances in the field and are also of great practical importance. I believe that the extraordinary properties of lattices have the potential to revolutionize many other areas of computer science such as complexity, cryptography, machine learning theory, quantum computation, and more. My scientific goals include obtaining stronger and more practical lattice-based cryptographic constructions, resolving important questions regarding the complexity of lattice problems, finding sub-exponential time algorithms for lattice problems and exploring some novel applications of lattices to areas such as Markov chains and machine learning theory.","822000","2008-07-01","2012-08-31"
"LBITAC","Lower Bounds and Identity Testing for Arithmetic Circuits","Amir Benbenishty Shpilka","TEL AVIV UNIVERSITY","The focus of our proposal is on arithmetic circuit complexity. Arithmetic circuits are the most common model for computing polynomials, over arbitrary fields. This model was studied by many
researchers in the past 40 years but still not much is known on many of the basic problems concerning this model.

In this research we propose to study some of the most exciting fundamental open problems in theoretical computer science: Proving lower bounds on the size of arithmetic circuits and finding
efficient deterministic algorithms for checking identity of arithmetic circuits. Proving a strong lower bound or finding efficient deterministic algorithms to the polynomial identity testing problem are the most important problems in algebraic complexity and solving either of them will be a dramatic breakthrough in theoretical computer science.

The two problems that we intend to study are closely related to each other - there are several known results showing that a solution to one of the problems may lead to a solution to the other. Thus, we propose to study strongly related problems that lie in the frontier of algebraic complexity. Any advance will be a significant contributions to the field of theoretical computer
science.","1427485","2011-03-01","2017-02-28"
"LEAD","Lower Extremity Amputee Dynamics: Simulating the Motion of an Above-Knee Amputee’s Stump by Means of a Novel EMG-Integrated 3D Musculoskeletal Forward-Dynamics Modelling Approach","Oliver Röhrle","UNIVERSITAET STUTTGART","""Wearing sub-optimally fitted lower limb prosthesis cause disorders of the stump that strongly lessens the well-being and the performance of an amputee. As experimental measurements are currently not capable of providing enough insights in the dynamic behaviour of the stump, simulations need to be employed to achieve the necessary knowledge gain to significantly improve the socket design and, hence, to increase the amputee’s well-being and performance. The overall goal of this proposal is to provide the enabling technology in form of novel computational and experimental methodologies to assist the design process of next-generation prosthetic devices. The focus hereby is to gain a better understanding of the dynamics of the musculoskeletal system of a lower extremity amputee, here, the stump of an above-knee amputee. To achieve this, LEAD pursues two aims. The first and main aim focuses on substantially changing existing modelling philosophies and methodologies of forward dynamics approaches such that they are capable of representing muscles, bone, and skin as 3D continuum-mechanical objects.  To counteract the increase of computational cost by switching from 1D lumped-parameter models to 3D models, novel, elegant, and efficient algorithms, e.g. nested iteration techniques tuned for efficiency through model-based coupling strategies and optimised solvers, need to be developed. The second aim is to experimentally measure physical quantities that provide the necessary input to drive the forward dynamics model, e.g. EMG, and to provide means of validation, e.g. with respect to pressure measurements, ultrasound recordings, and motion capture. Given the non-existing field of forward dynamics appealing to continuum-mechanical skeletal muscle models, LEAD creates a new field of research.""","1676760","2012-11-01","2017-10-31"
"LEAP","LEarning from our collective visual memory to Analyze its trends and Predict future events","Josef Sivic","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","People constantly draw on past visual experiences to anticipate future events and better understand, navigate, and interact with their environment, for example, when seeing an angry dog or a quickly approaching car. Currently there is no artificial system with a similar level of visual analysis and prediction capabilities. LEAP is a first step in that direction, leveraging the emerging collective visual memory formed by the unprecedented amount of visual data available in public archives, on the Internet and from surveillance or personal cameras - a complex evolving net of dynamic scenes, distributed across many different data sources, and equipped with plentiful but noisy and incomplete metadata. The goal of this project is to analyze dynamic patterns in this shared visual experience in order (i) to find and quantify their trends; and (ii) learn to predict future events in dynamic scenes.
With ever expanding computational resources and this extraordinary data, the main scientific challenge is now to invent new and powerful models adapted to its scale and its spatio-temporal, distributed and dynamic nature. To address this challenge, we will first design new models that generalize across different data sources, where scenes are captured under vastly different imaging conditions. Next, we will develop a framework for finding, describing and quantifying trends that involve measuring long-term changes in many related scenes. Finally, we will develop a methodology and tools for synthesizing complex future predictions from aligned past visual experiences.
Breakthrough progress on these problems would have profound implications on our everyday lives as well as science and commerce, with safer cars that anticipate the behavior of pedestrians on streets; tools that help doctors monitor, diagnose and predict patients’ health; and smart glasses that help people react in unfamiliar situations enabled by the advances from this project.","1496736","2014-11-01","2019-10-31"
"LEASP","Learning spatiotemporal patterns in longitudinal image data sets of the aging brain","Stanley Durrleman","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","Time-series of multimodal medical images offer a unique opportunity to track anatomical and functional alterations of the brain in aging individuals. A collection of such time series for several individuals forms a longitudinal data set, each data being a rich iconic-geometric representation of the brain anatomy and function. These data are already extraordinary complex and variable across individuals. Taking the temporal component into account further adds difficulty, in that each individual follows a different trajectory of changes, and at a different pace. Furthermore, a disease is here a progressive departure from an otherwise normal scenario of aging, so that one could not think of normal and pathologic brain aging as distinct categories, as in the standard case-control paradigm.

Bio-statisticians lack a suitable methodological framework to exhibit from these data the typical trajectories and dynamics of brain alterations, and the effects of a disease on these trajectories, thus limiting the investigation of essential clinical questions. To change this situation, we propose to construct virtual dynamical models of brain aging by learning typical spatiotemporal patterns of alterations propagation from longitudinal iconic-geometric data sets.

By including concepts of the Riemannian geometry into Bayesian mixed effect models, the project will introduce general principles to average complex individual trajectories of iconic-geometric changes and align the pace at which these trajectories are followed. It will estimate a set of elementary spatiotemporal patterns, which combine to yield a personal aging scenario for each individual. Disease-specific patterns will be detected with an increasing likelihood.

This new generation of statistical and computational tools will unveil clusters of patients sharing similar lesion propagation profiles, paving the way to design more specific treatments, and care patients when treatments have the highest chance of success.","1499894","2016-09-01","2021-08-31"
"LEBMEC","Laser-engineered Biomimetic Matrices with Embedded Cells","Aleksandr Ovsianikov","TECHNISCHE UNIVERSITAET WIEN","Traditional 2D cell culture systems used in biology do not accurately reproduce the 3D structure, function, or physiology of living tissue. Resulting behaviour and responses of cells are substantially different from those observed within natural extracellular matrices (ECM). The early designs of 3D cell-culture matrices focused on their bulk properties, while disregarding individual cell environment. However, recent findings indicate that the role of the ECM extends beyond a simple structural support to regulation of cell and tissue function. So far the mechanisms of this regulation are not fully understood, due to technical limitations of available research tools, diversity of tissues and complexity of cell-matrix interactions.
The main goal of this project is to develop a versatile and straightforward method, enabling systematic studies of cell-matrix interactions. 3D CAD matrices will be produced by femtosecond laser-induced polymerization of hydrogels with cells in them. Cell embedment results in a tissue-like intimate cell-matrix contact and appropriate cell densities right from the start.
A unique advantage of the LeBMEC is its capability to alter on demand a multitude of individual properties of produced 3D matrices, including: geometry, stiffness, and cell adhesion properties. It allows us systematically reconstruct and identify the key biomimetic properties of the ECM in vitro. The particular focus of this project is on the role of local mechanical properties of produced hydrogel constructs. It is known that, stem cells on soft 2D substrates differentiate into neurons, stiffer substrates induce bone cells, and intermediate ones result in myoblasts. With LeBMEC, a controlled distribution of site-specific stiffness within the same hydrogel matrix can be achieved in 3D. This way, by rational design of cell-culture matrices initially embedding only stem cells, for realisation of precisely defined 3D multi-tissue constructs, is possible for the first time.","1440594","2013-03-01","2018-02-28"
"LEDA","The challenging quest for low-mass dark structures","Simona VEGETTI","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Using strong gravitational lensing, I will constrain with my unique modelling technique and acquired knowledge the properties of dark matter and potentially revise the current standard paradigm for the formation of all structures which is at the core of modern cosmology and galaxy formation theories. 

Numerical simulations of cosmic structure formation have shown that the amount of mass in low-mass objects depends strongly on the assumed nature of dark matter. My goal is to constrain the nature of dark matter by measuring the dark matter mass function down to ~10^6 M_sol, where the predictions from different currently viable dark matter models differ by large factors.

To this end, I will use the gravitational imaging technique, an advanced modelling tool that I have developed and pioneered, and state-of-the-art strong gravitational lensing data for 12 systems observed with cm- and mm-interferometers. At present, this is the only observational probe of low-mass structure in the dark matter distribution beyond the Local Universe. 

This will represent an important milestone in our understanding of the dark Universe and will provide a key observational test of the Cold Dark Matter model in a regime that has not been probed before. This ERC project will challenge our standard model for small-scale structure formation and will distinguish between “warm” and “cold” hypothesis for the nature of dark matter. This ERC project will have significant implications for the fields of cosmology and galaxy formation.

I am in a unique position to achieve the scientific goal here proposed. I have extended experience in studying gravitational lenses and low mass dark structures. I have an unmatched gravitational lens modelling code and high quality data. With this ERC I will build upon my previous successes and create a top-class research group for studying dark matter with gravitational lensing.","1359688","2018-02-01","2023-01-31"
"LENA","non-LinEar sigNal processing for solving data challenges in Astrophysics","Jérôme Bobin","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","Astrophysics has arrived to a turning point where the scientific exploitation of data requires overcoming challenging analysis issues, which mandates the development of advanced signal processing methods. In this context, sparsity and sparse signal representations have played a prominent role in astrophysics. Indeed, thanks to sparsity, an extremely clean full-sky map of the Cosmic Microwave Background (CMB) has been derived from the Planck data [Bobin14], a European space mission that observes the sky in the microwave wavelengths. This led to a noticeable breakthrough: we showed that the large-scale statistical studies of the CMB can be performed without having to mask the galactic centre anymore thanks to the achieved high quality component separation [Rassat14]. 
Despite the undeniable success of sparsity, standard linear signal processing approaches are too simplistic to capture the intrinsically non-linear properties of physical data. For instance, the analysis of the Planck data in polarization requires new sparse representations to finely capture the properties of polarization vector fields (e.g. rotation invariance), which cannot be tackled by linear approaches. Shifting from the linear to the non-linear signal representation paradigm is an emerging area in signal processing, which builds upon new connections with fields such as deep learning [Mallat13]. 
Inspired by these active and fertile connections, the LENA project will: i) study a new non-linear signal representation framework to design non-linear models that can account for the underlying physics, and ii) develop new numerical methods that can exploit these models. We will further demonstrate the impact of the developed models and algorithms to tackle data analysis challenges in the scope of the Planck mission and the European radio-interferometer LOFAR. We expect the results of the LENA project to impact astrophysical data analysis as significantly as deploying sparsity to the field has achieved.","1497411","2016-09-01","2021-08-31"
"Lensless","High-resolution microscopy without lenses: a new generation of imaging technology","Stefan Witte","STICHTING VU","Lensless imaging is an elegant approach to microscopy, in which a sharp image of an object is retrieved by numerical means rather than by actual optical components such as lenses. Such an approach allows high-resolution imaging using only a light source and an image sensor. Multi-wavelength phase retrieval (Witte et al., patent application PCT/NL2013/050618) provides a general framework for lensless imaging, which allows the development of surprisingly simple yet high-resolution imaging devices. By detecting wavelength-dependent differences in wave propagation, I have recently shown that amplitude and phase of an object field can be reconstructed with high contrast and resolution. This approach works for complex objects in both reflection and transmission. As such, lensless microscopes have advantages in size, cost, robustness and simplicity, and provide a powerful alternative to conventional imaging systems in many applications.
My aim with this proposal is to advance this new technology towards high-impact applications, and to forge a coordinated research program on lensless imaging technology and its applications. I will develop lensless microscopes to image many cell cultures in parallel, as well as smartphone-based lensless microscopes for low-cost point-of-care diagnostics in third-world countries. Lensless microscopy through optical fibers will be implemented as a novel tool for minimally invasive endoscopic imaging. Since the technology does not require precision optical components, I will be able to extend it to shorter wavelengths. Applications in EUV metrology and soft-X-ray imaging become feasible, such as ultra-high-resolution surface profiling of lithographic wafers.
This program provides exciting new prospects for fundamental science, industrial metrology and medical diagnostics alike. My expertise in lensless imaging and laser development provide me with an excellent starting point for the realization of such a new generation of imaging technology.","1500000","2015-05-01","2020-04-30"
"LeviTeQ","Levitated Nanoparticles for Technology and Quantum Nanophysics: New frontiers in physics at the nanoscale.","James MILLEN","KING'S COLLEGE LONDON","Technology is continuously miniaturizing. As it reaches the nanoscale we face unique challenges, such as managing thermal. From the other direction, advances in the quantum physics of a few atoms, ions, and solid-state qubits mean that we increasingly wish to scale up quantum systems, or interface them with nanoscale devices.

Opto- and electro-mechanical (NEMS and MEMS) devices have been controlled at the quantum level in recent years, an amazing advance allowing even entanglement between light and mechanical motion. However, all such systems are plagued by unavoidable environmental contact, and energy dissipation through strain, limiting the potential of mechanical devices to participate in both classical and quantum technologies.

By levitating the mechanical element, these problems are overcome. LEVITEQ will, for the first time, cool the motion and rotation of tailor-made silicon particles, enabling full quantum level control. This ultra-low dissipation system offers exquisite force sensitivity, by driving the rotation of a levitated nanorod. LEVITEQ will pioneer the control of nanoparticles by electronic circuits, allowing simple technological integration in a room temperature environment. This all-electrical system will challenge existing quartz crystal oscillator technology.

LEVITEQ will explore new regimes of physics, by working in extreme vacuum, elucidating thermodynamics on the nanoscale. This research will pave the way for a levitated quantum object acting as a node in a quantum network, for coherent signal storage and conversion.","1498018","2019-02-01","2024-01-31"
"LIA","Light Field Imaging and Analysis","Bastian Goldlücke","UNIVERSITAT KONSTANZ","One of the most fundamental challenges in computer vision is to reliably establish correspondence - how to match a location in one image to its counterpart in another. It lies at the heart of numerous important problems, for example stereo, optical flow, tracking and the reconstruction of scene geometry from several photographs. The most popular approaches to solve these problems are based on the simplification that a scene point looks the same from wherever and whenever it is observed. However, this is fundamentally wrong, since its color changes with viewing direction and illumination. This invariably leads to failures when dealing with reflecting or transparent surfaces or changes in lighting, which commonly occur in natural scenes.

We therefore propose to radically rethink the underlying assumptions and work with light fields to describe the visual appearance of a scene. Compared to a traditional image, a light field offers information not only about the amount of incident light, but also the direction where it is coming from. In effect, the light field implicitly captures scene geometry and reflectance properties. In the following, we will argue that variational algorithms based on light field data have the potential to considerably advance the state-of-the-art in all image analysis applications related to lighting-invariant robust matching, geometry reconstruction or reflectance estimation.

Since computational cameras are currently making rapid progress, we believe that light fields will soon become a focus of computer vision research. Already, commercial plenoptic cameras allow to easily capture the light field of a scene and are suitable for real-world applications, while a recent survey even predicted that in about 20 years time, every consumer camera will be a light field camera. Our research will investigate fundamental mathematical tools and algorithms which will substantially contribute to drive this development.","1466100","2014-07-01","2019-06-30"
"LIC","Loop models, integrability and combinatorics","Paul Georges Zinn-Justin","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The purpose of this proposal is to investigate new connections which
have emerged in the recent years between problems from statistical
mechanics, namely two-dimensional exactly solvable models, and a variety
of combinatorial problems, among which:  the enumeration of plane partitions,
alternating sign matrices and related objects;
combinatorial properties of certain
algebro-geometric objects such as orbital varieties or the Brauer loop scheme;
or finally certain problems in free probability. One of the key methods
that emerged in recent years is the use
of quantum integrability and more precisely the quantum Knizhnik--Zamolodchikov
equation, which itself is related to many deep results in representation theory.
The fruitful interaction between all these ideas has led to many advances
in the last few years, including proofs of some old conjectures but
also completely new results. More specifically, loop models
are a class of statistical models where the PI has made
significant progress, in particular in relation to the so-called
Razumov--Stroganov conjecture (now Cantini--Sportiello theorem).

New directions that should be pursued include:
further applications to enumerative combinatorics such as proofs of various
open conjectures relating Alternating Sign Matrices, Plane Partitions
and their symmetry classes;
a full understanding of the quantum integrability of the
Fully Packed Loop model,
a specific loop model at the heart of the Razumov--Stroganov correspondence;
a complete description of the Brauer loop scheme, including its
defining equations, and of the underlying poset; the extension
of the work on Di Francesco and Zinn-Justin on the loop model/6-vertex vertex
relation to the case of the 8-vertex model
(corresponding to elliptic solutions of the Yang--Baxter equation);
the study of solvable tilings models, in relation to
generalizations of the Littlewood--Richardson rule, and the determination
of their limiting shapes.","840120","2011-11-01","2016-10-31"
"LIE ANALYSIS","Lie Group Analysis for Medical Image Processing","Remco Duits","TECHNISCHE UNIVERSITEIT EINDHOVEN","The aim of this project is to substantially improve computer algorithms for image analysis in medical imaging. Currently available techniques often require significant application-specific tuning and have a limited application scope. This is mostly due to the use of non-generic feature spaces that involve many physical dimensions and lack mathematical foundation.

Instead, we derive inspiration from the superior generic pattern recognition capabilities of the human brain and propose a novel operator design aiming at better results and  wider applicability.
This novel operator design combines (partial and ordinary) differential equations on non-compact Lie groups (induced by stochastic processes and sub-Riemannian geometric control) with wavelet transforms. Many mathematical challenges arise in the analysis and (numerical) solutions of these operators.

The research departs from previously developed insights of the PI on 'invertible orientation scores', which can be regarded as a specific instance in a general Lie group theoretical framework. Within this general framework one obtains a comprehensive invertible score defined on a higher dimensional Lie group beyond position space. The key challenge is to appropriately exploit these scores, their survey of multiple features per position, their underlying group structure, and their invertibility. We will tackle this via left-invariant evolutions and left-invariant sub-Riemannian optimal control within the score.

The orientation score approach will be systematically extended towards multi-scale-and-orientation, multi-velocity and multi-frequency encoding and processing, widening the application scope.  Moreover, improvements in contextual enhancement via invertible scores and improvements in optimal curve extractions in the Lie group domain of the score will be pursued.
We will develop and apply the resulting algorithms to a wide range of medical imaging challenges in neurological, retinal and cardiac applications.","1267550","2014-01-01","2018-12-31"
"Life-Cycle","Life-like Supramolecular Materials based on Reaction Cycles with Designed Feedback","Thomas HERMANS","CENTRE INTERNATIONAL DE RECHERCHE AUX FRONTIERES DE LA CHIMIE FONDATION","This “Life-Cycle” ERC proposal aims to develop a new class of artificial supramolecular materials that are kept in sustained non-equilibrium states by continuous dissipation of chemical fuels. Supramolecular polymers in current artificial materials stick together through weak reversible bonds that can be exchange by thermal energy. In contrast, natural supramolecular polymers such as those in the cytoskeletal network use chemical fuels such as adenosine triphosphate (ATP) to achieve an incredible adaptivity, motility, growth, and response to external inputs. Development of chemically fueled artificial supramolecular polymers should therefore lead to more life-like materials that could perform functions so far reserved only for living beings. 
The proposed materials are based on supramolecular reaction cycles that have both positive and negative feedback in order to achieve emergent properties, such as oscillations and waves. Two different approaches are used: i) supramolecular polymers that are fueled by redox reactions, and ii) enzyme-switchable supramolecular polymers that consume one of the natural fuels, namely ATP. The proposed polymers self-assemble cooperatively, which is used as a positive feedback mechanism. Using other co-assembling species we can engineer negative feedback in our reaction cycles to obtain unique supramolecular dynamics. Since the building blocks react, but also self-assemble they have built-in chemomechanical properties, much like in living materials such as the cytoskeleton. 
First we study the temporal behavior (part A) of our reaction cycles in well-stirred environments. Next, we move to non-stirred conditions (part B), where spatiotemporal behavior can be studied. And lastly, we develop free-standing non-equilibrium interactive materials based on our reaction cycles (part C). Overall, our approach opens a new way to obtain more life-like artificial materials that can eventually perform complex (biological) functions.","1762488","2018-01-01","2022-12-31"
"LIGHT","advanced Light mIcroscopy for Green cHemisTry","Maarten Blanka Jozef Roeffaers","KATHOLIEKE UNIVERSITEIT LEUVEN","""Optimization of catalytic materials and hence of chemical processes heavily relies on gaining detailed insight into the complex dynamics underlying the outcome of a catalytic process and using this information in the rational design of improved catalysts. So far, spectroscopic approaches have already contributed importantly; however a strong need for new and improved in situ spectroscopic methods with micro- and nanometer resolution still remains. This project aims to develop advanced light microscopy tools that will significantly contribute to this goal.""","1999485","2012-10-01","2017-09-30"
"Light4Function","Light-controlled and Light-driven Molecular Action","Stefan Hecht","HUMBOLDT-UNIVERSITAET ZU BERLIN","Important processes carried out by Nature’s machinery rely on proper regulation mechanisms. To achieve such control over various functions in man-made materials and devices light offers a superior advantage as an external stimulus and beyond as energy source. Photoswitchable entities provide an ideal platform to interface light with matter and therefore the proposed research program “Light 4 Function” aims at designing and developing new functional photochromic systems.

Building on the PI’s previous achievements and expertise, this broad program will be developing photochromic systems in four project areas for:
i) light-gated ligation, i.e. photocontrolling reversible attachment to complementary molecules, (bio)scaffolds, and surfaces to construct “smart” tags;
ii) light-controlled catalysis, i.e. phototuning the activity and selectivity in living polymerization processes to obtain remote-controlled catalysts;
iii) light-gated charge transport, i.e. photocontrolling current flow in single molecular junctions as well as in thin film organic transistors to create photoadressable organic devices;
iv) light-driven molecular motion, i.e. photoswitching macromolecules to generate maximum geometry changes in order to create sensitive optomechanic materials and devices.
The four project areas will be supported by efforts to further develop and optimize the utilized azobenzene and diarylethene photochromic components.

The photochromic systems will be designed such that they allow for optimal control over the chosen physico-chemical processes by light. Therefore, photons will be exploited to stimulate and drive molecular “action” at specific locations and/or at defined times. This superior control will enable complicated molecular processes to be orchestrated with the “flip of a light switch” and should lead to the development of light-responsive “smart” tags, catalysts, materials, and devices.","1494000","2013-01-01","2017-12-31"
"LightCas","Light-controlled synthetic enzyme cascades","Dorte ROTHER","FORSCHUNGSZENTRUM JULICH GMBH","There is an urgent need for the development of greener syntheses procedures if mankind wants to maintain an environment worth living in but is at the same time unwilling to accept a reduction in material comfort. The establishment of more biocatalytic steps in chemical syntheses is one possible solution, as enzymes and whole cells offer sustainable advantages, such as biodegradability, intoxicity, high selectivity, and many more. As a myriad of enzymatic reactions exist for almost any product, their potential is immense. Great scientific achievements and new techniques recently developed have enabled the design of economically and ecologically feasible multi-step enzyme cascades. However, with these new opportunities, also new challenges arise. The more enzyme steps are combined in one pot, the higher the risk of undesired cross-reactivity is. There is thus an urgent need for a tight control of each biocatalytic step in a cascade in order to obtain the desired product in a high purity and to make use of all advantages that enzyme cascades intrinsically offer. With LightCas, I aim to break new grounds in the area of multi-step (bio)catalysis by enabling an orthogonal, selective and thus flexible on/off tuning of enzymes in a cascade. By entrapping enzymes into light-switchable microgels, using photo-switchable active site lids and light-induced enzyme deactivation, three methods providing the opportunity to control enzyme activity in vitro and in vivo on demand will be (further) developed. The ultimate goal is to set up a one-pot multi-step light-controlled enzyme reactor yielding the desired product in high selectivity and concentration in a technically self-regulated manner.
Beyond the ground-breaking direct impact in the field of enzyme catalysis, huge gains in knowledge are expected from LightCas with respect to the application of intelligent stimuli-responsive materials as well as new, advanced methods for applications in the clinical and research environment.","1498125","2018-01-01","2022-12-31"
"LightCrypt","New Directions in Lightweight Cryptanalysis","Nathan Keller","BAR ILAN UNIVERSITY","Over the next five years, fifty billion new smart devices will be connected to the Internet of Things (IoT), creating a revolution in the way we interact with our environment. Such resource constrained devices will require lightweight cryptography to protect them and us from bad actors. Unfortunately, such schemes can be highly vulnerable: Two notable examples are the encryption schemes used in GSM cellular phones and in car remote controls - both broken by the PI. We claim that it is not sufficient to adjust the current design and analysis tools to the constrained environment. Instead, we must establish a new research methodology, aiming directly at the problems arising in the 'lightweight realm'. 
We plan to concentrate on four main directions. First, we will go 'a level up' to study the security of generic lightweight building blocks in order to find the minimal number of operations required to transition from insecure to secure designs. Second, when considering specific ciphers we will pursue practical low complexity attacks, which are more relevant to the lightweight realm than standard theoretical attacks. Third, we will pursue new directions toward establishing 'white-box cryptography' – a central challenge in IoT cryptography. Finally, we will explore further applications of discrete analysis to lightweight cryptography, trying to establish rigorous conditions under which the standard cryptanalytic techniques apply in order to avoid unnecessarily pessimistic security estimates.   
For the near future, we hope that our research will make it possible to detect and fix weaknesses in existing lightweight ciphers before they can be exploited by the 'bad guys'. Looking forward farther, we hope to understand how to design new secure lightweight ciphers for the billions of IoT devices to come.","1487500","2017-10-01","2022-09-30"
"lightMaterInt","Exploiting light and material interaction","Anat Levin Keslassy","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","The interaction between light and material leads to beautiful visual phenomena that greatly enrich our perception of the world.  The ability to measure and model light scattering is central to almost any field of science. However, light transport in rich scenes is a complex process involving a long sequence of scattering events. Computationally modeling, reproducing and acquiring the processes generated so easily by Mother Nature is an extremely challenging task.  While several computational models have been proposed, they are all making various simplifying assumptions that cannot capture the full complexity of light transport processes in nature. In the proposed research, we suggest new measurement strategies and new inference algorithms that will allow us to infer more information on light and material interaction. 
 
Specifically, the research will focus on the following tasks: (i) Acquiring internal sub-scattering, and recovering the volumetric structure of partially translucent objects using transient imaging data; (ii) Acquiring external illumination from its reflection on diffused objects; (iii) Exploiting illumination for developing digital light sensitive displays, capable of presenting 3D scenes with spatially varying reflectance properties. 
 
As light scattering is such a fundamental phenomenon, our envisioned new tools have applications in almost any field of science, from astronomy to microscopy, and in medicine. We plan to push the bound on the penetration depth of medical imaging devices, and allow chemists to infer more information on material decomposition through scattering.  In earth science we can infer aerosol density from the scattering of sunlight in the atmosphere and ocean, a central challenge in any study of climate and pollution. In addition, we will pursue new technological developments such as light sensitive displays, offering a novel form of immersive visual experience, and new technologies of coded security cameras.","1999825","2015-12-01","2021-08-31"
"LIGHTNING","Charge separation, lightning and radio emission in low-mass objects","Christiane Helling","THE UNIVERSITY COURT OF THE UNIVERSITY OF ST ANDREWS","This project will investigate the hypothesis that dust clouds are a major source of charge separation and discharge processes in very low mass, extrasolar objects like M-dwarfs, Brown-Dwarfs, and planets. The aim is to model charging, dust formation and sedimentation in dusty media to understand how the atmospheric ionisation mechanisms change at the border from stars to planets in the M-dwarf--Brown-Dwarf transition region where radio emission starst to exceed X-ray emission, and to investigate the physics and the occurrence of intra-cloud lightning outside our solar system.  Lightning is suggested to have triggered the occurrence of life on Earth.

Dusty media are generally very common on Earth and in space, for example in volcano plumes that influence the local climate on Earth, on Mars where it blocks Mars-Rover's wheels, in dust-clouds in Brown Dwarfs and planets which determine their chemistry and their detectability, or in planet-forming disks. All have in common that dust of mixed composition is abundant in a turbulent environment in a variety of sizes. This project will perform a characterisation of dusty astrophysical plasma, systemically study charge separation processes and draw comparison to known scenarios in volcanos and Martian plasmas. The project determines stellar parameter and dust cloud characteristics (e.g. cloud height) for which dust cloud charging becomes important, and under which conditions lightning can occur.  A charge conservation model will be coupled to a non-equilibrium chemistry to search for discharge-related molecules and for pre-biotic molecules that might occur during lightning. Applications to standard model atmospheres will be carried out to study the influence on the spectral energy distribution and the object's albedo. The long-term aim of this project is to solve the dust and charge conservation equations together with the magnetic field equations in order to study the development of radio emission in  low-mass objects.","1500000","2011-03-01","2017-02-28"
"LIGHTPORT","From light-stimulated anion receptors to transmembrane carriers and pumps","Sander WEZENBERG","RIJKSUNIVERSITEIT GRONINGEN","The transport of anions across the cell membrane, which is mediated by transport proteins, is essential to many important biological processes. Dysregulation of this transport has been associated to various diseases and therefore, chemists endeavour  to develop artificial receptors that mimic the function of natural transporters. Despite much progress over the last decade, the current artificial systems are mostly static, while proteins are able to change their activity dynamically in response to stimuli in the environment. To integrate such stimuli-controlled behavior in synthetic systems is a key contemporary challenge. In view of this, the goal of the proposed research program is to develop anion receptors in which the binding properties can be effectively modulated by light and to apply these receptors as transmembrane carriers and pumps, in order to regulate passive transport (i.e. down a concentration gradient) and to induce active transport (i.e. against a concentration gradient). This interdisciplinary program is divided into three work packages: WP1 aims at the development of structurally rigid and visible-light-actuated photoswitches and their use as platforms for constructing anion receptors; WP2 deals with the development of mechanically interlocked structures as photoswitchable anionic hosts; WP3 is directed at utilizing these receptors for light-gated transport and light-driven pumping of anions across phospholipid bilayers, whereas also an alternative dual-responsive anion channel will be prepared. Eventually, it is expected that this work will open a new route toward light-based localized pharmacological treatment, e.g. via light-triggered cancer or bacterial cell death. Furthermore, active transport systems, that are able to build up and maintain concentration gradients across membranes, could provide a completely new view on how to convert and store light (solar) energy.","1499461","2019-03-01","2024-02-29"
"LiKo","From Liouville to Kolmogorov: 2d quantum gravity, noise sensitivity and turbulent flows","Christophe Garban","UNIVERSITE LYON 1 CLAUDE BERNARD","This research project is organized along three seemingly unrelated directions:

(1) Mathematical Liouville gravity deals with the geometry of large random planar maps. Historically, conformal invariance was a key ingredient in the construction of Liouville gravity in the physics literature. Conformal invariance has been restored recently with an attempt of understanding large random combinatorial planar maps once conformally embedded in the plane. The geometry induced by these embeddings is conjecturally described by the exponential of a highly oscillating distribution, the Gaussian Free Field. This conjecture is part of a broader program aimed at rigorously understanding the celebrated KPZ relation. The first major goal of my project is to make significant progress towards the completion of this program. I will combine for this several tools such as Liouville Brownian motion, circle packings, QLE processes and Bouchaud trap models.

(2) Euclidean statistical physics is closely related to area (1) through the above KPZ relation. I plan to push further the analysis of critical statistical physics models successfully initiated by the works of Schramm and Smirnov. I will focus in particular on dynamics at and near critical points with a special emphasis on the so-called noise sensitivity of these systems.

(3) 3d turbulence. A more tractable ambition than solving Navier-Stokes equation is to construct explicit stochastic vector fields which combine key features of experimentally observed velocity fields. I will make the mathematical framework precise by identifying four axioms that need to be satisfied. It has been observed recently that the exponential of a certain log-correlated field, as in (1), could be used to create such a realistic velocity field. I plan to construct and analyse this challenging object by relying on techniques from (1) and (2). This would be the first genuine stochastic model of turbulent flow in the spirit of what Kolmogorov was aiming at.","935000","2016-09-01","2021-08-31"
"LIMITS","Limits of Structures in Algebra and Combinatorics","Lukasz GRABOWSKI","LANCASTER UNIVERSITY","The project is concerned with Borel and measurable combinatorics, sparse
graph limits, approximation of algebraic structures and applications to
metric geometry and measured group theory. Our research will result in
major advances in these areas, and will create new research directions in
combinatorics, analysis and commutative algebra.

The main research objectives are as follows.
1) Study equidecompositions of sets and solve the Borel version of the Ruziewicz problem. 
2) Give a new characterisation of amenable groups in terms of  measurable Lovasz Local Lemma.
3) Study rank approximations of infinite groups and commutative algebras.","1139333","2019-02-01","2024-01-31"
"LIMITTRANDOMMEDIA","Limit theorems for processes in random media","Noam Berger","TECHNISCHE UNIVERSITAET MUENCHEN","Classical random walks (CRW) have been studied for centuries, and very detailed information is known about them. However, most of the techniques for studying CRW are based on the complete regularity and the group structure of the medium. When modeling real world phenomena, this regularity assumption rarely holds, and therefore CRW is not a sufficient model. As a result, a number of non-classical models of random walk have been suggested. These models are believed to better model actual natural processes. One of the most studied of non-classical random walk models is random walk in random environment (RWRE). In RWRE the medium (&quot;environment&quot;) in which the process takes place is random, and the law of the random walk varies as a function of the location. RWRE can model, for instance, the motion of an electron in an alloy, the movement of enzymes along a DNA sequence and many other processes. Since the CRW methodology does not work for RWRE (and, in fact, neither for other non-classical models of random walk), new methodology needed to be developed. The purpose of this project is to contribute to the study of RWRE by improving the existing methods and by developing new ones. We will work on some of the most important problems in the field, namely convergence and rate of convergence to Brownian motion for various RWRE models (e.g. reversible, perturbative and others), trapping and slowdown for RWRE models (e.g. ballistic and perturbative), ballisticity conditions, zero-one laws, and others. The output of this project is expected to contribute significantly to the understanding of RWRE systems.","504000","2009-11-01","2014-10-31"
"LINCE","Light INduced Cell control by Exogenous organic semiconductors","Maria Rosa ANTOGNAZZA","FONDAZIONE ISTITUTO ITALIANO DI TECNOLOGIA","LINCE will develop light-sensitive devices based on organic semiconductors (OS) for optical regulation of living cells functions.
The possibility to control the activity of biological systems is a timeless mission for neuroscientists, since it allows both to understand specific functions and to manage dysfunctions. Optical modulation provides, respect to traditional electrical methods, unprecedented spatio-temporal resolution, lower invasiveness, and higher selectivity. However, the vast majority of animal cells does not bear specific sensitivity to light. Search for new materials capable to optically regulate cell activity is thus an extremely hot topic. OS are ideal candidates, since they are inherently sensitive to visible light and highly biocompatible, sustain both ionic and electronic conduction, can be functionalized with biomolecules and drugs. Recently, it was reported that polymer-mediated optical excitation efficiently modulates the neuronal electrical activity.
LINCE will significantly broaden the application of OS to address key, open issues of high biological relevance, in both neuroscience and regenerative medicine. In particular, it will develop new devices for: (i) regulation of astrocytes functions, active in many fundamental processes of the central nervous system and in pathological disorders; (ii) control of stem cell differentiation and tissue regeneration; (iii) control of animal behavior, to first assess device biocompatibility and efficacy in vivo. LINCE tools will be sensitive to visible and NIR light, flexible, biocompatible, and easily integrated with any standard physiology set-up. They will combine electrical, chemical and thermal stimuli, offering high spatio-temporal resolution, reversibility, specificity and yield. The combination of all these features is not achievable by current technologies. Overall, LINCE will provide neuroscientists and medical doctors with an unprecedented tool-box for in vitro and in vivo investigations.","1866250","2019-03-01","2024-02-29"
"LINKSPM","Linking atomic-scale properties of 2D correlated materials with their mesoscopic transport and mechanical response","Miguel MORENO UGEDA","FUNDACION DONOSTIA INTERNATIONAL PHYSICS CENTER","Fundamental material properties become highly susceptible to external perturbations in low dimensions. This presents tremendous new opportunities for manipulating the behavior of novel 2D layered materials and ultimately achieving unprecedented control over their performance when integrated into highly specific functional devices. However, strategies that enable such control are sorely lacking to date and remain an outstanding challenge for the materials science community. Progress here requires of a comprehensive microscopic picture of the fundamental properties of 2D materials in clear connection to their macroscopic behavior, a knowledge that is still missing due to the lack of experimental techniques that simultaneously probe multiple length regimes. 

The main objective of the proposed research is to demonstrate control over the electronic ground states of 2D materials via external strain and electromagnetic fields to build links of applicability for signal processing in electromechanical nanodevices. We will focus on 2D correlated materials exhibiting collective electronic phases such as superconductivity, which respond dramatically to external perturbations. The project aims to understand the interplay between these external stimuli and microscopic electronic phases, and to unambiguously correlate them with mesoscopic electrical transport and mechanical response. This project comprises three research thrusts: (i) Development of new instrumentation that provides a direct way to correlate atomic-scale and mesoscopic properties of materials, and to establish links between (ii) the electrical conductivity and (iii) the mechanical response of 2D correlated materials with their atomic-scale structure and stimulus-dependent electronic phase diagram. This project has the potential to transform this field by providing new pathways to control the behavior of layered nanostructures.","1734625","2018-10-01","2023-09-30"
"LIQUISWITCH","External Control of Liquid Nanofilms for Switchable Friction and Adhesion","Susan Perkin","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The overall vision of the project is to demonstrate methods for switching friction and adhesion – such as “switch off friction” or “switch on adhesion” – reversibly and with remote control. My hypothesis is that fluids confined to nanofilms between solid objects can be designed to dramatically alter their surface properties under the influence of applied fields, and so can be used to switch friction, lubrication or adhesion in a controlled and reversible way. The emphasis of the project is on creating well-defined model experiments, with high resolution in both film thickness and interaction forces, in order to reach a fundamental understanding of the new concepts and mechanisms involved. The novelty lies in both the newly-proposed mechanisms of switching friction and adhesion interactions, and the new instrumentation constructed to detect and analyse the forces. The new methodologies necessary for these pioneering experiments involve two new versions of a Surface Force Balance (SFB) providing molecular-resolution (0.1nm) control and measurement of interfacial liquid films, controlled application of electric and magnetic fields, and ultra-sensitive measurement of friction and adhesion. Notably, one of these new instruments will have macroscopic graphene electrodes for confinement of liquid nanofilms (0 – 100nm) between atomically smooth electrodes. The wide range of ‘switchable’ liquids studied will include polyelectrolytes, magnetic fluids, ionic liquids and self-assembled systems. Ultimately, this project will change the way we think about surface interactions into something we can ‘switch and control’.","1492828","2016-04-01","2021-03-31"
"LITHIUM","From planetary birth with aperture masking interferometry to nulling with Lithium Niobate technology","Sylvestre Mathieu André Lacour","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Observing the process of planetary accretion is crucial to inform models of planet formation. Most of the key action is expected to happen in the gaps of protostellar disks – a spatial realm over which aperture masking interferometry has demonstrated a unique ability to deliver incisive imaging. Masking offers twin advantages of higher dynamic range at the diffraction limit (lambda/D) than differential imaging, while at the same time giving nearly complete Fourier coverage compared to long baseline interferometry. The founding objective of this proposal is to create expertise and technology to understand the astrophysical phenomena so far only glimpsed in faint detections in stellar gaps such as those published in T Cha (Huelamo et al. 2011), HD142527 (Biller et al. 2012) and FL Cha (Cieza et al. 2013). But the central goal of this project is to further advance the experimental technique. Reaching even higher dynamic range for fainter detections is essential for probing planetary birth. The way to improve the dynamic range is clear: increase the accuracy of the primary closure phase observable. To do so, we will follow two paths. The first will use laboratory experimentations to analyse and understand the sources of bias to the closure phase. The resulting end-product will be better software offered to the community, and better techniques for a next generation of aperture masking devices. The second path is to amplify the closure phase signal by combining nulling with closure phase (Lacour et al. 2014). This second path is the most challenging, but will be an important breakthrough to the field. Nulling is to aperture masking what coronagraphy is to classical imaging. Without a first level of nulling, the aperture masking technique will always be limited by the photon noise due to the stellar light. We propose to build on our experience of Lithium Niobate integrated optics devices to bring aperture masking to a new level of performance in high dynamic range imaging.","1851881","2015-03-01","2020-02-29"
"LOBENA","Long Beamtime Experiments for Nuclear Astrophysics","Hans Otto Uldall Fynbo","AARHUS UNIVERSITET","The goal of LOBENA is to measure key properties needed for understanding nuclear processes in the Cosmos. Nuclear Astrophysics plays a key role in our quest to understand the origin and distribution of the chemical elements in our galaxy. Nuclear processes are crucial for understanding the energy production in the universe and are essential for describing the creation of chemical elements from the ashes of the Big Bang. Uncertainties in the nuclear physics can therefore influence our understanding of many astrophysical processes, both those involving stable stellar burning phases and explosive phenomena such as X-ray bursts, gamma-ray bursts and supernovae.
In LOBENA (LOng Beamtime Experiments for Nuclear Astrophysics) I will initiate a series of studies in Nuclear Astrophysics, which have in common the need for long beam times and the use of complete kinematics detection of several particles emitted in reactions. The core of the project will focus on the systems 8Be, 12C and 16O where today key open questions of great importance remain to answered. These questions can be addressed by reactions induced by low energy (<5MeV) beams of protons and 3He on light targets such as 6,7Li, 9Be,  10,11B and 19F using a newly developed complete kinematics detection procedure. The department of Physics and Astronomy in Aarhus provides a unique scene for doing these measurements since it provides accelerators where long beam time can be guarantied.  LOBENA will also include complimentary experiments at international user facilities such as ISOLDE (CERN), KVI (Groningen), JYFL and (Jyväskylä).
With this ERC starting grant proposal I wish to start up my own group around Nuclear Astrophysics experiments in house and at international user facilities. With two Post Doc.s and a Ph.D. I will be much better able to fully exploit the scientific potential of the proposed research, which will also help to consolidate my own research career and give me more independence.","1476075","2012-11-01","2018-10-31"
"LocalOrder","Localization and Ordering Phenomena in Statistical Physics, Probability Theory and Combinatorics","Ron Peled","TEL AVIV UNIVERSITY","Mathematical statistical physics has seen spectacular progress in recent years. Existing problems which were previously unattainable were solved, opening a way to approach some of the classical open questions in the field. The proposed research focuses on phenomena of localization and long-range order in physical systems of large size, identifying several fundamental questions lying at the interface of Statistical Physics, Probability Theory and Combinatorics.
One circle of questions concerns the fluctuation behavior of random surfaces, where the PI has recently proved delocalization in two dimensions answering a 1975 question of Brascamp, Lieb and Lebowitz. A main goal of the research is to establish some of the long-standing universality conjectures for random surfaces. This study is also tied to the localization features of random operators, such as random Schrodinger operators and band matrices, as well as those of reinforced random walks. The PI intends to develop this connection further to bring the state-of-the-art to the conjectured thresholds.
A second circle of questions regards long-range order in high-dimensional systems. This phenomenon is predicted to encompass many models of statistical physics but rigorous results are quite limited. A notable example is the PI’s proof of Kotecky’s 1985 conjecture on the rigidity of proper 3-colorings in high dimensions. The methods used in this context are not limited to high dimensions and were recently used by the PI to prove the analogue for the loop O(n) model of Polyakov’s 1975 prediction that the 2D Heisenberg model and its higher spin versions exhibit exponential decay of correlations at any temperature.
Lastly, statistical physics methods are proposed for solving purely combinatorial problems. The PI has applied this approach successfully to solve questions of existence and asymptotics for combinatorial structures and intends to develop it further to answer some of the tantalizing open questions in the field.","1136904","2016-01-01","2020-12-31"
"LOCALSTAR","Modelling star formation in the local universe","Clare Dobbs","THE UNIVERSITY OF EXETER","The goal of this proposal is to revolutionize our understanding of star formation in nearby galaxies, using numerical simulations. Traditionally, research in star formation has considered the contraction of a giant molecular cloud (GMC), or more commonly a star forming core, under gravity. However there has been relatively little research on molecular clouds themselves, even though they provide the initial conditions for star formation, and thus determine the main assumptions for theories of star formation. The proposed research will focus on the scales of giant molecular clouds, the clouds of molecular hydrogen (H2) where most star formation takes place in nearby galaxies. These objects link galactic scale physics with the small scale physics of star formation. Only now are the computational resources becoming available to study the interstellar medium (ISM) numerically on galactic scales, and model the complex processes involved in GMC and star formation. Simultaneously observational programs (e.g. ALMA, Herschel, CARMA) are starting to resolve GMCs in nearby galaxies. Our research will involve performing calculations on scales from individual GMCs to interacting galaxies, and comparing to forthcoming observations to answer some of the most fundamental questions in star formation, such as why star formation is inefficient, how do GMCs form and what are their lifetimes.","1169586","2011-11-01","2017-05-31"
"LoCoMacro","Local Control of Macroscopic Properties in Isolated Many-body Quantum Systems","Maurizio FAGOTTI","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Studies of many-body quantum systems in the last century have been mainly focussed on equilibrium properties; the systems of interest were typically coupled to an environment, which brings about relaxation after short times. The situation changed with the advent of experiments on clouds of ultra-cold, trapped atoms. These are by design almost 
isolated, and allow for investigations into nonequilibrium dynamics before the onset of dissipative processes. The characterization of such dynamics is now a main frontier of theoretical physics. One of the most exciting phenomena observed was the emergence of a new kind of relaxation, not caused by dissipation. The system acts as its own bath 
and at late times it is as if the state were prepared at a different temperature, or, especially in low-dimensional systems, in some exotic state of matter. Recently, some progress has been made in extending this picture to inhomogeneous systems. In particular, an exceptional phenomenon was pointed out: a localized perturbation can have global effects on the stationary properties of the observables. LoCoMacro is born of this observation and has the ultimate aim of finding novel ways to control the macroscopic properties of a nonequilibrium state by acting on a small part of the system. We address the fundamental questions of relaxation and emergence of nonequilibrium steady states in the presence of inhomogeneities; we study the effects of localized perturbations on the key elements of the dynamics, as the conservation 
laws. In integrable models we use the most advanced analytic techniques to obtain exact results, e.g., for correlation functions and entanglement measures. More generally, we rely on state-of-the-art numerical simulations. For the defining characteristics of the models studied, LoCoMacro creates a bridge between two fascinating topics: thermalization in homogeneous systems and many-body localization in disordered ones.","1499716","2019-01-01","2023-12-31"
"LODESTONE","LODESTONE: Unifying the Radio Spectrum to Map the Magnetic Universe","Anna Scaife","THE UNIVERSITY OF MANCHESTER","Clusters of galaxies are a unique cosmological probe. Beyond the potential wells of individual clusters the vast filamentary cosmic web which binds them together contains two thirds of the baryonic matter in the Universe and represents a rich scientific resource. In order to use such resources correctly it is essential  to understand the energetics, dynamics and redshift dependence of the intra/inter-cluster media. Absolutely fundamental to this understanding is a detailed knowledge of the strength, morphology and evolution of their magnetic fields. In addition, using clusters of galaxies and cosmic large scale structure as a laboratory in which to probe these fields is also the key to unlocking the more fundamental long standing problems of magnetic evolution and structure, and ultimately to determining the very origins of cosmic magnetism itself. To tackle these fundamental questions I will use a combination of two different observational approaches, effectively unifying the radio spectrum from meter to millimeter wavelengths: Rotation Measure Synthesis and the resolved Sunyaev–Zel’dovich Effect. Individually these approaches both offer unique insights; combined they provide an almost complete picture of the internal baryonic dynamics of clusters and large scale structure. I will use a set of leading and next generation radio telescopes around the globe. These telescopes span the radio spectrum, but are united by my leading role in the magnetism and/or cluster science case for each one. I will build an infrastructure which combines their operations to achieve a scientific outcome much greater than the individual instruments can produce independently. This will include development of world leading image reconstruction techniques; novel approaches to data combining; and state of the art analysis methods - as well as building a program which integrate disparate communities in astronomy. This program is called LODESTONE, to reflect the aligning nature of magnetism.","1928369","2013-01-01","2017-12-31"
"LOFAR","Searching for The Origin of Cosmic Rays and Neutrinos with LOFAR","Stijn Buitink","VRIJE UNIVERSITEIT BRUSSEL","The origin of cosmic rays remains one of the largest mysteries in astrophysics. Innovative and accurate radio measurements of cosmic rays and neutrinos with LOFAR promise to provide new answers.
It is generally believed that ultra-high-energy cosmic rays are produced in extragalactic sources like gamma- ray bursts or active galactic nuclei, while the lower energy cosmic rays come from our own Galaxy. At what energy this transition takes place is still unknown. Here we focus on disentangling Galactic and extragalactic components by studying the mass composition between 10^17 and 10^18 eV, a regime that is also crucial for understanding the origin of the extraterrestrial neutrinos detected by IceCube.
We do this with LOFAR, the first radio telescope that can detect individual cosmic rays with hundreds of antennas. This incredible level of detail allowed us to finally understand the complicated radiation mechanism and to perform the first-ever accurate mass analysis based on radio measurements. Our first data reveal a strong proton component below 10^18 eV, suggesting an early transition to an extragalactic component. With upgrades to our detector and techniques we will be able to improve our sample size by an order of magnitude, resolve more mass components, and identify the origin of high-energy cosmic rays and neutrinos.
The technique may be scaled up to higher energies, measured at the Pierre Auger Observatory, where mass information is needed to correlate cosmic rays with their astrophysical sources and to confirm the nature of the cutoff at ~10^19.6 eV.
We can even search for particles beyond the GZK limit. With the Westerbork telescope we have already set the best limit on cosmic rays and neutrinos above 10^23 eV. With LOFAR we will achieve a much better sensitivity at lower energies, also probing for new physics, like the decays of cosmic strings predicted by supersymmetric theories.","1500000","2015-06-01","2020-05-31"
"LOLITA","Information Theory for Low-Latency Wireless Communications","Tobias Mirco KOCH","UNIVERSIDAD CARLOS III DE MADRID","The majority of wireless connections in the fifth generation (5G) of wireless systems will most likely be originated by autonomous machines and devices rather than by the human-operated mobile terminals for which traditional broadband services are intended. It is thus expected that enhanced mobile-broadband services will be complemented by new services centered on machine-type communications (MTC). An important emerging area among MTC systems is that of low-latency communications, which targets systems that require reliable real-time communication with stringent requirements on latency and reliability.

The design of low-latency wireless communication systems is a great challenge, since it requires a fundamentally different design approach than the one used in current high-rate systems. Indeed, current systems exchange packets of several thousand bits. For such packet lengths, there are error-correcting codes that can correct transmission errors with high probability at rates close to the capacity. Consequently, the design of current systems is supported by the extensive information-theoretical knowledge we have about wireless communications. In contrast, low-latency systems exchange packets of only several hundred bits, so the rate of the error-correcting code must be significantly below the capacity to achieve the desired reliability. Consequently, for such systems, capacity is not a relevant performance measure, and design guidelines that are based on its behavior will be misleading.

Currently, we are lacking the theoretical understanding of low-latency wireless communication systems that would be crucial to design them optimally. The presented project addresses this problem by establishing the theoretical framework required to describe the fundamental tradeoffs in low-latency wireless communications. The project's vision is that finite-blocklength information theory will play the same role for low-latency systems as information theory has for current systems.","1424000","2017-03-01","2022-02-28"
"LONGSPIN","Long-range coupling of hole spins on a silicon chip","Romain MAURAND","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","With the miniaturization of electronic devices, the semiconductor industry has to deal with complex technical barriers and is forced to introduce novel and innovative concepts. The project proposal is exactly in line with this new paradigm as it proposes to divert CMOS technology to explore a new path for quantum spintronics. Concretely the project aims at using spin-orbit interaction present in the valence band of silicon to drive ultra-fast and ultra-coherent hole spin quantum bits (qubits). The proposal builds on the first demonstration by the principal investigator of a hole spin qubit electrically driven in silicon.
While spins are excellent quantum bits, their long-range coupling remains a challenge to tackle towards complex quantum computing architectures. Here I propose to take up this challenge using a microwave photon as a quantum mediator between qubits in silicon. 
The LONGSPIN project presents a unique approach by leveraging a standard silicon-on-insulator CMOS process for the implementation of the qubits co-integrated with superconducting microwave resonators. 
This research project will provide a CMOS quantum toolkit with optimized designs and materials for fast and coherent qubits with a profound understanding of the physical limitations to hole spin coherence and hole qubit gate fidelity in silicon.  Eventually a microwave photon used as a quantum bus will allow the transfer of quantum information between distant spin qubits.","1998423","2018-03-01","2023-02-28"
"LONGWOOD","Long-term woodland dynamics in Central Europe: from estimations to a realistic model","Péter Szabó","BOTANICKY USTAV AV CR, V.V.I.","The vegetation of Central Europe has been directly influenced by humans for at least eight millennia; the original forests have been gradually transformed into today’s agricultural landscape. However, there is more to this landscape change than the simple disappearance of woodland. Forests have been brought under various management regimes, which profoundly altered their structure and species composition. The details of this process are little known for two main reasons. The greatest obstacle is the lack of cooperation among the disciplines dealing with the subject. The second major problem is the differences in spatio-temporal scaling and resolution used by the individual disciplines. Existing studies either concern smaller territories, or cover large areas (continental to global) with the help of modelling-based generalizations rather than primary data from the past. Using an extensive range of primary sources from history, historical geography, palaeoecology, archaeology and ecology, this interdisciplinary project aims to reconstruct the long-term (Neolithic to present) patterns of woodland cover, structure, composition and management in a larger study region (Moravia, the Czech Republic, ca. 27,000 km2) with the highest spatio-temporal resolution possible. Causes for the patterns observed will be analyzed in terms of qualitative and quantitative factors, both natural and human-driven, and the patterns in the tree layer will be related to those in the herb layer, which constitutes the most important part of plant biodiversity in Europe. This project will introduce woodland management as an equal driving force into long-term woodland dynamics, thus fostering a paradigm shift in ecology towards construing humans as an internal, constitutive element of ecosystems. By integrating sources and methods from the natural sciences and the humanities, the project will provide a more reliable basis for woodland management and conservation in Central Europe.","1413474","2012-01-01","2016-12-31"
"LUSI LAB","Lusi: a unique natural laboratory for multidisciplinary studies of focussed fluid flow in sedimentary basins","Adriano Mazzini","UNIVERSITETET I OSLO","The 29th of May 2006 several gas and mud eruption sites suddenly appeared along a fault in the NE of Java, Indonesia. Within weeks several villages were submerged by boiling mud. The most prominent eruption site was named Lusi. To date Lusi is still active and has forced 50.000 people to be evacuated and an area of more than 7 km2 is covered by mud. The social impact of the eruption and its spectacular dimensions still attract the attention of international media. Since 2006 I have completed four expeditions to Indonesia and initiated quantitative and experimental studies leading to the publication of two papers focussing on the plumbing system and the mechanisms of the Lusi eruption. However still many unanswered questions remain. What lies beneath Lusi? Is Lusi a mud volcano or part of a larger hydrothermal system? What are the mechanisms triggering the eruption? How long will the eruption last?
LUSI LAB is an ambitious project that aims to answer these questions and to perform a multidisciplinary study using Lusi as a unique natural laboratory. Due to its relatively easy accessibility, the geological setting, and the vast scale, the Lusi eruption represents an unprecedented opportunity to study and learn from an ongoing active eruptive system. The results will be crucial for understanding focused fluid flow systems in other sedimentary basins world-wide, and to unravel issues related to geohazards and palaeoclimate aspects. The project will use multisensory sampling devices within the active feeder channel and a remote-controlled raft and flying device to access and sample the crater and the erupted gases. UV-gas camera imaging to measure the rate and composition of the erupted gases will be coupled with a network of seismometers to evaluate the impact that seismicity, local faulting and the neighbouring Arjuno-Welirang volcanic complex have on the long-lasting Lusi activity. This information will provide robust constraints to model the pulsating Lusi behaviour.","1422420","2013-01-01","2018-12-31"
"M&M´S","New Paradigms for MEMS & NEMS Integration","Frank Niklaus","KUNGLIGA TEKNISKA HOEGSKOLAN","Micro- and nanoelectromechanical system (MEMS and NEMS) components are vital for many industrial and consumer products such as airbag systems in cars and motion controls in mobile phones, and many of these MEMS and NEMS enabled applications have a large impact on European industry and society. However, the potential of MEMS and NEMS is being critically hampered by their dependence on integrated circuit (IC) manufacturing technologies. Most micro- and nano-manufacturing methods have been developed by the IC industry and are characterized by highly standardized manufacturing processes that are adapted for extremely large production volumes of more than 10.000 wafers per month. In contrast, the vast majority of MEMS and NEMS applications only demands production volumes of less than 100 wafers per month in combination with different non-standardized manufacturing and integration processes for each product. If a much wider variety of diverse and even low-volume MEMS and NEMS products shall be exploited, the semiconductor manufacturing paradigm has to be broken. In this project, we therefore will focus on frontier research on new paradigms for flexible and cost-efficient manufacturing and integration of MEMS and NEMS within three related research areas:
(1)	Wafer-Level Heterogeneous Integration for MEMS and NEMS, where we explore new and improved wafer-level heterogeneous integration technologies for MEMS and NEMS devices;
(2) 	Integration of Materials into MEMS Using High-Speed Wire Bonding Tools, where we explore new ways of integrating various types of wire materials into MEMS devices;
(3) 	Free-Form 3D Printing of Mono-Crystalline Silicon Micro- and Nanostructures, where we explore entirely novel ways of implementing mono-crystalline silicon MEMS and NEMS structures that can be arbitrarily shaped.","1495982","2011-11-01","2017-10-31"
"M-PAC","Miniature beam-driven Plasma ACcelerators","Sebastien CORDE","ECOLE POLYTECHNIQUE","As we push the frontier of particle physics to higher particle energies, conventional accelerator techniques are attaining their limits and new concepts are emerging. The use of an ionized gas —or plasma— circumvents the most significant barrier of conventional techniques by increasing the energy gained per unit length by several orders of magnitude. One class of plasma accelerators, relevant for high energy physics applications, consists in using a particle beam, « the driver », to excite a plasma wave, that is then used to accelerate the main particle beam. Research in this field requires large facilities, due to stringent conditions on the driver. In the M-PAC project, I propose to power plasma accelerators with laser-accelerated electron beams based on 100-TW-class laser systems, so as to miniaturize the so-called “beam-driven plasma accelerators”. The project crosses the boundary of the fields of research of laser acceleration and of beam-driven plasma acceleration. With these innovative miniature versions, the goal of the M-PAC project is then to tackle, through experiments and simulations, the next Grand Challenges facing the field of beam-driven plasma acceleration, bringing plasma accelerator technology to viability for high energy physics collider applications. They include the generation and preservation of the excellent beam quality required for high-energy colliders and next-generation light sources, the demonstration of high drive-to-main-beam energy efficiency and the acceleration of the antimatter counterpart of the electron, the positron. Finally, the miniature beam-driven plasma accelerators open new opportunities to push university-scale plasma-based light sources to the next level, both in terms of brightness and spectral range.","1499472","2017-01-01","2021-12-31"
"MACONS","A multi-microscopy approach to the characterisation of Nitride semiconductors (MACONS)","Rachel Angharad Oliver","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","""The commercial market for electronic and optoelectronic devices based on nitride semiconductors is growing extremely fast, but the fundamental science underlying these devices is lagging behind.  This proposal aims to explore the vital link between structure and properties in nitride materials, in order to reveal the limitations of current devices and to pave the way for new, improved technology.  The key strategy of the proposal is to combine multiple microscopy techniques to develop a comprehensive understanding of nanostructures and defects in the nitrides, and to link these discoveries to nanoscale measurements of the optical and electrical properties.  This will require a synergy of different techniques, from techniques commonly used on metals (such as atom-probe tomography) to techniques which focus exclusively on semiconductors (such as scanning capacitance microscopy).  It will also require the development of new approaches to the application of these techniques, to allow the same nanoscale regions of material to be assessed in multiple microscopes, so that the structure and composition of a specific nanostructure may be linked directly and unambiguously to its electrical and optical properties.   Overall, the aim is to provide a much more complete picture of nitride materials science than has ever previously been achieved, and to apply this new understanding to engineering improved materials for nitride optoelectronic devices.""","1371894","2011-12-01","2017-08-31"
"MAD-ESEC","Magmas at Depth: an Experimental Study at Extreme Conditions","Chrystèle Sanloup","UNIVERSITE PIERRE ET MARIE CURIE - PARIS 6","Magmas, i.e. silicate melts, have played a key role in the chemical and thermal evolution of the Earth and other planets.  The Earth's interior today is the outcome of mass transfers which occurred primarily in its early history and still occur now via magmatic events. Present day magmatic and volcanic processes are controlled by the properties of molten silicate at high pressure, considering that magmas are produced at depth. However, the physical properties of molten silicates remain largely unexplored across the broad range of relevant P-T conditions, and their chemical properties are very often assumed constant and equal to those known at ambient conditions. This blurs out our understanding of planetary differentiation and current magmatic processes.
The aim of this proposal is to place fundamental constraints on magma generation and transport in planetary interiors by measuring the properties of silicate melts in their natural high pressures (P) and high temperatures (T) conditions using a broad range of in situ key diagnostic probes (X-ray and neutron scattering techniques, X-ray absorption, radiography, Raman spectroscopy). The completion of this proposal will result in a comprehensive key database in the composition-P-T space that will form the foundation for modelling planetary formation and differentiation, and will provide answers to the very fundamental questions on magma formation, ascent or trapping at depth in the current and past Earth.
This experimental program is allowed by the recent advancements in in situ high P-T techniques, and comes in conjunction with a large and fruitful theoretical effort; time has thus come to understand Earth's melts and their keys to Earth's evolution.","1332160","2011-06-01","2017-05-31"
"MADE-IN-EARTH","Interplay between metamorphism and deformation in the Earth’s lithosphere","Lucie Tajcmanova","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","""A key to understanding the processes operating in the outer part of the Earth is to look at the metamorphic rocks produced in orogenic belts. These rocks now exhumed to the Earth’s surface provide a record of what they experienced, if only they can be correctly interpreted.
The recent use of high resolution devices has revealed the three-dimensional size, shape, composition and distribution of microstructural features in metamorphic rocks down to the nanometre-scale. The new observations show that mechanically maintained pressure variations can be significant (~1 GPa) even on a micro-scale. However, there is currently no satisfactory thermodynamic methodology for a quantitative interpretation of systems with such pressure variations in metamorphic rocks. Ignoring such pressure variations in petrological analysis can lead to errors in depth estimates that are comparable to the typical thickness of the whole continental crust. Such an error may then significantly influence the quality of geodynamic reconstructions.
Here, I propose to develop a revolutionary theoretical and computational method to understand microstructures that reflect pressure variations, based on the chemical and mechanical properties of their constituent minerals. Using the novel theoretical approach, I and my team will perform 3D numerical simulations and give the criteria to correctly understand the key microstructures.
This emerging multi-disciplinary research will provide a quantitative and physically-based framework for interpreting common microstructures in metamorphic rocks. Furthermore, the new approach will not only make a critical contribution to understanding the interplay between metamorphic processes and deformation on the grain scale, but it will also form the basis for a new generation of models for application to large-scale geological scenarios. The results of the project will thus significantly increase our understanding of key processes in the Earth’s lithosphere.""","1499820","2013-10-01","2018-09-30"
"MADNA","Modular Assembly of DNA-based systems; bio-inspired artificial allosteric assemblies","Johannes Gerhardus Roelfes","RIJKSUNIVERSITEIT GRONINGEN","Artificial allosteric systems, i.e. (semi-)synthetic systems that give an autonomous response to changes in their environment, are of great interest for applications as sensors, drug delivery and drug targeting.

The overall aim of this proposal is the development of bio-inspired DNA-based artificial allosteric systems that are of interest for biological and bio-medical applications.

In the present proposal, DNA will be used as the general scaffold for the design and construction of artificial allosteric systems and DNA-strand exchange will be used as a key mechanism to affect a predictable structural change in the system, resulting in a measurable response. Structure switching aptamers will be used to make this response small molecule dependent.","1430003","2011-09-01","2016-08-31"
"MAEROSTRUC","Multicomponent Aerogels with Tailored Nano-, Micro-   Macrostructure","Nadja-Carola BIGALL","GOTTFRIED WILHELM LEIBNIZ UNIVERSITAET HANNOVER","Aerogels and hydrogels from nanocrystal building blocks are a fascinating novel class of materials with extremely low densities and large specific surfaces, which partially exhibit the advantageous properties of their nanoscopic building blocks (e.g. size quantized fluorescence or catalytic activity). In the present project, multicomponent gels with controlled mechanical properties, plasmon enhanced fluorescence, photocatalytic properties, and with controlled conductivity properties will be synthesized. These new materials will not only exhibit the nanoscopic properties of their building blocks, but they will also exhibit new properties which are neither accessible from nanoparticle nor from bulk material. This will e.g. be achieved due to nanoscopic interactions between the materials or due to synergistic combination effects caused by appropriate material combination.

Synthetic routes for nanostructuring, microstructuring and macrostructuring nanocrystal hydrogels and aerogels will be developed. Nanostructuring involves advancement of colloidal nanocrystal synthesis as well as postsynthetic gel modifications. Microstructuring involves synthesizing multicomponent gels with defined contact points of the materials and intercalating multicomponent gels. Macrostructuring involves implementation of the gelation techniques into 3D printing, and gel deformation by external triggers and will enhance the applicability of gels. The materials developed will be tailored for several physicochemical effects and hence applications. 

While the project focuses on the synthesis of these new materials with defined physicochemical properties, the outcome of this project will influence many different research and application fields, such as electrodes and batteries, sensors, photocatalysis and catalysis, solar cells, air and solar batteries, and even membranes and touch screen devices.","1499769","2017-03-01","2022-02-28"
"MagBURST","Exploding stars from first principles: MAGnetars as engines of hypernovae and gamma-ray BURSTs","Jérôme GUILET","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","The birth of a neutron star with an extremely strong magnetic field, called a magnetar, has emerged as a promising scenario to power a variety of outstanding explosive events. This includes gamma-ray bursts, among the most luminous events observed up to high redshift and therefore useful as cosmological probes, but also supernovae with extreme kinetic energies called hypernovae and other classes of super-luminous supernovae. Simple phenomenological models, where the magnetar rotation period and magnetic field are adjusted, can explain many of these observations but lack a sound theoretical basis. The goal of this proposal is to develop an ab initio description of magnetar powered explosions in order to delineate the role they play for the production of gamma-ray bursts and super-luminous supernovae. This is urgently needed to interpret the growing diversity of explosions observed with ongoing transient surveys (iPTF, CRTS, Pan-STARRS) and in the perspective of future programs of observations such as SVOM and LSST. By using state-of-the-art numerical simulations, the following outstanding questions will be addressed:
1) What is the origin of the gigantic magnetic field observed in magnetars? The physics of the magnetic field amplification in a fast-rotating nascent neutron star will be investigated thoroughly from first principles. By developing the first global protoneutron star simulations of this amplification process, the magnetic field strength and geometry will be determined for varying rotation rates.
2) What variety of explosion paths can be explained by the birth of fast-rotating magnetars? Numerical simulations of the launch of a hypernova explosion and a relativistic GRB jet will provide the first self-consistent description of both events from a millisecond magnetar. Furthermore, the new understanding of magnetic field amplification will be used to improve the realism of these simulations.","1500000","2017-05-01","2022-04-30"
"MAGCOW","The Magnetised Cosmic Web","Franco VAZZA","ALMA MATER STUDIORUM - UNIVERSITA DI BOLOGNA","On large scales cosmic matter is distributed in a web consistent of clusters, filaments, walls and voids.
 While the dark-matter skeleton of the cosmic web is closely traced by galaxies and galaxy clusters, the gaseous distribution has never been directly imaged at any wavelength. This situation might change within the next decade, thanks to the new generation of radio instruments that will survey the sky: LOFAR, MWA, ASKAP and the Square Kilometer Array. Non-thermal components, relativistic particles and magnetic fields  are thought to have a spatial distribution that is broader than that of thermal baryons. For this reason, the new generation of radio telescopes should might be able to detect the tip of the iceberg from the  rarefied intergalactic medium, provided that magnetic fields are sufficiently amplified in these regions.  The detectable signal is expected to be weak and complex because of the contribution from radio galaxies and to the presence of diffuse fore- and backgrounds.  
 The developments proposed in this ERC proposal are exactly designed to address this complexity, and turn future radio observations into a unique probe of the growth of magnetic fields and of the acceleration of particles. This will be possible through the theoretical exploration of  plasmas in extreme conditions with sophisticated numerical simulations. With these simulations I will be able to predict the specific radio signature for the origin of extragalactic fields. This will enable the community to use radio surveys  in a quantitative way and to determine the origin of extragalactic magnetism, a longstanding puzzle connected to many open questions of modern astrophysics. 
 The legacy of this project will be its quantitative representation of non-thermal processes on the largest scales, ultimately going to be fully exploited by the Square Kilometer Array.","1465943","2017-09-01","2022-08-31"
"Magma Degassing","Defusing volcanic eruptions: the escape of volcanic gas","Kim Berlo","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Volcanic eruptions are driven by the exsolution and escape of dissolved volatiles. Fast and efficient escape of volatiles leads to a lower potential for an explosive eruption: defusing it. Yet, despite recognition of the importance of volatile escape, the mechanisms and kinetics of degassing remain unclear. This study aims to use a pioneering approach to reconstruct the escape of volcanic gases.
Exsolved gases are ephemeral and do not survive eruption. However textural evidence such as vesicles, fractures and veins in erupted magma lingers. Moreover, new data shows that chemical signals of degassing endure, not only in minerals, but also in quenched melt.
Volcanic gases are enriched in metals such as Hg, Tl, and Cu resulting in ore deposits and contributing to global metal emissions. Such enrichment is based on the preference of these metals for a gas phase. This project will establish how metals partition between volcanic gas and melt (basalt and rhyolite), how quickly such equilibrium partitioning is reached, and what can be learned regarding magma degassing from gas emissions and melt compositions as measured at volcanoes.
The first part of the project focuses on obtaining gas-melt partition coefficients and diffusivities of metals. The second part of the project involves comparison to natural samples. Metal concentration variations will be mapped within an exposed magmatic conduit and in recent explosively erupted volcanic rocks. The third part of the project aims to model the escape of volcanic gases using reactive flow modeling.
The combined results of this project will not only show how and how fast volcanic gases escape, but also form the basis of a new approach to quantifying historic (from glass shards) and future (from gas emissions) magmatic metal release to potential ore forming systems as well as to the atmosphere. Moreover, linking gas chemistry to dynamic degassing processes in a quantitative model will aid prediction of eruption style and timing.","1604211","2013-05-01","2018-04-30"
"MAGMIST","From the magnetized diffuse interstellar medium to the stars","Patrick Hennebelle","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","""Understanding star formation remains one of the greatest challenges of modern astronomy. Indeed in this field the progresses have been limited due, first, to the huge dynamics of spatial and temporal relevant scales and, second, the great variety and non-linearity of the physical processes involved in the formation of stars. The present proposal will contribute to provide a complete and coherent picture of the star formation process by self-consistently following the evolution of the interstellar matter from the very diffuse gas up to the protostars. This will be achieved by performing a series of heavy MHD numerical simulations with an adaptive mesh refinement code while subdivising the problem in three major steps namely the formation of large scale molecular clouds, the formation of star forming cores and the collapse of protostellar cores. In particular, the impact of the magnetic field and the radiative processes will be self-consistently treated using appropriate schemes. At each step, comparisons with both analytical models and observations will be performed by using existing models or developing new ones and calculating synthetic observations. The simulation results will also be used to test and improve the methods and the algorithms used by observers to extract the physical information from their data. An existing database, where the simulation results are available, will be further developed. The present proposal pursues two aims: i) achieving a global understanding of the star formation process, in particular by elucidating the link between the physical properties of the large scale ISM and the characteristics of the protostars, such as their mass, magnetisation and angular momentum ii) provide a better insight of the structure, nature and role of the magnetic field and the turbulence from the diffuse to the dense parts of the ISM.""","1312267","2013-01-01","2017-12-31"
"MAGNETALS","Tunable array of magnetic nano-crystals designed at the atomic scale: engineering high performance magnetic materials using hybrid organic-inorganic nano-architectures","Fabien Nicolas Silly","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","The storage density of computer hard drives is growing so rapidly that for new computer drive generations not only optimized materials are needed but also new concepts for data storage. Last decades, higher storage densities on computer disks were achieved by optimization of magnetic materials, i.e. the magnetic grains were gradually shrunk while, at the same time, the magnetic stability was increased. The nowadays smallest storage unit is made up 100 to 600 grains, that form one bit. Each grain is about 10 nanometres in size. These grains are arranged next to each other on substrates that are plated with magnetic metals. Decreasing further the size and amount of the grains necessary for one bit is now irremediably affecting the signal/noise ratio, weaker signals leading to loss of information. Therefore, new concepts for magnetic storage media have to be found.
Material reduced size leads to novel properties totally different from bulk properties. In our project we will engineer matter at the atomic and molecular level and develop advanced construction methods to build new functionalised materials for magnetic storage. We propose a multidisciplinary research project, that aims to explore various aspects related to magnetic properties of highly organised organic-inorganic nano-architectures. We will engineer tunable supramolecular assemblies to host and organise inorganic shape-selected magnetic nanocrystals. Due to the sensitive interrelation of magnetism and the atomic structure of these systems, any induced nanostructure modification will result in changes of the magnetism. Our ability to tailor nanocrystal size, composition, structure, shape and position will allow us to tune magnetism at the atomic scale. We will thus be able to design and produce new high density hybrid nano-architectures having gigantic magnetic performance, i.e., huge magnetostatic energy stored and a high blocking temperature. This research therefore has the potential to make a considerable impact on the high density data storage industry","1499725","2010-11-01","2016-10-31"
"MAGNETIC BEAMS","Magnetically manipulated molecular beams; a novel ultra-sensitive approach for studying the structure and dynamics of water surfaces","Gil Alexandrowicz","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","This proposal is aimed at developing and applying novel ultra-sensitive methods for studying the structure and dynamics of surfaces on an atomic scale, focusing on water surfaces in particular.
The proposal consists of two main instrument development projects which are based on magnetic manipulation of molecular beams: (1) Developing a ground-breaking apparatus which uses a pre-polarized H2O molecular beam in order to perform NMR measurements on dilute surface science systems, measurements which were impossible using conventional NMR approaches. (2)  Developing a unique second-generation helium spin echo spectrometer which is sensitive to motion on an unprecedentedly wide time scale range. This instrument will be capable of measuring atomic scale surface dynamics of systems which were previously beyond the realm of experimentalists.

Both of these novel instruments will be primarily used to study the atomic scale structure and dynamics of water surfaces. Studying these systems is particularly challenging due to the delicate and complex nature of the surface, nevertheless, there is an extensive interest in studying water surfaces due to the key role they play in a wide range of research fields and applications.  Examples include atmospheric chemistry, where ozone depleting reactions are catalyzed on ice surfaces, Material sciences and nano-technology, where the interaction and reactivity of a surface with water can determine the performance of novel miniature devices and even astrophysics where star birth reactions take place on ice surfaces.  We intend to exploit the new contrast mechanisms and the unique time scales made available by the novel instruments we will develop, in order to obtain new experimental insights into this exciting research field.","1850000","2012-10-01","2017-09-30"
"MAGNETIC-SPEED-LIMIT","Understanding the speed limits of magnetism","Stefano BONETTI","STOCKHOLMS UNIVERSITET","While the origin of magnetic order in condensed matter is in the exchange and spin-orbit interactions, with time scales in the subpicosecond ranges, it has been long believed that magnetism could only be manipulated at nanosecond rates, exploiting dipolar interactions with external magnetic fields. However, in the past decade researchers have been able to observe ultrafast magnetic dynamics at its intrinsic time scales without the need for magnetic fields, thus revolutionising the view on the speed limits of magnetism. Despite many achievements in ultrafast magnetism, the understanding of the fundamental physics that allows for the ultrafast dissipation of angular momentum is still only partial, hampered by the lack of experimental techniques suited to fully explore these phenomena. However, the recent appearance of two new types of coherent radiation, single-cycle THz pulses and x-rays generated at free electron lasers (FELs), has provided researchers access to a whole new set of capabilities to tackle this challenge. This proposal suggests using these techniques to achieve an encompassing view of ultrafast magnetic dynamics in metallic ferromagnets, via the following three research objectives: (a) to reveal  ultrafast dynamics driven by strong THz radiation in several magnetic systems using table-top femtosecond lasers; (b) to unravel the contribution of lattice dynamics to ultrafast demagnetization in different magnetic materials using the x-rays produced at FELs and (c) to directly image ultrafast spin currents by creating femtosecond movies with nanometre resolution. The proposed experiments are challenging and explore unchartered territories, but if successful, they will advance the understanding of the speed limits of magnetism, at the time scales of the exchange and spin-orbit interactions. They will also open up for future investigations of ultrafast magnetic phenomena in materials with large electronic correlations or spin-orbit coupling.","1967755","2017-02-01","2022-01-31"
"MagneticYSOs","Interpreting Dust Polarization Maps to Characterize the Role of the Magnetic Field in Star Formation Processes","Anaëlle Julie Maury","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","""Rotation and angular momentum transport play a critical role in the formation and evolution of astrophysical objects, including the fundamental bricks of astrophysical structures: stars. Stars like our Sun form when rotating dense cores, in the interstellar medium, collapse until they eventually reach temperatures at which nuclear fusion begins; while planets, including the Earth, form in the rotationally supported disks around these same young stars. One of the major challenges of modern astrophysics is the “angular momentum problem"""": observations show that a typical star-forming cloud needs to reduce its specific angular momentum by 5 to 10 orders of magnitude to form a typical star such as our Sun. It is also crucial to solve the angular momentum problem to understand the formation of protoplanetary disks, stellar binaries and the initial mass function of newly formed stars. Magnetic fields are one of the key ways of transporting angular momentum in astrophysical structures: understanding how angular momentum is transported to allow star formation requires characterizing the role of magnetic fields in shaping the dynamics of star-forming structures. The MagneticYSOs project aims at characterizing the role of magnetic field in the earliest stage of star formation, during the main accretion phase.
The simultaneous major improvements of instrumental and computational facilities provide us, for the first time, with the opportunity to confront observational information to magnetized models predictions. Polarization capabilities on the last generation of instrument in large facilities are producing sensitive observations of magnetic fields with a great level of detail, while numerical simulations of star formation are now including most of the physical ingredients for a detailed description of protostellar collapse at all the relevant scales, such as resistive MHD, radiative transfer and chemical networks. These new tools will undoubtedly lead to major discovery in the fields of planets and star formation in the coming years. It is necessary to conduct comprehensive projects able to combine theory and observations in a detailed fashion, which in turn require a collaboration with access to cutting edge observational datasets and numerical models. Through an ambitious multi-faceted program of dedicated observations probing magnetic fields (polarized dust emission and Zeeman effect maps), gas kinematics (molecular lines emission maps), ionization rates and dust properties in Class 0 protostars, and their comparison to synthetic observations of MHD simulations of protostellar collapse, we aim to transform our understanding of:
1) The long-standing problem of angular momentum in star formation
2) The origin of the stellar initial mass function
3) The formation of multiple stellar systems and circumstellar disks around young stellar objects (YSOs)
Not only this project will enable a major leap forward in our understanding of low-mass star formation, answering yet unexplored questions with innovative methods, but it will also allow to spread the expertise in interpreting high-angular resolution (sub-)mm polarization data. Although characterizing magnetic fields in astrophysical structures represents the next frontier in many fields (solar physics, evolved stars, compact objects, galactic nuclei are a few examples), only a handful of astronomers in the EU community are familiar with interferometric polarization data, mostly because of the absence of large european facilities providing such capabilities until the recent advent of ALMA. It is now crucial to strengthen the European position in this research field by training a new generation of physicists with a strong expertise on tailoring, analyzing and interpreting high angular resolution polarization data.""","1500000","2016-07-01","2021-06-30"
"MAGNETO","Active Magnetorheological Elastomers: from Hierarchical Composite Materials to tailored Instabilities","Konstantinos Danas","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","In recent years, there has been an increased effort by scientists to obtain new composite materials with extreme properties. Inspired by natural and biological processes, scientists have proposed the use of hierarchical architectures (i.e., assembly of structural components) spanning several length scales from nanometer to centimeter sizes. Depending each time on the desired properties of the composite material, optimization with respect to its stiffness, weight, density, toughness and other properties is carried out. In the present subject, the interest is in magneto-mechanical coupling and tailored instabilities. Hierarchical materials, such as magnetorheological elastomers (MREs) which combine magnetic particles (at the scale of nanometers and micrometers) embedded in a soft polymeric non-magnetic matrix, give rise to a coupled magneto-mechanical response at the macroscopic (order of millimeters and centimeters) scale when they are subjected to combined magneto-mechanical external stimuli. These composite materials can deform at very large strains due to the presence of the soft polymeric matrix without fracturing. From an unconventional point of view, a remarkable property of these materials is that while they can become unstable by combined magneto-mechanical loading, their response is well controlled in the post-instability regime. This, in turn, allows us to try to operate these materials in this critically stable region, similar to most biological systems. These instabilities can lead to extreme responses such as wrinkles (for haptic applications), actively controlled stiffness (for cell-growth) and acoustic properties with only marginal changes in the externally applied magnetic fields. Unlike the current modeling of hierarchical composites, MREs require the development of novel experimental techniques and advanced coupled nonlinear magneto-mechanical models in order to tailor the desired macroscopic instability response at finite strains.","1499206","2015-04-01","2020-03-31"
"MagnonCircuits","Nano-Scale Magnonic Circuits for Novel Computing Systems","Andrii Chumak","TECHNISCHE UNIVERSITAET KAISERSLAUTERN","Magnons – quanta of spin waves – propagating in magnetic materials having nano-scale wavelengths and carrying information in the form of a spin angular momentum, can be used as data carriers in next-generation nano-sized low-loss information processing systems. The low losses of magnonic systems can be reached due to the absence of translational electron motion associated with Joule heat-ing and extremely low magnetic damping in the dielectric Yttrium-Iron-Garnet (YIG) material used.

The recent revolutionary progress in the growth of high-quality YIG films with nanometer thickness, and in the patterning of these films, opened a way to the practical development of nano-scale mag-nonic computing systems. However, the decrease in sizes of YIG structures to sub-100 nm requires the development of the physical knowledge base for understanding linear and nonlinear magnetization dynamics in nanostructures.

The strategic goal of the proposed MagnonCircuits research program is to make a transformative change in the data processing paradigm from traditional electronics to magnon spintronics. The ingre-dients required for such a transformation and addressed by MagnonCircuits are: (i) The fabrication of magnon conduits of sub-100 nm width, the development of a toolbox enabling excitation and de-tection of fast exchange magnons, and the understanding of the physics underlying magnon dynamics at the nano-scale in the exchange interaction regime. (ii) Employment of such novel physical phenom-ena as spin pumping, spin transfer torque and spin Hall effect to overcome the fundamental limita-tions of the state-of-the-art approaches in magnon spintronics, and to compensate the dissipation in magnonic circuits. (iii) Realization of two-dimensional magnonic circuits required for transport and processing of magnon-carried data. A proof-of-concept models of two nano-scale devices – majority gate and magnon transistor – will be developed in the course of MagnonCircuits.","1487969","2016-06-01","2021-05-31"
"MAHNOB","Multimodal Analysis of Human Nonverbal Behaviour in Real-World Settings","Maja Pantic","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","Existing tools for human interactive behaviour analysis typically handle only deliberately displayed, exaggerated expressions. As they are usually trained only on series of such exaggerated expressions, they lack models of human expressive behaviour found in real-world settings and cannot handle subtle changes in audiovisual expressions typical for such spontaneous behaviour. The main aim of MAHNOB project is to address this problem and to attempt to build automated tools for machine understanding of human interactive behaviour in naturalistic contexts. MAHNOB technology will represent a set of audiovisual spatiotemporal methods for automatic analysis of human spontaneous (as opposed to posed and exaggerated) patterns of behavioural cues including head pose, facial expression, visual focus of attention, hands and body movements, and vocal outbursts like laughter and yawns. As a proof of concept, MAHNOB technology will be developed for two specific application areas: automatic analysis of mental states like fatigue and confusion in Human-Computer Interaction contexts and non-obtrusive deception detection in standard interview settings. A team of 5 Research Assistants (RAs), led by the PI and having the background in signal processing and machine learning will develop MAHNOB technology. The expected result after 5 years is MAHNOB technology with the following capabilities: - analysis of human behaviour from facial expressions, hand and body movements, gaze, and non-linguistic vocalizations like speech rate and laughter; - interpretation of user behaviour with respect to mental states, social signals, dialogue dynamics, and deceit/veracity; - near real-time, robust, and adaptive processing by means of incremental processing, robust observation models, and learning person-specific behavioural patterns; - provision of a large, annotated, online dataset of audiovisual recordings providing a basis for benchmarks for efforts in machine analysis of human behaviour.","1736800","2008-09-01","2013-08-31"
"MajorNet","Majorana neutrino discovery strategy with CMS","Lesya SHCHUTSKA","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","The Large Hadron Collider at CERN is successfully operating at a record-breaking energy and is expected to deliver an enormous dataset of 300/fb of proton-proton collisions in the next 6 years. I propose to take advantage of both the unprecedented collision energy and the large number of events to be recorded in order to search for signs of new particles which could uncover the nature of dark matter, provide an explanation for the matter-antimatter asymmetry of the universe or answer questions about the origin of neutrino masses. To achieve this, I will build on my expertise devising and organizing new physics searches with leptons and will establish and lead a new multi-signature research programme at the CMS experiment. This programme will exploit both the intensity and energy frontiers opening up at the LHC and can be further expanded at the high-luminosity LHC (HL-LHC). Since the masses and couplings of the new particles are unconstrained, my proposed search strategy will be such that a wide mass range will be covered also encompassing many orders of magnitude in the coupling strength.","1500000","2018-01-01","2022-12-31"
"MAKEITSIMPLE","Make it simple: towards a new era for organic synthesis","Igor Larrosa Guerrero","THE UNIVERSITY OF MANCHESTER","Organic synthesis has undeniably made tremendous progress over the past two centuries. Nevertheless, our ability to efficiently synthesise molecules is mostly limited to targets of low structural complexity. Traditional synthetic strategies require the presence of reactive functional groups that are used as handles for further functionalisation. This requirement is one of the factors dramatically enhancing the difficulty of syntheses. The last two decades have seen the emergence of a more straightforward alternative: the direct functionalisation of C-H bonds. Through this strategy the typically inert C-H bonds, ubiquitous in organic molecules, can be activated by transition metal catalysts and subsequently functionalised. This approach has allowed us to dream of a future where any organic molecule could be synthesised in a direct manner by simply replacing the C-H bonds of a substrate with the required functionalities, as if building a ball-and-stick molecular model with our hands. The development of a full set of C-H functionalisation methodologies will impact on all applied areas, such as the synthesis of pharmaceuticals, agrochemicals, and new materials. Furthermore, their atom efficiency and low waste generation ensures a privileged position among the green chemistry methods.

For this strategy to succeed, numerous challenges are still to be overcome. In this research proposal we aim at addressing one of them: the C-H functionalisation of aromatic compounds. We will build up a toolkit of complementary methodologies to functionalise aromatic C-H bonds under mild conditions (energy efficient), with broad functional group tolerance (general), and with absolute control of the regioselectivity. By the end of the five years we aim to have developed a robust general methodology allowing the coupling of any two arenes via double C-H bond activation.","1493855","2011-10-01","2016-09-30"
"MALIG","A mathematical approach to the liquid-glass transition: kinetically constrained models, cellular automata and mixed order phase transitions","cristina Toninelli","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","This proposal focuses on the mathematics of three cross-disciplinary, very active and deeply interlaced research themes:  interacting particle systems with kinetic constraints, bootstrap percolation cellular automata and mixed order phase transitions. These topics belong to the fertile area of mathematics at the intersection of probability and mathematical statistical mechanics. They are also extremely important in physics. Indeed they are intimately connected to the fundamental problem of understanding the liquid-glass transition, one of the longstanding open questions in condensed matter physics.
The funding of this project will allow the PI to lead a highly qualified team with complementary expertise. Such a diversity  will allow a novel, interdisciplinary and potentially groundbreaking approach. Even if research on each one of the above topics has been lately quite lively, very few exchanges and little cross-fertilization occurred among them. One of our main goals is to overcome the barriers among the three different research communities and to explore the interfaces of these yet unconnected fields. We will open two novel and challenging chapters in the mathematics of interacting particle systems and cellular automata: interacting particle glassy systems and bootstrap percolation models with mixed order critical and discontinuous transitions. In order to achieve our groundbreaking goals we will have to go well beyond the present mathematical knowledge. We believe that the novel concepts and the unconventional  approaches that we will develop will have a deep impact also in other areas including combinatorics, theory of randomized algorithms and complex systems.  
The scientific background and expertise of the PI, with original and groundbreaking contributions in each of the above topics and with a broad and clearcut vision of the mathematics of the proposed research as well as of the fundamental physical questions,make the PI the ideal leader of this project.","883250","2016-09-01","2021-08-31"
"MaMBoQ","Macroscopic Behavior of Many-Body Quantum Systems","Marcello PORTA","EBERHARD KARLS UNIVERSITAET TUEBINGEN","This project is devoted to the analysis of large quantum systems. It is divided in two parts: Part A focuses on the transport properties of interacting lattice models, while Part B concerns the derivation of effective evolution equations for many-body quantum systems. The common theme is the concept of emergent effective theory: simplified models capturing the macroscopic behavior of complex systems. Different systems might share the same effective theory, a phenomenon called universality. A central goal of mathematical physics is to validate these approximations, and to understand the emergence of universality from first principles.

Part A: Transport in interacting condensed matter systems. I will study charge and spin transport in 2d systems, such as graphene and topological insulators. These materials attracted enormous interest, because of their remarkable conduction properties. Neglecting many-body interactions, some of these properties can be explained mathematically. In real samples, however, electrons do interact. In order to deal with such complex systems, physicists often rely on uncontrolled expansions, numerical methods, or formal mappings in exactly solvable models. The goal is to rigorously understand the effect of many-body interactions, and to explain the emergence of universality.

Part B: Effective dynamics of interacting fermionic systems. I will work on the derivation of effective theories for interacting fermions, in suitable scaling regimes. In the last 18 years, there has been great progress on the rigorous validity of celebrated effective models, e.g. Hartree and Gross-Pitaevskii theory. A lot is known for interacting bosons, for the dynamics and for the equilibrium low energy properties. Much less is known for fermions. The goal is fill the gap by proving the validity of some well-known fermionic effective theories, such as Hartree-Fock and BCS theory in the mean-field scaling, and the quantum Boltzmann equation in the kinetic scaling.","982625","2019-02-01","2024-01-31"
"MANITOP","Massive Neutrinos: Investigating their Theoretical Origin and Phenomenology","Werner Rodejohann","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","The aim of the proposed project is to shed light on the theoretical origin of neutrino masses and to explore the phenomenological consequences of the model predictions and of possible mechanisms giving rise to neutrino mass. The results of many upcoming experiments in the neutrino sector and beyond will be a crucial discriminator for models and will have to be followed closely. Apart from the usual neutrino oscillation observables, there are more model-dependent implications of neutrino mass models, for instance lepton flavor violating decays and electric dipole moments in the charged lepton sector, or processes involving new particles at colliders such as the LHC. The connection to the baryon asymmetry of the Universe, to dark matter and to proton decay will also be studied. Phenomenology will also be focussed on: in particular, the implications of upcoming (precision) experiments on the neutrino mass and mixing parameters or the neutrino mass matrix will be investigated. The prospects of using high energy neutrino cosmic rays, neutrinoless double beta decay (including analogous processes) and new experimental ideas to probe the parameters of neutrino physics will also be explored.","790800","2008-09-01","2012-08-31"
"MANYBO","Many-body physics in gauge fields with ultracold Ytterbium atoms in optical lattices","Fabrice Gerbier","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","In this project, we will investigate the many-body physics of interacting ultracold atoms in presence of strong gauge fields. The practical implementation will use Ytterbium atoms in optical lattices. We will use two atoms in two internal states- the ground state and a long-lived excited state- trapped in suitably designed state-dependent lattice potentials. Coherent coupling between the two states will be used to ``write'' a spatially-dependent phase on the atomic wavefunction, which under suitable conditions will mimic the Aharonov-Bohm phase accumulated by charged particles moving in a gauge field. Using this technique, we will study the behavior of interacting bosonic and fermionic quantum gases in such artificial gauge potentials for different lattice geometries. We will look for strongly correlated states analogous to those observed for 2D electrons experiencing the fractional quantum Hall effect, and study the unusual behavior of their elementary excitations (``anyons''). These novel quantum phases will be primarily characterized using high-sensitivity imaging with single-site resolution, enabling spatially-resolved measurements of the spatial distribution and of its correlation functions. The project will first investigate the simpler case of an Abelian gauge potentials for bosons and fermions, then move to the more complex case of a non-Abelian $SU(2)$ gauge field using two-component fermions. The resulting system can be seen as a laboratory playground to study interacting quantum matter (bosonic or fermionic) coupled to well-defined gauge fields, a situation encountered in many domains of Physics, from high-energies to condensed matter.","1099913","2010-11-01","2015-10-31"
"MapCat","High spatial resolution mapping of catalytic reactions on single nanoparticles","Elad Gross","THE HEBREW UNIVERSITY OF JERUSALEM","Catalytic nanoparticles are heterogeneous in their nature - and even within the simplest particles structural and compositional differences exist and affect the overall performances of a catalyst. Thus non-disruptive, detailed chemical information at the nanoscale is essential for understanding how surface properties direct the reactivity of these particles. Infrared spectroscopy offers a low-energy route towards conducting in-situ, high spatial resolution mapping of catalytic reactions on the surface of single nanoparticles, yielding the influence of various physiochemical properties on the catalytic reactivity. 
In the project my team will employ recently developed Infrared nanospectroscopy measurements to provide high spatial resolution mapping of catalytic reactions on the surface of metallic nanoparticles, while using chemically active N-heterocyclic carbene molecules as indicators for surface reactivity. With this setup I will address fundamental questions in catalysis research and identify, on a single particle basis and under reaction conditions, the ways by which the size, structure, composition and metal-support interactions direct the reactivity of metallic nanoparticles in hydrogenation, oxidation and functionalization reactions. My research group demonstrated recently the feasibility of this novel approach by which structure-reactivity correlations were identified within single nanoparticles. Knowledge gained in this project will provide in-depth understanding of the basic elements that control the reactivity of heterogeneous catalysts and enable the development of optimized catalysts based on rational design. Moreover, one can foresee wide application potential for this experimental approach in various other research fields like batteries and fuel cells, in which high spatial resolution analysis of reactive surfaces is essential for understanding structure-reactivity correlations.","1846009","2019-01-01","2023-12-31"
"MAQD","Mathematical Aspects of Quantum Dynamics","Benjamin Schlein","UNIVERSITAT ZURICH","The main goal of this proposal is to reach
a better mathematical understanding of
the dynamics of quantum mechanical
systems. In particular I plan to work
on the following three projects along
this direction. A. Effective Evolution
Equations for Macroscopic Systems.
The derivation of effective evolution
equations from first principle microscopic
theories is a fundamental task of statistical
mechanics. I have been involved in
several projects related to the derivation
of the Hartree and the Gross-Piteavskii
equation from many body quantum
dynamics. I plan to continue to work on
these problems and to use these results
to obtain new information on the many
body dynamics. B. Spectral Properties
of Random Matrices. The correlations
among eigenvalues of large random
matrices are expected to be independent
of the distribution of the entries. This
conjecture, known as universality, is
of great importance for random matrix
theory. In collaboration with L. Erdos and
H.-T. Yau, we established the validity of
Wigner&amp;apos;s semicircle law on
microscopic scales, and we proved the
emergence of eigenvalue repulsion. In
the future, we plan to continue to study
Wigner matrices to prove, on the longer
term, universality. C. Locality Estimates in
Quantum Dynamics. Anharmonic lattice
systems are very important models in
non-equilibrium statistical mechanics.
With B. Nachtergaele, H. Raz, and R.
Sims, we proved Lieb-Robinson type
inequalities (giving an upper bound on
the speed of propagation of signals), for
a certain class of anharmonicity. Next, we
plan to extend these results to a larger
class of anharmonic potentials, and to
apply these bounds to establish other
fundamental properties of the dynamics
of anharmonic systems, such as the
existence of its thermodynamical limit.","750000","2009-12-01","2014-11-30"
"MARCAN","Topographically-driven meteoric groundwater – An important geomorphic agent","Aaron Micallef","HELMHOLTZ ZENTRUM FUR OZEANFORSCHUNG KIEL","Topographically-driven meteoric (TDM) recharge is a key driver of offshore groundwater systems because sea level has been lower than at present for 80% of the last 2.6 million years. Groundwater has been implicated as an important agent in the geomorphic evolution of passive continental margins and the canyons that incise them. However, the geomorphic efficacy of groundwater remains dubious, and a diagnostic link between landscape form and groundwater processes remains poorly quantified, especially for bedrock and cohesive sediments. Obstacles that prevent going beyond the current state-of-knowledge include: (i) a focus on terrestrial contexts and a lack of mechanistic understanding of groundwater erosion/weathering; (ii) limited information on offshore groundwater architecture, history and dynamics. By addressing the role of TDM offshore groundwater in the geomorphic evolution of the most prevalent types of continental margins, MARCAN is expected to open new scientific horizons in continental margin research and bring about a step-change in our understanding of some of the most widespread and significant landforms on Earth. The project’s methodology is rooted in an innovative, multi-scale and multidisciplinary approach that incorporates: (i) the most detailed 3D characterisation of TDM offshore groundwater systems and their evolution during an integral glacial cycle, based on state-of-the-art marine data and hydrogeologic models, and (ii) the development of a comprehensive continental margin geomorphic evolution model, based on realistic laboratory simulations, accurate field measurements and advanced numerical solutions. By placing better constraints on past fluid migration histories, MARCAN will also have strong applied relevance, primarily by improving assessment and exploitation of offshore freshwater as a source of drinking water.","1757432","2017-01-01","2021-12-31"
"MARCHES","Modelling of Architectures Ruled by Coupled or Heightened Excited States","Denis, Marc, Hugues, Marie Jacquemin","UNIVERSITE DE NANTES","The goal of the MARCHES project is to rationalise and optimize the interplay between electronically excited-states in complex molecular architectures. The simulation of the properties of large conjugated architectures is to be performed with ab initio tools explicitly taking into account environmental effects.  Though efficient methods able to tackle such task are to be conceived during this project, we aim to enlighten coupled excited-states so to pave the way towards chemically-intuitive designs of new molecules. Indeed, the rationalisation and optimisation of the excited-state properties of large compounds is not only one of the major challenges of computational chemistry and physics, it also opens new horizons for emergent properties. In that framework, this project will allow to design molecular switches usable as building blocks for complex logic gates, subsequently unlocking crucial steps towards more efficient storage materials. To this end, compounds containing several photochromic switches coupled at the excited state have to be designed: this is an important challenge. Indeed, photochromes are actually limited to uncoupled or simply additive systems: emergent multi-addressable features are impossible to achieve.","1500000","2012-01-01","2016-12-31"
"MASE","Modelling the Archaean Subduction Environment","Jeroen Van Hunen","UNIVERSITY OF DURHAM","""Today, subduction dominates the Earth’s appearance: it drives plate tectonics, and plays a dominant role in continental crust formation. If and how subduction operated 2.5-4 billion years ago, in the Archaean, is debated, primarily on the basis of the sparse Archaean geological record. It seems likely that some form of subduction occurred at least by the late Archaean, but may well have looked different from today’s. A proper understanding of this Archaean ‘subduction’ is essential, since so many processes are likely to depend on it.

Observations of the geological (mostly isotope-geochemical) record have provided an invaluable window to peer into the Archaean world. But inferred Archaean geodynamics from these observations are non-unique. Various models fit the same data within uncertainty, and often lack a firm physical basis. To overcome these shortcomings, I propose a novel, forward approach of predicting synthetic geochemical fingerprints from numerical, geodynamically consistent physical models, and comparing those with geochemical observations. This will be used to constrain and better understand the two most pressing questions in Earth sciences: How did plate tectonics evolve, and how did continents form? In particular, this project aims to:
1) assess quantitatively the geodynamical and geochemical viability of intermittent plate tectonics;
2) test the various proposed models for the formation of Archaean continental crust;
Comparison of calculated synthetic geochemistry (e.g. Re-Os data, rare-Earth element data) from geodynamical models with available datasets will provide powerful diagnostics to distinguish viable models.

In addition, this work will also directly relevant for the evolution of the Earth’s surface, and to the differences with the other terrestrial planets. Finally, there are potential economic benefits, since the world’s largest mineral deposits (e.g. gold) occur in Archaean terrains and have been associated to subduction.""","1490738","2012-01-01","2016-12-31"
"MASPIC","Spin currents in magnetic nanostructures","Mathias Kläui","JOHANNES GUTENBERG-UNIVERSITAT MAINZ","MaSpic will create an autonomous team at the University of Konstanz to investigate the interaction between magnetization, spin - polarized and pure diffusive spin currents using novel instrumentation and innovative theoretical approaches. A thorough understanding of the fundamental charge and spin transport interaction mechanisms, key to use of the spin degree of freedom for Spintronics, will be developed. To understand the interplay between spin-polarized charge currents and magnetization configurations (adiabatic vs. non-adiabatic electron transport), the reciprocal effects of magnetization on the current (magnetoresistance) and of the current on magnetization (spin transfer torque) will be correlated for the same spin structures. Non-intrusive high resolution imaging at variable temperature will be used to probe the non-adiabaticity and help understand the hotly debated influence of thermal excitations on transport. Pure diffusive spin currents will be efficiently generated and used to manipulate magnetization with ultra-low power dissipation. The poorly understood spin current generation by the Spin Hall Effect and spin current propagation will be probed by direct imaging and the sign of the spin accumulation and influence of scattering determined to separate intrinsic and extrinsic effects. For the measurements a unique variable temperature high resolution SEMPA imaging system will be acquired and further developed including ultra-fast electrical contacts. Theoretical modelling using an atomistic Heisenberg approach will go beyond the conventional micromagnetic calculations that are limited to 0K. To understand thermal transport effects, temperature dependent simulations with adiabatic and non-adiabatic spin torque terms will accompany experiments. Realistic lattice structures and heterostructures will be modelled to simulate the influence of the pure spin currents on the magnetization using spatially varying interface torque terms, not previously possible.","1610786","2008-08-01","2014-04-30"
"MATERIALIZABLE","MATERIALIZABLE: Intelligent fabrication-oriented Computational Design and Modeling","Bernd BICKEL","INSTITUTE OF SCIENCE AND TECHNOLOGYAUSTRIA","While access to 3D-printing technology becomes ubiquitous and provides revolutionary possibilities for fabricating complex, functional, multi-material objects with stunning properties, its potential impact is currently significantly limited due to the lack of efficient and intuitive methods for content creation. Existing tools are usually restricted to expert users, have been developed based on the capabilities of traditional manufacturing processes, and do not sufficiently take fabrication constraints into account. Scientifically, we are facing the fundamental challenge that existing simulation techniques and design approaches for predicting the physical properties of materials and objects at the resolution of modern 3D printers are too slow and do not scale with increasing object complexity. The problem is extremely challenging because real world-materials exhibit extraordinary variety and complexity.

To address these challenges, I suggest a novel computational approach that facilitates intuitive design, accurate and fast simulation techniques, and a functional representation of 3D content. I propose a multi-scale representation of functional goals and hybrid models that describes the physical behavior at a coarse scale and the relationship to the underlying material composition at the resolution of the 3D printer. My approach is to combine data-driven and physically-based modeling, providing both the required speed and accuracy through smart precomputations and tailored simulation techniques that operate on the data. A key aspect of this modeling and simulation approach is to identify domains that are sufficiently low-dimensional to be correctly sampled. Subsequently, I propose the fundamental re-thinking of the workflow, leading to solutions that allow synthesizing model instances optimized on-the-fly for a specific output device. The principal applicability will be evaluated for functional goals, such as appearance, deformation, and sensing capabilities.","1497730","2017-02-01","2022-01-31"
"MathAm","Mathematical Structures in Scattering Amplitudes","Claude Duhr","EUROPEAN ORGANIZATION FOR NUCLEAR RESEARCH","Among the most important mathematical quantities of interest in high-energy particle physics are the so-called scattering amplitudes, which allow us to make predictions for physical observables. Despite their importance, performing explicit computations of scattering amplitudes is still one of the bottlenecks of high-energy physics, mostly due to the complexity of the integrals involved and a lack of understanding of the underlying mathematics. 

Over the last couple of years, a deep connection between scattering amplitudes and certain branches of modern mathematics has slowly started to emerge. The goal of MathAm is investigate in detail the relationship between scattering amplitudes, number theory and algebraic geometry, with the final aim of developing novel computational techniques for scattering amplitudes that are beyond reach of conventional state-of-the-art technology.

The ultimate goal of MathAm is threefold: By revealing unexpected connections between seemingly disconnected areas of mathematics and physics, MathAm will 
1. shed new light on the mathematical underpinnings of the fundamental laws of Nature in general.
2. play a decisive role in testing recent conjectures about the all-loop structure of certain special classes of gauge 3. theories by confronting them to the explicit results for scattering amplitudes,
3. set a new standard for making predictions for collider experiments like the LHC by performing computations that are beyond reach of current technology. 

To sum up, MathAm has a unique multi-disciplinary character and, by applying novel technology from modern mathematics, its results will have impact in various seemingly disconnected disciplines, ranging from the frontiers of research in pure mathematics over formal aspects of Quantum Field Theory all the way to making the most precise theoretical predictions for the LHC experiments.","1365060","2015-09-01","2020-08-31"
"MATHANA","Mathematical modeling of anaesthetic action","Axel Hutt","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","General anaesthesia is an important method in today's hospital practice and especially in surgery. To supervise the depth of anaesthesia during surgery, the anaesthesist applies electroencephalography (EEG) and monitors the brain activity of the subject on the scalp. The applied monitoring machine calculates the change of the power spectrum of the brain signals to indicate the anaesthetic depth. This procedure is based on the finding that the concentration increase of the anaesthetic drug changes the EEG-power spectrum in a significant way. Although this procedure is applied world-wide, the underlying neural mechanism of the spectrum change is still unknown. The project aims to elucidate the underlying neural mechanism by a detailed investigating a mathematical model of neural populations.
The investigation is based on analytical calculations in a neural population model of the cortex involving intrinsic neural properties of brain areas and feedback loops to other areas, such as the loop between the cortex and the thalamus. Currently, there are two proposed mechanisms for the charactertisic change of the power spectrum: a highly nonlinear jump in the activation (so-called phase transition) and a linear behavior. The project mainly focusses on the nonlinear jump to finally rule it out or support it. A subsequent comparison to previous experimenta results aims to fit the physiological parameters.  Since the cortex population is embedded into a network of other cortical areas and the thalamus, the corresponding analytical investigations takes into account external stochastic (from other brain areas) and time-periodic (thalamic) forces. To this end it is necessary to develop several novel nonlinear analysis technique of neural populations to derive the power spectrum close to the phase transition and conditions for physiological parameters.","856500","2011-01-01","2015-10-31"
"MATISSE","Middle Infrared Broadly Tunable Compact Cavity-Less Source based on Parametric Conversion","Camille-Sophie Bres","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","""The middle-infrared (Mid-IR) is a core band for molecular detection and identification as nearly every molecule strongly absorbs in this wavelength range. Hence, it hosts critical applications ranging from spectroscopy, health monitoring, sensing as well as free space communication. Unfortunately, the advantages of this wavelength range have not been fully exploited due to the lack of Mid-IR sources that can simultaneously satisfy stringent requirements these diverse applications impose. To this end, MATISSE seeks to transfigure light generation in the Mid-IR spectral band by designing new classes of nonlinear frequency mixers. We aim at overturning the limitations of conventional resonant (cavity) processes by simultaneously enabling mode-hop free tuning, narrow linewidth and wide tunable range operation. In addition, we also plan in developing the first modulation-capable source across a wide range of Mid-IR frequencies. To overcome these challenges, MATISSE will leverage the cavity-less nature and parametric origin of the proposed source. The project will cascade two distinct  compact nonlinear platforms to enable the efficient wavelength conversion from NIR to Mid-IR. This two-step approach takes advantage of attributes each distinct platform has to offer by 1) utilizing the advanced technology available at the NIR wavelengths and 2) exploiting the Mid-IR capabilities of non-silica platforms.  We believe only an approach such as ours can provide a compact, freely tunable Mid-IR source without resorting to conventional cavity (resonant) physics. The MATISSE project therefore promises substantial impact in spectroscopy, sensing and free space communication by providing presently non-existing tools that will not only improve the sensitivity of absolute and relative spectroscopic methods, but also introduce free space sensing/communication using emitted powers that are considered non-viable in conventional systems.""","1499972","2013-02-01","2018-01-31"
"MATKIT","Mathematical Aspects of Kinetic Theory","Clément Mouhot","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","""The main goal of the project is to reach a better mathematical understanding of the (integro)-partial differential equations from kinetic theory, in particular their qualitative and asymptotic behavior, derivation from many particle systems, and singular limits. Although various evolution problems from physics shall be considered, the paradigmatic ones are the Boltzmann equation for gas dynamics and the Vlasov-Poisson equation for plasmas and galactic dynamics.

The methodology is focused on the developement of conceptual tools and mathematical techniques. It shall put therefore the emphasize on the structures common to several problems, with a view to their possible application to other fields of mathematical analysis. The methodology is also characterized by the search, whenever possible, of constructive quantitative methods of proofs, and by the attention payed to the qualitative meaning of the mathematical results obtained for physics.

The tasks related to the general goal of the project are organized into the following four parts:
I. Space-independent kinetic equations for describing microscopic interactions (Cauchy problem for long-ranged interactions, granular gases and self-similarity).
II. Transport equations and phase mixing (Landau damping for Vlasov equations, inviscid damping for 2-dimensional incompressible fluids).
III. How transport and collisions mix: hypocoercivity (spectral and stability analysis of hypocoercive collisional operators according to the local equilibrium space and the geometry of confinement).
IV. Derivation of kinetic equations (mean-field and Boltzmann-Grad limits by semigroup approach).

I have been involved in many recent progresses related to these aspects and I aim at constructing a team around me in order to achieve these tasks and objectives. Kinetic theory is developing a growing rate, and the construction of such a team in Europe would be timely.""","1150000","2011-09-01","2016-08-31"
"MatMech","Live Tapings of Material Formation: Unravelling formation mechanisms in materials chemistry through Multimodal X-ray total scattering studies","Kirsten Marie Ørnsbjerg Jensen","KOBENHAVNS UNIVERSITET","With this proposal, I want to develop a new, multimodal approach to in situ X-ray scattering studies to unravel formation mechanisms of the solid state. The aim of the project is to develop a unified view of metal oxide nucleation processes on the atomic scale: From precursor complexes over pre-nucelation clusters to the final crystalline particle.
The development of new materials relies on our understanding of the relation between material structure, properties and synthesis. While the intense focus on ‘materials by design’ have made it possible to predict the properties of many materials given an atomic arrangement, actually knowing how to synthesize it is a completely different story. Material synthesis methods are to a large degree developed by extensive parameter studies based on trial-and-error experiments. Specifically, our knowledge of particle nucleation is lacking, as even non-classical views on nucleation such as the concept of pre-nucleation clusters do not apply an atomistic view of the formation process. Here, I want to use new methods in X-ray total scattering and Pair Distribution Function analysis to follow nucleation processes to establish the framework needed for predictive material synthesis. One of the large challenges in studying nucleation is the lack of a characterization method that can give structural information on materials without long-range order. I have demonstrated that time-resolved X-ray total scattering gives new possibilities for following structural changes in a synthesis, and the use of total scattering has opened for a new view on material formation. However, the complexity of the structures involved in nucleation processes is too large to obtain sufficient information from X-ray total scattering alone. Here, I will combine X-ray total scattering data with complementary techniques using a new multimodal approach for complex modelling analysis, providing a unifying view on material nucleation.","1493269","2019-02-01","2024-01-31"
"MATRIX","Mixed-Matrix Interfaces for Enhanced Fine Chemicals Downstream Processing and Monitoring","Thomas Schäfer","UNIVERSIDAD DEL PAIS VASCO/ EUSKAL HERRIKO UNIBERTSITATEA","In the production of fine chemicals the downstream processing may amount up to 75 % of the total manufacturing costs, particularly when the product is aimed at nutritional or therapeutic use which demands a maximum purity. Current downstream processing strategies can face limitations whenever existing purification and separation processes are based on physico-chemical principles and thus exhibit a far more limited selectivity for the target compounds than is known from interactions involving biomolecules. The latter, on the other hand, are less robust and often require specific operating conditions (typically close to ambient) and feed compositions, which often restricts their broad application. This proposal presents an innovative, interdisciplinary approach on designing selective and tuneable interfaces for sensors and separations to be employed in downstream processing techniques. By combining recent advances in biology/biochemistry and chemistry and applying them to chemical engineering principles, the approach goes beyond concepts currently known, involving primarily engineered nucleic acids (aptamers) and tuneable ionic liquids (IL) incorporated in an appropriate support structure. The aim of this proposal is to make use of the unique properties of both biomolecules and ionic liquids in order to create supported mixed-matrix interfaces which exhibit properties that bulk materials can hardly achieve. Another novelty of this proposal is the aim to develop these interfaces in a modular way, namely by joining individually optimised elements during interface design in order to yield the desired overall properties. This approach warrants a maximum of degrees of freedom in the overall interface design while keeping the basic interface architecture constant, and thus overcomes one of the limitations that many conventional separation/purification techniques encounter during optimisation of the bulk material.","1498138","2008-08-01","2014-07-31"
"MAtrix","In silico and in vitro Models of Angiogenesis: unravelling the role of the extracellular matrix","Hans Pol S Van Oosterwyck","KATHOLIEKE UNIVERSITEIT LEUVEN","Angiogenesis, the formation of new blood vessels from the existing vasculature, is a process that is fundamental to normal tissue growth, wound repair and disease. The control of angiogenesis is of utmost importance for tissue regenerative therapies as well as cancer treatment, however this remains a challenge. The extracellular matrix (ECM) is a one of the key controlling factors of angiogenesis. The mechanisms through which the ECM exerts its influence are poorly understood. MAtrix will create unprecedented opportunities for unraveling the role of the ECM in angiogenesis. It will do so by creating a highly innovative, multiscale in silico model that provides quantitative, subcellular resolution on cell-matrix interaction, which is key to the understanding of cell migration. In this way, MAtrix goes substantially beyond the state of the art in terms of computational models of angiogenesis. It will integrate mechanisms of ECM-mediated cell migration and relate them to intracellular regulatory mechanisms of angiogenesis.
Apart from its innovation in terms of computational modelling, MAtrix’ impact is related to its interdisciplinarity, involving computer simulations and in vitro experiments. This will enable to investigate research hypotheses on the role of the ECM in angiogenesis that are generated by the in silico model. State of the art technologies (fluorescence microscopy, cell and ECM mechanics, biomaterials design) will be applied –in conjunction with the in silico model- to quantity cell-ECM mechanical interaction at a subcellular level and the dynamics of cell migration. In vitro experiments will be performed for a broad range of biomaterials and their characteristics. In this way, MAtrix will deliver a proof-of-concept that an in silico model can help in identifying and prioritising biomaterials characteristics, relevant for angiogenesis. MAtrix’ findings can have a major impact on the development of therapies that want to control the angiogenic response.","1497400","2013-04-01","2018-03-31"
"Matryoshka","Fast Interactive Verification through Strong Higher-Order Automation","Jasmin Christian Blanchette","STICHTING VU","Proof assistants are increasingly used to verify hardware and software and to formalize mathematics. However, despite the success stories, they remain very laborious to use. The situation has improved with the integration of first-order automatic theorem provers -- superposition provers and SMT (satisfiability modulo theories) solvers -- through middleware such as Sledgehammer for Isabelle, codeveloped by the PI; but this research has now reached the point of diminishing returns. Only so much can be done when viewing automatic provers as black boxes.

To make interactive verification more cost-effective, we propose to deliver very high levels of automation to users of proof assistants by fusing and extending two lines of research: automatic and interactive theorem proving. This is our grand challenge. Our starting point is that first-order automatic provers are the best tools available for performing most of the logical work. Our approach will be to enrich superposition and SMT with higher-order reasoning in a careful manner, to preserve their desirable properties. We will design proof rules and strategies, guided by representative benchmarks from interactive verification.

With higher-order superposition and higher-order SMT in place, we will develop highly automatic provers building on modern superposition provers and SMT solvers, following a novel stratified architecture. To reach end users, these new provers will be integrated in proof assistants, including Coq, Isabelle, and the TLA+ Proof System, and will be available as backends to more specialized verification tools. The users of proof assistants and similar tools stand to experience substantial productivity gains: In the past five years, the success rate of automatic provers on interactive proof obligations from a representative benchmark suite has risen from 47% to 77%; with this project, we aim at 90%--95% proof automation.","1498438","2017-03-01","2022-02-28"
"MATTERDESIGN","New Science and Technology of Artificial Layered Structures and Devices","Rahul Raveendran Nair","THE UNIVERSITY OF MANCHESTER","The extensive and growing library of layered crystals opens up the exciting possibility of exfoliating them into single atomic layers and stacking these atomic layers back together in new sequences to fabricate a new class of artificial van der Waals solids, with novel properties and functionalities. The principal objectives of this proposal are to study the fundamental physics and chemistry of novel materials, structures and devices synthesised by assembling different two-dimensional (2D) crystals and to exploit their unique properties for diverse applications. 

The applicant primarily aims to explore the new science and technology of extremely narrow 2D capillaries synthesised by assembling various precisely patterned 2D crystals. Molecular transport through nano-capillaries is highly important due to its applicability in filtration and separation technology and also because of the unusual fundamental behaviour arising at the molecular scale. In this proposal, the applicant plans to study the behaviour of molecules, atoms and ions confined in the ultra-narrow 2D channels and explore the use of these devices for nanofluidic applications. Furthermore, the potential of these 2D channels for the synthesis of artificial structures by encapsulating various materials inside the channels will be investigated. The applicant plans to use a variety of techniques, such as chemical exfoliation and mechanical exfoliation, together with already established layer-by-layer coating or multiple transfer techniques, to produce novel materials and devices. The applicant will also investigate the potential of these novel layered materials for the fabrication of novel superconductors by intercalating them with various elements. The urge in pursuing this project relies on the fact that many intercalated graphite and other layered compounds are superconducting. The above research directions should create a solid basis for a world-leading research group led by the applicant.","1456253","2016-05-01","2021-04-30"
"MAZEST","M- and Z-estimation in semiparametric statistics : applications in various fields","Ingrid Van Keilegom","UNIVERSITE CATHOLIQUE DE LOUVAIN","The area of semiparametric statistics is, in comparison to the areas of fully parametric or nonparametric statistics, relatively unexplored and still in full development. Semiparametric models offer a valid alternative for purely parametric ones, that are known to be sensitive to incorrect model specification, and completely nonparametric models, which often suffer from lack of precision and power. A drawback of semiparametric models so far is, however, that the development of mathematical properties under these models is often a lot harder than under the other two types of models. The present project tries to solve this difficulty partially, by presenting and applying a general method to prove the asymptotic properties of estimators for a wide spectrum of semiparametric models. The objectives of this project are twofold. On one hand we will apply a general theory developed by Chen, Linton and Van Keilegom (2003) for a class of semiparametric Z-estimation problems, to a number of novel research ideas, coming from a broad range of areas in statistics. On the other hand we will show that some estimation problems are not covered by this theory, we consider a more general class of semiparametric estimators (M-estimators called) and develop a general theory for this class of estimators. This theory will open new horizons for a wide variety of problems in semiparametric statistics. The project requires highly complex mathematical skills and cutting edge results from modern empirical process theory.","750000","2008-07-01","2014-06-30"
"MB2","Molecular Biomimetics and Magnets Biomineralization: Towards Swimming Nanorobots","Damien Faivre","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Nature not only provides inspiration for designing new materials but also teaches us how to use interparticle and external forces to structure and assemble these building blocks into functional entities. Magnetotactic bacteria and their chain of magnetosome represent a striking example of such an accomplishment where a simple living organism precisely tune the properties of inorganics that in turn guide the cell movement thereby providing an energetic advantage vs. the non-magnetotactic counterparts.
In this project, we will develop a bio-inspired research based on magnetotactic bacteria. We will combine the recent developments of nanoscale engineering in the chemical science and the latest advances in molecular biology to create a novel methodology enabling first, the understanding of the control of biological determinants over single inorganic building blocks at the nanoscale and over highly-organized hierarchical structures, and second, the use of these biomacromolecules to construct new functional materials.
We will use phage display to genetically select peptides specifically binding to magnetite and look for homology within the available genomes of the different strains of magnetotactic bacteria in order to detect promising biological determinants. We will screen the identified compounds by our in-house developed high-throughput technique based on force microscopy. On the one hand, the effect of the high potential biological determinant on the properties of magnetic nanoparticles will be tested under physiological conditions in biomimetic reactor. On the other hand, we will use the knowledge gained from the binding capacities of the peptides to functionalize magnetite nanoparticles and assemble them in order to eventually form a swimming nanorobots that can be directed by an external magnetic field while transporting beads.","1485000","2011-01-01","2015-12-31"
"MC2","Mixed-phase clouds and climate (MC2) – from process-level understanding to large-scale impacts","Trude STORELVMO","UNIVERSITETET I OSLO","The importance of mixed-phase clouds (i.e. clouds in which liquid and ice may co-exist) for weather and climate has become increasingly evident in recent years. We now know that a majority of the precipitation reaching Earth’s surface originates from mixed-phase clouds, and the way cloud phase changes under global warming has emerged as a critically important climate feedback. Atmospheric aerosols may also have affected climate via mixed-phase clouds, but the magnitude and even sign of this effect is currently unknown. Satellite observations have recently revealed that cloud phase is misrepresented in global climate models (GCMs), suggesting systematic GCM biases in precipitation formation and cloud-climate feedbacks. Such biases give us reason to doubt GCM projections of the climate response to CO2 increases, or to changing atmospheric aerosol loadings. This proposal seeks to address the above issues, through a multi-angle and multi-tool approach: (i) By conducting field measurements of cloud phase at mid- and high latitudes, we seek to identify the small-scale structure of mixed-phase clouds. (ii) Large-eddy simulations will then be employed to identify the underlying physics responsible for the observed structures, and the field measurements will provide case studies for regional cloud-resolving modelling in order to test and revise state-of-the-art cloud microphysics parameterizations. (iii) GCMs, with revised microphysics parameterizations, will be confronted with cloud phase constraints available from space. (iv) Finally, the same GCMs will be used to re-evaluate the climate impact of mixed-phase clouds in terms of their contribution to climate forcings and feedbacks. Through this synergistic combination of tools for a multi-scale study of mixed-phase clouds, the proposed research has the potential to bring the field of climate science forward, from improved process-level understanding at small scales, to better climate change predictions on the global scale.","1500000","2018-03-01","2023-02-28"
"MCUNLEASH","Model Checking Unleashed","Martin Lange","UNIVERSITAET KASSEL","Model checking is a technique which is being used successfully in automatic program. More than 25 years of research on model checking has brought forward various techniques and tricks which can handle complex tasks or just massive structures as caused by the state-space explosion problem for instance.
Mathematically, the model checking problem is to determine whether or not a given relational structure is a model of a given formula, typically of a temporal logic. In the domain of program verification, very weak temporal logics suffice in order to express relevant properties. Temporal logics of higher expressive power can naturally express more complex properties. In particular, there already are logics which can express all kinds of decision problems in the range of NP and
above.
In this project we will investigate the use of advanced model checking techniques for the solving of decision and computation problems in all kinds of diverse areas like bio-informatics, database querying, network routing, computational linguistics, etc. We will, for example, identify important problems in this area, encode them as model checking problems in logics of suitable expressive power, and use partial evaluation, manual optimisations, and the vast toolbox of model checking specialisations in order to obtain new and competitive algorithms for and insights into such problems. We will design suitable temporal logics where there are none, and build model checking algorithms which serve as a generic starting point for the solving of such problems.","1364405","2010-12-01","2015-11-30"
"MDEPUGS","Measuring Dark Energy Properties Using Galaxy Surveys","William James Percival","UNIVERSITY OF PORTSMOUTH HIGHER EDUCATION CORPORATION","Observations have shown that the expansion of the Universe is accelerating. At present, there is no physically motivated model to explain this, directly demonstrating that current theories of fundamental particles and gravity are either incorrect or incomplete. Understanding this phenomenon is consequently one of the most important outstanding problems in the whole of science. Perhaps the simplest solution is to postulate a new component of energy density with an effective negative pressure that forms 75\% of the present energy density of the Universe. There are many other theoretical ideas for explaining this phenomenon, which are often collectively referred to as ``dark energy''. The research in this proposal is designed to help to understand dark energy by setting constraints on the cosmological expansion. The spatial distribution of matter and its evolution are sensitive probes of the matter/energy content of the Universe. Consequently galaxy surveys, which trace this distribution, provide key avenues for understanding dark energy through a number of different techniques. One technique, which is the focus of the research proposed here, uses ``Baryon Acoustic Oscillations'' to constrain dark energy. Baryon Acoustic Oscillations are patterns of galaxies, initiated by sound waves in the early Universe, that form a fixed scale in the Universe whose true size we know. The apparent scale of these ``standard rulers'', when observed in galaxy surveys, constrains the distance--redshift relation and consequently how dark energy is driving the acceleration of the Universe. In this proposal, I request funding to set up a team to develop the novel techniques required to extract these distance measurements from the next generation of galaxy surveys, and apply them to measure dark energy properties. This will develop European leadership in a key future field of observational cosmology.","880000","2008-10-01","2014-03-31"
"ME4OER","Mechanism Engineering of the Oxygen Evolution Reaction","Marcel RISCH","HELMHOLTZ-ZENTRUM BERLIN FUR MATERIALIEN UND ENERGIE GMBH","I propose innovative strategies to elucidate and engineer the electrocatalytic mechanism of earth-abundant transition metal oxides with the aim of enhancing the low efficiency of the oxygen evolution reaction (OER). Mastering multi-electron reactions such as the OER is critical for the transition from dwindling fossil fuels to ecologically and economically sustainable fuels based on renewable energy. Water is the most abundant source of hydrogen bonds on earth and fuels based on these bonds have the highest energy densities, which makes water an attractive resource for sustainable fuels production. However, the production of any hydrogen-based fuel from water is currently thwarted by the low efficiency of the OER. Improved catalysts are presently designed by optimizing a single step in the reaction sequence. In contrast, I target the low efficiency of the OER by engineering multiple steps of the mechanism to (i) control the number of electron transfers before the limiting step; and (ii) enforce a reaction path close to the thermodynamic limit. Combining these two strategies increases the catalytic current of transition metal oxides at typical overpotentials by a factor of 100,000. Rational design of the mechanism on this fundamental level calls for unprecedented insight into the active state of electrocatalysts. My team will achieve this firstly by novel approaches to prepare catalytically limiting states for their elucidation by synchrotron-based X-ray spectroscopy and secondly by studying transitions between these states in pioneering time-resolved experiments. Both the required breakthroughs in method development and the innovative scientific strategies are generalizable to other multi-electron reactions, which opens the door for industrial catalysts that store energy sustainably in hydrogen-based fuels on a global scale.","1499980","2019-03-01","2024-02-29"
"MECHANO-FLUO","Mechanofluorochromism: from molecular engineering to the elaboration of smart materials","Clémence Alice Françoise ALLAIN","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The MECHANO-FLUO project aims at preparing mechanofluorochromic molecules and materials, understand and tune their photophysical and mechanofluorochromic properties, and implement these new materials as quantitative mechanical force sensors. Mechanofluorochromism relates to fluorescent compounds in the solid state for which emission spectrum changes upon application of a mechanical stimulus. The interest for this phenomenon has enormously increased for the last 4 years but studies aiming at understanding the structure-mechanofluorochromic properties relationships are still in their infancy and many examples remain purely qualitative. In the MECHANO-FLUO project, I will synthesize a library of mechanofluorochromic molecules responsive to different mechanical stimuli (pressure, shearing), with various sensitivities. I aim at relating the molecular structure to the sensitivity to different mechanical stimuli. Two aspects seldom explored in the literature so far will be particularly studied. The first one is the study at the micro- to nanoscale, so as to obtain an amplification of the mechanofluorochromic response and develop ultra-sensitive mechanofluorochromic probes. The second one is the quantification of the mechanofluorochromic response, from the nano- to the macroscopic scales. Two applications will be investigated. The first one is stress metrology: fluorescent materials with a quantitative mechanofluorochromic response could give access to the stress level in various materials and thus constitute an entirely new method for in situ control of the damaging of materials classically used in the nuclear industry or the aeronautics. The second one is mechanobiology: I hope to provide a tool for direct force measurement at the cellular scale, which would have tremendous implications for the comprehension of biological phenomena where mechanotransduction is implied, especially embryogenesis and tumor proliferation.","1499172","2017-03-01","2022-02-28"
"MECHANOBIO","Finite element simulations of mechanobiology in tissue engineering","Damien Lacroix","THE UNIVERSITY OF SHEFFIELD","The influence of mechanical stimuli on cell behaviour also known as mechanobiology has led to the development of mechano-regulation theories and finite element simulations that predict tissue formation in regenerative medicine. Computer simulations can explore mechanotransduction processes on the cellular level which are not possible to measure experimentally. However, most studies are limited to continuum macroscopic description of the tissues and therefore are inadequate to relate macroscopic loading to microscopic mechanical stimuli. The main objective of this project is to reach new frontiers in mechanobiology with the development of a new approach in the modelling of tissue engineering with an integration of the microscopic modelling of cells with the macroscopic modelling of the scaffold. A discrete approach to model the porous scaffold will be combined with a multitude analyses of single cell biomechanics attached onto the scaffold. In order to validate the overall methodology, each of the different modelling steps will be modelled through the development of in vitro experiments on adult human mesenchymal stem cells. The breakthrough of this project will change considerably the methodology used previously by the scientific community in this field and in the development of the biomedical field regarding computer modelling. The impact of this project will bring a better understanding of the local mechanical stimuli on cells and the understanding of the translation of mechanical macroscopic loading onto microscopic loading.","1498497","2011-10-01","2016-09-30"
"MechanoSignaling","Predicting cardiovascular regeneration: integrating mechanical cues and signaling pathways","Sandra LOERAKKER","TECHNISCHE UNIVERSITEIT EINDHOVEN","The key challenge in regenerative medicine is to re-establish a physiological tissue organization as this is conditional for proper tissue functionality. In the cardiovascular field, tissue engineering of blood vessels and heart valves requires the development of a tri-laminar structure. Previous attempts to establish this organization have been mainly trial-and-error based. Therefore, to force breakthroughs and accelerate clinical translation, computational modeling is critical to understand and predict the process of neo-tissue regeneration starting from non-living biodegradable materials (i.e. scaffolds).
The main drivers of regeneration are (1) hemodynamic loads that trigger mechanically-driven tissue growth and remodeling, and (2) signaling interactions between cells that control the emergence of global tissue organization (e.g. layering of vessels and valves). While the first aspect currently receives vast attention, the modeling of cell signaling in the context of tissue engineering remains an unexplored area. In this project, I aim to obtain a mechanistic understanding of how a critical pathway in the cardiovascular system, i.e. the Notch signaling pathway, drives the emergence of global tissue organization while interacting with mechanical cues. I will adopt a unique, multi-disciplinary approach, where quantitative in vitro experiments will be performed to inform novel multi-scale computational models of Notch signaling and its consequences on regeneration. I will leverage these models to understand and predict in vivo regeneration of engineered cardiovascular tissues starting from various initial conditions.
If successful, this project will have a tremendous impact on the development of rational guidelines for ensuring functional tissue regeneration, which represents a breakthrough towards creating cardiovascular replacements that are superior to current treatment options. Moreover, it enables me to start my own independent research group in this field.","1498526","2019-01-01","2023-12-31"
"MechJointMorph","The role of mechanical forces induced by prenatal movements in joint morphogenesis","Niamh Catherine Nowlan","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","Most joints start off the same during embryonic development, as two opposing cartilage surfaces, and are moulded into the diverse range of shapes seen in the adult in a process known as morphogenesis. While we understand very little of the biological or mechanobiological processes driving joint morphogenesis, there is evidence to suggest that fetal movements play a critical role in joint shape development. Developmental Dysplasia of the Hip (DDH), where the hip is partly or fully dislocated, is much more common when the baby’s movement is restricted or prevented. This proposal will determine how mechanical forces influence joint shape morphogenesis, which is of key relevance to neonatal joint conditions such as DDH, to adult joint health and disease, and to tissue engineering of cartilage. A mouse line in which mutant embryos have no skeletal muscle will be studied, providing the first in depth analysis of mammalian joint shape development for normal and abnormal mechanical environments. The mouse line could provide the first mammalian model system for prenatal onset DDH. ‘Passive’ movements of these mutant embryos will then be induced by massage of the mother, and the effects on the joints measured. If the effects on joint shape of absent spontaneous movement are mitigated by the treatment, this technique could eventually be used as a preventative treatment for DDH. Next, an in vitro approach will be used to quantify how much movement is needed for joint shape development. This research will provide an optimised protocol for applying biophysical stimuli to promote cartilage growth and morphogenesis in culture, providing valuable cues to cartilage tissue engineers. Finally, a computational simulation of joint shape morphogenesis will be created, which will integrate the new understanding gained from the experimental research in order to predict how different joints shapes develop in normal and abnormal mechanical environments.","1499501","2014-01-01","2018-12-31"
"MECTRL","Measurement-based dynamic control of mesoscopic many-body systems","Jacob Friis Sherson","AARHUS UNIVERSITET","Quantum control is an ambitious framework for steering dynamics from initial states to arbitrary desired final states. It has over the past decade been used extensively with immense success for control of low- dimensional systems in as varied fields as molecular dynamics and quantum computation. Only recently have efforts been initiated to extend this to higher-dimensional many-body systems. Most generic quantum control schemes to date, however, put quite heavy requirements on the controllability of either the system Hamiltonian or a set of measurement operators. This will in many realistic scenarios prohibit an efficient realization.

Within this proposal, I will develop a new quantum control scheme, which is minimalistic on system requirements and therefore ideally suited for the efficient and reliable optimization of many-body control problems. The fundamentally new ingredient is the total quantum evolution dictated by a combination of fixed many-body time evolution and the precise knowledge of the quantum back-action due to repeated quantum non-destruction (QND) measurements of a single projection operator. 
The main focus of this proposal is theoretical and experimental quantum engineering of the dynamics in systems, which are sufficiently small to calculate the measurement back-action exactly and sufficiently large to have interesting many-body properties.
Recent experimental advances in single site manipulation of bosons in optical lattices have enabled the high fidelity preparation exactly such mesoscopic samples of atoms (5-50). This forms an ideal starting point for many-body quantum control, and we will i.a. demonstrate engineering of quantum phase transitions and preparation of highly non-classical Schödinger cat states.

Finally, using the results from an online graphical interface allowing users of the internet to solve quantum problems we will attempt to build next-generation optimization computer algorithms with a higher level of cognition built in.","1499406","2015-05-01","2020-04-30"
"MEGA-XUV","Efficient megahertz coherent XUV light source","Thomas Südmeyer","UNIVERSITE DE NEUCHATEL","""Coherent extreme ultraviolet (XUV) light sources open up new opportunities for science and technology. Promising examples are attosecond metrology, spectroscopic and structural analysis of matter on a nanometer scale, high resolution XUV-microscopy and lithography. The most promising technique for table-top sources is femtosecond laser-driven high-harmonic generation (HHG) in gases. Unfortunately, their XUV photon flux is not sufficient for most applications. This is caused by the low average power of the kHz repetition rate driving lasers (<10 W)  and the poor conversion efficiency (<10-6). Following the traditional path of increasing the power, numerous research teams are engineering larger and more complex femtosecond high-power amplifier systems, which are supposed to provide several kilowatts of average power in the next decade. However, it is questionable if such systems can easily serve as tool for further scientific studies with XUV light.
The goal of this proposal is the realization of a simpler and more efficient source of high-flux XUV radiation. Instead of amplifying a laser beam to several kW of power and dumping it after the HHG interaction, the generation of high harmonics is placed directly inside the intra-cavity multi-kilowatt beam of a femtosecond laser. Thus, the unconverted light is “recycled”, and the laser medium only needs to compensate for the low losses of the resonator. Achieving passive femtosecond pulse formation at these record-high power levels will require eliminating any destabilizing effects inside the resonator. This appears to be only feasible with ultrafast thin disk lasers, because all key components are used in reflection.
Exploiting the scientific opportunities of the resulting table-top multi-MHz coherent XUV light source in various interdisciplinary applications is the second major part of this project. The developed XUV source will be transportable, which will enable the fast implementation of joint measurements.""","1500000","2012-03-01","2017-02-28"
"MEGANTE","MEasuring the Gravitational constant with Atom interferometry for Novel fundamental physics TEst","Gabriele ROSI","ISTITUTO NAZIONALE DI FISICA NUCLEARE","Starting from the original experiment performed by Henry Cavendish more than two centuries ago, the precision determination of the gravitational constant G remains a challenging endeavour. It has been measured about a dozen times over the last 50 years, but the results have varied much more than what would be expected from random and
systematic errors. Likely, this is due to the fact that, so far, all the past experiments have relied on macroscopic classical instruments, which could all be governed by uncontrolled mechanical influences. On the other hand, a recent controversial study about correlations between the measured values of G and the variations of the length of day seems to suggest that some other not well-understood effects could be present.

MEGANTE will address all these issues by carrying out precision G determinations making use of original experimental strategies based on quantum sensors. Unprecedented accuracy levels will be achieved using cold atoms in free-fall to probe the gravitational field, surpassing thus the state-of-art measurements based on torsion balance and simple pendulum.

In parallel, MEGANTE will provide results that go far beyond the pure metrological interest. Indeed, owing the lack of a full understanding of gravity, several theoretical models predict new physics phenomena such violations of the inverse square law or a dependency of the G value from the local density of the matter. These aspects of the gravitational interactions will be thoroughly examined during the project, improving current constrains on those theories.

MEGANTE will define a novel paradigm for precision G measurements and experiments on gravitation, paving the way for a final resolution of a two-centuries-old problem in metrology.","1550000","2019-02-01","2024-01-31"
"MEGASIM","Million-core Molecular Simulation","Berk Hess","KUNGLIGA TEKNISKA HOEGSKOLAN","Molecular simulation has become a standard tool for studying the function of biomolecules, such as proteins, nucleic acids and lipids. Due to increasing computer power and decreasing length scales in engineering, molecular simulation is also increasingly used in microfluidics and the study of, for instance, small water droplets. All these applications would benefit strongly from simulations that are several orders of magnitude longer than the current state of art. Although currently Moore's law still holds, the performance of processor cores no longer doubles every 18 months, but rather the number of cores increases. Therefore to improve the performance and to scale to a million cores, each core should do less work. With the classical single-program multiple-data parallelism the communication time will quickly become a bottleneck. To advance the molecular simulation field and efficiently use upcoming million core computers, a switch to multiple-program multiple-data parallelism (MPMD) is required. Domain decomposition should be applied over the nodes, whereas within a node MPMD parallelism should be used. This requires workloads being divided and dispatched efficiently to different threads. To hide the communication times, calculation should be overlapped with communication. Because simulation time steps will soon take in the order of 100 microseconds, global communication will become a bottleneck. However,global communication is required for, among other things, full electrostatics algorithms. Thus new algorithms need to be derived to ensure parallel scaling. Only with such efforts we will be able to fully utilize the potential of upcoming hardware to solve current and future scientific problems.","899448","2011-05-01","2017-04-30"
"MEMBRANESACT","Biological Membranes in Action: A Unified Approach
to Complexation, Scaffolding and Active Transport","Ana-Suncana Barišic Smith","FRIEDRICH-ALEXANDER-UNIVERSITAET ERLANGEN NUERNBERG","In recent breakthrough publications, the effect of fluctuations on the affinity of membrane-confined molecules has been evaluated, and a quantitative model for the time evolution of small adhesion domains has been developed under my leadership. Now I propose to bring my research to a new level by tackling the problem of active and passive organisation of proteins into macromolecular structures on fluctuating fluid membranes, using a physicist’s approach across established disciplinary boundaries.

The formation and transport of supramolecular complexes in membranes is ubiquitous to nearly all functions of biological cells. Today, there is a variety of experiments suggesting that macromolecular complexes act as scaffolds for free proteins, overall yielding obstructed diffusion, counterbalanced by active transport by molecular motors. However, an integrative view connecting complexation and transport is largely missing. Furthermore, the effects of membrane mediated interactions and (non)-thermal fluctuations were so far overlooked. Gaining a quantitative insight into these processes is key to understanding the fundamental functioning of cells.

Together with my carefully selected team, I will address these intrinsically biological problems, by means of theoretical physics. Phenomena such as active and anomalous transport, as well as complexation are also currently subject to intense research in the statistical and soft matter physics communities. In this context, the aim of this proposal is to bridge the divide between the two worlds and significantly contribute to both physics and the life sciences by developing general principles that can be applied to processes in cells. Resolving these issues is of fundamental importance since it would identify how interactions on the cell surface arise, and may translate directly into pharmaceutical applications.","1500000","2013-10-01","2019-09-30"
"MEMCAD","Memory Compositional Abstract Domains:
Certification of Memory Intensive Critical Softwares","Xavier Philippe Rival","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","Every year, software bugs cost hundreds of millions of euros to compagnies and administrations. A number of disasters such as the Ariane 5 first flight failure can are due to faulty softwares. Static analysis aims at computing automatically properties of softwares, so as to prove they are exempt from some class of bugs. In the last ten years, static analysis of numeric intensive applications improved dramatically so that the certification of safety properties like the absence of runtime errors in industrial size control-command, numeric intensive applications, such as Airbus fly-by-wire softwares is now feasible.
By contrast, the situation is much worse for memory intensive softwares. Existing static analyzers for such softwares do not scale to large scale softwares, and fail to prove strong invariants on large classes of softwares. These limitations stem from the fact they use a monolithic algebra of logical formulas (or abstract domain).
Our proposal is based on the observation that the complex memory properties that need be reasoned about should be decomposed in combinations of simpler properties. Therefore, in static analysis, a powerful memory abstract domain could be designed by combining several simpler domains, specific to common memory usage patterns. The benefit of this novel vision is twofold: first it would make it possible to simplify drastically the design of complex abstract domains required to reason about complex softwares, hereby allowing certification of complex memory intensive softwares by automatic static analysis; second, it would enable to split down and better control the cost of the analyses, thus significantly helping scalability.
This shift of focus will bring both theoretical and practical improvements to the program certification field. We propose to build a static analysis framework for reasoning about memory properties, and put it to work on important classes of applications, including large safety critical memory intensive softwares.","1489663","2011-10-01","2017-09-30"
"MEMETRE","From processes to modelling of methane emissions from trees","Mari PIHLATIE","HELSINGIN YLIOPISTO","Atmospheric concentration of the strong greenhouse gas methane (CH4) is rising with an increased annual growth rate. Biosphere has an important role in the global CH4 budget, but high uncertainties remain in the strength of its different sink and source components. Among the natural sources, the contribution of vegetation to the global CH4 budget is the least well understood. Role of trees to the CH4 budget of forest ecosystems has long been overlooked due to the perception that trees do not play a role in the CH4 dynamics. Methanogenic Archaea were long considered as the sole CH4 producing organisms, while new findings of aerobic CH4 production in terrestrial vegetation and in fungi show our incomplete understanding of the CH4 cycling processes. Enclosure measurements from trees reveal that trees can emit CH4 and may substantially contribute to the net CH4 exchange of forests.

The main aim of MEMETRE project is to raise the process-based understanding of CH4 exchange in boreal and temperate forests to the level where we can construct a sound process model for the soil-tree-atmosphere CH4 exchange. We will achieve this by novel laboratory and field experiment focusing on newly identified processes, quantifying CH4 fluxes, seasonal and daily variability and drivers of CH4 at leaf-level, tree and ecosystem level. We use novel CH4 flux measurement techniques to identify the roles of fungal and methanogenic production and transport mechanisms to the CH4 emission from trees, and we synthesize the experimental work to build a process model including CH4 exchange processes within trees and the soil, transport of CH4 between the soil and the trees, and transport of CH4 within the trees. The project will revolutionize our understanding of CH4 flux dynamics in forest ecosystems. It will significantly narrow down the high uncertainties in boreal and temperate forests for their contribution to the global CH4 budget.","1908652","2018-02-01","2023-01-31"
"MemoMOFEnergy","Constructing polar rotors in metal-organic frameworks for memories and energy harvesting","Monique VAN DER VEEN","TECHNISCHE UNIVERSITEIT DELFT","I seek to develop new ferroelectrics based on metal-organic frameworks with dipolar rotors. Ferroelectrics are targeted to be used as physically flexible memories and mechanical energy harvesters for biocompatible sensors and implantable monitoring devices. 

As ferroelectrics can store and switch their polarity, they can be used as memories. Via the piezoelectric effect, they can harvest mechanical vibrations. The materials most compatible with flexible substrates, are soft matter materials. However, these so far don’t meet the requirements. Especially lacking is a combination of  i) polarisation stability, ii) a sufficiently low energy barrier for polarisation switching and iii) fast switching. As energy harvesters, soft matter materials are hampered by low piezoelectric coefficients.

The main objective of this proposal is rational design of ferroelectrics by obtaining a fundamental understanding of the relation between structure and properties. I will achieve this by uniquely synthesizing polar rotors into 3D crystalline scaffolds that allow to alter the rotors’  nano-environement. I will achieve this via polar ligands in metal-organic frameworks (MOFs). The variability of MOFs allows to tune the nature of the hindrance towards rotation of the polar rotors. The tuneable flexibility allows to regulate the energy harvesting efficiency. Moreover, MOFs have already shown potential as biocompatible materials that can be integrated on physically flexible substrates. 

The research consists of i) synthesis of polar rotor MOFs with targeted variations, ii) reliable characterisation and computational modelling of the electronic properties, iii) nanoscopic insight in the switching dynamics. The approach allows to understand how ferro- and piezoelectricity are related to the materials’ structure, and hence to develop materials with exceptional performance. My recent observation of the ferroelectric behaviour of a nitrofunctionalised MOF is the basis for this proposal.","1500000","2018-01-01","2022-12-31"
"MERCURY ISOTOPES","Exploring the isotopic dimension of the global mercury cycle","Jeroen Sonke","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Mass-independent fractionation (MIF) of isotopes in terrestrial geochemical processes was first observed in 1983 for oxygen and in 2000 for sulfur isotopes. Recently mercury (Hg) was added to this shortlist when isotopic anomalies were observed for Hg s two odd isotopes, 199Hg and 201Hg in biological tissues. The objective of the MERCURY ISOTOPES project is to take Hg MIF beyond the initial discovery, and use it to address major outstanding scientific questions of societal and philosophical interest. Similar to the profound insights that carbon and oxygen isotope systematics have brought to climate research, we propose to use variations in Hg isotopic compositions to fingerprint natural and anthropogenic sources, quantify isotope fractionation processes, and provide new constraints on models of mercury cycling.

The MERCURY ISOTOPES project centres on the use of mercury MIF to understand global Hg dynamics at different time scales, from the Pleistocene to modern times. Three main themes will be investigated: 1. the modern Hg cycle focusing on Asian urban-industrial emissions related to coal burning, 2. recent atmospheric Hg deposition in the Arctic, recent Arctic Ocean Hg records from archived biological tissues, and post-glacial Hg deposition from 10,000 yr old ombrotrophic peat records along a mid-latitude   sub-Arctic gradient. 3 Continuous atmospheric Hg speciation and isotopic monitoring at the Pic du Midi Observatory (Pyrenees).

By tapping information from the isotopic dimension of Hg cycling, including revolutionary mass-independent effects, I expect a maximum scientific impact while supporting a socially relevant and urgently needed investigation at the frontier of isotope geosciences.","1176924","2010-12-01","2015-11-30"
"MesoFermi","Mesoscopic Fermi Gases","Henning Moritz","UNIVERSITAET HAMBURG","This proposal brings together the fields of ultracold Fermi gases and of mesoscopic systems. Starting with a two-dimensional (2D) Fermi gas, we will imprint small-scale potential structures onto the atoms. Thus, a mesoscopic system embedded in a 2D reservoir is produced.

Specifically, we will imprint optical dipole potentials varying on a micrometre scale onto a 2D gas of 6Li atoms. Due to the widely different energy scales, the entropy of the atoms in the mesoscopic structures will be massively reduced as compared to the reservoir atoms. The atoms in the mesoscopic structures will be characterised by an innovative detection scheme with single atom sensitivity. The combination of mesoscopic potentials, single atom detection and entropy reduction will put us in a unique position to access new regimes of many-body physics.

First, we will investigate a mesoscopic realisation of the 2D Hubbard model. Beyond the study of the fermionic Mott insulating phase and its excitations, the possibility to study staggered Hubbard models and create domain structures is a very attractive prospect. Most importantly, the massive entropy reduction inherent to the mesoscopic approach will enable us to observe antiferromagnetic ordering, the major milestone central to further progress in the field.

Going beyond periodic structures, we will focus on the direct creation of mesoscopic model systems. In a bottom-up approach, we will realise a plaquette consisting of 2x2 sites, the essential building block for models of d-wave superconductivity. The creation of 1D structures with local defects will open the possibility to study phenomena such as spin-charge separation, Friedel oscillations and the rectification of atomic transport. Finally, the physics of open quantum systems will become accessible when studying the interaction between mesoscopic system and reservoir.  In conclusion, I believe that the proposed research programme will bring a new level of functionality to the field.","1236060","2013-10-01","2018-09-30"
"MESOPROBIO","Mesoscopic models for propagation in biology","Vincent CALVEZ","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","According to biologists, there is a need for quantitative models that are able to cope with the complexity of problems arising in the field of life sciences. Here, complexity refers to the interplay between various scales that are not clearly separate. The great challenge of the MESOPROBIO project is to analyse complex PDE models for biological propagation phenomena at the mesoscale. By analogy with the kinetic theory of gases, this is an intermediate level of description between the microscale (individual-based models) and the macroscale (parabolic reaction-transport-diffusion equations). The specific feature common to all the models involved in the project is the local heterogeneity with respect to a structure variable (velocity, phenotypical trait, age) which requires new mathematical methods. I propose to push analysis beyond classical upscaling arguments and to track the local heterogeneity all along the analysis.

The biological applications are: concentration waves of bacteria, evolutionary aspects of structured populations (with respect to dispersal ability or life-history traits), and anomalous diffusion. The mathematical challenges are: multiscale analysis of PDE having different properties in different directions of the phase space, including nonlocal terms (scattering, competition), and possibly lacking basic features of reaction-diffusion equations such as the maximum principle. The outcomes are: travelling waves, accelerating fronts, approximation of geometric optics, nonlocal Hamilton-Jacobi equations, optimal foraging strategies and evolutionary dynamics of phenotypical traits. Emphasis will be placed on quantitative results with strong feedback towards biology.

The project will be conducted in Lyon, a French hub for mathematical biology and hyperbolic equations. There will be close interaction with biologists in order to establish the most appropriate questions to answer. Several collaborations in Europe (UK, Austria) will be developed.","1091688","2015-09-01","2020-08-31"
"MESOQMC","Quantum Monte-Carlo in mesoscopic devices","Xavier Waintal","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","Understanding electronic correlations remains one of the biggest challenges of theoretical condensed matter physics. Mesoscopic systems, where electronic confinement can be externally controlled, are natural test beds for understanding the effects of correlations, and the lack of proper techniques to take them into account is acute. This project aims at developing new tools for simulating correlated quantum mesoscopic devices. We will combine standard approaches for transport in mesoscopic quantum systems with new quantum Monte-Carlo algorithms designed to capture correlations in those devices. We will use modern programming paradigms to develop a versatile numerical platform designed to be easily used by other research groups. These numerical tools will be closely related to existing analytical approaches so that we shall be able to make contact with standard many-body theory while go beyond the limitations of the analytical approaches. We will apply this new set of techniques to several problems that have been puzzling the community for some time including quantum transport in low-density two-dimensional gases for both bulk disordered systems (“Two dimensional metal-insulator transition”) and quantum point contacts (“0.7 anomaly”). We will also apply our techniques to several new problems of increasing importance: at finite-frequency, electron-electron interactions play a central role and must be taken into account properly. We will discuss high frequency measurements such as quantum capacitances, ac conductance or photo-assisted transport in a variety of materials (twodimensional gases of electrons or holes, graphene, semi-conductor nanowires…) and leverage on our new numerical tools to go beyond the standard mean field description.","1222176","2011-01-01","2015-12-31"
"MESOTAS","Chatting with Neurons: A novel approach to the study of neurophysiologic responses of neuronal tissue in vitro, combining nanotechnology, tissue engineering, microfluidics and neuroelectrophysiology","Regina Luttge","TECHNISCHE UNIVERSITEIT EINDHOVEN","Laboratory-on-a-Chip technology was introduced in this field. To avoid the complexity of an animal model and to reduce the number of animals for pre-clinical research cell culture models are important. Here, the combination of microfluidics, tissue engineering and neuroelectrophysiology on MEA-chips is suggested. Because neuronal tissue on chip may act differently from the neurons in their natural environment, the first objective is to follow a systems engineering approach to realize a platform technology, which allows us to reliably co-culture cells in a 3D interconnected configuration, providing an artificially vascularized system on a MEA. For on-line monitoring of the culturing conditions, we will implement micro-total analysis systems (TAS) technology proposing microchip capillary electrophoresis, potentially coupled to mass spectrometry, to correlate electrophysiology with neurochemistry. Previously, it has been demonstrated that physical and chemical micro- and nanostructures influence cell guidance, viability and cell differentiation, so far, unfortunately without a unifying theory to explain the involved mechanisms. Therefore, our second objective is to further our understanding with respect to the influence of nanocues, implementing microfluidic programming to activate porous nanostructures on MEA and investigate cellular signaling and pathway reactions related to the cell’s adhesion mechanism. Combining the first and the second objective will allow us to work towards clinical questions of neurodynamic diseases as epilepsy, characterized by intermittent abnormal synchronization of different neuronal populations. We hypothesize that for these disorders, 3D cell co-culture models will resemble the natural neural networks more closely than 2D, which may subsequently serve as a model to study novel therapeutic procedures, for instance selective neurostimulation. Thus, we propose, as our third objective, nanostimulation of neuronal subsystem.","1260000","2011-10-01","2016-09-30"
"METAMECH","Template assisted assembly of METAmaterials using
MECHanical instabilities","Andreas Fery","LEIBNIZ-INSTITUT FUR POLYMERFORSCHUNG DRESDEN EV","The concept of optical metamaterials has opened completely new perspectives for manipulation of electromagnetic radiation. In contrast to homogenous materials, metamaterials enable qualitatively new features like negative index of refraction. These give rise to new fundamental physical effects, but as well to completely new applications like super-lensing or loss-free molding of the flow of light. Still, practical demonstrations of metamaterials for the near-IR and visible frequency range are scarce and limited to microscopic sizes. This is due to the fact that their assembly relies on complex and resource-consuming lithographic processes, which cannot be up-scaled well.
This proposal aims at establishing a radically different assembly route for metamaterials, which is up-scalable to macroscopic areas and greatly reduces processing effort.
The methodology relies on the synthesis of tailored colloidal building blocks, so-called meta-atoms, which combine metals and insulators in a well defined geometry. These meta-atoms will subsequently be assembled into hierarchical structures using templates fabricated by controlled wrinkling of elastomeric substrates. The use of mechanical instabilities in template formation eliminates lithographic steps in materials assembly. Structures and assembly processes will be optimized based on theory and simulation and morphological and optical properties will be investigated on meta-atom and metamaterials level.
We target negative-index metamaterials, metamaterials for transformation optics and a new class of elastically deformable metamaterials. Upscaling of metamaterial formation on macroscopic dimensions will become feasible and the materials will become available for a broader academic and industrial community, making them in the mid-term available for applications in energy (light harvesting, light concentrators), information (manipulation of light flow) and medical technology (sensing).","1469646","2012-10-01","2017-09-30"
"MetamorphChip","Dynamic Microfluidic Structures for Analysis of Single Cell Systems","Moran Bercovici","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","The interaction and communication between individual cells plays a central role in virtually all fields of biology, from the cooperative work of cells in the immune system, through the differentiation of stem cells, and to the proliferation of cancer cells. In recent years it has been shown that these processes are fundamentally coupled to cell-to-cell heterogeneity and variability. Despite the fact that studying cellular ensembles obscures these fundamental biological processes, most current studies consider cell populations, largely due to technological limitations in the ability to dynamically compartmentalize, manipulate, and analyze single cells. 

We propose to develop and demonstrate a new concept for a single-cell-level bioanalytical workspace that is dynamically configurable in real time. Making use of electrokinetically driven surface deformations, a physical mechanism recently invented in my lab, the MetamorphChip will be able to dynamically modify its own microfluidic structure, thus allowing complete freedom in the manipulation of individual cells and their environment, in real time. 

The project is divided into 5 aims:
 
1. Deepening our physical understanding of electrokinetically driven surface deformations, and using it to create a “library” of fundamental dynamic elements. 
2. Designing, building, and testing the first prototype MetamorphChip.
3. Demonstrating the ability of the MetamorphChip to manipulate single cells and their microenvironment.
4. Performing advanced biochemical analysis on single cells using the chip. 
5. Demonstrating the use of the MetamorphChip for experimental study of immune cell interaction. 

I strongly believe that successful implementation of this project would fundamentally change the way in which single-cell experiments are conceived and performed.","1744056","2016-04-01","2021-03-31"
"METCHACT","A New Blueprint for Chemical Synthesis via Metal-Catalyzed C H Bond Functionalization","Matthew James Gaunt","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Synthesis is a fundamental area of science that is crucial to advances in medicine and materials - fields that directly impact modern society. The increasing challenges that our society presents has raised the demands on synthesis to provide new molecules that can perturb biological function or provide a new physical property. This fellowship aims to provide solutions to these problems by developing pioneering synthesis blueprints for how chemists can make any molecule in an efficient, rapid and  green  fashion. In particular, direct benefits of this research will be realized in the treatment of disease.

The bond forming processes that underline our ability to make molecules, large or small, depends on the manipulation of pre-functionalized molecules (compounds that need to be prepared through additional synthetic steps). However, the most common type of chemical bond in almost all organic molecules is the C H bond. Usually, all but a few of these C H bonds are considered as inert and only useful in synthesis if they are in the vicinity of a one of these activating functional groups. To a synthetic chemist, permitting the use of any C H bond as a potential functional group would be like giving them a key to a new world Discovery of new chemical reactivity would lead to new reactions; comprised into a new tool kit for synthetic chemists, this would provide the basis for a molecule-building blueprint that would challenge synthetic dogma. Furthermore, using the traditionally  inert  C H bond as a versatile functional group would save synthesis time, make synthesis  greener , and hence more efficient and cost effective. Therefore, to be able to introduce a general blueprint for chemical synthesis based using any C H bond as a versatile functional group would be a truly paradigm shifting advance, and could have enormous impact on many of the scientific challenges that affect modern society.","1499983","2011-01-01","2016-12-31"
"METHID","Observation and Modelling of Radiocarbon in Atmospheric Methane for Methane Source Identification","Heather Dawn Graven","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","Observation and Modelling of Radiocarbon in Atmospheric Methane for Methane Source Identification

Greenhouse gas emissions are the primary cause of global climate change, and methane (CH4) is the second most important contributor after carbon dioxide (CO2). Major sources of methane are both natural (wetlands) and anthropogenic (agriculture, landfills and fossil fuels). Current efforts to assess the anthropogenic CH4 influence on climate change and the effectiveness of mitigation policies for CH4 are limited by large uncertainties in estimates of total methane emissions and their attribution to various sources by accounting-based techniques. This project will pioneer and apply innovative techniques for atmospheric observation and modelling of radiocarbon in CH4 that will enable unique quantification of fossil fuel vs. biogenic CH4 sources at regional and global scales, thereby improving the estimation and attribution of CH4 emissions of different types. The proposed work will significantly advance the frontier of current research on atmospheric methane and the characterization of anthropogenic sources on policy-relevant scales, and it has the potential to influence climate policy and industrial practices over the next 10-20 years.","1500000","2016-06-01","2021-05-31"
"MEViC","Molecular engineering of virus-like carriers","Giuseppe Battaglia","UNIVERSITY COLLEGE LONDON","In the last 5 years I have been working on the study of nanoscopic vesicles formed by the assembly in water of amphiphilic block copolymers. These polymer vesicles also known as polymersomes can be designed with size, topology and morphology similar to natural viruses. The synthetic nature of copolymers allows the design of interfaces with various classes of biochemically-active functional groups. This, in combination with precise control over the molecular architecture, determines the degree of order in self-organizing polymeric materials. Such bio-inspired ‘bottom-up’ supramolecular design principles can offer outstanding advantages in engineering structures at a molecular level, using the same long–studied principles of biological molecules. It is self-evident that the highly biocompatible nature of these new amphiphilic copolymer assemblies augurs well for biomedical applications. Indeed, related polymeric micelles and vesicles have already been reported and studied as delivery systems for drugs, gene, and image contrast agents. Herein I propose to engineer new generations of polymersomes whose size, topology, surface chemistry is exquisitely controlled by supramolecular interactions with the aim to control their bioactivity and explore new ways to target specific biological sites via multi-fictionalisation and steric controlled binding. This will be achieved by a balanced combination of novel physico-chemical techniques with tailor-made biological evaluation based on state-of-the-art cell culture methods as well as in vitro and in vivo high content screening. My long-term aim is to set-up new design principles for nanoparticles for biomedical applications together with a thorough biomedical fast screening that will enable safe and fast translation into the clinic as well as benchmarking nanotoxicological methodologies.","1643736","2011-10-01","2016-09-30"
"MFMF","Market Frictions in Mathematical Finance","Paolo Guasoni","DUBLIN CITY UNIVERSITY","For the past twenty years, Mathematical Finance has grown from the perfect fit between martingale methods and models of frictionless markets. But in last two years, the limits of this theory have become painfully clear, with the widespread failure of the valuation and risk control systems in the financial industry.

This proposal lays the groundwork for a new generation of models, which include nonlinear frictions such as transaction costs and liquidity as essential elements, not as extra features. This endeavor entails developing new notions of nonlinear stochastic integrals, and requires a theory that looks beyond the established setting of semimartingales. To become useful, this theory will need tools to solve related optimization problems, either explicitly, or with asymptotic methods. Convex duality and control theory will help develop such tools, together with partial differential equations techniques.

The proposed research aims at (i) understanding the natural setting of frictions models from well-posedness principles, (ii) developing a consistent integration theory, and (iii) investigating implications for optimization problems. These steps are central to nurture a new class of financial models, which can eventually remedy the pitfalls of the current ones.","1100000","2012-01-01","2017-12-31"
"MGATDE","Modified Gravity as an Alternative to Dark Energy","Kazuya Koyama","UNIVERSITY OF PORTSMOUTH HIGHER EDUCATION CORPORATION","Modified Gravity as an Alternative to Dark Energy","500000","2008-10-01","2013-09-30"
"MIAMI","Machine Learning-based Market Design","Sven SEUKEN","UNIVERSITAT ZURICH","""Market designers study how to set the """"rules of a marketplace"""" such that the market works well. However, markets are getting increasingly complex such that designing good market mechanisms """"by hand"""" is often infeasible, in particular when certain design desiderata (such as efficiency, strategyproofness, or fairness) are in conflict with each other. Moreover, human agents are boundedly-rational: already in small domains, they are best modeled as having incomplete preferences, because they may only know a ranking or the values of their top choices. In combinatorial domains, the number of choices grows exponentially, such that it quickly becomes impossible for an agent to report its full valuation, even if it had complete preferences. In this ERC grant proposal, we propose to combine techniques from """"machine learning"""" with """"market design"""" to address these challenges.

First, we propose to develop a new, automated approach to design mechanisms with the help of machine learning (ML). In contrast to prior ML-based automated mechanism design work, we explicitly aim to train the ML algorithm to exploit regularities in the mechanism design space. Second, we propose to study the """"design of machine learning-based mechanisms."""" These are mechanisms that use machine learning internally to achieve good efficiency and incentives even when agents have incomplete knowledge about their own preferences.

In addition to pushing the scientific boundaries of market design research, this ERC project will also have an immediate impact on practical market design. We will apply our techniques in two different settings: (1) for the design of combinatorial spectrum auctions, a multi-billion dollar domain; and (2) for the design of school choice matching markets, which are used to match millions of students to high school every year.
""","1375000","2018-12-01","2023-11-30"
"MICA","Mechanics of slow earthquake phenomena: an Integrated perspective from the Composition, geometry, And rheology of plate boundary faults","Ake Fagereng","CARDIFF UNIVERSITY","Major tectonic faults have, until recently, been thought to accommodate displacement by either continuous creep or episodic, damaging earthquakes. High-resolution geophysical networks have now detected ‘slow earthquakes’, transient modes of displacement that are faster than creep but slower than earthquakes. This project aims to illuminate the unknown mechanism behind slow earthquakes, through an integrated, multi-scale approach. MICA uses the unique natural laboratory of exhumed and active faults, to build numerical models constrained by observed fault geometry and microstructurally defined deformation mechanisms, to determine, for the first time, the rheology of slow slip.

The first objective is to create a model of the slow earthquake source, to constrain the micro- to kilometre-scale internal geometry of plate boundary faults, and the spatial distribution of deformation mechanisms. Fault rocks also retain a deformation sequence, allowing insight to how deformation style evolves with time. Thus, a combination of drill samples from active faults and outcrops of exhumed analogues, from a range of depths, allows for a 4-D model from micro- to plate boundary scale.

By knowing the geometrical distribution of fault rocks, and deciphering their evolution in time, this project will apply geologically constrained numerical models and laboratory constrained stress-strain relationships to determine bulk fault rheology as a function of space. Unique from past models, this project integrates scales from microstructures to plate boundary scale faults, and bases rheological models on deformation mechanisms and fault structures constrained through detailed fieldwork, and also considers the state-of-the-art of geophysical observation. The model focuses on understanding slow earthquakes, but also applies to understanding whether the slow earthquake source can also host fast seismic slip, and what differentiates slowly slipping faults from faults hosting major earthquakes.","1499244","2017-02-01","2022-01-31"
"MicMactin","Dissecting active matter: Microscopic origins of macroscopic actomyosin activity","Martin Sylvain Peter Lenz","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""Biological motion and forces originate from mechanically active proteins operating at the nanometer scale. These individual active elements interact through the surrounding cellular medium, collectively generating structures spanning tens of micrometers whose mechanical properties are perfectly tuned to their fundamentally out-of-equilibrium biological function. While both individual proteins and the resulting cellular behaviors are well characterized, understanding the relationship between these two scales remains a major challenge in both physics and cell biology.

We will bridge this gap through multiscale models of the emergence of active material properties in the experimentally well-characterized actin cytoskeleton. We will thus investigate unexplored, strongly interacting nonequilibrium regimes. We will develop a complete framework for cytoskeletal activity by separately studying all three fundamental processes driving it out of equilibrium: actin filament assembly and disassembly, force exertion by branched actin networks, and the action of molecular motors. We will then recombine these approaches into a unified understanding of complex cell motility processes.

To tackle the cytoskeleton's disordered geometry and many-body interactions, we will design new nonequilibrium self consistent methods in statistical mechanics and elasticity theory. Our findings will be validated through simulations and close experimental collaborations.

Our work will break new ground in both biology and physics. In the context of biology, it will establish a new framework to understand how the cell controls its achitecture and mechanics through biochemical regulation. On the physics side, it will set up new paradigms for the emergence of original out-of-equilibrium collective behaviors in an experimentally well-characterized system, addressing the foundations of existing macroscopic ""active matter"" approaches.""","1491868","2016-06-01","2021-05-31"
"MICROBONE","Multiscale poro-micromechanics of bone materials, with links to biology and medicine","Christian Hellmich","TECHNISCHE UNIVERSITAET WIEN","""Modern computational engineering science allows for reliable design of the most breathtaking high-rise buildings, but it has hardly entered the fracture risk assessment of biological structures like bones. Is it only an engineering scientist's dream to decipher mathematically the origins and the evolution of the astonishingly varying mechanical properties of hierarchical biological materials? Not quite: By means of micromechanical theories, we could recently show in a quantitative fashion how """"universal"""" elementary building blocks (being independent of tissue type, species, age, or anatomical location) govern the elastic properties of bone materials across the entire vertebrate kingdom, from the super-molecular to the centimetre scale. Now is the time to drive forward these developments beyond elasticity, striving for scientific breakthroughs in multiscale bone strength. Through novel, experimentally validated micromechanical theories, we will aim at predicting tissue-specific inelastic
properties of bone materials, from the """"universal"""" mechanical properties of the nanoscaled elementary components (hydroxyapatite, collagen, water), their tissue-specific dosages, and the """"universal"""" organizational patterns they build up. Moreover, we will extend cell population models of contemporary systems biology, towards biomineralization kinetics,in
order to quantify evolutions of bone mass and composition in living organisms. When using these evolutions as input for the aforementioned micromechanics models, the latter will predict the mechanical implications of biological processes. This will open unprecedented avenues in bone disease therapies, including patient-specific bone fracture risk assessment relying on micromechanics-based Finite Element analyses.""","1493399","2010-11-01","2015-10-31"
"microCODE","Microfluidic Combinatorial On Demand Systems: a Platform for High-Throughput Screening in Chemistry and Biotechnology","Piotr Garstecki","INSTYTUT CHEMII FIZYCZNEJ POLSKIEJ AKADEMII NAUK","This proposal addresses an important opportunity in the rapidly developing art of microfluidics. On one hand vast expertise is available on automation of single phase flows via microvalves or electrokinetics and on flow of drops on planar electrodes. These systems are perfectly suited for a range of applications but are inherently inefficient in handling massively large numbers of processes due to correspondingly large number of input/output controls that at best scales logarithmically in the number of processes. On the other hand conducting reactions in thousands micro droplets embodies many of the most acclaimed promises of microfluidics – ultra-miniaturisation, speed, rapid mixing and extensive control of physical conditions. Demonstrations of incubation of cells, in-vitro translation and directed evolution confirm that these techniques can reduce the cost and time of existing processes by orders of magnitude. Droplet microfluidics is at the moment, however, almost (except sorting) completely passive.
We recently demonstrated the use of external valves to automate formation and motion of droplets on simple disposable chips and screening up to 10000 compositions per hour. We propose to develop externally controlled programmable modules for i) multiplexed, on-demand generation of multiple emulsions, ii) aspiration of libraries of samples and multiplexing linear libraries into full cross matrices, iii) splitting drops into two, few and large numbers (e.g. 10000) drops, iv) optical monitoring of presence and content of droplets, v) counting cells inside the drops, vi) circulating drops, vii) titration, viii) holding paramagnetic beads in drops. Our design rules will allow to integrate these modules into externally controlled systems for research on i) combinatorial synthesis, ii) material science, iii) role of noise in metabolic networks, iv) evolution of bacteria, v) inexpensive multiplexed diagnostics systems, including cytometry, PCR and ELISA assays in drops.","1749600","2012-01-01","2016-12-31"
"microCrysFact","Microfluidic Crystal Factories (μ-CrysFact): a breakthrough approach for crystal engineering","Jose Puigmartí Luis","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","To study and understand the aggregation, nucleation, and/or self-assembly processes of crystalline matter is of crucial importance for research and applications in many disciplines. For example, understanding the formation of crystalline amyloid fibres could lead to advances in the treatment and prevention of both Alzheimer’s and Parkinson’s diseases, whereas controlling the process of crystal formation can play a significant role in obtaining chemicals and materials that are important for industry as well as society as a whole (e.g., drugs, superconductors, polarizers and/or frequency modulators). 

Despite the impressive progress made in molecular engineering during the last few decades, the quest for a general tool-box technology to study, control and monitor crystallisation processes as well as to isolate metastable states (dynamic capture) is still incomplete. That is because crystalline assemblies are frequently investigated in their equilibrium form, driving the system to its minimum energy state. This methodology limits the emergence of new chemicals and crystals with advanced functionalities, and thus hampers advances in the field of materials engineering. 

µ-CrysFact will develop tool-box technologies where diffusion-limited and kinetically controlled environments will be achieved during crystallisation and where the isolation of non-equilibrium species will be facilitated by pushing crystallisation processes out of equilibrium. In addition, µ-CrysFact’s technologies will be used to localise, integrate and chemically treat crystals with the aim of honing their functionality. This unprecedented approach has the potential to lead to the discovery of new materials with advanced functions and unique properties, thus opening new horizons in materials engineering research.","1814128","2016-09-01","2021-08-31"
"MICROFOX","Microbial formation of minerals by communities of Fe(II)-oxidizing bacteria in modern and ancient environments","Andreas Alfred Kappler","EBERHARD KARLS UNIVERSITAET TUEBINGEN","""Iron minerals are ubiquitously present in the environment. Their formation is linked to the global C and N cycle and they control the fate of nutrients, metals, and greenhouse gases. My recent work, published in international journals including Nature Geoscience, showed that Fe(II)-oxidizing bacteria form Fe(III) minerals and suggested that such bacteria were involved in the deposition of Precambrian Banded Iron Formations, the world’s largest iron mineral deposits. Three neutrophilic microbial groups contribute to Fe(III) mineral formation: microaerophiles, phototrophs and nitrate-reducing Fe(II)-oxidizers. However, as previous studies have always solely focused on only one particular Fe(II) metabolism, the contribution of the different Fe(II)-oxidizing groups to overall Fe(III) mineral formation in nature and the competition among them for Fe(II) within Fe(II)-oxidizing communities is still unknown. I propose to use an innovative and holistic approach to study for the first time the abundance, activity and spatial distribution of all three Fe(II)-oxidizing bacterial groups in one habitat in different environments. Quantification of microbial activity and both nutrient and metal sorption under varying geochemical conditions will allow us to study competition among the Fe(II)-oxidizing groups and evaluate the ecological importance of microbial Fe(III) mineral formation in both early Earth and modern environments. This requires an interdisciplinary frontier research effort at the scale of an ERC grant integrating microbiology, biogeochemistry and mineralogy. Central to this is the cultivation and characterization of Fe(II)-oxidizing bacteria and their mineral products, a research area spearheaded by my group. This frontier research will define the role of microbial iron mineral formation in modern and ancient Earth systems, open doors to new biotechnology applications and advance the search for life on the Fe-rich planet Mars.""","1499000","2013-01-01","2017-12-31"
"MicroMOUPE","Microscopy - Making optimal use of photons and electrons","Thomas JUFFMANN","UNIVERSITAT WIEN","The sensitivity of modern microscopy is limited by shot-noise. It limits the accuracy of measurements of specimen properties as well as the spatial resolution of electron microscopes when imaging sensitive specimens, such as proteins or DNA. But the shot-noise limit is not a fundamental limit. A technologically feasible and optimal approach to overcoming the shot-noise limit is to have each probe particle interact with the specimen multiple times. We recently introduced this concept to microscopy using self-imaging cavities.
Within this project, I want to demonstrate post-selection free sub-shot noise microscopy with both photons and electrons. Optically this will be possible by introducing a fast electro-optical switch into a multi-pass microscope, evading the need for temporal post-selection. After this proof-of principle experiment, the sensitivity enhancement offered by multi-pass microscopy shall be applied to the detection of nanometric particles, such as single molecules, proteins and metal nanoparticles. Linear signal enhancement with the number of interactions is expected for bright-field microscopy. For dark-field microscopy a quadratic enhancement is expected, due to coherent build-up of scattered fields. Finally, adaptive optics will be used to optimize multi-pass microscopy for the study of cells.
Multi-pass electron microscopy will be realized in collaboration with Stanford University. It will require several novel electron optical elements that will be designed and tested both at Stanford University and at the University of Vienna. One of these elements will be a pattern generator for electrons based on ponderomotive potentials. The required potential landscapes will be created using adaptive optics to shape intense laser pulses. With this novel electron optics tool fast beam-blanking, a phase plate for Zernike phase microscopy, arbitrary pattern creation and aberration correction will be demonstrated.","1672752","2018-03-01","2023-02-28"
"MICRONEX","Microbioreactor platforms as in vivo-like systems to probe the role of Neuroblastoma-derived Exosomes in cancer dissemination","Elisa CIMETTA","UNIVERSITA DEGLI STUDI DI PADOVA","Engineers can actively contribute to fields thought to be out of their “comfort zones”. We can be leaders of discoveries that translate into advances in the understanding of disease and improving human health. Engineers might use different language and tools than Life Sciences Scientists but we find a common ground, as the laws of Thermodynamics, Physics, and Mathematics also apply to biological phenomena.
The development of microbioreactors (μBRs) reconstructing biologically sound niches can revolutionize medical research.
In our bodies cells reside in a complex milieu, the microenvironment (μEnv), regulating their fate and function. Most of this complexity is lacking in standard laboratory models, leading to readouts poorly predicting the in vivo situation. This is particularly felt in cancer research, as tumors are extremely heterogeneous and capable of conditioning both the local μEnv and distant organs. Neuroblastoma (NB) is the most common and difficult to cure pediatric malignant solid tumor. Secreted exosomes are means by which NBs reshape their μEnv and induce local and long-range changes in cells, regulating progression and prognosis. But the mechanisms involved are yet not completely understood. A major limitation is the difficulty to model in vitro the local in vivo dynamic μEnv.
We hypothesize that μBRs exploiting classical engineering principles will solve the limitations of existing classical culture models.
We propose to develop platforms and test their edge over classical approaches in decoding the role of exosomes and μEnv in NB. Our μBRs generate time and space-resolved concentration gradients, support fast dynamic changes and reconstruct complex interactions between cells and tissues while performing multifactorial and parallelized experiments.
We expect that our technologies will bridge the gap between in vitro techniques and in vivo biological phenomena leading to significant and novel results, shedding light on previously unexplored scenarios.","1446250","2017-12-01","2022-11-30"
"MicroParticleControl","Controlled synthesis of particulate matter in microfluidics","Simon Kuhn","KATHOLIEKE UNIVERSITEIT LEUVEN","Despite the many advantages of microchemical systems and their successful applications in chemical
engineering research, one major drawback greatly limiting their use is their susceptibility to channel clogging
for flows containing particulate matter. Hence, the aim of the proposed research is to overcome the challenge
of clogging in microfluidic devices and to design microfluidic systems that can tolerate particulate matter
and synthesize solid materials according to their specifications (e.g. size, purity, morphology). To reach this
goal, we apply a combined experimental and theoretical approach, in which the experimental results will lead
to model development reflecting the particle formation and interaction kinetics and their coupling to the
hydrodynamics. The novel concept of the proposal is to devise engineering strategies to handle the
particulate matter inside the reactor depending on if the solid material is i) an unwanted and insoluble by-product
of a reaction, or ii) the target compound (e.g. nanoparticle synthesis or crystallization of organic
molecules). Depending on the case we will design different ultrasound application strategies and introduce
nucleation sites to control the location of particle formation within the microchannel. This project will
provide fundamental insight into the physico-chemical phenomena that result in particle formation, growth
and agglomeration processes in continuous flow microdevices, and will provide a theoretical tool for the
prediction of the dynamics of particle-particle, particle-wall and particle-fluid interactions, leading to
innovative microreactor designs.","1500000","2016-03-01","2021-02-28"
"MicroQuant","Microscopy of Tunable Many-Body Quantum Systems","Hanns-Christoph Nägerl","UNIVERSITAET INNSBRUCK","We propose to take the experimental investigation of strongly-correlated quantum matter in the context of ultracold gases to the next scientific level by applying “quantum gas microscopy” to quantum many-body systems with tunable interactions. Tunability, as provided near Feshbach resonances, has recently proven to be a key ingredient for a broad variety of strongly-correlated quantum gas phases with strong repulsive or attractive interactions and for investigating quantum phase transitions beyond the Mott-Hubbard type. Quantum gas microscopy, as recently demonstrated in two pioneering experiments, will be combined with tunability as given by bosonic Cs atoms to give direct access to spatial correlation functions in the strongly interacting regimes of e.g. the Tonks gases, to open up the atom-by-atom investigation of transport properties, and to allow the detection of entanglement. It will provide local control at the quantum level in a many-body system for entropy engineering and defect manipulation. It will allow the generation of random potentials that add to a periodic lattice potential for the study of glass phases and localization phenomena. In a second step, we will add bosonic and fermionic potassium (39-K and 40-K) to the apparatus to greatly enhance the capabilities of the tunable quantum gas microscope, opening up microscopy to fermionic and, in a third step, to fermionic dipolar systems of KCs polar ground-state molecules. In the case of atomic 40-K fermions with tunable contact interactions, the central goal will be to investigate magnetic systems, in particular to create anti-ferromagnetic many-body states. The Cs sample, for which we routinely achieve ultralow temperatures and extremely pure Bose-Einstein condensates, would serve as a perfect coolant and probe. With KCs, which is non-reactive and hence stable, we will enter a qualitatively new regime of fermionic systems with long-range dipolar interactions.","1477500","2012-01-01","2016-12-31"
"MIDAS","Multidimensional Spectroscopy at the Attosecond frontier","Nirit Dudovich","WEIZMANN INSTITUTE OF SCIENCE LTD","The invention of multidimensional spectroscopy was a major leap in nuclear magnetic resonance. Comparable schemes in the optical regime have led to significant advances in our understanding of ultrafast dynamics in complex molecular systems. Currently, these multidimensional approaches are the most powerful and complete measurement schemes for resolving molecular dynamics on femtosecond time scales. The goal of this project is to advance the basic ideas and concepts of multi-dimensional spectroscopy to the forefront of ultrafast science – the attosecond (10-18 second) regime.
Attosecond science is a young field of research that has rapidly evolved over the past decade. Leading researchers in the field have opened a door into a new area of research that allows the observation of multi-electrons dynamics on their own natural time scale. Attosecond science lies at the heart of strong field light-matter interactions. These interactions can lead to the generation of attosecond duration XUV and energetic electron pulses, thereby providing researchers with the tools for studying a broad range of fundamental phenomena in Nature which evolve on an attosecond time scale. While an extensive theoretical effort has been invested in studying these phenomena, their experimental observation remains limited. The main limitation is set by the complexity of the interaction that offers numerous channels in which electronic dynamics can evolve.
The proposed research program aims at introducing multidimensional spectroscopy in the attosecond regime, thus revealing the underlying complex dynamics behind many attosecond scale phenomena. Integrating state of the art experimental schemes, supported by advanced theoretical analysis, will lead to the discoveries of new phenomena previously inaccessible in many experimental observations. The impact of the proposed research is beyond attosecond spectroscopy – opening new paths in resolving phenomena at the extreme nonlinear limit.","1349833","2013-09-01","2018-08-31"
"MIDNP","Metal Ions Dynamic Nuclear Polarization:Novel Route for Probing Functional Materials with Sensitivity and Selectivity","Michal LESKES","WEIZMANN INSTITUTE OF SCIENCE LTD","Materials with specific electrical, optical or chemical properties often derive their special functions from small perturbations in their composition or structure. Thus, rational design of new functional materials demands sensitive and versatile determination of structural and compositional properties, a very difficult goal not presently available. The overarching goal of this ERC project is to develop a novel route for Magic-Angle Spinning Dynamic Nuclear Polarization (MAS-DNP) as an enabling methodology in materials science, introducing new opportunities for investigating and designing functional materials.
Solid State Nuclear Magnetic Resonance (ssNMR) spectroscopy is an excellent probe for local order/disorder, but unfortunately its sensitivity is limited.  DNP, a process whereby the large electron spin polarization is transferred to the nuclear spins, had greatly expanded the range of materials systems and questions that can be probed by ssNMR. However, it commonly relies on the use of exogenous nitroxide radicals, thereby limiting its utilization in materials science to nonreactive surfaces. 
We propose to develop Metal Ions DNP (MIDNP) utilizing paramagnetic dopants as endogenous polarization agents in the bulk.  To effectively harness the electron spin polarization of the dopants for higher sensitivity, we will (a) address challenges such as the effect of bonding, spin interactions and relaxation on DNP via a mechanistic study of carefully selected dopants in energy materials; (b) Develop new techniques for NMR spectral assignment and explore alternative DNP mechanisms for paramagnetic solids; (c) Expand the approach for sensitizing the detection of surfaces and interfaces and elucidate the critical role of surface chemistry in the efficacy of energy storage materials.
MIDNP will provide a novel, sensitive alternative for probing the structure and composition of new materials and will transform the utilization of ssNMR in the study of functional materials.","1700000","2019-01-01","2023-12-31"
"MIGRANT","Mining Graphs and Networks: a Theory-based approach","Jan Ramon","KATHOLIEKE UNIVERSITEIT LEUVEN","In this project we aim at formulating enhancing theoretical foundations for the emerging field of graph mining. Graph mining is the field concerned with extracting interesting patterns and knowledge from graph or network structured data, such as can be found in chemistry, bioinformatics, the world wide web, social networks etc. Recent work has shown that many standard data mining techniques can be extended to structured data and can yield interesting results, but also that when applied to complex real-world data, these standard techniques often become computationally intractable. In this project we aim at providing a better understanding of the complexity of the tasks considered in the field of graph mining, and at proposing techniques to better exploit the properties of the data. To this aim, we will bring together insights from the fields of data mining, graph theory, learning theory and different application fields, and add our own original contributions. Key features of the methodology include the ground-breaking integration of insights from graph theory in data mining and learning approaches, the development of efficient prototype algorithms, and the interdisciplinary collaboration with application domain experts to validate the practical value of the work, This potential impact of this project is significant, as it will be the first systematic study of the theory of graph mining, it will provide foundations on which later research can build further and it will have applications in the many domains with complex data.","1716066","2009-12-01","2015-05-31"
"MIMESIS","Microscopic Modelling of Excitonic Solar Cell Interfaces","Alessandro Troisi","THE UNIVERSITY OF WARWICK","Organic Photovoltaic Solar Cells and Dyes Sensitized Solar Cells (collectively referred to as Excitonic Solar Cells) are one of the major alternatives to silicon photovoltaics and the subject of the proposed investigation. The PI s expertise in the theory of single molecule electron transport, organic electronics and condensed phase simulations will be used to build a research team that will investigate all elementary processes that take place at the interface of excitonic solar cells. For the first time within a single theoretical research team, the same attention will be paid to the morphology of the relevant interfaces, their electronic structure at an atomistic level and the computation of the rates of the elementary processes (e.g. charge separation, charge recombination, triplet formation, etc.). Although the rates of the interfacial processes are what determine ultimately the efficiency of the cell, no theoretical tool so far has been used for their prediction and to guide the synthesis of new materials. This limitation of theory is related to the intrinsic complication of electron and exciton transfer across heterogeneous interfaces whose study does not fall within the remit of a single discipline. Breaking the traditional boundaries between soft-matter and quantum chemistry simulations and between solid state theory and molecular photochemistry, the proposed research aims at providing what is thought to be the best possible theoretical description of excitonic solar cells that can be achieved in 4 years time. The proposed investigation will provide a comprehensive understanding of the relation between chemical composition and efficiency in excitonic solar cells that will serve as a reference for future investigations in the field of photovoltaic research.","1050000","2009-10-01","2013-09-30"
"MINATRAN","Probing the Micro-Nano Transition: Theoretical and Experimental Foundations, Simulations and Applications","Aikaterini Aifanti","ARISTOTELIO PANEPISTIMIO THESSALONIKIS","The objective is to develop a robust multifunctional framework/probe for capturing the evolution of deformation and failure in a variety of processes at the micro-nano transition regime. An interdisciplinary approach will be pursued based on fundamental theory and experiment, in conjunction with multiscale simulations for micro/nanotechnology applications. The approach is unconventional as it ventures to extend continuum mechanics down to the micro/nano regime and verify this through nanoindentation and atomic force microscopy techniques. It is also unique as the new phenomenology introduced for establishing this extension (higher order gradients accounting for microscopic processes and interfacial energy terms accounting for nanoscopic phenomena) will be substantiated through hybrid (ab initio-atomistic-defect-finite element) simulations. The framework will be employed to consider fracture and size effects in a number of micro-nano scale transition configurations ranging from nanograined aggregates and nanolayered structures to nanotubes and micropillars, and from Li-ion battery electrodes to bioactive interfaces. Other micro/nano objects such as quantum dots, nanowires and NEMS/MEMS devices, as well as biomolecular microcrystalline membranes leading to living cell division will be considered. In a sense this “scale” transition theory is reminiscent in scope to Landau’s “phase” transition theory where a variety of different physical phenomena can be treated within a common framework. This optimism stems from the PI’s previous success with this approach, as well as Smalley’s remark that the “laws of continuum mechanics are amazingly robust for treating even intrinsically discrete objects only a few atoms in diameter”. A good mix of young researchers and mature scholars will be employed, thus connecting people and ideas through joint publications and scholarly activities in a critical area of fundamental and applied research.","1128400","2008-10-01","2013-09-30"
"MINE","Molecular Interfacial structure and dynamics of Nanoscopic droplets in Emulsions (MINE)","Sylvie Roke","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Emulsions consist of one liquid dispersed as nanoscopic droplets in another liquid, such as milk, and butter. The understanding of the structure and stability of emulsions is commonly obtained from empirical studies in which a macroscopic parameter (like temperature or concentration of constituents) is varied. Since the work of Irving Langmuir and others (published in 1917) it is well established that the stability and properties of these nanoscopic droplets are strongly influenced by the state of the droplet interface. However, despite the abundance and importance of emulsions in our daily lives, the molecular mechanisms that dictate the stability and properties of emulsions are still unknown. This lack of insight is caused by the system itself: the condensed surrounding medium forms an impenetrable barrier to most molecular probes. Nonlinear light scattering spectroscopy, a novel method I have developed (both theoretically and experimentally), offers a way of obtaining molecular information (chemical composition, molecular orientation, ordering and chirality) of the interfaces of nanoscopic particles in solution. With this method it should be possible to observe, in-situ, non-invasively and label-free, the molecules at the interface of the nanoscopic droplets in solution. I therefore propose to form a small group that investigates interfaces of nanoscopic droplets in emulsions on the molecular level and timescale. Using femtosecond nonlinear light scattering methods we can finally observe the molecules that dictate the structure and stability of emulsions in action.","1150000","2009-11-01","2014-10-31"
"MiniMasonryTesting","Seismic Testing of 3D Printed Miniature Masonry in a Geotechnical Centrifuge","Michalis VASSILIOU","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Earthquakes are responsible for more than half of the human losses due to natural disasters. Masonry structures have been proven the most vulnerable both in the developing and in the developed world. Even though Masonry is one of the oldest building materials, our understanding of its behavior at the level of the structure (system level) is limited. Therefore, there is a need for extended shake table testing. But shake table tests are expensive and full-scale system-level testing of large buildings is only possible in a handful of shake tables in the globe – and at a huge cost.

We propose to take advantage of research developments in 3D printing and develop a method to perform system-level testing at a small scale using 3D printers and a geotechnical centrifuge (to preserve similitude). The key is to print materials with behavior controllable and similar to masonry. MiniMasonry testing proposes to control the properties of masonry via controlling the geometry of a 3D printed “meta”-mortar. The method will be developed via typical static masonry tests performed on the 3D printed parts. It will be further validated via comparing shaking table tests (in a centrifuge) of miniature structures to existing results of full-scale tests. The cost of the dynamic tests is expected to be so low, that multiple tests can be performed, so that existing numerical methods can be validated in the statistical sense. As a case study, the method will be applied to explore the behavior of a low-cost seismic isolation method that has been proposed for masonry structures in developing countries.

With the rapid evolution of 3D printing, it will be possible to scale-up the methods developed in MiniMasonryTesting, so that other Civil Engineering materials can be tested faster and cheaper than now. This is a game changer in structural testing, as it will enable researchers to test structures that up to now it was impossible or very expensive to test at a system level.","1999477","2019-04-01","2024-03-31"
"MININEXACT","Exact Mining from In-Exact Data","Michail Vlachos","IBM RESEARCH GMBH","Data exchange and data publishing is an inherent component of our interconnected world. Industrial companies outsource datasets to marketing and mining firms in order to support business intelligence; medical institutions exchange collected clinical experiments; academic institutions create repositories and share datasets for promoting research collaboration. A common denominator in any data exchange is the 'transformation' of the original data, which usually results in 'distortion' of data. While accurate and useful information can be potentially distilled from the original data, operations such as anonymization, rights protection and compression result in modified datasets that very seldom retain the mining capacity of its original source. This proposal seeks to address questions such as the following:

- How can we lossy compress datasets and still guarantee that mining operations are not distorted?
- Is it possible to right protect datasets and provide assurances that this task shall not impair our ability to distill useful knowledge?
- To what extent can we resolve data anonymization issues and yet retain the mining capacity of the original dataset?

We will examine a fundamental and hard problem in the area of knowledge discovery, which is the delicate balance between data transformation and data utility under mining operations. The problem lies at the confluence of many areas, such as machine and statistical learning, information theory, data representation and optimization. We will focus on studying data transformation methods (compression, anonymization, right protection) that guarantee the preservation of the salient dataset characteristics, such that data mining operations on original and transformed dataset are retained as well as possible. We will investigate how graph-centric approaches, clustering, classification and visualization algorithms can be ported to work under the proposed mining-preservation paradigm. Additional research challenges i","1499999","2011-04-01","2016-03-31"
"MINOS","Nuclear magic numbers off stability","Alexandre Obertelli","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","Atomic nuclei are finite systems composed of fermions, the nucleons, and essentially governed by the strong force and quantum mechanical laws. Their structure is characterized by single-particle orbitals grouped in energy shells, separated by energy gaps. The numbers of nucleons that correspond to fully filled shells are called magic and represent the backbone of nuclear structure. In this proposal, we propose a new approach to investigate the most neutron-rich systems ever reached and establish the shell structure in new regions of the nuclear chart where new magic numbers or strong shell reordering are expected or controversial. This will open new horizons in the terra incognita of the nuclear landscape. Beyond the fundamental question of the nuclear force, the assessment of new shell closures in the nuclear landscape is of primary importance to better understand the stellar nucleosynthesis in the Universe.
In-flight gamma spectroscopy of rare isotopes at intermediate energy is one of the most efficient tools to populate and measure excited states in exotic nuclei. We propose to develop a new method that will increase the sensitivity of prompt-gamma spectroscopy by more than one order of magnitude compared to existing setups. Experiments will be performed at the most competitive fragmentation radioactive-beam facilities worldwide. In the future, this program will take advantage of the European FAIR facility, Germany, coupled to the European new-generation gamma array AGATA spectrometer. When coupled to AGATA, the improvement will reach a factor of several hundreds. This new experimental technique will be strengthened by original developments in the theory of reaction mechanisms, which are also included in this proposal.","1121520","2010-11-01","2015-10-31"
"MINT","Multiphoton Ionization Nano-Therapy","Dvir Yelin","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","The application of nanotechnology for addressing key problems in clinical diagnosis and therapy holds great promise in medicine and in cancer in particular. Recent works have shown significant progress in nanoparticle-mediated drug delivery and therapy. In these applications, however, the small dimensions of the nanoparticles have been used primarily for efficient delivery and specificity, while the effects mediated by the nanoparticles occur away from the particle itself, affecting the entire cell\tumour volume. We propose to study and develop, for the first time, a novel scheme for cancer therapy that treats cancer cells at nanoscale resolutions. Briefly, when noble-metal nanoparticles are illuminated with femtosecond laser pulses tuned to their plasmonic resonance, order-of-magnitude enhancements of the optical fields several nanometres away from their surfaces lead to local damage only to nearby molecules or cellular organelles. This process, which practically involves no toxic agents, is at the basis for this proposal; we will utilize techniques for targeting nanoparticles to cells, initiate and control cancer cell destruction using nanoparticles and femtosecond laser pulses, and develop technology for conducting image-guided minimally invasive cancer therapy in remote locations of the body. Preliminary results supporting the proposed scheme include nonlinear optical imaging and ablation of living cells, in vivo endoscopic imaging of cancerous tumour nodules, and computer simulations of light-nanoparticle interactions. Using state-of-the-art concepts in nanotechnology, biology, chemistry, and medicine, the proposed novel multidisciplinary research will attempt at offering a feasible and safe addition to existing forms of cancer therapy.","1782600","2009-12-01","2014-11-30"
"MINT","Mechanically Interlocked Carbon Nanotubes","Emilio Manuel Pérez Álvarez","FUNDACION IMDEA NANOCIENCIA","""We present a plan to design, synthesize and exploit the properties of mechanically interlocked carbon nanotubes (MINTs).
The scientific aim of the project is to introduce the mechanical bond as a new tool for the derivatization of carbon nanotubes. The mechanical link combines the advantages of covalent and supramolecular modifications, namely: kinetic stability (covalent) and conserved chemical structure (supramolecular). Besides this, its dynamic nature opens up unique opportunities for both fundamental studies and applications.
From a technological point of view, MINTs should have a practical impact in the fields of molecular electronics and molecular machinery. A general modular approach to MINT-based materials for photovoltaic devices and electrochemical sensors is presented. We also expect to exploit the rigidity and low dimensionality of SWNTs to construct molecular machines that utilize them as tracks to move across long distances, which is not possible in small-molecule molecular machines.
To achieve these goals we will exploit the PI’s expertise in the chemical modification of carbon nanostructures, in the self-assembly of electroactive materials and in the synthesis and characterization of mechanically interlocked molecules.""","1444999","2012-10-01","2017-09-30"
"MIRA","Next Generation Machine Intelligence for Medical Image Representation and Analysis","Ben GLOCKER","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","Machines capable of analysing and interpreting medical scans with super-human performance would transform healthcare as much as medical imaging itself did over the last century. With an increasing complexity and volume of data the interpretation of images and extraction of clinically useful information push human abilities to the limit. There is high risk that critical patterns of disease go undetected. We require powerful and trustworthy computational tools based on machine intelligence to support experts and go beyond human performance to tackle the major challenges in clinical practice. Two key ingredients are currently missing: 1) interpretable statistical representations that capture important information while reducing complexity; 2) intelligent algorithms that leverage knowledge across multiple tasks to solve the most challenging problems such as early detection of pathology.

This project is devoted to redefine the state-of-the-art in medical image analysis by developing a new generation of machine intelligence using powerful techniques of representation learning. Key to the project is its unique access to some of the largest and most comprehensive imaging databases combined with world-leading expertise in machine learning and medical imaging. An overarching objective is to harvest information from population data to construct what will be the most advanced statistical models of anatomy. In contrast to previous attempts that focus primarily on specific organs or pathology, here shared representations are learned from highly complex data by jointly solving multiple tasks. Linking the representations with demographics, lifestyle, genetics and disease allows probing of genetic and environmental determinants related to specific anatomical and pathological phenotypes across organs. This will provide insights into complex diseases, and enables a novel approach to abnormality detection that aims to automatically find subtle signs of pathology in new medical scans.","1499292","2018-02-01","2023-01-31"
"MIRACLE","Mid-InfraRed Active photonic integrated Circuits for Life
sciences and Environment","Gunther Roelkens","UNIVERSITEIT GENT","My group will explore the new field of photonic integrated circuits for the mid infrared (MIR) wavelength band based on high-index contrast waveguide structures. This research is fueled by the need for compact, integrated solutions for spectroscopic sensor systems in the MIR for biomedical applications and environmental monitoring, since most molecules have  fingerprint  absorption lines in this wavelength range. The project is based on the use of high index contrast group IV waveguide systems. To extend the functionality of the photonic integrated circuit, my group will heterogeneously integrate other materials (III-V semiconductors, LiNbO3, MCT, chalcogenides, polymers) on the high index contrast waveguide system for particular optical functions. My group will start exploring this field by focusing on three cornerstone applications. The research will focus on the realization of a MIR lab-on-a-chip spectroscopic system, a fully integrated MIR Fourier Transform InfraRed spectroscopy system (FTIR) and an integrated optical parametric oscillator to address new wavelength ranges, all integrated on a silicon photonic integrated circuit. Each of these three cornerstones would be world s first mid-infrared systems-on-a-chip and thus a breakthrough. Inherently this makes this a relatively high risk/very high gain proposal. My group will combine the strengths of the two institutes which support this proposal, i.e. the world class silicon photonics and heterogeneous integration technology at Ghent University/IMEC and the world class III-V and plasmonic technology at the Technical University of Eindhoven. I strongly believe that my program will open a whole new window of opportunities in the mid-infrared with a large impact on science and society.","1451400","2010-12-01","2015-11-30"
"MIRACLS","Multi Ion Reflection Apparatus for Collinear Laser Spectroscopy of radionuclides","Stephan Malbrunot","EUROPEAN ORGANIZATION FOR NUCLEAR RESEARCH","Employing laser spectroscopy (LS) to study radionuclides is equally rich in its long tradition as it is manifold in its active pursuit today as virtually all radioactive ion beam (RIB) facilities do or are planning to host dedicated setups. Probing the hyperfine structure of an atom or ion with laser light is a powerful technique to infer nuclear properties such as a nuclide’s spin, charge radius, or electromagnetic moments. This information provides insight into a wide range of contemporary questions in nuclear physics such as the mechanism driving the emergence and disappearance of nuclear shells far away from stability.
In the last decade, LS has benefited from the advent of ion traps in rare isotope science. The bunched beams released from these traps have led to an increase in sensitivity by several orders of magnitude due to an improved signal-to-background ratio when gating on the passing ion bunch.
This present proposal is determined to introduce another type of ion trap, an Electrostatic Ion Beam Trap, which has the potential to enhance the sensitivity of collinear LS by another factor of 20-800. This is achieved by increasing the laser-interaction and observation time by trapping the ion bunch between two electrostatic mirrors while keeping its beam energy at 30 keV to minimize Doppler broadening.
Such a device promises to extend collinear LS to nuclides so far out of reach given their low yields of typically <1000 ions/s at RIB facilities. Among the accessible nuclides are 34Mg in the island of inversion, 20Mg at the neutron shell closure N=8, or Sn isotopes towards the doubly magic 100Sn. Their charge radii will benchmark modern theoretical models utilizing 3-body forces in their quest to understand the evolution of nuclear shells.
Ultimately, the setup can be further enhanced in sensitivity when combined with other single-particle detection methods or by utilizing its multi-reflection time-of-flight aspect to suppress disturbing isobaric contamination.","1463750","2017-01-01","2021-12-31"
"MIRAGE 20-15","Mid Infra-Red near-field control by Adiabatic frequency Generation Enabling 20fs/15nm resolution","Haim Suchowski","TEL AVIV UNIVERSITY","The goal of this proposal is to allow observing and controlling ultrafast phenomena in a spatio-temporal window of 20fs-15nm at mid-IR by merging the extreme temporal resolution of the recently developed single-cycle mid-IR pulses with the spatial resolution of near field scattering optical microscope (aSNOM). The mid-infrared wavelength regime is of particular importance to materials science, chemistry, biology and condensed matter physics, as it covers the fundamental vibrational absorption bands of many gaseous molecules and bio-molecules.
Adiabatic frequency conversion, a recent advance in nonlinear optics based on my PhD research and my current collaboration with MIT, generates ultrashort pulses in this important wavelength regime, which outperform the currently available mid-IR ultrashort sources, and unlike other techniques allows complete control of the temporal evolution by amplitude and phase manipulation of the NIR input. Combining these capabilities with aSNOM will allow one-of-a-kind route to perform active coherent control of quantum dynamics and allow single shot spatio-temporal observation of fast dynamical processes at nanoscale-resolution.
Moreover, mid-IR ultrashort pulses delivered to the nanoscale can produce the high peak power needed to observe the nonlinear properties of the material under examination. Together with the richness of pulse shape manipulation it stands to enable, the currently impossible capability of intra-pulse multidimensional mid-IR spectroscopies at the nanoscale. This will open a gateway to all-optical, non-intrusive and label-free in situ studies of ultrafast processes in 2D materials and topological insulators, peptide evolution, photo-induced surface femtochemistry and protein folding. In particular, I plan to utilize these capabilities to explore nanoscale surface femtochemistry and to study energy pathways of hot carriers following the plasmonic decay in 2D materials and plasmonic nanostructures.","1493250","2015-03-01","2020-02-29"
"MIRNA","Metal Ions and Metal Ion Complexes Guiding Folding and Function of Single RNA Molecules","Roland Karl Oliver Sigel","UNIVERSITAT ZURICH","RNAs play crucial roles in cellular metabolic processes, e.g. ribozymes in RNA-processing or riboswitches in the regulation of protein expression. Metal ions thereby guide and determine folding and function of every complex nucleic acid structure. Recently, it has become increasingly evident that RNA folding and catalysis are extremely sensitive to changes in concentration and nature of the metal ion involved as well as to single-atom changes in metal ion complexes. The elucidation of the specific binding of certain metal ions and their complexes by nucleic acids poses an enormous challenge. This recognition process must depend solely on basic coordination chemical principles but is poorly understood. The goal of this project is to understand the effect of metal ions and their complexes on local and global structure formation of single large RNAs: Specifically, the influence of metal ions on the assembly of the catalytic core of group II intron ribozymes as well as the influence of single corrin side chains of coenzyme B12 to induce the structural change of its 202 nucleotide long riboswitch will be characterized. Combining classical Inorganic, Coordination, Analytical, and Organic Chemistry with Biophysics, we will apply single molecule Förster Resonance Energy Transfer spectroscopy (smFRET) together with hydrolytic cleavage experiments and chemical synthesis. SmFRET studies allow us to investigate every molecule individually instead of a bulk signal and thus to observe also minor populations. Our results will reveal how single metal ions and ligand atoms guide and influence global structure, folding, and function of ribozymes and riboswitches, and promise to have a significant impact on Biological Inorganic Chemistry, RNA Biochemistry, as well as Medicinal Chemistry.","1495729","2010-12-01","2015-11-30"
"mitochon","Artificial Mitochondria for Health","Ivan Lopez Montero","UNIVERSIDAD COMPLUTENSE DE MADRID","Mitochondria are cell organelles that provide the energetic requirements of the body. The majority of cellular ATP is produced by the membrane protein ATP synthase through a proton gradient across the mitochondrial inner membrane. Alterations in ATP synthase biogenesis can result in severe mitochondrial diseases affecting tissues with high energy requirements as brain and muscles. Mitochondrial diseases affect approximately 20 million people in the EU, causing 35 % of deaths during the first year of life of newborns. However, the available therapeutic approaches, are still extremely limited and there is no specific treatment for ATP synthase deficiencies. To improve the treatments currently available for mitochondrial diseases, the project will focus on the realization of artificial mitochondria (AM). Based on artificial lipid vesicles, AM will be fabricated by means of microfluidics methods, a powerful tool able to produce identical replicas of a given bio-inspired membrane-object. ATP synthase will be expressed and assembled within the lipid bilayer by encapsulating cell-free protein expression systems. To test the ability of AM as in-situ energy fabrication systems, targeting-AM will be endocytosed inside cultured cells and ATP synthesis will be triggered by taking advantage of the proton gradient provided by endosomes. Finally, by enclosing other plasmids encoding for diverse proteins, AM can be used as energy-factoring pockets to elicit protein expression just when internalized within cells. This novel approach may constitute an advanced new concept in gene therapy to more effectively create breakthroughs in improving human health.","1378000","2013-12-01","2018-11-30"
"MiTopMat","Microstructured Topological Materials: A novel route towards topological electronics","Philip MOLL","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Topological semi-metals such as Cd3As2 or TaAs are characterized by two bands crossing at isolated points in momentum space and a linear electronic dispersion around these crossing points. This linear dispersion can be mapped onto the Dirac- or Weyl-Hamiltonian, describing relativistic massless fermions, and thus relativistic phenomena from high-energy physics may appear in these materials. For example, the chirality, χ=±1, is a conserved quantity for massless fermions, separating the electrons into two distinct chiral species. A new class of topological electronics has been proposed based on chirality imbalance and chiral currents taking the role of charge imbalance and charge currents in electronics. Such devices promise technological advances in speed, energy efficiency, and quantum coherent processes at elevated temperatures.

We will research the basic physical phenomena on which topological electronics is based: 1) The ability to interact electrically with the chiral states in a topological semi-metal is an essential prerequisite for their application. We will investigate whether currents in the Fermi arc surface states can be induced by charge currents and selectively detected by voltage measurements. 2) Weyl materials are more robust against defects and therefore of interest for industrial fabrication. We will experimentally test this topological protection in high-field transport experiments in a wide range of Weyl materials. 3) Recently, topological processes leading to fast, tuneable and efficient voltage inversion were predicted. We will investigate the phenomenon, fabricate and characterize such inverters, and assess their performance. MiTopMat thus aims to build the first prototype of a topological voltage inverter. 

These goals are challenging but achievable: MiTopMat’s research plan is based on Focused Ion Beam microfabrication, which we have successfully shown to be a promising route to fabricate chiral devices.","1836070","2017-12-01","2022-11-30"
"MIXMETAPPS","Tailoring Mixed-Metal Chemistry for Frontier Synthetic and Catalytic Applications","Eva Hevia","UNIVERSITY OF STRATHCLYDE","Designed to meet many of the colossal challenges facing synthetic and organometallic chemists as demanded by current societal, environmental and economic issues, this project will accelerate the development of a  innovative mixed-metal chemistry, which combines two metals with markedly different polarities in the same molecule. Initially polar Mg and non-polar Zn “hybrids” will be developed followed by other metal pairs. These hybrids will exhibit a unique chemistry distinct to those of their parent monometallic compounds. Building on our recent pioneering work in this area (PNAS 2010, JACS 2010) that uses a tried and tested metal-structural-inorganic approach that allows rationale design of tailor-made mixed-metal reagents, this novel and ambitious research programme will deliver new chemo- and regioselective organobimetallic reagents designed as transformational tools for a broad spectrum of fundamentally important chemical reactions (deprotonation, metal-halogen exchange, alkylation, reduction, electrophilic amination, cross-couplings, etc), used every day in academia and industry. Catalytic, using cheap environmentally benign inorganic salts as catalysts, as well as stoichiometric advances will be made. Mixed-metal reagents will also be pioneered in Green Chemistry, by screening their reactivity using green solvents (e.g., 2-methyltetrahydrofuran) and ultimately, the holy grail, using water, which would inspire a synthetic revolution. Bulky ligand supported mixed-metal reagents will be constructed to activate organic heterocyclic molecules towards novel cascade reactions. Incorporating transition metals and lanthanides to this hybrid methodology will expand even more their opportunities in synthesis towards the development of hybrid catalysts for cross-coupling reactions as an alternative to expensive Pd and Ni methodologies. The award of an ERC Starting Grant will help the PI to consolidate her research team and propel her to an internationally-leading status.","1497180","2011-10-01","2017-03-31"
"MLCS","Machine learning for computational science:
statistical and formal modelling of biological systems","Guido Sanguinetti","THE UNIVERSITY OF EDINBURGH","Computational modelling is changing the face of science. Many complex systems can be understood as embodied computational systems performing distributed computations on a massive scale. Biology is the discipline where these ideas find their most natural application: cells can be viewed as input/ output devices, with proteins and organelles behaving as finite state machines performing distributed computations inside the cell. This led to the influential framework of cell as computation, and the successful deployment of formal verification and analysis on models of biological systems.

This paradigm shift in our understanding of biology has been possible due to the increasingly quantitative experimental techniques being developed in experimental biology. Formal modelling techniques, however, do not have mechanisms to directly include the information obtained from experimental observations in a statistically consistent way. This difficulty in relating the experimental and theoretical developments in biology is a central problem: without incorporating observations, it is extremely difficult to obtain reliable parametrisations of models. More importantly, it is impossible to assess the confidence of model predictions. This means that the central scientific task of falsifying hypotheses cannot be performed in a statistically meaningful way, and that it is very difficult to employ model predictions to rationally plan novel experiments.

In this project we will build and develop machine learning tools for continuous time stochastic processes to obtain a principled treatment of the uncertainty at every step of the modelling pipeline. We will use and extend probabilistic programming languages to fully automate the inference tasks, and link to advanced modelling languages to allow formal analysis tools to be deployed in a data modelling framework. We will pursue twoapplications to fundamental problems in systems biology, guaranteeing impact on exciting scientific questions.","1421944","2012-10-01","2017-09-30"
"MMA","Molecular Mechanical Adhesives","Michael NASH","UNIVERSITAT BASEL","Protein-based hydrogels are commonly used as adhesives and sealants in surgical settings. Fibrin gels, for example, are biocompatible, however their use is hampered by poor mechanical properties. Previous attempts to improve fibrin gel mechanics relied on interpenetrating networks in combination with PEO, collagen and other polymers, however, only modest improvements were observed. The important challenge lies in understanding how molecular design principles can influence gel mechanics on the macroscale. 

The goal of this research is to develop mechanically tunable protein hydrogels. Upon mixture of two liquid components, the systems I propose would spontaneously form a gel matrix consisting of oligomerized proteins that mimic the extracellular matrix and possess controllable mechanical responses. By understanding protein nanomechanics at the single-molecule level, and designing modes of energy dissipation into hydrogel networks, my project will have an impact by bridging the knowledge gap between single-molecule and macroscopic mechanical responses. 

My approach is ground-breaking because I am leveraging the discoveries I made on a family of super-stable receptor-ligand proteins (Cohesins & Dockerin (Coh-Doc)). These reversible receptor-ligands can be broken and reformed thousands of times, yet still maintain high stability (1/2 covalent bond strength). After having pioneered the application of these mechano-stable domains as molecular handles in single-molecule experiments, I propose the following frontier research:  

A) I will use molecular engineering of Coh-Doc complexes to test the hypothesis that mechanical properties of bulk materials can be rationally designed based on single-molecule mechanical behavior of receptor-ligands.
 
B) I will adapt the system to seamlessly merge with the native fibrin clotting pathway, providing a self-healing mechano-stable fibrin-based gel that could be applied as a liquid or spray and strongly adhere to cells and tissues.","1466916","2017-04-01","2022-03-31"
"MMPF","Molecular Movies of Protein Folding","Sander Woutersen","UNIVERSITEIT VAN AMSTERDAM","Protein folding, the process by which a protein assumes its three-dimensional shape, is one of the basic unsolved problems of biophysical and biochemical research. Many of the structural changes taking place during protein folding, especially during the early stages, are as yet poorly understood. This is because high-resolution structural techniques generally lack the time resolution necessary for observation of folding dynamics, whereas methods that have the required time resolution generally lack structural specificity. We propose an experimental approach that combines the structure-sensitivity of multi-dimensional NMR with the ultrafast time resolution of optical techniques. To do this, we use two-dimensional optical spectroscopy (in particular, two-dimensional optical spectroscopy and time-resolved vibrational circular dichroism) in combination with site-specific labeling of proteins. This will make it possible to obtain a structurally and temporally resolved picture of protein folding, which can be regarded as a 'molecular movie' of the folding process. With the proposed method, we will investigate structural changes during protein folding at increasing levels of complexity: from the dynamics of alpha-helix nucleation, to the formation and structural characteristics of intermediate states in small globular proteins and complex beta-sheet topologies, to the nature of biologically functional, short-lived unfolded states in signalling proteins. At each of these levels of complexity, the proposed method will be used to unravel the mechanisms behind the respective folding events.","1716321","2008-09-01","2014-08-31"
"MMUSCLES","Modification of Molecular structure Under Strong Coupling to confined Light modES","Johannes Maximilian FEIST","UNIVERSIDAD AUTONOMA DE MADRID","Understanding and controlling the properties of matter is one of the overarching goals of modern science. A powerful way to achieve is this by using light, usually in the form of intense laser beams. However, modern advances in nanophotonics allow us to confine light modes so strongly that their effect on matter is felt even when no external fields are present. In this regime of “strong coupling” or “vacuum Rabi splitting”, the fundamental excitations of the coupled system are hybrid light-matter states which combine the properties of both constituents, so-called polaritons. Little attention has been paid to the fact that strong coupling can also affect internal structure, such as nuclear motion in molecules. First experimental indications for this effect have been found, but current theory cannot explain or predict such changes. We will thus develop theoretical methods that can treat the modification of molecular structure under strong coupling to confined light modes. This will require advances in the microscopic description of the molecules under strong coupling by explicitly including their rovibrational degrees of freedom, as well as techniques to incorporate the influence of these modes in the macroscopic setting of collective strong coupling. In order to achieve this, we will adapt well-known techniques from quantum chemistry and combine them with the concepts of polariton physics. We will investigate what level of control can be gained through this approach, and whether confined light modes could act as a “photonic catalyst” to control molecular dynamics without requiring an active ingredient. This could present a novel tool to control photochemical reactions that are of paramount importance in the biological mechanisms of vision and photosynthesis, and hold great interest for use in memories, photoswitching devices, light-driven actuators, or solar energy storage. Consequently, this work could have wide-ranging impact on many different fields of science.","1499500","2017-04-01","2022-03-31"
"MNIQS","Mathematics and Numerics of Infinite Quantum Systems","Mathieu Lewin","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The purpose of the project is to study linear and nonlinear models arising in quantum mechanics and which are used to describe
matter at the microscopic and nanoscopic scales. The project focuses on physically-oriented questions (rigorous derivation of a
given model from first principles), analytic problems (existence and properties of bound states, study of solutions to timedependent
equations) and numerical issues (development of reliable algorithmic strategies). Most of the models are nonlinear and
describe physical systems possessing an infinite number of quantum particles, leading to specific difficulties.
The first part of the project is devoted to the study of relativistic atoms and molecules, while taking into account quantum
electrodynamics effects like the polarization of the vacuum. The models are all based on the Dirac operator.
The second part is focused on the study of quantum crystals. The goal is to develop new strategies for describing their behavior in
the presence of defects and local deformations. Both insulators, semiconductors and metals are considered (including graphene).
In the third part, attractive systems are considered (like stars or a few nucleons interacting via strong forces in a nucleus). The
project aims at rigorously understanding some of their specific properties, like Cooper pairing or the possible dynamical collapse of
massive gravitational objects.
Finally, the last part is devoted to general properties of infinite quantum systems, in particular the proof of the existence of the
thermodynamic limit","905700","2010-10-01","2015-09-30"
"MOCAPAF","Role of Molecular Clusters in Atmospheric Particle Formation","Hanna Vehkamäki","HELSINGIN YLIOPISTO","Climate change is currently one of the central scientific issues in the world, and the ability to reliably forecast climate is crucial for making political decisions that affect the lives of billions of people. Aerosols remain the dominant uncertainty in predicting radiative forcing and future climate change, and also have adverse effects on human health and visibility. One of the least-well understood aerosol-related processes is nucleation: the formation of new particles from condensable vapours. While nucleation is related primarily to neutral clusters, state-of-the-art experimental methods measure only charged clusters.

The main scientific objectives of this project are 1) to understand the chemical composition of charged and especially neutral atmospheric clusters from molecular to multi-nanometre scale, and explain the mechanism by which they nucleate, and 2) to direct current intense instrument development and provide theoretical tools to maximize the information on neutral clusters that can be obtained from experimental results on charged clusters.

Our scientific plan consists of a multilevel computational effort to provide formation rates and properties of atmospheric clusters and particles to aerosol dynamic and climate modellers. To capture the properties of the smallest clusters, we need to perform quantum chemical calculations, combined with simulations on cluster formation kinetics. Unfortunately, these methods are computationally far too demanding to describe the entire nucleation process. Thus, we will feed quantum chemical results to classical thermodynamic models, the results of which in turn must be parameterized for efficient use in larger-scale models.","1476418","2011-02-01","2016-01-31"
"MODEL","Mechanics Of Deformation of the Earth's Lithosphere","Boris Jozef Paul Kaus","JOHANNES GUTENBERG-UNIVERSITAT MAINZ","The deformation of lithospheric plates result in a wide variety of geodynamical processes such as mountain belts, volcanic eruptions, and earthquakes.
Since most lithospheric processes occur on a million-year timescale and involve rocks which have a nonlinear rheology, they are difficult to reproduce with laboratory experiments. Moreover, the geological record yields an incomplete picture of such processes and geophysical techniques mainly give a snapshot of how the Earth looks like today. As a result, most geological reconstructions remain interpretations that are not always mechanically consistent.
Here, we will employ computer models that are capable of simulating lithospheric deformation under geological conditions, while employing realistic laboratory-derived creep laws of rocks. We propose to:
1) Constrain the present-day rheology and structure of the lithosphere in active mountain belts (European Alps and the Himalaya) by combining forward models with inverse techniques constrained with available geophysical datasets.
2) Develop mechanically consistent reconstructions of mountain belts by using the best-fit rheologies from step 1 as an input for 3D models that are performed on geological timescales, and which are constrained with geological datasets.
3) Obtain insights into the physics of processes related to the deformation of (i) fold and thrust-belts and (ii) salt-related structures in sedimentary basins.
The project will significantly advance our understanding of dynamics of the lithosphere and deliver models that satisfy both geophysical and geological constraints.
Therefore, the outcome of the proposed work will provide a solid framework for understanding most geological processes that are related to the deformation of the lithosphere and crust.","1420920","2010-12-01","2015-11-30"
"MODELAGE","Is your heart aging well? A systems biology approach to characterize cardiac aging from the cell to the body surface","ESTHER Pueyo Paules","UNIVERSIDAD DE ZARAGOZA","Europe is facing a striking change in its demographics with an increasingly larger proportion of citizens aged 65 years and over. Aging is characterized by a progressive decline in the physiological functions of the body, with very notable effects on the heart. These effects are associated with a higher prevalence of arrhythmias, which, on top of deteriorating quality of life, increase the risk of other cardiovascular diseases like stroke, heart failure and neurological sequelae. Investigations targeting cardiac aging have often focused on assessing the effects of a specific contributing factor, at a single evaluation scale (molecular, cell, tissue, organ) and in many cases using animal species not relevant to humans in terms of aging mechanisms. MODELAGE proposes a multi-scale, multi-factorial research that is expected to make an important step in the characterization of human heart aging at both the population and individual levels. MODELAGE will work on an integrative methodological framework in which in silico modeling will be combined with in vitro cell and tissue analysis and in vivo electrocardiographic evaluation to investigate how cardiac aging manifests at a range of scales, from cell to body surface, and how electrical, structural and autonomic alterations contribute to such manifestations in humans. Indices describing spatio-temporal dynamics of cardiac electrophysiology will be evaluated in a population of young to senescent individuals using a novel feedback control approach. Inter-individual age-related variations in those indices will be assessed and correlated with markers of biological age (as opposed to chronological age). By investigating the mechanisms underlying inter-individual differences in cardiac dynamics, MODELAGE will set links to arrhythmia susceptibility and will propose novel non-invasive markers to identify high-risk senescent individuals for which preventive anti-arrhythmic treatment should be considered.","1498636","2015-10-01","2020-09-30"
"MODELING DC","Modelling platforms for high-power resonant DC hub and power networks with multiple converter systems","Dragan Jovcic","THE UNIVERSITY COURT OF THE UNIVERSITY OF ABERDEEN","This research proposal aims developing modelling tools for designing high-power multi-terminal DC transformers based on resonant topologies. The DC transformers with fault isolation property are expected to play a crucial role in developing DC grids, like the proposed North Sea supergrid. The resonant DC hubs with inherent fault current limitation are proposed as a replacement of traditional AC substation, and represent fundamentally different approach to power system operation control and protection. The second parallel aim is the development of modelling platform for dynamics studies of future (AC or DC) transmission grids with numerous converter systems. This modelling platform will be capable of representing unlimited number of converter systems in analytical form (parametric domain) and supporting eigenvalue studies in frequency range below 150Hz.","718016","2011-03-01","2015-08-31"
"MODEM","Multipoint Optical DEvices for Minimally invasive neural circuits interface","Ferruccio Pisanello","FONDAZIONE ISTITUTO ITALIANO DI TECNOLOGIA","A primary goal of experimental neuroscience is to dissect the neural microcircuitry underlying brain function, ultimately to link specific neural circuits to behavior. There is widespread agreement that innovative new research tools are required to better understand the incredible structural and functional complexity of the brain. To this aim, optical techniques based on genetically encoded neural activity indicators and actuators have represented a revolution for experimental neuroscience, allowing genetic targeting of specific classes of neurons and brain circuits. However, for optical approaches to reach their full potential, we need new generations of devices better able to interface with the extreme complexity and diversity of brain topology and connectivity.
This project aspires to develop innovative technologies for multipoint optical neural interfacing with the mammalian brain in vivo. The limitations of the current state-of-the-art will be surmounted by developing a radically new approach for modal multiplexing and de-multiplexing of light into a single, thin, minimally invasive tapered optical fiber serving as a carrier for multipoint signals to and from the brain. This will be achieved through nano- and micro-structuring of the taper edge, capitalizing on the photonic properties of the tapered waveguide to precisely control light delivery and collection in vivo. This general approach will propel the development of innovative new nano- and micro-photonic devices for studying the living brain.
The main objectives of the proposals are: 1) Development of minimally invasive technologies for versatile, user-defined optogenetic control over deep brain regions; 2) Development of fully integrated high signal-to- noise-ratio optrodes; 3) Development of minimally invasive technologies for multi-point in vivo all-optical “electrophysiology” through a single waveguide; 4) Development of new optical methodologies for dissecting brain circuitry at small and large scale","1996250","2016-10-01","2021-09-30"
"MODES","Modal analysis of atmospheric balance, predictability and climate","Nedjeljka Zagar","UNIVERZA V LJUBLJANI","Despite large progress in modelling of atmospheric processes and computing capabilities and concentrated efforts to increase complexity of the atmospheric models, the assessment of accuracy of natural atmospheric climate variability, its predictability and interaction with anthropogenic influences is far from well understood. This project aims to advance scientific understanding of dynamical properties of the atmosphere and climate systems over many spatial and temporal scales.
It is proposed to study atmospheric balance and predictability in terms of the energy percentage which is associated with various types of motions, balanced or Rossby-type of motions and unbalanced or inertio-gravity motions. This representation of the atmosphere is called the normal-mode function representation and it is a heart of methodology proposed in this project.
The projects is built on theoretical foundation set in 1970s at the National Center for Atmospheric Research in USA and with the support of original developers it will apply normal-mode function representation tool to issues for which it could not have been reliably applied earlier. The project relies on accomplishments of the proposal’s PI in weather and data assimilation modeling which this project will extend to new research areas.
The project will quantify balance in analysis datasets and ensemble forecasting systems and use the results as a starting point for climate model assessment for their ability to represent the present climate and possible changes of balance in model simulations of future climate scenarios. Results will allow dynamical classification of climate models based on their balance properties. Predictability issues will be studied by comparing temporal variability of balance in the forecasts in terms of various spatial scales. An important project outcome will be a free-access, user-friendly tool for carrying out a physically-based analysis of weather and climate model outputs.","495482","2011-12-01","2016-11-30"
"MODES","Multimode light shaping: from optical fibers to nanodevices","Massimiliano GUASONI","UNIVERSITY OF SOUTHAMPTON","The project MODES arises in the framework of the emerging interest for nonlinear multimode processes in optical fibers,  and wants to extend it to on-chip waveguides and nanoparticles, where the study of the nonlinear multimode dynamics is still on its infancy.
This project is based on a central key-idea: by properly engineering a multimode system, we can shape and master the nonlinear interaction between the modes into play, and finally exploit it for novel applications in several strategic areas.
This project has therefore a dual nature: one key-idea but multidisciplinary, heterogeneous applications.  It focuses on 4 main strategic areas (SA) and identifies an objective (OBJ) for each one, which is related to the exploitation of a specific  nonlinear multimode process:

SA1: Support technology for Spatial Division Multiplexing (SDM) >>> OBJ1: the project investigates the development of wideband multimode wavelength converters and amplifiers 

SA2: High-capacity SDM data-transmission >>>OBJ2: the project investigates the existence of multimode solitons leading to an undistorted, high-quality propagation in multicore and multimode optical fibers 

SA3: On-chip infrared optical sources >>>OBJ3: the project targets the development of on-chip, widely tunable optical sources that may be used to selectively detect important environmental gases in the whole infrared spectrum 

SA4: Shaping the nonlinear radiation at nanoscale >>>OBJ4: the project aim at developing a new theoretical insight into the way higher-harmonic radiation is emitted in complex nanostructures. Finally, it wants to and to exploit this new knowledge in view of an ultrafast conversion from invisible to visible light.

To conclude, by addressing new theoretical problems and unveiling a new multimode technology, MODES aim at opening new frontiers in nonlinear optics and being pioneer in the field of nonlinear multimode nanophotonics.","1450455","2018-12-01","2023-11-30"
"ModGravTrial","Modified Gravity on Trial","Lavinia HEISENBERG","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","The main goal of this project is to study the fundamental properties of field theories of space-time, their cosmological consequences and observational signatures. My approach aims to use the next generation of cosmological and astrophysical observations to test the validity of General Relativity on scales where it has not been fully tested yet and its resilience against alternative theories with modifications on cosmological scales. 

I will first study the implications of the quantum aspects of modified gravity theories beyond the tree level analyses for their viability, consistency and predictability. This will reduce the allowed alternative theories significantly. I will then investigate the physical consequences of these theoretically promising theories for cosmological and astrophysical scenarios. As part of my approach, I will use large galaxy surveys to constrain effects on the dynamics of cosmic structure formation from these modifications of gravity. In addition, I will confront modified theoretical predictions with Planck measurements of the Cosmic Microwave Background, type-Ia supernova data, measurements of the Baryon Acoustic Oscillations and gravitational lensing. Along the way, my group and I will crucially contribute to the development of the necessary analytical and numerical tools to exhaustively analyse such data in the search for modifications of gravity. 

In addition, the recent detection of gravitational waves by the LIGO team has paved an exciting new avenue for testing gravitational theories. These new observations will put even more stringent constraints on alternative theories. As part of the proposed research, I will also extensively exploit this new observational channel to test the validity of General Relativity and put new effects of modified gravity on trial. In particular, the propagation speed of tensor perturbations in modified gravity theories will be severely restricted by observations of gravitational waves.","1500000","2019-01-01","2023-12-31"
"ModRed","The geometry of modular representations of reductive algebraic groups","Simon Riche","UNIVERSITE CLERMONT AUVERGNE","The main theme of this proposal is the Geometric Representation Theory of reductive algebraic groups over algebraically closed fields of positive characteristic. Our primary goal is to obtain character formulas for simple and for indecomposable tilting representations of such groups, by developing a geometric framework for their categories of representations.
Obtaining such formulas has been one of the main problems in this area since the 1980's. A program outlined by G. Lusztig in the 1990's has lead to a formula for the characters of simple representations in the case the characteristic of the base field is bigger than an explicit but huge bound. A recent breakthrough due to G. Williamson has shown that this formula cannot hold for smaller characteristics, however. Nothing is known about characters of tilting modules in general (except for a conjectural formula for some characters, due to Andersen). Our main tools include a new perspective on Soergel bimodules offered by the study of parity sheaves (introduced by Juteau-Mautner-Williamson) and a diagrammatic presentation of their category (due to Elias-Williamson).","882844","2016-09-01","2021-08-31"
"MODULAR","Modular mechanical-atomic quantum systems","Philipp Treutlein","UNIVERSITAT BASEL","Atomic ensembles are routinely prepared and manipulated in the quantum regime using the powerful techniques of laser cooling and trapping. To achieve similar control over the vibrations of nanofabricated mechanical oscillators is a goal that is vigorously pursued, which recently led to the first observations of ground-state cooling and quantum behavior in such systems.

In this project, we will explore the new conceptual and experimental possibilities offered by hybrid systems in which the vibrations of a mechanical oscillator are coupled to an ensemble of ultracold atoms. An optomechanics setup and an ultracold atom experiment will be connected by laser light to generate long-distance Hamiltonian interactions between the two systems. This modular approach avoids the technical complications of combining a cryogenic optomechanics experiment and a cold atom experiment into a highly integrated setup. At the same time, it allows to investigate intriguing conceptual questions associated with the remote control of quantum systems.

The coupled mechanical-atomic system will be used for a range of experiments on quantum control and quantum metrology of mechanical vibrations. We will implement new schemes for ground-state cooling of mechanical vibrations that overcome some of the limitations of existing techniques, explore coherent mechanical-atomic interactions and Einstein-Podolsky-Rosen entanglement, and use such entanglement for measurements of mechanical vibrations beyond the standard quantum limit. The extensive experience of the PI in atomic quantum metrology and hybrid optomechanics will be a valuable asset in this endeavor.

Besides the interesting perspective of observing quantum phenomena in engineered mechanical devices that are visible to the bare eye, the project will open up new avenues for quantum measurement of mechanical vibrations with potential impact on the development of mechanical quantum sensors and transducers for accelerations, forces and fields.","1498961","2016-01-01","2020-12-31"
"MODULISPACES","Topology of moduli spaces of Riemann surfaces","Dan PETERSEN","STOCKHOLMS UNIVERSITET","The proposal describes two main projects. Both of them concern cohomology of moduli spaces of Riemann surfaces, but the aims are rather different.

The first is a natural continuation of my work on tautological rings, which I intend to work on with Qizheng Yin and Mehdi Tavakol. In this project, we will introduce a new perspective on tautological rings, which is that the tautological cohomology of moduli spaces of pointed Riemann surfaces can be described in terms of tautological cohomology of the moduli space M_g, but with twisted coefficients. In the cases we have been able to compute so far, the tautological cohomology with twisted coefficients is always much simpler to understand, even though it “contains the same information”. In particular we hope to be able to find a systematic way of analyzing the consequences of the recent conjecture that Pixton’s relations are all relations between tautological classes; until now, most concrete consequences of Pixton’s conjecture have been found via extensive computer calculations, which are feasible only when the genus and number of markings is small.

The second project has a somewhat different flavor, involving operads and periods of moduli spaces, and builds upon recent work of myself with Johan Alm, who I will continue to collaborate with. This work is strongly informed by Brown’s breakthrough results relating mixed motives over Spec(Z) and multiple zeta values to the periods of moduli spaces of genus zero Riemann surfaces. In brief, Brown introduced a partial compactification of the moduli space M_{0,n} of n-pointed genus zero Riemann surfaces; we have shown that the spaces M_{0,n} and these partial compactifications are connected by a form of dihedral Koszul duality.  It seems likely that this Koszul duality should have further ramifications in the study of multiple zeta values and periods of these spaces; optimistically, this could lead to new irrationality results for multiple zeta values.","1091249","2018-01-01","2022-12-31"
"MOLBIL","Molecular Billiards in Slow Motion","Sebastiaan Yvonne Theodorus Van De Meerakker","STICHTING KATHOLIEKE UNIVERSITEIT","It is a long held dream of chemical physicists to study (and ultimately control!) the interactions between individual molecules in completely specified collisions. This project brings this goal within reach. I will develop a novel crossed molecular beam scattering apparatus in which precise control over the molecules prior to the collision is obtained, and in which the scattering products are detected with the highest possible resolution. The velocity and quantum state of molecules is brought fully under control using Stark and Zeeman decelerators. The angular and velocity distributions of the scattering products will be probed using velocity map imaging. The monochromatic molecular beam pulses afforded by the Stark and Zeeman decelerators will yield scattering images with unprecedented sharpness, adding a new dimension to the information that can be extracted from the measured differential cross sections. This “best of both worlds” combination allows for bimolecular scattering studies at unexplored energies and with unprecedented resolution. I will exploit these new possibilities to study scattering phenomena that provide insights in molecular scattering mechanisms that were previously beyond the realm of experimentalists. These include quantum tunneling phenomena and scattering resonances in low-energy collisions, rotational product-pair correlations in bimolecular collisions, and non-adiabatic effects in the multi-surface dynamics beyond the Born-Oppenheimer approximation for radical-radical collisions. The scattering data that will be obtained will challenge the most sophisticated theoretical models to calculate molecular potential energy surfaces to date, and will foster major steps forward in our understanding of molecular interactions. The approach proposed here will open up a new and intellectually rich research field in chemical physics, and will comprise a major breakthrough in the upcoming research field of cold molecules.","1500000","2013-08-01","2018-07-31"
"MOLECSYNCON","Controlling Tunneling Charge Transport with Organic Synthesis","Ryan Chiechi","RIJKSUNIVERSITEIT GRONINGEN","""This project pushes Molecular Electronics (ME) beyond simple distant-dependence studies towards controlling tunneling charge transport with organic synthesis by manipulating the intrinsic properties of organic molecules to shape the tunneling barrier. The measurements will be done with two tools that I have developed; Eutectic Ga-In (EGaIn), which is increasingly being used by the ME community as a robust method for measuring charge-transport through self-assembled monolayers (SAMs) and SAM-templated nanogap (STAN) electrodes, which is a newer tool that allows the facile coupling of light and electric fields into SAM-based tunneling junctions. These tools are critical for performing physical-organic studies in practical tunneling junctions in which the molecules themselves define the smallest dimension of the junction; spectroscopic tools that rely on AFM or STM define the junction with a piezo and are not directly applicable to practical devices, which is the underlying motivation for all research in ME.

Two sets of molecules, one cross-conjugated and one combining flexible alkane tails with rigid oligophenylene moieties will be synthesized and investigated; more as necessary. Both series of molecules are designed around straightforward physical-organic studies meant to elucidate structure/property relationships empirically by measuring the influence of systematic structural/electronic changes on the electrical properties of tunneling junctions. The molecules will be incorporated into both EGaIn and STAN junctions to unravel the effects of the energies of the frontier orbitals, dipole moments, and other non-length-dependent synthetic handles. In the STAN junctions, the molecules will be gated with electric fields to examine more closely dynamic shifts in orbital energies and the role of polarizability.""","1494863","2013-08-01","2018-07-31"
"MOLFOUNTAIN","Precision measurements on cold molecules in a fountain","Hendrick Lucas Bethlem","STICHTING VU","In a recent series of experiments, it has been shown that polar molecules can be decelerated, bunched, cooled, and trapped using time-varying electric fields. These experiments demonstrate an unprecedented level of control over molecules, which enables a variety of applications of great scientific interest. Here, I propose to use these techniques to create a molecular fountain. In this fountain, the first of its kind, polar molecules are decelerated, cooled, and subsequently launched upwards some 10-50 cm before falling back under gravity, thereby passing a microwave cavity or laser beam twice – as they fly up and as they fall back down. The effective interrogation time in such a Ramsey type measurement scheme includes the entire flight time between the two traversals through the driving field, which can be up to a second. This long interrogation time will allow for extreme precision measurements on molecular structure to a level at which fundamental physics theories can be tested. I will use the inversion frequency in ammonia around 23 GHz as a test case. This transition is very well studied and was used in the first ‘atomic’ clock and the first demonstration of a MASER. The fountain should make it possible to measure the inversion frequency with a relative accuracy of 10^{-12}–10^{-14}; that is more than a thousand fold improvement as compared to the best previous measurement. Besides serving as a proof-of-principle, this measurement may be used as a test of the time-variation of fundamental constants – an issue that has profound implications on how we understand the universe. The inversion frequency in ammonia is determined by the tunneling rate of the protons through the barrier between the two equivalent configurations of the molecule, and is exponentially dependent on the proton mass. By monitoring the inversion frequency over a period of a few years, a possible variation of the proton-electron mass ratio can be constrained or measured.","1100000","2008-08-01","2013-07-31"
"MOLIGHT","Light in moving media","Daniele Faccio","HERIOT-WATT UNIVERSITY","""The interaction of light with matter is traditionally studied in media that are either still or moving at a negligible fraction of the speed of light. In this regime standard text-book results apply. However, if the medium or, more generally speaking, certain parameters that distinguish the medium such as the refractive index are made to change or move close to the speed of light then new and unexpected scenarios emerge. For example, if the medium refractive index is not moving but is made to oscillate at high, e.g. optical frequencies, then a parametric interaction occurs with an incident light beam amplified transformed into a new pair of light beams. If closed within cavity mirrors, amplification by the oscillating medium is enhanced and even vacuum photons are be excited in correlated pairs in a process analogous to the dynamical Casimir effect. We will also consider the opposite regime in which the refractive index profile does not oscillate but is made to move forwards close to, or even faster than the speed of light. Again we encounter surprising effects: light propagating in such a moving medium finds an elegant description in terms of a space-time metric in which space is flowing in much the same way in which water flows in a river or in which space flows in a gravitational field. Under appropriate conditions the analogy can be extended to black holes and the analogue of an event horizon for light is formed by the perturbation. Light is frequency shifted at the horizon and is also amplified: the horizon acts as a parametric amplifier. The perturbations required for these and other related effects described in the proposal will be experimentally obtained using intense laser pulses and nonlinear optics and thus open a fascinating new area of laser physics.""","1556580","2012-11-01","2017-10-31"
"MOLMESON","Molecular Mesoscopics for Organic Nano-Optoelectronics","John Mark Lupton","UNIVERSITAET REGENSBURG","This project explors the boundary between the individual molecule and the bulk solid in the context of polymeric organic semiconductors by constructing and studying molecular aggregates from the single molecule level upwards. Using time-resolved and steady-state spectroscopies at elevated and at cryogenic temperatures, the interaction of individual molecular units will be revealed. For example, the question arises as to how large a molecular aggregate can become to still behave as an individual quantum-mechanical entity, emitting just one photon at a time. How far can photoexcitations migrate in self-organized mesoscopic aggregates, and what is the interaction length with quenching species such as charges? Under which conditions does the coupling between molecular units weaken to become incoherent and irreversible? The work program combines routes to controlling self-assembly in-situ and monitoring conformational dynamics of the polymer chain as well as aggregation effects in real-time. Superresolution microscopic techniques will be applied to spatially localize excitations on a polymer chain and watch their migration. Single-molecule fluorescence will be combined with spin-resonance techniques to study charge formation und unravel radical-based material breakdown processes. Besides this bottom-up control of spectroscopic features, a top-down approach to device engineering will be explored with the goal of identifying the smallest-possible device features below which the effects of discreteness dominate leading to single-electron and single-photon devices. Breakthroughs with implications beyond organic electronics are anticipated, since the materials provide models for polymer physics, quantum optics and solid-state mesoscopics. Sensory functions are expected to derive from the control and understanding of light-matter interactions on super-molecular sub-ensemble length scales.","1480556","2012-12-01","2017-11-30"
"MOLSPINTRON","Synthetic Expansion of Magnetic Molecules Into Spintronic Devices","Paul Kögerler","RHEINISCH-WESTFAELISCHE TECHNISCHE HOCHSCHULE AACHEN","""Molecular spintronics is emerging as a rapidly growing field at the interface of inorganic molecular chemistry, surface sciences, and condensed matter physics fueled by both the fundamental interest in the underlying charge and spin transport mechanisms, and the prospects of the combined exploitation of molecular charge and spin states in a revolutionary new class of molecular-based ultra-low power devices translating their spin/charge response characteristics into novel, non-trivial functionalities. The research project proposes a range of innovative synthetic functionalization strategies of magnetic molecules that allow for targeted multi-terminal contacting of individual molecules in an approach representing a paradigm shift from existing top-down contact techniques in molecular spintronics. The project aims to reverse this existing approach and focuses on multi-step growth, controllable at the molecular level, of metallic electrode structures directly originating at a molecular magnet, as well as on controlled anchoring of the magnetic molecules to metal oxide surfaces of gate electrodes. Central to the proposal are magnetically functionalized polyoxometalates which provide a range of advantages relevant to molecular spintronics such as high stability, redox activity, structural versatility, tuneability of their molecular magnetic structures, as well growth strategies of metallic nanostructures such as quantum size-effect-controlled growth of metallic island structures. The synthetic expansion of molecule-attached metal nanocluster precursor structures into functional multi-terminal contacts addressable by multi-tip STM setups will lead to a breakthrough in reproducible charge transport measurements of single magnetic molecules and access to their fascinating Kondo physics, while the targeted technological breakthrough targets a chemically controlled integration  of single magnetic molecules into nanostructured environments of spintronic devices.""","1467200","2012-11-01","2017-10-31"
"MONIFAULTS","Monitoring real faults towards their critical state","Piero POLI","UNIVERSITE GRENOBLE ALPES","The last seismic sequence in Italy, responsible for 298 fatalities and important economic loss, remind us how urgent it is to improve our knowledge about earthquake physics to advance earthquake forecasting. While direct observations during laboratory earthquakes permit us to derive exhaustive physical models describing the behaviour of rocks and to forecast incoming lab-earthquakes, the complex physics governing the nucleation of earthquakes remain poorly understood in real Earth, and so does our ability to forecast earthquakes. I posit that this ‘ignorance’ emerges from our limited ability to unravel information about fault physics from geophysical data.The objective of this proposal is to introduce a new and integrated methodology to monitor the spatiotemporal evolution of elastic properties on real faults using seismological and geodetic data. We will apply machine learning and covariance matrix factorization for improved earthquake detection, and to discover ‘anomalous’ seismological signals, which will reveal unknown physical processes on faults. These novel observations will be integrated with time dependent measurements of rheology and deformation, obtained from cutting-edge techniques applied to continuous seismological and geodetic data. Our integrated monitoring approach will be applied to study how faults respond to known stress perturbations (as Earth tides). In parallel, we will analyse periods preceding significant earthquakes to assess how elastic properties and deformation evolve while a fault is approaching a critical (near rupture) state. Our natural laboratory will be Italy, given its excellent geodetic and seismological instrumentation, deep knowledge about faults geometry and the relevant risk posed by earthquakes. Our research will provide new insights about the complex physics of faults at critical state, necessary to understand how real earthquakes nucleate. This project will also have a major impact on observational earthquake forecast.","1393174","2019-01-01","2023-12-31"
"MoNTeS","Molecular Networks with precision Terahertz Spectroscopy","Roland Wester","UNIVERSITAET INNSBRUCK","Terahertz frequencies match the vibrations between large functional groups in molecular networks from macromolecules, nano-droplets to proteins. If we are able to measure these oscillations we can decipher the structure and the long-range interactions in large molecular systems. This yields a precise fingerprint of the molecule that is highly useful for sensitive trace analysis. However, despite of a lot of research in the field, high precision spectroscopy in the former terahertz gap for isolated large molecular networks has not been developed yet.

In this project I will develop the necessary tools to measure terahertz transition frequencies in large, mass-selected molecular systems with high resolution. For this purpose a cryogenic radiofrequency ion trap will be coupled to a terahertz resonator cavity. This will allow excitation of a dilute sample of molecular ions in well-defined internal quantum states with single-frequency terahertz radiation. My vision is to achieve high spectral resolution and single-ion sensitivity for almost arbitrarily large molecular systems in the terahertz regime which will initiate a new field for molecular spectroscopy.

To explore the potential of the newly-developed methods, I propose to study molecular networks of fundamental importance in chemistry, biology and astronomy. Vibration-tunneling dynamics will be studied in water cluster ions. Torsional motion of biological chromophores and its role in the quenching of the fluorescent state will be investigated. And the spectral signatures of molecules that are promising candidates for detection in the interstellar medium will be determined.","1471200","2012-01-01","2016-12-31"
"MORE","Advanced Mathematical Tools for Complex Network Engineering","Mérouane Debbah","Ecole Superieure D'Eiectricite","""The objective of this research project is to develop a comprehensive mathematical framework to analyze and optimize future complex systems characterized by their large dimensions, their stochastic aspects, and their being self-organized, such as smart grids and small cell telecommunication networks.

Complex systems are inherently not tractable by existing mathematical tools due to their large physical dimensions and to their many stochastic parameters.
In the past decades, several mathematical tools have emerged that enable the analysis of simplified models of complex systems. Among those tools, we mention importantly large dimensional random matrix theory, decentralized stochastic approximation, and mean field games. Only recently have those tools started to improve to encompass broader ranges of system models, allowing one to obtain relevant results in realistic scenarios. The target of our research project is to develop these tools in order to address broad problems of complex systems and to encompass them in an original unified theoretical framework.

This project represents an opportunity to develop common methodologies for many engineering communities sharing similar challenges in large dimensional stochastic networks, within a new world leading group on mathematical foundations for complex networks.""","1499714","2012-10-01","2017-09-30"
"MORPHOSIS","Morphing Locally and Globally Structures with Multiscale Intelligence by Mimicking Nature","Giulia Lanzara","UNIVERSITA DEGLI STUDI ROMA TRE","The objective of the proposed research is to engineer novel multifunctional morphing materials drawing inspiration from biological systems that are known to possess distributed sensing capabilities which in turn guide their local and global morphing. This will be achieved through the development of novel multi-scale technologies (nano- to macro) and materials that, once integrated, will allow distributed local/global sensing and morphing capabilities that can be exploited for structural as well as for eminently flexible applications. The distributed local/global morphing and sensing will be delivered by fabricating at the microscale a non-invasive, light-weight, flexible and highly expandable active network with enhanced actuation capabilities and a neurological sensor network. The networks are then expanded to the macro-scale prior being integrated in a flexible material or in an innovative multi-stable shape memory carbon-fiber composite. The sensor network has to monitor environmental and loading conditions. These data are then used to control the deformation of the active network which can deliver local (roughness changes as in dolphins skin for instance for drag reduction) or global morphing (e.g. for deformable textiles as in insect wings) in flexible materials. The multi-stable carbon-fiber composite can be used in conjunction with these two functions so as to achieve advanced morphing in structural applications (e.g., birds wings vs. aircrafts wings). The composite, with a shape memory resin as hosting matrix, due to its rigidity and sensitivity to temperature variations, can snap from one configuration to the other. The speed of the purposefully-introduced snapping-through process will be tuned with the help of the integrated active network. This research has the potential to pave the way toward the development of new multidisciplinary research fields and could revolutionarize the design and production of future structures in a variety of fields.","1664600","2013-01-01","2018-12-31"
"MOSAIC","Patterning the surface of monolayer-protected nanoparticles to obtain intelligent nanodevices","Fabrizio Mancin","UNIVERSITA DEGLI STUDI DI PADOVA","While chemical science is still striving in the search for such molecular machinery, real and perfectly working molecular machines have been developed millions of years ago by Nature. When biological systems are considered, one striking feature that emerges is their intrinsic functional simplicity, since only a few building blocks are used to build complex structures. Apparently, what matters is not chemical complexity but the ability to precisely control the spatial arrangement and organization.
Functional nanoparticles offer an unmatched opportunity to build complex structures with simple building blocks and relatively simple manipulations. The main goal of the Mosaic project is to gain the ability to hierarchically control the self-assembling of metal nanoparticles coating monolayers and take advantage from such ability to obtain complex function from the materials realized. This objective will require reaching a complete understanding of the structure and dynamic of nanoparticles coating monolayers developing new tools, mainly based on NMR spectroscopy, for their investigation. Then, we plan to learn how to use supramolecular interactions to control the monolayer organization and to gain, in this way, the ability to program functional groups patterns on the surface of the particles. In this way, it will possible to achieve a degree of organization comparable to that of biologic systems, such as enzymes or membranes. This organization of functional groups will be then used to obtain highly sophisticated function by these nanosystems, such as recognition, sensing, in particular NIR sensing, catalysis and transport.","1499000","2010-12-01","2015-11-30"
"MOTORS","On the move: Motor-cargo and motor-microtubule interactions studied with quantitative, high spatio-temporal resolution microscopy in vivo","Melike Lakadamyali","FUNDACIO INSTITUT DE CIENCIES FOTONIQUES","Intracellular transport plays a key role in many cellular processes. Cells rely on a well-regulated, two-way transport system that consists of molecular motors and cytoskeletal filaments for their correct functioning. In nerve cells, breakdown of transport is tightly linked to neurodegenerative diseases.
Understanding what goes wrong with intracellular transport during disease requires knowledge of how motors work inside cells to transport cargo. While much effort has been devoted to understanding motor mediated intracellular transport, our knowledge of motor-cargo and motor-microtubule interactions in the cellular environment is still very rudimentary. The small size of motors, the complex architecture of their microtubule tracks, and the inherently dynamic nature of transport have made these interactions virtually inaccessible to observation in living cells until recently. With new advances in fluorescence microscopy, in particular with the development of ground-breaking methods that surpass the diffraction limit, we can finally begin to address this challenging problem.
In this ambitious proposal we will study, at an unprecedented level of detail, the nanoscale organization and stoichiometry of motor proteins on their cargo and interactions of motor proteins with microtubules in living cells. We will achieve this goal by using a multidisciplinary approach that combines cutting-edge biophysical tools such as single particle tracking, quantitative single molecule counting and super-resolution nanoscopy with novel genetic manipulation and fluorescence labeling methods. Using these unique set of tools we will unravel the molecular mechanisms that regulate motors to achieve efficient transport. The results obtained in this proposal will provide, for the first time, a detailed picture of how motors function inside living cells, greatly enhancing our knowledge of a fundamental cell biological process and of its implications in disease.","1288535","2013-09-01","2017-12-31"
"MottMetals","Quantitative approaches for strongly correlated quantum systems in equilibrium and far from equilibrium","Olivier Paul Emile Parcollet","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","Understanding electronic correlations remains one of the most important challenges in theoretical condensed matter physics.  The interaction-induced metal-to-insulator Mott transition plays a major role in many transition metal oxides, f-electron materials and now in quantum optics.  Upon doping or application of a strong electric field, strongly correlated Mott metals emerge from the Mott insulators, with fascinating properties.  Moreover, the out-of-equilibrium behaviour of these systems is only beginning to be systematically explored experimentally.  While these systems strongly challenge the standard concepts and methods of the quantum many-body theory, a new era is progressively unfolding, in which quantitative and detailed comparisons between theory and experiments is becoming possible in strong correlation regimes, even out of equilibrium.

The goal of this proposal is to construct, in close contact with experiments and phenomenology, a new generation of theoretical methods and algorithms in order to i) study the new states of matter induced by non-equilibrium phenomena in strongly correlated quantum systems, first in simple models, and then in realistic computations for real materials; ii) elucidate the mystery of high temperature superconductivity.  Open source implementations of the methods and algorithms developed during this project will also be provided for a better knowledge diffusion.","1130800","2012-01-01","2017-12-31"
"MOTZETA","Motivic zeta functions and the monodromy conjecture","Johannes Nicaise","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","The monodromy conjecture, formulated in the seventies by the Japanese mathematician Igusa, is one of the most important open problems in the theory of singularities. It predicts a remarkable connection between certain geometric and arithmetic invariants of a polynomial f with integer coefficients. The conjecture describes in a precise way how the singularities of the complex hypersurface defined by the equation f = 0 influence the asymptotic behaviour of the number of solutions of the congruence f = 0 modulo powers of a prime.  Some special cases have been solved, but the general case remains wide open. A proof of the conjecture would unveil profound relations between several branches of mathematics.
In the past years, we have developed a new interpretation of the monodromy conjecture, based on non-archimedean geometry, and we have generalized it to a larger framework. A significant success of this approach was our proof of the monodromy conjecture for one-parameter degenerations of abelian varieties. The aim of our proposal is to generalize this proof to degenerations of Calabi-Yau varieties, and to adapt the arguments to the local case of the conjecture (hypersurface singularities). Degenerations of Calabi-Yau varieties play a central role in Mirror Symmetry, a mathematical theory in full development that emerged from string theory. We will explore in detail the connections between the monodromy conjecture and recent breakthroughs in Mirror Symmetry (tropical constructions of degenerating Calabi-Yau varieties).  We hope to achieve these goals by combining advanced tools from several research domains, in particular: motivic integration, non-archimedean geometry, Hodge theory, logarithmic geometry and tropical geometry. We are convinced that all these research domains will greatly benefit from the systematic exploration of their mutual interactions, and that the impact of our project will go far beyond the monodromy conjecture.","1044980","2013-05-01","2018-04-30"
"MPGR","Mathematical Problems in General Relativity","Michail Dafermos","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The proposed project concerns the study of mathematical aspects of the general theory of relativity. Major unsolved problems in the area include the problem of nonlinear stability of black hole spacetimes and the problem of weak and strong cosmic censorship. Mathematically, these problems concern the global dynamics of solutions to the Einstein equations. Resolution of these problems is of fundamental importance both to our understanding of the nature of the theory of relativity and to our assessment of its validity. In previous work, I have addressed these problems either under symmetry reductions or in linearised settings. These studies have revealed much of the mathematical structure of the dynamics of the Einstein equations in the context of black holes. Further progress extending beyond symmetry or linearised theory now appears possible. A research programme with this as a goal will be outlined in the proposal.","500000","2008-09-01","2013-08-31"
"MST","Moonshine and String Theory","Chih-Ning Cheng","UNIVERSITEIT VAN AMSTERDAM","The purpose of the proposed research is to forward the understanding of the umbral moonshine discovered recently by myself. I plan to study it in the context of string theory. Moreover, I aim to use this new discovery to gain a deeper understanding of certain fundamental aspects of the theory. 

The term moonshine refers to the astonishing and puzzling relation between functions with special symmetries (modular properties) and finite groups. The novel type of moonshine involves the so-called mock modular forms, and was first noticed in the study of K3 surfaces. In a recent paper I constructed 23 instances of such a new ""umbral moonshine"" phenomenon  in a completely uniform way using the 23 special lattices classified by Niemeier as the starting point, and thereby provided the general framework in which this paradigm should be studied.  

From a physical point of view, it is well-known that K3 surfaces play a crucial role in not only the specific constructions of compactifications but also the fundamental dualities in string theory. Hence, the new quantum symmetries of K3 surfaces, as suggested by umbral moonshine, will have a wide range of important implications for string theory. Moreover, I believe the solution of the moonshine puzzle will lead to a new understanding of the long sought-after algebraic structure of the supersymmetric (or BPS) spectrum of supersymmetric quantum theories. More ambitiously, I aim to draw lessons from these special theories with large symmetries to shed light on the structure of the ""landscape"" of string theory vacua. 

From a mathematical point of view, to understand and to prove such a mysterious and beautiful relation would be a triumph in its own right. Moreover, the development of umbral moonshine will undoubtedly lead to new important results in the study automorphic forms, K3 geometry, and extended algebras.","1256624","2015-09-01","2020-08-31"
"MuDiLingo","A Multiscale Dislocation Language for Data-Driven Materials Science","Stefan SANDFELD","TECHNISCHE UNIVERSITAET BERGAKADEMIE FREIBERG","Crystalline defects in metals and semiconductors are responsible for a wide range of mechanical, optical and electronic properties. Controlling the evolution of dislocations, i.e. line-like defects and the carrier of plastic deformation, interacting both among themselves and with other microstructure elements allows tailoring material behaviors on the micro and nanoscale. This is essential for rational design approaches towards next generation materials with superior mechanical properties.

For nearly a century, materials scientists have been seeking to understand how dislocation systems evolve. In-situ microscopy now reveals complex dislocation networks in great detail. However, without a sufficiently versatile and general methodology for extracting, assembling and compressing dislocation-related information the analysis of such data often stays at the level of “looking at images” to identify mechanisms or structures. Simulations are increasingly capable of predicting the evolution of dislocations in full detail. Yet, direct comparison, automated analysis or even data transfer between small scale plasticity experiments and simulations is impossible, and a large amount of data cannot be reused.

The vision of MuDiLingo is to develop and establish for the first time a Unifying Multiscale Language of Dislocation Microstructures. Bearing analogy to audio data conversion into MP3, this description of dislocations uses statistical methods to determine data compression while preserving the relevant physics. It allows for a completely new type of high-throughput data mining and analysis, tailored to the specifics of dislocation systems. This revolutionary data-driven approach links models and experiments on different length scales thereby guaranteeing true interoperability of simulation and experiment. The application to technologically relevant materials will answer fundamental scientific questions and guide towards design of superior structural and functional materials.","1499145","2017-11-01","2022-10-31"
"MULT2D","Multiscale Mechanics of Bone Fragility in Type-2 Diabetes","Ted VAUGHAN","NATIONAL UNIVERSITY OF IRELAND GALWAY","Type-2 (T2) Diabetes is associated with a 3-fold increase in bone fracture risk, despite the fact that bone volume is not reduced. This implies that T2 diabetes impairs bone quality, whereby the intrinsic material properties of the bone matrix are altered. However, current diagnostic techniques are unable to predict fracture probability in T2 diabetes as they are based on measures of bone quantity. While it is believed that non-enzymatic cross-linking of organic proteins (also known as AGE accumulation) in the bone matrix is responsible for bone fragility in T2 diabetes, there is a distinct lack of understanding how altered protein configurations impair whole-bone biomechanics. In this project, the applicant will embark on frontier research that will develop a state-of-the-art multiscale computational framework that couples behaviour from the molecular to whole-bone level, providing a basis to interrogate and elucidate the physical mechanisms that are responsible for diabetic bone fragility. A multiscale experimental framework will, for the first time, establish relationships between AGE crosslink-density and whole-bone fragility in animal and human T2 diabetic bone tissue. Together, this data will inform a probabilistic mutli-level model of hip fracture, which will be used to quantitatively evaluate the relationship between hip fracture probability, bone quantity and bone quality. The research programme will also establish a novel strategy for clinical fracture risk assessment that employs existing protocols to measure bone quantity, in combination with a surrogate measure of bone quality. The surrogate measure of bone quality proposed is a systemic measure of AGE content, which is clinically-obtainable through a blood sample and therefore widely-applicable. Overall, the project will provide a ground-breaking advance in our understanding of bone fragility, with remarkable potential to innovate novel solutions for clinical assessment of T2 diabetic bone disease.","1499659","2019-02-01","2024-01-31"
"MultiBD-CHALLENGE","The Pursuit of Group 13-Group 15 (E13≡E15) Triple Bonds. Their Reactivity and Applications for Materials","Diego Andrada","UNIVERSITAT DES SAARLANDES","Multiple bonds have an enourmous impact on our lives as they are extremely useful functionalities in important industrial chemical transformations and products. The new millennium has witnessed considerable progress in the chemistry of main group compounds with multiple bonds. In case of elements other than carbon, the utilization of bulky ligands, with the appropriate steric and electronic effects, is a crucial factor in the stabilization of such species. Nevertheless, heteronuclear compounds containing triple bonds between the heavier elements of Group 13 and Group 15 are so far unknown. This proposal will address this knowledge gap by the use of donor-acceptor interactions to stabilize such compounds. The hypothesis rests not only in the stabilization provided by the Lewis base species but also on the electronic features enforced by them. This proposal will utilize the tools of experimental and computational chemistry in tandem, as an efficient and predictive strategy to gain synthetic access to the hitherto unknown triple bonds. These structures will present a multifunctional character, by having two pi-bonds, a lone pair on the Group 15 element, and Lewis base donors. Thus, given the unique bonding situation, they are expected to serve as innovative reagents for the activation of organic small molecules, as well as excellent metal-free catalysts and versatile coordination ligands toward transition metals. Furthermore, the heteroatomic triple bond motif is expected to provide unprecedented precursors for growing high-quality III-V semiconductor films. Therefore, the specific aims of this project are: (i) to comprehensively design the syntheses of these unique compounds; (ii) to develop and exploit their reactivity and; (iii) to harvest their potential in materials science. Achieving these aims will have a tremendous impact on various areas of academic and industrial interest ranging from catalysis and energy storage materials to photovoltaic devices.","1500000","2019-02-01","2024-01-31"
"MULTIBIOPHOT","Multiphoton processes using plasmonics   towards advanced nanobiophotonics","Janina Kneipp","HUMBOLDT-UNIVERSITAET ZU BERLIN","Proposal summary
Biophotonics is an emerging interdisciplinary field. Modern laser spectroscopic methods in combination with
microscopy open up exciting new ways to study biological objects. For biological applications, multi-photon
or non-linear spectroscopy can offer several advantages over one-photon excitation. The aim of this project
is to explore novel spectroscopic strategies based on multi-photon excitation. In order to overcome lowsignal
problems related to multi-photon spectroscopy, we will exploit plasmonics and perform multi-photon
spectroscopy in the enhanced local optical fields of gold- and silver nanostructures. I propose a combination
of three different plasmonics-enhanced two-photon spectroscopic methods for multimodal two-photon
sensing and imaging. This combination can provide information on morphological structures and function of
biological systems along with chemical information about molecular composition, structure, and interactions.
A key aspect of this new concept will be the implementation of spectroscopically multifunctional
nanosensors with plasmonic nanoparticles as basic building blocks. Following multi-photon excitation, these
sensors deliver information on their environment inferred from a set of multiple surface-enhanced
spectroscopic signatures.
The proposed research will generate fundamental knowledge about multi-photon driven processes in
enhanced local optical fields and about multi-photon interaction of light and biological matter. This might
open up entirely new directions to advance nanobiophotonics. In particular, the outcome of the research will
stimulate the new field of two-photon sensing and imaging, which has the capability to advance our
understanding of biological systems and processes. The project is expected to have broad impact and
contribute to the development of multi-photon optical sensing and multi-photon excited photophysics and
photochemistry with applications in physics, chemistry, energy technology, biotechnology and medicine.","1394679","2011-01-01","2015-12-31"
"MULTICELL","Microfluidic multiplexed cell chips","Charles Baroud","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","There exist very few techniques for studying a group of cells containing
a large number compared to a single cell but small compared to a whole
tissue. This implies that statistics are exceedingly difficult to obtain
from measurements of individual cells. Microfluidics provides a way to
amend this by allowing ways to observe individual cells and automate
such measurements. The aim in this project is to develop a cell
manipulation platforms based on microfluidics techniques developed in
our lab, while answering relevant biological questions.

The first question concerns Sickle Cell Anemia, a genetic disease for
which no treatment exists. We will study the polymerization of
hemoglobin within red blood cells, as they are submitted to cycles of
oxygenation and deoxygenation. Quantitative measurements of the response
of the cells to oxygen variations will allow physiological conditions to
be simulated, including in the presence of therapeutic candidates or
other biological agents.

The second question concerns the motility of adherent cells in a
three-dimensional environment. This question will be to understand the
migration of cells in a 3D gradient of chemo-attractant, as well as
gradients of rigidity of the environment. This part will require the
development of new technological tools which can later be applied to a
wide range of biological problems. The long term aim is to replace the
current tools of biological labs with miniaturized and integrated lab on
a chip devices.","1494744","2012-02-01","2017-01-31"
"MULTIJEDI","Multilingual Joint Word Sense Disambiguation","Roberto Navigli","UNIVERSITA DEGLI STUDI DI ROMA LA SAPIENZA","In the information society the language barrier represents one of the main obstacles to the automatic use, integration and manipulation of knowledge, and this is manifested in the lack of intelligent systems able to perform unified semantic processing of textual resources in a multitude of different languages. To create such systems, a necessary step is to assign the appropriate meanings to the words in documents, a task referred to as Word Sense Disambiguation (WSD). But while WSD is typically performed in a monolingual setting, in order to enable multilingual processing, the semantic connections between word senses (i.e. meanings) in different languages need to be exploited. However, current state-of-the-art systems mainly rely on the existence of bilingual aligned text collections or limited-coverage multilingual resources to perform cross-lingual disambiguation, an unrealistic requirement when working with an arbitrary number of language pairs.
Here we propose a research program that will investigate radically new directions for performing multilingual WSD. The key intuition underlying our proposal is that WSD can be performed globally to exploit at the same time knowledge available in many languages. The first stage will involve the development of a methodology for automatically creating a large-scale, multilingual knowledge base. In a second stage, using this lexical resource, novel graph-based algorithms for jointly performing disambiguation across different languages will be designed and experimented. Crucially, we aim to show that these two tasks are mutually beneficial for going beyond current state-of-the-art WSD systems. The proposed project will have an impact not only on WSD research, but also on related areas such as Information Retrieval and Machine Translation.","1288400","2011-02-01","2016-01-31"
"MULTIMATE","A Research Platform Addressing Outstanding Research Challenges for Nanoscale Design and Engineering of Multifunctional Material","Johanna Rosen","LINKOPINGS UNIVERSITET","""Nanoscale engineering is a fascinating research field spawning extraordinary materials which revolutionize microelectronics, medicine,energy production, etc. Still, there is a need for new materials and synthesis methods to offer unprecedented properties for use in future applications.

In this research project, I will conduct fundamental science investigations focused towards the development of novel materials with tailor-made properties, achieved by precise control of the materials structure and compostition. The objectives are to: 1) Perform novel synthesis of graphene. 2) Explore nanoscale engineering of """"graphene-based"""" materials, based on more than one atomic element. 3) Tailor uniquely combined metallic/ceramic/magnetic materials properties in so called MAX phases. 4) Provide proof of concept for thin film architectures in advanced applications that require specific mechanical, tribological, electronic, and magnetic properties.

This initative involves advanced materials design by a new and unique synthesis method based on cathodic arc. Research breakthroughs are envisioned: Functionalized graphene-based and fullerene-like compounds are expected to have a major impact on tribology and electronic applications. The MAX phases are expected to be a new candidate for applications within low friction contacts, electronics, as well as spintronics. In particular, single crystal devices are predicted through tuning of tunnel magnetoresistance (TMR) and anisotropic conductivity (from insulating to n-and p-type).

I can lead this innovative and interdisciplinary project, with a unique background combining relevant research areas: arc process development, plasma processing, materials synthesis and engineering, characterization, along with theory and modelling.""","1484700","2010-09-01","2015-08-31"
"MultiMT","Multi-modal Context Modelling for Machine Translation","Lucia Specia","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","Automatically translating human language has been a long sought-after goal in the field of Natural Language Processing (NLP). Machine Translation (MT) can significantly lower communication barriers, with enormous potential for positive social and economic impact. The dominant paradigm is Statistical Machine Translation (SMT), which learns to translate from human-translated examples.

Human translators have access to a number of contextual cues beyond the actual segment to translate when performing translation, for example images associated with the text and related documents. SMT systems, however, completely disregard any form of non-textual context and make little or no reference to wider surrounding textual content. This results in translations that miss relevant information or convey incorrect meaning. Such issues drastically affect reading comprehension and may make translations useless. This is especially critical for user-generated content such as social media posts -- which are often short and contain non-standard language -- but applies to a wide range of text types. 

The novel and ambitious idea in this proposal is to devise methods and algorithms to exploit global multi-modal information for context modelling in SMT. This will require a significantly disruptive approach with new ways to acquire multilingual multi-modal representations, and new machine learning and inference algorithms that can process rich context models. The focus will be on three context types: global textual content from the document and related texts, visual cues from images and metadata including topic, date, author, source. As test beds, two challenging user-generated datasets will be used: Twitter posts and product reviews.

This highly interdisciplinary research proposal draws expertise from NLP, Computer Vision and Machine Learning and claims that appropriate modelling of multi-modal context is key to achieve a new breakthrough in SMT, regardless of language pair and text type.","1493771","2016-07-01","2021-06-30"
"MULTISCALE","Precision Multi-Scale Predictions for the LHC: Higgs, Jets and Supersymmetry","Wouter - Jonathan Waalewijn","UNIVERSITEIT VAN AMSTERDAM","My project will boost the precision of theoretical predictions for collisions at the Large Hadron Collider. Precise predictions are crucial to further constrain the properties of the recently-discovered Higgs boson, and uncover a faint signal of Beyond-the-Standard Model physics. I will focus on the strong interactions, which dominate the theoretical uncertainty and play a role at multiple energy scales, including those related to the incoming protons, the hard scattering, the masses of (new) particles, the transverse momentum and size of jets.

The critical progress of this proposal lies in taking this intrinsically multi-scale nature into account, moving beyond the current trade-off between precision and realism in the three dominant calculational paradigms. Fixed-order calculations are systematically improvable but assume that there is no hierarchy between perturbative scales. Monte Carlo event generators provide a fully exclusive description of the final state, but are currently limited to leading-logarithmic order and lack theoretical uncertainties. Resummed calculations can reach a higher logarithmic accuracy, but have been restricted to single observables. 

In a recent breakthrough, I constructed a new effective field theory that simultaneously achieves higher logarithmic accuracy in two independent observables, by factorizing the physics at the corresponding scales. Moving beyond this prototypical study, I will develop the general effective field theory framework that accounts for the relevant scales in realistic measurements, which overcomes the limitations of all three paradigms. This research will be carried out in the context of several important LHC applications: precision Higgs measurements, jet substructure techniques for identifying boosted heavy particles and  supersymmetry searches. My new field-theoretic insights and more precise predictions will be critical as the LHC starts Run 2, searching for new physics at even higher energies.","1500000","2016-09-01","2021-08-31"
"MULTISCALECHEMBIO","Electronic Structure of Chemical, Biochemical, and Biophysical Systems: Multiscale Approach with Electron Correlation","Leonardo Guidoni","UNIVERSITA DEGLI STUDI DI ROMA LA SAPIENZA","The currently available computational methods have often serious limitations to treat systems where electron correlation plays and important role. Many issues concerning the electronic structure of radicals, photoreceptors near-half-filled transition metals (Cr,Mo,Fe,Ni) are of paramount relevance in basic and applied research in Chemistry and Biochemistry, but still out of the capabilities of standard and conventional tools such as Density Functional Theory. On the other hand, post Hartree-Fock methods computationally more expensive and their application is limited to few atoms. The objective of the present proposal is to overcome these limitations and to develop and apply a multiscale, innovative and unconventional computer simulation technique to unravel the electronic properties of strongly correlated chemical and biochemical systems. The methodology is based on a combined approach between Quantum Monte Carlo (QMC), DFT and Molecular Mechanics. The proposed approach has a faster scaling of the calculation time with the system size N with respect others standard quantum chemistry methods of equivalent level (~ N4 vs ~ N7). es to address challenging open problems in the chemistry and biochemistry of radical compounds, photoreceptors, and transition metal catalysis and enzymatic activity. Application to photoreceptors include the study of the spectral properties of rhodopsin, the integral membrane protein responsible of the light detection in the retina. Applications on transition metal molecules will shed the light on the catalytic strategies of iron-based enzymes and their corresponding biomimetic compounds.","1200000","2009-10-01","2015-09-30"
"MUSE","Multi-perspective Ultrasound Strain Imaging & Elastography","Richard LOPATA","TECHNISCHE UNIVERSITEIT EINDHOVEN","Ultrasound (US) is the modality of choice for imaging and functional measurements of the cardiovascular system due to its high spatial and temporal resolution. In recent years, the use of US has been on the rise owing to huge advancements in acquisition speed and resolution. Nevertheless, because of physical constraints, several issues —limited field-of-view, refraction, resolution and, contrast anisotropy— cannot be resolved using a single probe. 
This proposal will aim at tackling these issues introducing Multi-perspective Ultrasound Strain Imaging & Elastography (MUSE). MUSE will push the frontiers of 3-D US imaging by introducing a novel, multi-perspective 3-D US system. The revolutionary system will consist of two synchronously controlled 3-D matrix arrays and advanced signal and image processing to improve geometric and functional measurements (strain, elasticity). Validation will be performed for two applications: cardiac strain imaging in patients with aortic valve stenosis (AoS) and elastography of abdominal aortic aneurysms (AAA).
Fusion of dual-probe data will be challenged and achieved by new algorithms, preserving important features and improving both contrast and field-of-view. Advanced 3-D processing of the raw US data will be developed for motion and strain imaging. A novel compounding technique, fusion strain imaging, will combine multi-perspective strain data to improve accuracy and precision. A comprehensive framework for system verification and validation will be built, comprising US simulations, ex vivo experiments, and in vivo pilot studies on healthy volunteers. The proposed technique will be validated in AoS and AAA patients. 
Ultimately, MUSE will introduce a non-invasive, ground-breaking US platform for functional screening and follow-up, and a breakthrough in early diagnosis, clinical decision making, and risk assessment of cardiovascular disease. Moreover, MUSE has the potential to replace invasive or costly imaging modalities with US.","1998505","2018-02-01","2023-01-31"
"MuSES","Muon Spectroscopy of Excited States","Alan John Drew","QUEEN MARY UNIVERSITY OF LONDON","Muon spin spectroscopy has shown itself to be a very powerful probe of material properties, with Europe leading this research due to it operating half of the available muon sources in the world, but to date there has been very little work done on spectroscopy of excited states. In the first instance, this proposal will design and build an upgrade to an existing spectrometer (HiFi at ISIS) that will comprise a high-power tunable laser to provide the electronic excitation. This will be followed by a study of the physics of excited state muon spectroscopy, an entirely unexplored area of the technique. The fundamental mechanisms of charge carrier transport in organic semiconductors will then be investigated, and the ground-work for directly measuring the recombination zone in organic LEDs will be done. Perhaps most importantly, the fundamental physics of electron transfer in peptides will be performed - which is responsible for many biological processes and not well understood. The muon technique has recently been shown by the applicant to offer both spatial and temporal information on the electron’s progress through the molecule - this is the only technique that can measure the electron’s wavefunction at different locations on the molecule as a function of time after the excitation is formed.","1476891","2012-12-01","2017-11-30"
"MUSIC","Modeling and Simulation of Cancer Growth","Hector Gómez Díaz","UNIVERSIDADE DA CORUNA","Nowadays, the treatment of cancer is based on the so-called diagnostic paradigm. We feel that the shift from the traditional diagnostic paradigm to a predictive patient-specific one may lead to more effective therapies. Thus, the objective of this project is to introduce predictive models for cancer growth. These predictive models will take the form of mathematical models developed from first principles and the fundamental features of cancer biology. For these models to be useful in clinical practice, we will need to introduce new numerical algorithms that permit to obtain fast and accurate simulations based on patient-specific data.

We propose to develop mathematical models using the framework provided by the mixtures theory and the phase-field method. Our model will account for the growth of the tumor and the vasculature that develops around it, which is essential for the tumor to grow beyond a harmless limited size. We propose to develop new algorithms based on Isogeometric Analysis, which is a recent generalization of Finite Elements with several advantages. The use of Isogeometric Analysis will simplify the interface between medical images and the computational mesh, permitting to generate smooth basis functions necessary to approximate higher-order partial differential equations like those that govern cancer growth. Our modeling and simulation tools will be examined and validated by experimental and clinical observations. To accomplish this, we propose to use anonymized patient-specific data through several patient imaging modalities.

Arguably, the successful undertaking of this project, would have the potential to transform classical population/statistics-based treatments of cancer into patient-specific therapies. This would elevate mathematical modeling and simulation of cancer growth to a stage in which it can be used as a quantitatively accurate predictive tool with implications for clinical practice, clinical trial design, and outcome prediction.","1405420","2012-10-01","2017-09-30"
"MUSICA","Multi-platform remote sensing of isotopologues for investigating the cycle of atmospheric water","Matthias Schneider","KARLSRUHER INSTITUT FUER TECHNOLOGIE","MUSICA aims to understand the atmospheric water cycle and its interplay with climate change applying unique long-term high quality and global remote sensing observations of tropospheric stable water vapour isotopologues. It is well established that water in its various forms plays a dominant role in nearly all aspects of the Earth s climate system. Understanding the full cycle of evaporation, cloud formation, and precipitation is of highest scientific priority for predicting climate change.
The ratio of the isotopologues (e.g. HD16O/H216O) is affected by evaporation, condensation, and cloud processes, and offers a unique opportunity for investigating how water moves through the troposphere. Incorporating isotopologues in atmospheric general circulation models (AGCM) and comparing the isotopologue simulations to observations has the potential to test the models  ability of reproducing the global atmospheric water cycle and its interplay with climate change. So far this research field has not been explored due to the lack of consistent, long-term, high-quality, and area-wide observational data. MUSICA will for the first time combine long-term ground- and space-based remote sensing measurements in a consistent manner, and will generate novel tropospheric HD16O/H216O data, taking benefit from both the high and well documented quality of the ground-based observations and the wide geographical coverage of the space-based observations. This unique observational data set will allow a new dimension of water cycle research.
MUSICA will collaborate with the Stable Water Isotope Intercomparison Group (SWING) in order to improve current state-of-the-art water isotope AGCMs. MUSICA will investigate and improve the understanding of tropospheric water vapour sources and transport pathways, and empirically assess how well climate feedbacks are captured by current climate models and thereby it will constrain a major uncertainty of climate projections.","1500000","2011-02-01","2016-07-31"
"MUSTANG","Magnonics Using Spin Torque, spin caloritronics, And Nanoplasmonic engineerinG","Johan Åkerman","GOETEBORGS UNIVERSITET","My overall aim is to develop a Magnonic technology platform where Spintronic, Spin-Caloritronic and Nano-plasmonic devices and structures combine to create ground-breaking functionality from novel interactions between charge, spin, heat and light. With traditional Magnonic studies typically geared towards the low GHz range, and nanoplasmonic phenomena primarily focusing on visible light, my proposed platform will also attempt to bridge the so-called “THz gap” and create ultra-broadband and rapidly tuneable spin wave (SW) based signal generators, manipulators, detectors, and even spectrometers, in the 10–200 GHz frequency range. I will reach this goal by transferring my documented nano-contact spin torque oscillator (NC-STO) expertise into the magnonics world of both metal and insulator based SW propagation, add recently discovered spin hall (SHE) and inverse spin hall effect (ISHE) SW manipulation/detection, and combine it with my recently acquired know-how in nanoplasmonics.

My specific aims are:

1. SW generation and manipulation using metal and YIG based NC-STOs

2. SW-light/heat interaction using nanoplasmonic structures and Spin-Caloritronics

3. ISHE/SHE detection and control of propagating SWs in metals and YIG","1500000","2013-01-01","2017-12-31"
"MUSTANG","Multi-scale Star Formation Across Nascent Galaxies","Diederik KRUIJSSEN","RUPRECHT-KARLS-UNIVERSITAET HEIDELBERG","One of the fundamental questions in modern astrophysics is how gas is converted into stars during galaxy formation, and how this may have changed with galactic environment and cosmic time. It is widely accepted that the unknown physics of star formation (SF) in molecular clouds and feedback from massive stars represent the main uncertainties in our understanding of galaxy formation. The key open questions are: which physical mechanisms govern the time-evolution of gas towards SF and its subsequent expulsion by feedback? How do these physics change with environment or cosmic time? How did the most extreme stellar systems in the Universe (i.e. globular clusters) form at high redshift? How do the cloud-scale physics of SF and feedback connect cold dark matter cosmology to the observable galaxy population? It is now possible to answer these questions thanks to two crucial developments. (1) The latest generation of billion-euro observatories allow the necessary observational data to be taken. (2) The techniques needed to solve the problem have been developed, tested, and validated, through major theoretical efforts that I have led. These developments position me ideally to answer the above questions with a unique combination of world-leading observational data, fundamental theory, innovative analysis techniques, and state-of-the-art simulations. I will use these to formulate a multi-scale description of SF and feedback, focussing on the cloud-scale physics and advancing the field beyond the phenomenology of galactic scaling relations. I will introduce an empirically-motivated, physical model for cloud-scale SF and feedback in galaxy formation simulations, thereby overcoming their main limitation and making the crucial step of linking the observable galaxy population to cold dark matter cosmology. This ambitious programme bridges observations, theory, and simulations, and can only be realised with the human resources provided by an ERC Starting Grant.","1499629","2017-04-01","2022-03-31"
"MUSYX","Multiscale Simulation of Crystal Defects","Christoph Ortner","THE UNIVERSITY OF WARWICK","""The MUSYX project will develop a rigorous numerical analysis framework for assessing the accuracy of multiscale methods for simulating the dynamics of crystalline defects. The core focus of the research will be the analysis of approximation errors of atomistic-to-continuum (a/c) coupling methods and related multiscale schemes. The rigorous mathematical foundations, which will be the outcome of this work, will also lead to the construction of more robust and more efficient numerical algorithms.

The research will be undertaken within four distinct but closely related themes: Theme A: quasistatic evolutions up to and including bifurcation points (defect nucleation and evolution); Theme B: Transition paths, saddles, and transition rates between local minima (defect nucleation and diffusion at finite temperature); Theme C: Computation of defect formation energies within the framework of equilibrium statistical mechanics; Theme D: Fully dynamic problems. The four themes are connected through the focus on crystal defects and model interfaces (e.g., atomistic/continuum).

Themes A and B build on and significantly extend the theory of a/c coupling pioneered by the PI, which combines classical techniques of numerical analysis (consistency, stability) with modern concepts of multiscale and atomistic modeling. Theme C aims to develop an analogous theory for multiscale free energy calculations (precisely, defect formation energies). Theme D approaches the analysis of a fully dynamic multiscale scheme by analyzing its qualitative statistical properties.""","1111793","2014-01-01","2018-12-31"
"MYCAP","DEVELOPMENT OF A TECHNOLOGY TO PRODUCE MICROCAPSULES, based on the formation of drops from viscous non-Newtonian liquids sprayed through fan-jet nozzles, TO USE IN CANCER THERAPY","Eva Maria Martin Del Valle","UNIVERSIDAD DE SALAMANCA","The main aim of this project is the development of a new technology to produce smart drug delivery
system for chemotherapeutic agents per recognition event. For that affinity-microparticles (10-20 microns
diameter) loaded of paclitaxel (PTX) or Endotastin will be produced.
Microcapsules will be made using a new technology based on the formation of drops from viscous non-
Newtonian liquids sprayed through fan-jet nozzles. This process is based on generation of kinetic energy to a
liquid jet resulting on controlled spray generation. The technique will be modelled in order to ensure the scale-up
the process.
The microparticles, based on alginate polymer, will be functionalised on his surface by affinity ligand, epidermal
growth factor (EGF) which will be able to recognize a specific protein of the tumoral cell, (EGFR) epidermal
growth factor receptor.
Surface plasma resonance will be carried to control the interaction between the microparticle and the protein and
therefore to ensure the efficiency of the microparticles produced. This information will be used to developed a
dynamic model to assess the importance of spatial phenomena and then we will evaluate the accuracy of partial
differential equations (PDEs) in transient when spatial effects are important.
Control release from microcapsules loaded of anticancer agent will be characterized by control release kinetics,
mass transfer, mechanic stability and permeability studies. Mass transfer through the tissue, or therapeutic
leakage from storage cavities and their consequent transport through the organ, are among the several physical
processes, where knowledge of the unsteady transport of a scalar quantity (mass of an active) is of importance
for cell therapy. For that reasons it is necessary to derive an analytical solution for the unsteady mass transport
problem in a porous medium under torsional flow to simulate the diffusion of active materials in body cavities
(Mixed mechanic-electrical model), assuming body cavities as ideally isotropic porous medium.
Finally, characterized microcapsules will be tested in lung tissues with lung cancer. Cell viability (MTT) and
apoptosis after PTX exposure in non small cell lung cancer (NSCLC) will be studied. Morphological distribution
of particles in areas of interest (lung, pleura and lymph nodes) will be examined. The experimental results found
in vitro will compared with experimental animal models developed for tumoral cell death.","1367229","2011-02-01","2016-01-31"
"MYKI","A Bidirectional MyoKinetic Implanted Interface for Natural Control of Artificial Limbs","Christian Cipriani","SCUOLA SUPERIORE DI STUDI UNIVERSITARI E DI PERFEZIONAMENTO S ANNA","MYKI aims at developing and clinically evaluating a dexterous hand prosthesis with tactile sensing which is naturally controlled and perceived by the amputee. This will be possible by overcoming the conventional approaches based on recording electrical signals from the peripheral nervous system (nerves or skeletal muscles) through the development of a radically new Human-Machine Interface (HMI) based on magnetic field principles, both able to decode voluntary motor commands and to convey sensory feedback to the individual. Core of this system is a multitude of magnets implanted in independent muscles and external magnetic readers/drivers (MRDs) able to (i) continuously localize the movements of the magnets and, at specific times, (ii) induce subtle movements in specific magnets. In fact, as a magnet is implanted it will travel with the muscle it is located in, and its localization will provide a direct measure of the contraction/elongation of that muscle, which is voluntarily controlled by the central nervous system. In this way it will be possible to decode the efferent signals sent by the brain by observing a by-product of the muscle fibres recruitment. On the other hand, a movement induced in the implanted magnet by the external MRD, could provide a perceivable stimulus, conveyed to the brain by means of the peripheral sensory receptors present in the muscle (e.g. muscle spindles or Golgi tendon organ) or in the neighbouring skin (tactile mechanoreceptors). In this way we aim to provide tactile and/or proprioceptive sensory information to the brain, thus restoring the physiological sensorimotor control loop. Remarkably, with passive magnetic tags (that do not require to be powered-on) and wearable readers/drivers, it will be possible to implement a wireless, bidirectional HMI with dramatically enhanced capabilities with respect to the state of the art interfaces, as illustrated in this proposal.","1475269","2016-09-01","2021-08-31"
"N-BNP","New directions in Bayesian Nonparametrics","Igor Pruenster","UNIVERSITA COMMERCIALE LUIGI BOCCONI","The popularity of Bayesian nonparametric (BNP) inference is rapidly growing within both the academic community and practitioners. Indeed the BNP viewpoint naturally allows for rich and flexible probabilistic modeling and, via conditional (or posterior) distributions, for accurate function estimation, most notably of probability distributions, regression functions and hazard rates. After de Finetti’s theoretical foundation of the BNP paradigm in the ‘30s, the first methodological breakthroughs in the ‘70s, and major theoretical and computational progress in the following 40 years, further significant developments of BNP are nowadays needed for providing successful answers to the practical challenges of the XXI century emerging from diverse applied fields.
Therefore, the main objective of the present research project is to introduce and investigate novel methodologies and procedures for BNP inference. The advances will include the development of new types of covariate-dependent random discrete distributions in contexts of partial exchangeability, the derivation of general classes of nonparametric estimators suitable for several prediction problems, the construction of various types of dynamic particle systems and associated diffusion approximations, the frequentist asymptotic validation of the most up-to-date Bayesian procedures. The theoretical investigation will be complemented by the implementation of the obtained results in a variety of applied contexts, among which nonparametric regression, meta-analysis, competing risks, macroeconomic dynamics, population processes with spatial immigration and time-varying mutation rate, credit markets with heterogeneous agents, biodiversity assessment and prediction in Ecology and Genomics. The modern probabilistic techniques needed to address challenging inferential issues explain the interplay between theory and applications which is a major headline of this project and represents one of the distinctive features of BNP.","957939","2012-10-01","2018-09-30"
"NAC","Nuclear Atomic Clock","Thorsten Schumm","TECHNISCHE UNIVERSITAET WIEN","""Atoms, as building blocks of nature, consist of an atomic nucleus and the electron shell. Both systems are governed by similar laws and forces. However, the required energies to create changes in the nucleus or the electron shell differ by many orders of magnitudes. This reflects in largely different tools and methods used for their investigation: atomic physics probes the electron shell mainly by means of lasers. Nuclear physicists create excitations at high energies using particle accelerators such as CERN.
The radio isotope 229Thorium is the only atom with the potential to bridge the gap between atomic and nuclear physics. It provides an unnaturally low-energy nuclear excited state, accessible to atomic physics tools, most notably laser excitation. It is the aim of the proposed research project to identify the  optical nuclear transition  and make it usable for fundamental investigations and applications.
Currently, our  second  is defined as 9.192.631.770 oscillations of a light wave that leads to a specific excitation in the electron shell of the Cesium atom. Using the nuclear excited state of 229Thorium instead would increase the time standard accuracy by many orders of magnitudes, at the same time reducing the experimental complexity considerably. Building such a  nuclear clock  is the main goal of the research proposal. This will directly lead to improved accuracy in satellite based navigation (GPS) and enhanced bandwidth in communication networks. Furthermore, vomparing a  nuclear atomic clock  to standard time standards will hence allow addressing one of the most fundamental questions in physics: """"are nature s constants really constant?"""".""","1245884","2010-12-01","2015-11-30"
"NAMASTE","Thermodynamics of the Climate System","Valerio Lucarini","UNIVERSITAET HAMBURG","The investigation of the global structural properties of the climate system (CS) plays a central role for the provision of a unifying picture of climate variability and climate change on a large variety of scales and is of outstanding importance for the quest for reliable metrics to be used in the validation of climate models (CMs). The CS can be seen as a complex, non-equilibrium system, transforming potential into mechanical energy as a thermal engine, generating entropy by irreversible processes, and keeping an approximate steady state by balancing the thermodynamic fluxes with the surrounding environment. We move from the thermodynamical perspective pioneered by Lorenz by means of theoretical studies, numerical simulations performed with hierarchies of CMs, ranging from minimal models to state-of-the-art coupled atmosphere-ocean models, and, where possible, observations. We take advantage of both the tools of the phenomenological theory of non-equilibrium thermodynamics and, from a more fundamental point of view, of the recent developments of non-equilibrium statistical mechanics, along the lines of the response theory developed by Ruelle. The main goals of this interdisciplinary project can be summarized as follows:
- Advances in the thermodynamic description of the CS and planetary bodies, re-analysis of the hydrological cycle and of the atmosphere- ocean interaction;
- Thermodynamic re-examination of mechanisms involved in past, present and future climate variability and change with CMs of various degrees of complexity;
- Definition and implementation of a new generation of diagnostic tools for auditing CMs and for analysing observative (satellite) data;
- Application of the response theory and Kramers-Kronig relations to CMs of various degrees of complexity for analysing generalized climate sensitivities and climate response to periodic forcings;
- Analysis of the impact of stochastic perturbations on the statistical properties of the forced and free fluctuations of simplified CMs;
- Study of the climatic tipping points and feedbacks by analysis of the resonances and divergence of the climatic response, and by investigation of the fluctuations of the large scale thermodynamical properties.","1393440","2010-09-01","2015-08-31"
"NaMic","Nanowire Atomic Force Microscopy for Real Time Imaging of Nanoscale Biological Processes","Georg Ernest Fantner","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Short summary:

The ability to measure structures with nanoscale resolution continues to transform physics, materials science and life science alike. Nevertheless,  while there are excellent tools to obtain detailed molecular-level static structure (for example in biology), there are very few tools to develop an understanding of how these structures change dynamically as they fulfill their biological function. New biologically-compatible, high-speed nanoscale characterization technologies are required to perform these measurements. In this project, we will develop a nanowire-based, high-speed atomic force microscope (NW-HS-AFM) capable of imaging the dynamics of molecular processes on living cells. We will use this instrument to study the dynamic pore-formation mechanisms of novel peptide antibiotics. This increase in performance over current AFMs will be achieved through the use of electron-beam-deposited nanogranular tunneling resistors on prefabricated nanowire AFM cantilevers. By combining these cantilevers with our state of the art high-speed AFM technology, we expect to obtain nanoscale-resolution images of protein pores on living cells at rates of tens of milliseconds per image. This capability will open a whole new arena for seeing nanoscale life in action.","1264640","2012-12-01","2017-11-30"
"Nano Harvest","Flexible nanowire devices for energy harvesting","Maria Tchernycheva","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The goal of NanoHarvest is to explore novel solutions for flexible photovoltaic and piezoelectric converters enabled by semiconductor nanowires. The first objective is to demonstrate an innovative concept of flexible solar cells based on free-standing polymer-embedded nanowires which can be applied to almost any supporting material such as plastic, metal foil or even fabrics. The second objective it to develop high-efficiency flexible and compact piezo-generators based on ordered arrays of nanowire heterostructures. The crucial ingredient - and also the common basis - of the two proposed research axes are the advanced nanowire heterostructures: we will develop nanowires with new control-by-design functionalities by engineering their structure at the nanoscale. The main focus of NanoHarvest will be on the III-nitride semiconductors, which are characterized by a strong piezoelectric response and have also demonstrated their ability for efficient photon harvesting in the blue and green parts of the solar spectrum. Our strategy is to address the physical mechanisms governing the energy conversion from the single nanowire level up to the macroscopic device level. The deep understanding gained at the nanoscale will guide the optimization of the device architecture, of the material growth and of the fabrication process. We will make use of Molecular Beam Epitaxy to achieve ultimate control over the nanowire morphology and composition and to produce control-by-design model systems for fundamental studies and for exploration of device physics. The original transfer procedure of the ordered nanowire arrays onto flexible substrates will enable lightweight flexible devices with ultimate performance, which will serve as energy harvesters for nomad applications.","1496571","2015-04-01","2020-03-31"
"NANO-ARCH","Assembly of Colloidal Nanocrystals into Unconventional Types of Nanocomposite Architectures with Advanced Properties","Liberato Manna","FONDAZIONE ISTITUTO ITALIANO DI TECNOLOGIA","Nanoscience promises innovative solutions in a large variety of sectors, ranging from cost-effective optoelectronic devices to energy generation, and to highly performing materials and interfaces. Realizing this promise will rely heavily on a bottom-up approach. This can only succeed if self assembly of advanced nanoscale building blocks will be developed intensively, to enable creation of useful macroscopic architectures. The unconventional assembly of nanocrystals towards functional materials is the area where this proposal aims at providing a key contribution. This will be achieved via ground-breaking advances in the fabrication of shape controlled nanocrystals, via solution approaches, in their organization following radically new concepts and in the study of their assembly related properties. The bottom line here is to tune the assembly process of nanocrystals so as to generate a desired functionality or a combination of functionalities. This would represent a dramatic leap forward from the trial-and-error approach to controlling the various properties that is currently prevalent in many of the communities working in the field of nanocrystals. The primary motivation of this proposal is therefore to correlate strongly the structural properties with the behaviour of nanostructured assemblies. This is clearly a cutting edge research program, at the frontier of chemistry, physics, materials science and engineering, and whose successful outcome will be of tremendous benefit in several fields.","1299960","2009-11-01","2013-10-31"
"NANO-GRAPHENE","Understanding the Electronic Properties of Carbon Nanotubes and Graphene as Quantum Conductors","Viorica Cristina Bena","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","In low-dimensional systems the strength of electronic interactions is enhanced, which can give rise to fascinating phenomena such as charge fractionalization, spin-charge separation and fractional or non-Abelian statistics. Furthermore, the effects of disorder and external factors (such as the substrate, the leads, magnetic fields, or the coupling with a gate or an STM tip), are much stronger in low-dimensional systems than in three-dimensional systems, and can greatly alter their properties. The first goal of this project is to find experimental signatures of the exotic phenomena caused by interactions, both in carbon nanotubes, and in regular and graphene fractional quantum Hall systems. The second goal is to understand how the interplay between disorder, interactions and external factors impacts the physics and the possible technological use of nanotubes and graphene in electronic nanodevices. To achieve these goals I intend to calculate theoretically quantities measurable by electronic transport, such as the conductance and the noise, in particular the noise at high-frequencies, as well as quantities measurable by scanning tunneling microscopy (STM), such as the local density of states (LDOS). Furthermore I intend to analyze and explain the recently developed STM experiments on graphene, and to propose new STM measurements that will elucidate the physics of graphene in the fractional quantum Hall regime. Some of the theoretical techniques that I plan to use are the perturbative non-equilibrium Keldysh formalism, conformal field theory and the Bethe ansatz, the T-matrix approximation, the Born approximation and numerical methods such as ab-initio and recursive Green's functions.","1041240","2011-05-01","2016-04-30"
"NANO-JETS","Next-generation polymer nanofibers: from electrified jets to hybrid optoelectronics","Dario Pisignano","UNIVERSITA DEL SALENTO","""This project ultimately targets the application of polymer nanofibers in new, cavity-free lasers. To this aim, it wants to tackle the still unsolved problems of the process of electrospinning in terms of product control by the parameters affecting the dynamics of electrified jets. The electrospinning is based on the uniaxial elongation of polymeric jets with sufficient molecular entanglements, in presence of an intense electric field. It is a unique approach to produce nanofibers with high throughput. However, the process is still largely suboptimal, the most of nanofiber production being still carried out on an empirical basis. Though operationally simple, electrospinning is indeed complex as the behavior of electrified jets depends on many experimental variables making fully predictive approaches still missing. This project aims to elucidating and engineering the still unclear working principles of electrospinning by solutions incorporating active materials, with a tight synergy among modeling, fast-imaging characterization of electrified jets, and process engineering. Once optimized, nanofibers will offer an effective, well-controllable and cheap material for building new, cavity-free random laser systems. These architectures will enable enhanced miniaturization and portability, and enormously reduced realization costs. Electrospun nanofibers will offer a unique combination of optical properties, tuneable topography and light scattering effectiveness, thus being an exceptional bench tool to realize such new low-cost lasers, which is the second project goal. The accomplishment of these ambitious but well-defined objectives will have a groundbreaking, interdisciplinary impact, from materials science to physics of fluid jets in strong elongational conditions, from process to device engineering. The project will set-up a new, internationally-leading laboratory on polymer processing, making a decisive contribution to the establishment of scientific independence.""","1491823","2013-03-01","2018-02-28"
"NANO-TEC","Nano-engineered high performance Thermoelectric Energy Conversion devices","Maria De La Soledad Martin-Gonzalez","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","Providing a sustainable supply of energy to the world s population will become a major societal problem for the 21st century. Thermoelectric materials, whose combination of thermal, electrical, and semiconducting properties, allows them to convert waste heat into electricity, are expected to play an increasingly important role in meeting the energy challenge of the future. Recent work on the theory of thermoelectric devices has led to the expectation that their performance could be enhanced if the diameter of the wires could be reduced to a point where quantum confinement effects increase charge-carrier mobility (thereby increasing the Seebeck coefficient) and reduce thermal conductivity. The predicted net effect of reducing diameters to the order of tens of nanometres would be to increase its efficiency or ZT index by a factor of 3. The objective of this five year proposal is to investigate and optimise the fabrication parameters influencing ZT in order to achieve a power conversion efficiency of &gt;20%. For that, low dimensional nanowires arrays of state of art n and p-type materials will be prepared by cost-effective mass-production electrochemical methods. In order to obtained devices with a ZT &gt;2 for application in energy scavenging and as cooler/heating devices, three approaches will be followed: a) determination of the best materials for each temperature range (n and p type) optimizing composition, microstructure, shapes (core/shell, nanowire surface texture, heterostructures), interfaces and orientations, b) advanced characterization, device development and modeling will be used iteratively during nanostructures and materials optimization, and c) nano-engineering less conventional thermoelectric like cage compounds by electrodeposition methods. This proposal aims to generate a cutting edge project in the thermoelectric field and, if successful, a more efficient way to harness precious, but nowadays wasted energy.","1228000","2010-03-01","2016-02-29"
"Nano@Energy","Novel Design of Nanostructures for Renewable Energy:
Fundamental Questions and Advanced Applications","Taleb Mokari","BEN-GURION UNIVERSITY OF THE NEGEV","Photovoltaics and liquid fuels are poised as major contributors to the global energy market, promising cleaner, renewable sources of energy than fossil fuels. However, the technologies required to make this possibility a reality are limited by their high cost per kWh, and current share of photovoltaics and liquid fuels in the energy market is thus extremely small. One method of reducing the costs of photovoltaics lies in the use of semiconductor nanocrystals to absorb and convert solar photon energy to usable electricity and liquid fuel. Among the advantages of a nanocrystal-based design for photovoltaics are the requirement for thinner absorbing layers, the less energy-intensive refining processes, and their scalability with respect to photovoltaic production.
To address these challenges, I plan to initiate a multidisciplinary research project that comprises three separate, but interrelated and complementary, parts that will be conducted in parallel. The first and the main part will be the preparation of novel hybrid nanostructures that have potential for PV and fuel cells applications. The second will focus on a systematic study of the fundamental processes of charge dynamics in the nanoscale regime. The materials and knowledge generated can then be applied in the third part of the project—development of PV and photoelectrochemical devices with scale-up potential for large-scale solar energy exploitation, and examination of benchmark properties (overall efficiency, I V characteristics, external quantum efficiency, hydrogen and liquid fuel production) of our new hybrid materials and devices. These properties will be used as feedback for the synthesis of more complex hybrid structures and for improving our device assembly methods and the choice of materials and/or composites for the devices.","1500000","2011-10-01","2016-09-30"
"NanoBeam","Quantum Coherent Control: Self–Interference of Electron Beams with Nanostructures","Nahid TALEBI SARVARI","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","NanoBeam will develop new directions in electron microscopy, materials, and optical sciences to control and characterize the ultrafast responses of polaritons and electronic states in materials. This will be achieved by (i) a ubiquitous control of slow and fast electron wave packets and (ii) realization of fully coherent light sources using shaped electron wave packets interacting with nanostructures. Quantum coherent control traditionally employs a sequence of optical pulses to direct the response of condensed matter systems towards a desired state, as a tool for novel quantum technologies. This control system has been only recently implemented in electron microscopes, by combining lasers and photoemission electron guns. However, this field is still it its infancy because it does not provide us with important aspects of the sample response such as spectral phase and time-energy evolution of electronic states in samples, which happens at the attosecond time scale. 
NanoBeam aims at quantum coherent control within electron microscopes by triggering both electron wave packets and their mechanisms of radiation, using carefully engineered nanostructures. This innovative and unconventional control system is to be achieved by an unprecedented combination of theory and experiment. On the theoretical side, I plan to develop a Maxwell-Schrödinger self-consistent numerical toolbox, to fully understand the interaction of electron wave packets with light and nanostructures beyond the routinely used adiabatic approximations, but also to utilize our expertise in theoretical modelling to propose novel methodologies for coherent control and shaping of the electron beams. On the experimental side, I intend to develop a novel spectral interferometry technique with the ability to retrieve and control the spectral phase in a scanning electron microscope to overcome the challenges in meeting both nanometer spatial and attosecond time resolution.","1499125","2019-02-01","2024-01-31"
"NANOBRAIN","On-chip memristive artificial nano-synapses and neural networks","Julie Grollier","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","These last fifty years have seen Von Neumann computing architectures boom. Nevertheless, even the most powerful digital computers cannot rapidly solve apparently simple problems such as image interpretation. However, because its structure is
massively parallel and analog, the human brain is able to perform such tasks in a fraction of second. Neuromorphic circuits allow to go beyond conventional digital architectures. An on-chip implementation of these circuits requires to be able to fabricate nanometer sized, analog, reconfigurable, fast components. While the spiking neurons can easily be fabricated with classical CMOS technology, the synapse plasticity is challenging to achieve. In 1971 L. Chua has introduced a new circuit element, called memristor , a non-linear resistance which by definition includes a memory effect. Only last year, a team in Hewlett-Packard has for the first time proposed a device for synaptic applications showing memristive properties based on electromigration of oxygen vacancies in Titanium Oxide. The project NanoBrain aims first at developing alternative memristors based on different physical principles (spintronics and ferroelectricity), avoiding in particular the potential over-heating and fragility of the electromigration-based devices. The final goal of the project is to prove the efficiency of these new nano-synapses by integrating them into functional neural networks.","1495803","2010-11-01","2015-10-31"
"NANOCAT","Catalysis at the Nanoscale","Johannes Albertus Antonius Wilhemus Elemans","STICHTING KATHOLIEKE UNIVERSITEIT","Is it possible to really 'see' individual molecules in action as they are involved in a chemical reaction at a surface? And can we, in this way, get a complete understanding of reaction mechanisms, at the resolution of atoms? The importance of studying chemical reactions at surfaces has recently been highlighted by Gerhard Ertl being awarded the Nobel Prize in chemistry in 2007, for elucidating mechanisms of chemical processes on heterogeneous catalysts at the single molecule level with Scanning Tunneling Microscopy (STM). Although ground-breaking, these studies were carried out in ultra-high vacuum, which is, however, an unrealistic condition for conventional chemical or biological reactions which usually occur in a liquid medium. The aim of this ERC proposal is to establish a research area at the interface of chemistry and physics which has so far been nearly completely unexplored: the investigation of chemical reactions at solid-liquid interfaces at the highest detail possible, by visualizing molecules with STM while they are involved in a reaction. By doing so, unique information about reaction mechanisms can be obtained by looking at individual molecules, instead of ensembles where the behaviour of many molecules is averaged.
Towards this goal I propose to use a newly developed catalysis-STM setup, which is equipped with a liquid-cell and a bell-jar, and in which the conditions that are commonly applied in chemical laboratory processes (e.g. addition and withdrawal of chemicals, working under different atmospheres) can be closely resembled. In this setup I intend to carry out chemical reactions at a surface and monitor the behaviour of individual adsorbed catalysts, while they are in action. More specifically, it is my aim to investigate in detail the relation between structure and reactivity at the nanoscale","1500000","2010-09-01","2015-08-31"
"NanoCellActivity","Nanoscale live-cell activity sensing using smart probes and imaging","Peter Robert L. DEDECKER","KATHOLIEKE UNIVERSITEIT LEUVEN","Fluorescence microscopy is the tool of choice for live-cell imaging. Its usefulness has been further enhanced by the availability of genetically-encoded biosensors, which enable the visualisation of when and where a certain activity arises. In addition, the development of diffraction-unlimited imaging has dramatically improved the spatial resolution of fluorescence imaging. However, these techniques have had difficulty working with biosensors, largely limiting the information to the spatial location of the labels. 
This project seeks to develop diffraction-unlimited imaging of biosensors, creating activity maps with a diffraction-unlimited spatial resolution in living systems. I propose to meet this challenge using a two-pronged approach, focusing both on the development of labels and sensors as well as new imaging tools and strategies. Based on existing scaffolds, we will develop sensors that display strong photochromism, providing reversible fluorescence dynamics intrinsically suited to superresolution imaging. In tandem, we will develop imaging strategies that focus on robustness and work well in living systems, in exchange for a spatial resolution of a 50 to 70 nm and a temporal resolution of a few seconds or less. 
We will use these developments in the study of the nanoscale spatiotemporal regulation of G-protein-coupled receptor (GCPR) signalling in living systems. By extending sub-diffraction imaging to the molecular environment, this project will contribute new insights into long-standing research questions that directly involve the health and well-being of all of us, while also enabling exciting prospects for further research.","1368250","2017-02-01","2022-01-31"
"NanoChemBioVision","Next Generation Label-free Chemical Nanoscopy for Biomedical Applications","Sumeet Mahajan","UNIVERSITY OF SOUTHAMPTON","Imagine if one could simply use an optical microscope and see whether a particular virus has infected a biological specimen or not! Or if a single disease causing molecular structure could be detected, 20 years before the disease manifests itself! Conventional microscopy simply does not have such spatial resolution! The challenge is to image endogenous molecules and structures composed of them specifically, in real-time, without tampering and sample destruction. Non-Linear optical techniques such as vibrational sum frequency generation (vSFG) and coherent Raman scattering (CRS), which use the intrinsic properties of molecules for selectively imaging them, provide a solution. They are non-invasive, label-free, chemically selective and non-destructive with capability for video-rate imaging of biomolecules and biochemical structures. However, they need to overcome the frontier of spatial resolution to be able to provide information at <100 nm level, which is much below the limit for these techniques and conventional microscopy. The proposal addresses this challenge by developing and implementing a generic, simple optical ultra-high resolution technology using a novel approach based on super-oscillatory modulation of light coupled with wavelength mixing. We will uniquely apply this approach to the chemically selective vSFG and CRS techniques. Ultra-high spatial resolution with these techniques will allow unprecedented new insight into many biochemical phenomena. To demonstrate the utility of ‘chemical nanoscopy’ developed in this proposal vesicular transport in axons of neurons will be studied, which is highly relevant to cognitive decline observed in ageing and neurodegenerative disorders. The project outcomes have the potential to revolutionize research and biomedical understanding by opening doors to ‘unseen biology’, unravelling disease, viral infection and allergy mechanisms and ultimately, yielding better diagnostics and therapeutics.","1916076","2015-04-01","2020-03-31"
"NANOCOMP","Complex Dynamics of Clusters in High-Aspect Ratio Hollow Nanostructures:A Nanoscale Platform for High-Performance Computing","Maria del Carmen Gimenez Lopez","UNIVERSIDAD DE SANTIAGO DE COMPOSTELA","Practical aspects and understanding of frontier-computing concepts such as memcomputing (a brain-inspired computational paradigm), quantum computing and spintronics are hindered because of the lack of suitable nanostructured materials. The NANOCOMP project aims to develop a technology for the integration of nano-switches within the confined space of high-aspect ratio hollow carbon nanostructures, yielding a totally new class of hybrid metal-carbon nanomaterials with different dimensionality as model systems enabling the realisation of these computing schemes. This research will also pave the way for developing new energy-storage concepts. The main objectives are: 1) To develop protocols for successful transport and encapsulation of intact nano-switches within tubular carbon nanostructures (TCN); 2) To understand and control the effects of the confined nano-switches on the carbon nanocontainer (and vice versa); 3) To unravel and develop new methodologies for exploiting the functional properties of the confined nano-switches; 4) To fabricate nanodevices, novel 2D ordered arrays and highly-porous 3D networks for a variety of applications ranging from quantum processors to flexible spintronic devices and supercapacitors.","1689554","2016-04-01","2022-01-31"
"NANOCONTACTS","Structural and electronic properties of nanoscale metallic contacts fabricated by thermally assisted electromigration","Regina Hoffmann-Vogel","KARLSRUHER INSTITUT FUER TECHNOLOGIE","The key aim of the project is to correlate the electronic transport properties of nanoscale metallic contacts with their structure. The electronic transport properties through a metallic contact of atomic dimensions are governed by the atomic structure and by the chemical properties of the contact as well as by the wave nature of electrons. This leads to plateaus of the conductance measured as a function of contact size that do not necessarily correspond to integer multiples of the conductance quantum. I will investigate whether and how atomic as well as electronic shell effects influence the atomic structure of nanoscale metallic contacts. We will measure both electronic transport properties and structural properties concurrently and determine their mutual relation on each individual contact. The contacts will be fabricated by Joule heating a nanowire until thermally assisted electromigration sets in and thins the nanowire to form a contact. The structural properties of these nanocontacts will be studied using scanning force microscopy and scanning tunneling microscopy with atomic resolution in ultrahigh-vacuum. This approach will allow us to use clean superconducting contacts and to exploit superconductivity in order to study the electronic transport properties of the contacts. The electronic transport properties will be studied employing multiple Andreev reflections to determine the number and transmission coefficient of electronic conduction channels. Eventually, a deeper understanding of the relation between structure and electronic transport properties will be obtained which is a prerequisite to tailor the electronic transport properties of nanoscale metallic contacts.","1513000","2010-01-01","2015-12-31"
"NANOENABLEDPV","Novel Photovoltaics Enabled by Nanoscience","Erik Christian Garnett","STICHTING NEDERLANDSE WETENSCHAPPELIJK ONDERZOEK INSTITUTEN","The “NanoEnabledPV” research program will exploit the fundamental benefits of nanomaterials and address their challenges to make low-cost solar cells a reality. NanoEnabledPV contains three focus areas necessary to reach our goal:

1)	“Nano surface doping” – surface-controlled nanomaterial properties.  We will explore using charged surface oxides and surface ligands with dipole moments as a novel doping mechanism.  We will make the first nanowire solar cell using a surface “p-n” junction.  The lessons learned from single nanowire studies will be extended to make large-scale, high efficiency metal-insulator-semiconductor solar cells.
2)	“Solar highways” – metal nanowire core-semiconductor shell photovoltaics.  We will examine the optical and electrical properties of silver and copper nanowires coated with various semiconductor shells for the first time.  This novel device structure can achieve complete absorption using 10 times thinner semiconductor layers compared to standard thin-film structures and also enables facile charge extraction via the metal core.
3)	“Nanophotography” – hierarchical synthesis and assembly based on optical resonances in nanostructures.  We will develop a new type of mask-free photolithography in solution with resolution far below the diffraction limit.  This will enable rational, large-scale synthesis of ordered hierarchical structures that can be assembled into complex 3-D networks.

Together, these programs that sit at the intersection of physics, chemistry, materials science and engineering will provide the active light-absorbing materials needed for next generation solar energy conversion schemes, a deep understanding of how they work at the nanoscale and methods for integrating them into macroscale devices.  We are requesting 1.5 Million Euros over a period of 5 years that will be used to hire 2 PhD students, 2 postdoctoral researchers and buy the equipment needed to build a unique nanowire solar cell fabrication and analysis lab.","1499310","2013-08-01","2018-07-31"
"NanoFab2D","Novel 2D quantum device concepts enabled by sub-nanometre precision nanofabrication","Levente Tapaszto","MAGYAR TUDOMANYOS AKADEMIA ENERGIATUDOMANYI KUTATOKOZPONT","In today’s electronics, the information storage and processing are performed by independent technologies. The information-processing is based on semiconductor (silicon) devices, while non-volatile data storage relies on ferromagnetic metals. Integrating these tasks on a single chip and within the same material technology would enable disruptively new device concepts opening the way towards ultra-high speed electronic circuits. Due to the unique versatility of its electronic and magnetic properties, graphene has a strong potential as a platform for the implementation of such devices. By engineering their structure at the atomic level, graphene nanostructures of metallic, semiconducting, as well as magnetic properties can be realized. Here we propose that the unmatched precision and full edge orientation control of our STM-based nanofabrication technique enables the reliable implementation of such graphene nanostructures, as well as their complex, functional networks. In particular, we propose to experimentally demonstrate the feasibility of (1) semiconductor graphene nanostructures based on the quantum confinement effect, (2) spin-based devices from graphene nanostructures with magnetic edges, as well as (3) novel operation principles based on the interplay of the electronic and spin-degrees of freedom. We propose to demonstrate the electrical control of magnetism in graphene nanostructures, as well as a novel switching mechanism for graphene field effect transistors induced by the transition between two magnetic edge configurations. Exploiting such novel operation mechanisms in graphene nanostructure engineered at the atomic scale is expected to lay the foundations of disruptively new device concepts combining electronic and spin-based mechanisms that can overcome some of the fundamental limitations of today’s electronics.","1496500","2016-07-01","2021-06-30"
"NANOFIB","Nano fibrous materials - structure, design and application","Christian Clasen","KATHOLIEKE UNIVERSITEIT LEUVEN","The performance and physical attributes of a material and product can be tailored to so far unmatched material strengths and properties by creating new nano fibrous structures from polymers by electrospinning. The electrospinning process uses an electric field to produce charged jets of polymer solutions or melts. Bending instabilities of the jet, caused by the surface charge, lead to extremely high local extension rates of the jet and produce fibres with diameters of the order of a few nanometer that consist of highly aligned polymer strands. However, the biggest unsolved problem of the electrospinning process is the sensitive equilibrium between surface tension, viscosity, elasticity and conductivity of the polymer solutions. These are controlled by molecular parameters as the molar mass, chemical microstructure, conformation in solution or supramolecular structures via intermolecular interactions. The optimal combination of these parameters is, as yet, unknown. Within this project, a novel and unique technical platform will be developed and installed, that is generally capable to image and analyse high speed free surface flows in miniaturised dimensions. This platform will then be utilized to analyse electrospinning process parameters and to connect them to the material properties and the molecular structure of the polymer solution. Only such a fundamental understanding of the relation of these properties to the flow and mass transfer phenomena on the micro-time and -dimensional scale will allow to design in the second part of this project the required structural and material properties of nano-scale fibres for: -novel fibre/matrix composites for the creation of ultra-high-strength hydrogel membranes; -short fibre morphologies created by a novel controlled disruptive spinning process at the boundaries of the parameter space; -tailoring of fibre properties from renewable resources by modification of the chemical side-chain structure of polysaccharides.","1228736","2008-07-01","2013-06-30"
"NanoFLP","Nanoparticles as Partners in Frustrated Lewis Pairs: Boosting the Surface Reactivity of Inorganic Nanoparticles","Sophie CARENCO","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Associating bulky and strong Lewis acid and base creates a Frustrated Lewis Pair (FLP). Traditionally, both FLP partners are molecules. Molecular FLPs have shown excellent abilities to catch and dissociate small molecules such as H2 in a heterolytic way, under mild conditions. The driving force is the destabilization of the initial acid-base adduct, sterically frustrated: it liberates a reactive pocket that catches the small molecule guest, and strongly lowers the activation energy for bond dissociation. 
The pristine and challenging concept of NanoFLP consists in replacing one of the molecular FLP partner, either the acid or the base, by an inorganic nanoparticle: the other molecular partner will adsorb on the surface and boosts the reactivity of the nanoparticle by creating a frustrated active site. 
I will demonstrate the versatility of NanoFLPs with three families of inorganic nanoparticles (metals, acidic oxides, basic oxides), illustrating the two schemes: nanoparticle is either the Lewis acid or the Lewis base. I will use probe molecules (CO2, H2, SO2 and N2O) to investigate the nature and reactivity of the active sites. All reactions should be achieved under much milder conditions (rt.-150 °C, 1-3 bars) than those required using similar nanoparticles in the absence of the molecular partner. 
I will fully describe the nanoparticle surface and the dynamics of the molecular partner using benchtop and synchrotron spectroscopies with in situ cells: infrared, nuclear magnetic resonance in solution, X-ray absorption and near-ambient-pressure X-ray photoelectron spectroscopy. 
In the last stage of the project, I will take advantage of the several active sites that one nanoparticle can bear to achieve combined reactions of two small molecules (reactants) on a single NanoFLP. 
NanoFLP proposes a new type of active site for utilizing small molecules as sources of C, N, S and O. It will open an avenue in the design of reactive interfaces, eg. for catalysis and sensors.","1499875","2018-01-01","2022-12-31"
"NANOFORCELLS","Development of a nanomechanical tool-box for the investigation of cell mechanics","Montserrat Calleja Gomez","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","The fact that biophysical and biomechanical properties of cells and subcellular structure influence and are influenced by onset and progression of human diseases is now attracting the physiologists attention. This opens up new routes for disease diagnosis and treatment. The first traditional way to discover a tumor was by palpation, such as sweetness in urine was indicative of diabetes. Nowadays, the biochemical search for diagnosis markers has experienced amazing advancements, while the physical cues have remained almost forgotten; this is mainly due to a lack of powerful tools able to unravel the mechanics of individual cells. This proposal aims to develop a set of tools and demonstrate their potential  for the throughout study of individual cell mechanics and sub-cellular structures.  The proposed tools will provide the route to develop mechanical and physical assays to extract the elastic and viscoelastic deformability of f.e. cancer cells as compared to healthy cells, providing with mechanical biomarkers for diagnosis.  NANOFORCELLS proposes an innovative approach combining optical interferometry together with advanced nanomechanical systems and AFM local characterization. This mechanical lab for cells will provide new knowledge that can not be attained today with present technologies. Also, the proposed nanomechanical devices of this project will provide not only a direct measurement for single cell rigidity but the capability for parallel measurement of hundreds of individual cells per minute, opening the route for portable tests that could prove very general to determine the health status of cells from blood samples. Also, interaction of cell with the environment can be studied in real time and thus the devices will provide tools for the study of drug effects and drug delivery vehicles, or for the assessment of the toxicity of nanoparticles.","1492854","2011-11-01","2016-10-31"
"NANOGEN","Polymer-based piezoelectric nanogenerators for energy harvesting","Sohini Kar-Narayan","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Energy harvesting (EH) from ambient vibrations originating from sources such as moving parts of machines, fluid flow and even body movement, has enormous potential for small-power applications such as wireless sensors, flexible, portable and wearable electronics, and bio-medical implants, to name a few. Nanoscale piezoelectric energy harvesters, also known as nanogenerators (NGs), can directly convert small scale ambient vibrations into electrical energy. Scavenging power from ubiquitous vibrations in this way offers an attractive route to supersede fixed power sources such as batteries that need replacing/recharging, and that do not scale with the diminishing size of modern electronics. This proposal aims to develop NGs for future self-powered smart devices. Ceramics such as lead zirconium titanate and semiconductors such as zinc oxide are the most widely used piezoelectric EH materials. This proposal however focuses on a different class of piezoelectric materials, namely ferroelectric polymers, such as polyvinlyidene fluoride (PVDF), its copolymers, and nylon. These are potentially superior EH materials as they are flexible, robust, lightweight, easy and cheap to fabricate, as well as being lead-free and bio-compatible. The key strategy of this proposal is in combining i) materials engineering to create novel piezoelectric polymer-ceramic nanocomposite materials with enhanced EH functionalities, ii) state-of-the art nanoscale characterization to explore and exploit these novel materials, and iii) fabrication of high performance NGs for implementation into commercial devices, using insight gained from modelling of materials and device parameters. The proposed research will culminate in a well-defined process for the large-scale production of highly efficient and low cost piezoelectric NGs with reliable EH performance to power the next generation of autonomous devices, thus steering the field into the renewable energy market as a clean and competitive technology.","1635710","2015-04-01","2020-03-31"
"NANOGRAPHOUT","Design, synthesis, study and applications of distorted nanographenes","María Araceli González Campaña","UNIVERSIDAD DE GRANADA","Graphene is considered a very promising material. Perfect samples of graphene without structural defects are extremely electrical and thermal conductive. However, defects usually appear during the production of graphene, modifying its thermal, electrical and mechanical properties. If we understand the influence of imperfections on the properties of graphene, we may tune its local electrical properties by controlling the presence of defects, leading to new organic semiconductor materials. We aim to embed seven- and higher membered rings into an otherwise planar NANOGRAPHene lattice as a new tool for the preparation of innovative materials for organic electronics. These defects would induce a curvature in the planar sheet distorting the structure OUT of the plane. NANOGRAPHOUT focuses on providing a general synthetic method for the construction of a variety of distorted nanographenes with good control on size, shape and the edges of the final compounds. Key synthetic steps include alkyne cyclotrimerization and cyclodehydrogenation reactions. By evaluating the morphology, optical and electronic properties and electron transport of synthesized nanographenes, we aim to establish the first comprehensive study clarifying the influence of defects on the properties of nanographenes. We will test electrical transport properties of selected compounds in organic thin-film field-effect transistors (OTFTs) laying the foundation for using distorted nanographenes as organic semiconductors based on pi-pi interactions. With the same bottom-up approach based on organic synthesis we intend to present nanographenes with helical chirality. Adding chiroptical response to the semiconductor properties of nanographenes will provide the new devices the added value of their potential application in photonics. As proof-of-concept, we plan to implement helically chiral distorted nanographenes as active layer in OTFTs and evaluate their use as elliptically polarized light emitters and detectors.","1492675","2016-04-01","2021-03-31"
"NANOINFER","Intelligent Memories that Perform Inference with the Physics of Nanodevices","Damien QUERLIOZ","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Cognitive tasks are increasingly necessary in modern electronics. The energy efficiency of associated algorithms, which rely on abundant stored parameters, is severely limited by the separation of computation and memory elements in conventional computers. In NANOINFER, I will directly address this challenge by developing intelligent memory chips that natively perform both memory and computing functions, using CMOS and emerging nanodevices. These chips will perform modern Bayesian inference algorithms, which allow cognitive-type reasoning. The project includes theoretical investigations as well as intelligent memory chip designs, which will be supported by proof-of-concept experimental demonstrations. The proposed architectures, based on spintronic and memristive memories, will maximize energy efficiency by leveraging the complex physics of these emerging devices for inference operations and the storage of model parameters, and by minimizing exchanges between computation units and memory. Inference will be performed using sampling algorithms that allow tackling difficult problems and are robust to nanodevice imperfections. The inference circuits will be composed of digital CMOS logic as well as spiking neurons circuits. Two standard Bayesian approaches will be employed to enable learning, permitting highly adaptive systems. Preliminary results on a system that performs naïve Bayesian inference have validated this concept and its use with novel memory technologies. NANOINFER will resolve critical interdisciplinary challenges to permit intelligent memories to perform non-naïve tasks, ensuring a correspondence between device physics and Bayesian concepts while maintaining a fusion between computation and memory. This project will deepen our understanding of novel memory technologies and develop a toolbox for creating intelligent memory chips. These will allow smart devices to perform cognitive/sensory-motor tasks at low energy without requiring large computing machines.","1499609","2017-03-01","2022-02-28"
"NANOMOL","From Nano Test Tube to Nano Reactor: Visualisation, Manipulation and Synthesis of Molecules at Nanoscale","Andrey Nikolaevich Khlobystov","THE UNIVERSITY OF NOTTINGHAM","High aspect ratio (quasi-1D) nanostructures have potential to revolutionise the way we use, make and study molecules. This ambitious project is designed to enable characterisation and manipulation of molecules at a single-molecule level, visualisation of mechanisms of chemical reactions in real space and time, and synthesis of molecules within nano-sized containers. Understanding interactions of molecules with nanostructures of different types (nanofibres, nanotubes) and different chemical composition (carbon, bron nitride, titanium dioxide) forms a fundamental core of this project, as the 1D nanomaterials will serve as structural and functional bridges between the molecular world and the macro world. This project opens up new broad horizon for molecular disciplines, such as organic chemistry, molecular physics and the science of nanomaterials. Molecules possessing optical (polyaromatic hydrocarbons, complexes of transition metals and lanthanides), magnetic (single-molecule magnets, free radicals) or redox (metallocenes, molecular wires, tetrathiafulvalene) properties wired to 1D nanostructures will be delivered for next generation of electronic devices, harnessing functional properties of individual molecules for a variety of applications ranging from ultrasensors to quantum information processors. This project will help to establish a precise control of geometries and orientations of extended molecular arrays urgently needed for nano-device applications. Understanding of how molecules interact with 1D nanostructures and how they react with each other when confined within nano-reactors will give a new powerful set of tools to control the direction, selectivity and kinetics of chemical reactions. Methodology of molecular confinement at the nanoscale developed in this project will offer new opportunities for preparative synthetic chemistry of the XXI century leading to high-value isomerically and enantiomerically pure products that cannot be synthesised otherwise.","1446108","2011-12-01","2016-11-30"
"NANOP","Nanoporous Membranes for High Throughput Rare Event Bio-analysis","Joshua Edel","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","A novel analytical platform is proposed to detect and identify DNA at low concentration in a high throughput manner at the single molecule level.

The potential impact of this research is significant and will result in single molecule detection becoming a mainstream tool within the medical diagnostics and analytical communities. 'Rare event' detection plays an important role in the early detection of illnesses and disease (e.g. cancers and bacterial infections). Using analytical technologies that exist today it is almost impossible to detect a single DNA strand within a standard blood sample (of a few mLs) within a reasonable time frame. The technology that will be developed within the current project will allow for such detection to be performed both rapidly and efficiently. If successful, the core technology described will become a mainstream analytical tool that will be of significant benefit within biomedical laboratories, hospitals, and clinics around the world.

Specifically, chemical and semiconductor processing methods will be developed to define a novel approach to high throughput DNA quantification at the single molecule level. This innovative technology will function by introducing biological samples in micro- and nanofluidic chips and using electric fields to direct DNA strands through nanometre-sized pores on a membrane. Detection and sizing of the individual DNA strands (labelled with fluorophores) is then accomplished using confocal fluorescence spectroscopy.

This new approach to high-throughput, single molecule DNA analysis harnesses the strengths of both analytical spectroscopy and silicon fabrication technology to allow the creation of hybrid devices in which molecular quantification can be realized. I expect this work to have major impact and open up new possibilities for nano-analytical tools in the chemical and biological sciences.","1497620","2012-01-01","2017-09-30"
"NanoPacks","NanoPacks: Assembling nanoparticles via evaporation-driven droplet collapse for ultrasensitive detection techniques","Alvaro Marin","UNIVERSITEIT TWENTE","The foundation of nanophotonics and nanoplasmonics has boosted the development of ultrasensitive detection techniques. Some of these techniques, such as Surface Enhanced Raman Spectroscopy or Surface Enhanced Fluorescence, are able to detect femtomolar concentrations of analytes or even single molecules, only relying on the adsorption of the analytes on a nanostructured surfaces. 

The development of nanotechnology requires a high control on the building blocks of the structures. The concept of self-assembly has been introduced and successfully applied in recent years to build all sorts of nanostructures. However, self-assembly generally involves an attractive interaction of the elements which requires the use of specially designed nanoparticles, thus imposing severe limitations in the applicability of self-assembly. 

The approach I want to explore in this project is a complete change of paradigm which consists on assembling nanostructures through the collapse of evaporating drops: A droplet, containing both metallic nanoparticles and a tiny amount of analyte molecules, evaporates until the whole solvent vanishes and only the solutes are left. By manipulating the way the droplet evaporates, we can control the shape and properties of the remains, and therefore assemble metallic nanoparticles together with the molecules of interest in a passive way.
 
The project will increase the reach of plasmonic-based techniques for the early detection of diseases: First, the approach does not rely on expensive fabrication techniques, but only on the thermodynamics and the statistical physics of the particle packings. Secondly, by using a physical approach to form nanoparticle and analyte aggregates, we avoid adverse interactions with the analyte’s chemistry. 

The packing of metallic nanoparticles presents new challenges and brings several scientific questions that I will address experimentally through microfluidics, but also via simulations and modeling.","1500000","2016-08-01","2021-07-31"
"NanoPhennec","Nanophononic devices: from phonon networks to phonon CQED","Norberto Daniel LANZILLOTTI KIMURA","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Phonons (quanta of vibration) play a major role in many of the physical properties of condensed matter. One of the most striking features of acoustic phonons is their ability to interact with virtually any other excitation in solids. Recent progress in the design, fabrication and control of nanomechanical systems has paved the way to explore new frontiers in the classical and quantum worlds. Devices based on semiconductor quantum dots (QDs) have been recently demonstrated to perform as near-ideal single photon sources, a very promising platform for developing a solid-state quantum network. The phonon engineering, however, remains an unexplored knob in the quantum information toolbox.

The goal of this project is to explore new horizons in nanophononics by developing novel phononic networks with full control on the phonon dynamics, and unprecedented structures capable of acoustically interact with single QDs, bridging the gap between nanophononics and semiconductor QD quantum optics. 

AlGaAs based semiconductor cavities are capable of confining simultaneously photons and phonons. The building blocks of the proposed research are semiconductor pillar microcavities and single QDs deterministically positioned to maximize their interaction with the confined electromagnetic and elastic fields.  To achieve our main goal we set three major objectives: 1) To develop novel one- and three-dimensional optophononic resonators and develop appropriate phononic measuring techniques; 2) To engineer nanophononic networks working in the tens-of-GHz range; and 3) To demonstrate first phonon cavity quantum electrodynamics phenomena for a single artificial atom coupled to a phononic cavity. Shaping the phononic environment opens exciting perspectives for solid state quantum applications, by providing a full control over the main source of decoherence and actually using it as a powerful resource to eventually transfer the quantum information.","1499375","2017-02-01","2022-01-31"
"NANOPHOM","Nanophosphor-based photonic materials for next generation light-emitting devices","Gabriel LOZANO","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","Energy-efficient and environmentally friendly light sources are an essential part of the global strategy to reduce the worldwide electricity consumption. Light-emitting diodes (LEDs) emerge as a key alternative to conventional lighting, due to their high power-conversion efficiency, long lifetime, fast switching, robustness, and compact size. Nonetheless, their implementation in the consumer electronic industry is hampered by the limited control over brightness, colour quality and directionality of LED emission that conventional optical elements relying on geometrical optics provide.
This project exploits new ways of controlling the emission characteristics of nanophosphors, surpassing the limits imposed by conventional optics, through the use of exciting nanophotonic concepts - an approach that has not been explored so far due to the strong multiple light-scattering that standard micrometre-sized phosphors present. The development of reliable and scalable nanophosphor-based photonic materials will allow ultimate spectral and angular control over the light emission properties, addressing the critical shortcomings of current LEDs. The new optical design of these devices will be based on multilayers, surface textures and nano-scatterers of controlled composition, size and shape, to attain large-area materials possessing photonic properties that will enable a precise management of the visible radiation. To prove and on-demand control over the colour appearance and the angular emission pattern of emitting devices, the project will culminate in an experimental demonstration of two paradigmatic cases: i) directional white-light emission within a narrow angular cone; ii) omnidirectional emission of monochromatic light.
Nanophom will significantly advance our comprehension of fundamental phenomena like the formation of photonic modes in complex optical media to which light can couple, as well as advancing the state of the art of high-efficiency solid-state lighting devices.","1499739","2017-04-01","2022-03-31"
"NANOPOTS","Nanotube Based Polymer Optoelectronics","Andrea Carlo Ferrari","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The target of this project is to develop a new class of polymer based optoelectronic devices embedding the optical and electronic functionalities of carbon nanotubes (CNTs). These devices will combine the fabrication advantages of polymer photonics, with the tunable active and passive optical properties of CNTs. This is an ambitious frontier research program, with a strong interdisciplinary nature, across engineering, physical, chemical and soft matter sciences. The ERC grant will consolidate the newly funded Research Group lead by the PI at the newly built centre for Advanced Photonics and Electronics of the University of Cambridge. CNTs will be grown by chemical vapour deposition at low temperatures, compatible with polymer processing. Direct deposition of CNT on optical components (such as fibres and mirrors) will be studied. Fundamental understanding of ultra-fast non-linear optics will be sought by a combination of theory and experiments. A range of novel photonic polymers incorporating CNTs will be produced: index matching gels, optical adhesives and silicones. These new materials, incorporating the optical functionality of CNTs, will be used to build a variety of photonic devices. Nanowires are also promising for photonic applications, since they exhibit a size-tunable absorption resonance at telecommunications wavelengths, and their use will also be considered.","1799964","2008-10-01","2013-09-30"
"NANOPUZZLE","Multifunctional Magnetic Nanoparticles: Towards Smart Drugs Design","Jesús Martínez De La Fuente","UNIVERSIDAD DE ZARAGOZA","Nature has been utilizing nanostructures for billion of years. The following two properties, (i) being about the size of typical biological objects and (ii) the possibility of tailoring their properties by changing their size, make nanoparticles attractive for biomedical applications. Using nanoparticles to deliver drugs to tumours offers an attractive possibility to avoid obstacles that occur during conventional systemic drug administration. This NANOPUZZLE project pretends to develop an innovative controlled release methodology, based on hyperthermia and magnetic nanoparticles, as platform for the incorporation of different molecules with different functionalities, to obtain a multifunctional system for cancer treatment and diagnose that leads antitumoral drugs discharge only in the tumoral area. Multifunctional magnetic nanoparticles loaded with a targeting agent (folic acid) and a potent antitumoral drug (doxorubicin) will be prepared. These active molecules will be coupled to the magnetic nanoparticles (MNPs) due to complementary oligonucleotides strands (oligo-zipper). Due to the magnetic properties of these nanomaterials, a local heating induced by an alternating magnetic field, will release the drug in the desired target as a consequence of the DNA denaturation (oligo-unzipping). For this approach, the increase of temperature is only required directly in the nanoparticles and the heating of the surroundings is not needed. For instance, less quantity of nanoparticles and a weaker external magnetic field will be required, avoiding the main inconveniences of conventional hyperthermia treatments. Furthermore, the superparamagnetic properties of these MNPs will also allow their use as contrast agents for tracking and diagnosis by magnetic resonance imaging (MRI).","1541310","2010-02-01","2015-12-31"
"NanoREAL","Real-time nanoscale optoelectronics","Alexander Walter Holleitner","TECHNISCHE UNIVERSITAET MUENCHEN","Is it possible to really ‘see’ how fast electrons flow in nanoscale optoelectronic circuits? Can we, in this way, get a complete understanding of the real-time dynamics of electrons in nanoscale circuits?

The vision of this ERC proposal is to establish a research area at the interface of condensed matter physics, ultrafast optics, and electrical engineering which has so far been nearly completely unexplored: the investigation of real-time dynamics of photoexcited charge carriers in electrically contacted nanosystems with the highest precision possible. By doing so, unique information about the optoelectronic processes in nanoscale circuits shall be obtained. Four interconnected visions are pursued all with applications in information technology and electrical engineering. The approach is risky, however, it promises very interesting physics on the way. We will: (i) explore the fastest and smallest photoswitches fully integrated in electric circuits, (ii) probe single and collective charge excitations for the fastest nanoscale optoelectronic devices, (iii) determine the radiative and non-radiative lifetimes in photovoltaic circuits time-resolved, (iv) discover how fast nanoscale photo-thermoelectric devices operate. Towards these visions, I propose to use a real-time optoelectronic ‘on-chip’ detection scheme for nanoscale circuits, which was developed by us very recently. In this setup, I intend to carry out time-of-flight experiments of photoexcited electrons in nanoscale circuits, to investigate the ultimate switching speed of optoelectronic devices, and to explore the ultrafast dynamics of photothermo-electric currents in electrically contacted nanosystems.
The project gives essential insights for designing and implementing nanoscale circuits into optoelectronic switches, photodetectors, solar cells, thermo-electric devices as well as high-speed off-chip/on-chip communication modules to make ultrafast nanoscale optoelectronics real.","1272196","2012-11-01","2017-10-31"
"NANOSCOPY","High-speed chip-based nanoscopy to discover real-time sub-cellular dynamics","Balpreet Singh Ahluwalia","UNIVERSITETET I TROMSOE - NORGES ARKTISKE UNIVERSITET","Optical nanoscopy has given a glimpse of the impact it may have on medical care in the future. Slow imaging speed and the complexity of the current nanoscope limits its use for living cells. The imaging speed is limited by the bulk optics that is used in present nanoscopy. In this project, I propose a paradigm-shift in the field of advanced microscopy by developing optical nanoscopy based on a photonic integrated circuit. The project will take advantage of nanotechnology to fabricate an advance waveguide-chip, while fast telecom optical devices will provide switching of light to the chip, enhancing the speed of imaging. This unconventional route will change the field of optical microscopy, as a simple chip-based system can be added to a normal microscope.  In this project, I will build a waveguide-based structured-illumination microscope (W-SIM) to acquire fast images (25 Hz or better) from a living cell with an optical resolution of 50-100 nm. I will use W-SIM to discover the dynamics (opening and closing) of fenestrations (100 nm) present in the membrane of a living liver sinusoidal scavenger endothelial cell. It is believed among the Hepatology community that these fenestrations open and close dynamically, however there is no scientific evidence to support this hypothesis because of the lack of suitable tools. The successful imaging of fenestration kinetics in a live cell during this project will provide new fundamental knowledge and benefit human health with improved diagnoses and drug discovery for liver. Chip-based nanoscopy is a new research field, inherently making this a high-risk project, but the possible gains are also high. The W-SIM will be the first of its kind, which may open a new era of simple, integrated nanoscopy. The proposed multiple-disciplinary project requires a near-unique expertise in the field of laser physics, integrated optics, advanced microscopy and cell-biology that I have acquired at leading research centers on three continents.","1490976","2014-02-01","2019-01-31"
"NanoSOFT","Fluid transport at the nano- and meso- scales : from fundamentals to applications in energy harvesting and desalination process","Alessandro SIRIA","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","New models of fluid transport are expected to emerge from the confinement of liquids at the nanoscale, where the behaviour of matter strongly departs from common expectations. 
This is the field of the Nanofluidics: taking inspiration from the solution found by evolved biological systems, new functionalities will emerge from the nanometre scale, with potential applications in ultrafiltration, desalination and energy conversion.
Nevertheless, advancing our fundamental understanding of fluid transport on the smallest scales requires mass and ion dynamics to be ultimately characterized across channels with dimensions close to the molecular size. A major challenge for nanofluidics thus lies in building distinct and well-controlled nanochannels, amenable to the systematic exploration of their properties.
This project will tackle several complementary challenges. On the first hand the realization of new kind of fluidic devices allowing the study of fluid and ion transport at the nanoscale: these new experimental devices will be obtained by using nanostructures like building blocks as already shown by realising a fluidics set-up based on transmembrane nanotubes; in parallel a dedicated plateform for the characterization of fluid transport will be developed based on electrokinetics and optical detection set-ups. On the other hand, profiting of such experimental set-ups, I will look for the limit of the classical description of the fluid dynamics, focusing on new functionalities emerging from exotic behaviour of fluids at the nanometer level. This will be done by studying different kind of nanofluidics set-up such as carbon and boron-nitride nanotube, ultrathin pierced graphene and h-BN sheet and composite materials.

I aim the creation of a link between fundamental research on soft matter and nanoscience-condensed matter with a an attention on the energy production domain, assuring a fruitful transfer between the fundamental findings and new industrial applications.","1494000","2015-04-01","2020-03-31"
"NANOSOLID","Chemically Engineered Nanocrystal Solids","Maksym Kovalenko","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","""Many materials in the form of well-defined nanoscale crystals (“nanocrystals”) exhibit unique properties due to size effects and large surface-to-volume ratios. Yet it is clear that the utilization of nanomaterials in modern technologies requires their integration into solid-state structures with programmable electronic, magnetic and optical properties. The clear challenge is the rational design of this novel type of condensed matter, in which the size-tunable individual properties of nanoscale building blocks are enhanced by their interactions and by the macroscopic properties of their ensembles. The project NANOSOLID will rethink existing approaches and propose radically new strategies for the bottom-up assembly of inorganic entities of various dimensionalities into functional inorganic materials. We identified two clear and interlinked needs that will be addressed: the proper design and understanding of nanocrystal surface chemistry, and the unconventional assembly of nanocrystals into dense nanostructured solids. The union of modern concepts from molecular and supramolecular chemistry will be used to develop nanosolids with predictable geometries and functionalities. We will combine colloidal nanocrystals with other well-established classes of materials aiming at previously unknown crystalline structures composed of strongly interacting species in search for ground-breaking advances in materials design. Among the possibilities for these investigations are covalent and non-covalent, directional and non-directional binding modes, and specific and non-specific interparticle interactions. Together, this project will contribute significantly to the fundamental knowledge about the nanocrystal surface, and will develop new synthetic design tools for complex inorganic solids. Overall, the new materials design platform is expected to bring the long-awaited innovative solutions in energy research, particularly in the areas of thin-film devices for energy conversion and storage.""","1490319","2012-12-01","2017-11-30"
"NANOSPEC","Novel Out-of-Equilibrium Spectroscopy Techniques to Explore and Control Quantum Phenomena in Nanocircuits","Frédéric Pierre","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","We plan to develop and make use of novel out-of-equilibrium spectroscopy techniques that give access to energy transfers in
electronic nanocircuits. The unveiled information will be used to investigate promising quantum phenomena and to explore new
routes to control the mechanisms that limit their potentialities for nanoelectronics.
The proposals backbone is the spectroscopy of the fundamental electronic states energy distribution function f(E) that we
demonstrated this fall 2009: by using a quantum dot as an energy filter, we performed the first measurement of a non-equilibrium
f(E) in a semiconductor nanocircuit. We plan not only to employ it, but also to develop complementary techniques which will further
widen our range of investigation. We anticipate this f(E) toolbox will be crucial for the rising field of out-of-equilibrium mesoscopic
physics.
We will first examine through the unexplored facet of heat transport the quantum Hall effect regimes, which exhibit a large variety
of puzzling many-body quantum phenomena and are of particular interest for their metrology applications and quantum information
potentialities. The planed experiments will be done for various out-of-equilibrium situations, which will permit us to address longstanding
open questions, such as the nature of pertinent excitations, and to test original ways to increase quantum effects.
We will also perform direct energy exchange measurements to investigate the inelastic mechanisms that set the length and energy
scales of coherent and out-of-equilibrium physics in nanocircuits. The novel f(E) spectroscopy will permit us to take advantage of
the two-dimensional electron gas circuits high modularity to study many transport regimes and geometries that remain unexplored
from this revealing viewpoint.","1454400","2010-12-01","2015-11-30"
"NANOSTORM","Design of Nanomaterials for Targeted Therapies Guided by Super Resolution Imaging","Lorenzo ALBERTAZZI","TECHNISCHE UNIVERSITEIT EINDHOVEN","Nanomaterials revolutionized the field of targeted cancer therapies introducing innovative approaches towards the molecular recognition of diseased cells. However, despite the large investments in nanotechnology-based drug delivery the translation into clinical applications is still unsatisfactory and up to date there are no actively-targeted materials approved for clinical use. One of the main reasons is the lack of knowledge about the behaviour of nanostructures in the biological environment that makes the rational design of effective drug delivery carriers extremely challenging. 

NANOSTORM proposes the use of an innovative optical imaging technique such as super resolution microscopy to visualize and understand the molecular interactions of nanomaterials with their cellular targets in unprecedented detail. We recently reported for the first time the ability of Stochastic Optical Reconstruction Microscopy (STORM) to image self-assembled synthetic materials in vitro with nanometric resolution. NANOSTORM aims to bring this to the next level, using STORM to unveil the structure-activity relations of therapeutic nanomaterials in the biological environment at the single molecule level. The knowledge arising from this investigation will provide novel design principles for the next generation of nanomaterials for targeted therapies. In particular, in the framework of NANOSTORM novel nanomaterials for the targeted treatment of prostate cancer will be synthesized and evaluated. 
This interdisciplinary research program will advance our understanding of nanostructures for targeted drug delivery and guide the formulation of novel materials for cancer therapy.","1497588","2018-02-01","2023-01-31"
"NANOSTRUCTURE","Solving the nanostructure problem: Understanding, exploiting and designing functional disordered materials","Andrew Leslie Goodwin","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Many materials of fundamental importance possess structures that do not exhibit long-range periodicity. The absence of Bragg reflections in the diffraction patterns of these materials precludes the use of traditional crystallographic techniques as a means of determining their atomic-scale structures. Yet it is clear that these materials do possess well-defined local structure on the nanometre scale; moreover it is often this local structure that is implicated in the particular physical properties of interest. For this reason, the development of systematic information-based methodologies for the determination of local structure in disordered materials remains one of the key challenges in modern structural science; this is sometimes referred to as the “nanostructure problem”. This proposal addresses this issue by aiming (i) to develop robust methodologies for determining nano-scale structure in amorphous and highly-disordered systems, with an emphasis on laboratory-based techniques, (ii) then to use these techniques to develop structural models that will help address key scientific questions in a broad range of fields, and (iii) to apply the intuition gained to design new materials that exploit disorder to yield next-generation materials with desirable functionalities.","1694608","2011-10-01","2016-09-30"
"NANOthermMA","Advanced Simulation Design of Nanostructured Thermoelectric Materials with Enhanced Power Factors","Neophytos Neophytou","THE UNIVERSITY OF WARWICK","Roughly one-third of all energy consumption ends up as low-grade heat. Thermoelectric (TE) materials could potentially convert vast amounts of this waste heat into electricity and reduce the dependence on fossil fuels. State-of-the-art nanostructured materials with record-low thermal conductivities (κ~1-2W/mK) have recently demonstrated large improvements in conversion efficiencies, but not high enough to enable large scale implementation. Central to this low efficiency problem lies the fact that the Seebeck coefficient (S) and the electrical conductivity (σ), the parameters that determine the TE power factor (σS2), are inversely related. Relaxing this inverse interdependence has never been achieved, and TE efficiency remains low. My recent work in nanostructured materials, however, demonstrated for the first time how such a significant event can be achieved, and unprecedentedly large power factors compared to the corresponding bulk material were reported. This project focuses around four ambitious objectives: i) Theoretically establish and generalize the strategies that relax the adverse interdependence of σ and S in nanostructures and achieve power factors >5× compared to the state-of-the-art; ii) Experimentally validate the theoretical propositions through well-controlled material design examples; iii) Provide a predictive, state-of-the-art, high-performance, electro-thermal simulator to generalize the concept and guide the design of the entirely new nanostructured TE materials proposed. Appropriate theory and techniques will be developed so that the tool includes all relevant nanoscale transport physics to ensure accuracy in predictions. Simulation capabilities for a large selection of materials and structures will be included; iv) Develop robust, ‘inverse-design’ optimization capabilities within the simulator, targeting maximum performance. In the long run, the simulator could evolve as a core platform that impacts many different fields of nanoscience as well.","1498813","2016-07-01","2021-06-30"
"NanoTrigger","Triggerable nanomaterials to modulate cell activity","Lino Da Silva Ferreira","CENTRO DE NEUROCIENCIAS E BIOLOGIACELULAR ASSOCIACAO","The advent of molecular reprogramming and the associated opportunities for personalised and therapeutic medicine requires the development of novel systems for on-demand delivery of reprogramming factors into cells in order to modulate their activity/identity. Such triggerable systems should allow precise control of the timing, duration, magnitude and spatial release of the reprogramming factors. Furthermore, the system should allow this control even in vivo, using non-invasive means. The present project aims at developing triggerable systems able to release efficiently reprogramming factors on demand. The potential of this technology will be tested in two settings: (i) in the reprogramming of somatic cells in vitro, and (ii) in the improvement of hematopoietic stem cell engraftment in vivo, at the bone marrow. The proposed research involves a team formed by engineers, chemists, biologists and is highly multidisciplinary in nature encompassing elements of engineering, chemistry, system biology, stem cell technology and nanomedicine.","1699320","2012-11-01","2017-10-31"
"NanoVirus","Deciphering virus-host interactions using correlated confocal-atomic force microscopy","David ALSTEENS","UNIVERSITE CATHOLIQUE DE LOUVAIN","Viruses are a major class of pathogens that infect a variety of organisms. Infection is a multistep process that involves the concerted action of both virus and host cell machineries. The first steps of virus infection include cell binding, cell entry and release of the viral genetic material. Entry pathways are largely defined by the preliminary interactions between viruses and their receptors at the cell surface. Those interactions determine the mechanisms of virus attachment, uptake, and, ultimately, penetration into the cytosol. Elucidating the complex interplay between viruses and their receptors at the cell surface is an essential step towards establishing a full picture of the infection process. 

Currently, a crucial challenge in virology is to develop a quantitative method to decipher the entry pathways of a virus, thus allowing the probing of the kinetics and energetic parameters of the interactions established between the virus and the cell surface. While current methods successfully describe the entry pathways, they fail in identifying in a quantitative manner the key steps such as energy intensive and high-affinity steps. To overcome this limitation, the ambition of this ERC proposal is to combine the latest generations of atomic force microscopes (AFM) with confocal laser scanning microscopes (CLSM). This will allow us to investigate and quantitatively characterize the early steps of single virus entry directly on living cells. At the frontiers of nanotechnology, biophysics and biology, this project aims at pushing the limits of AFM to enable us to better understand the molecular mechanisms of virus entry. 

This project will have strong scientific and medical impacts. In virology, it will significantly improve the understanding of the mechanisms of virus infection. In medicine, the new method will help us and other researchers to screen new compounds that are targeting viral infection.","1998125","2018-01-01","2022-12-31"
"NAPOLI","Nanoporous Asymmetric Poly(Ionic Liquid) Membrane","Jiayin Yuan","STOCKHOLMS UNIVERSITET","Nanoporous polymer membranes (NPMs) play a crucial, irreplaceable role in fundamental research and industrial usage, including separation, filtration, water treatment and sustainable environment. The vast majority of advances concentrate on neutral or weakly charged polymers, such as the ongoing interest on self-assembled block copolymer NPMs. There is an urgent need to process polyelectrolytes into NPMs that critically combine a high charge density with nanoporous morphology. Additionally, engineering structural asymmetry/gradient simultaneously in the membrane is equally beneficial, as it would improve membrane performance by building up compartmentalized functionalities. For example, a gradient in pore size forms high pressure resistance coupled with improved selectivity. Nevertheless, developing such highly charged, nanoporous and gradient membranes has remained a challenge, owing to the water solubility and ionic nature of conventional polyelectrolytes, poorly processable into nanoporous state via common routes. 
Recently, my group first reported an easy-to-perform production of nanoporous polyelectrolyte membranes. Building on this important but rather preliminary advance, I propose to develop the next generation of NPMs, nanoporous asymmetric poly(ionic liquid) membranes (NAPOLIs). The aim is to produce NAPOLIs bearing diverse gradients, understand the unique transport behavior, improve the membrane stability/sustainability/applicability, and finally apply them in the active fields of energy and environment. Both the currently established route and the newly proposed ones will be employed for the membrane fabrication.
This proposal is inherently interdisciplinary, as it must combine polymer chemistry/engineering, physical chemistry, membrane/materials science, and nanoscience for its success. This research will fundamentally advance nanoporous membrane design for a wide scope of applications and reveal unique physical processes in an asymmetric context.","1500000","2015-03-01","2021-01-31"
"NAQUOP","Nanodevices for Quantum Optics","Valery Zwiller","KUNGLIGA TEKNISKA HOEGSKOLAN","We propose developing a nanodevice toolbox for single photon quantum optics. A scalable scheme to generate indistinguishable single photons, an interface to couple single photon polarization to a single electron spin and high efficiency single photon detectors represent the core of the scientific problems to be addressed in this project.
We set the following research objectives: 1- Understand to what extent quantum dots can be made indistinguishable. 2- Interface coherently single photons to single electron spins via strain engineering in quantum dots. 3- Gain a better understanding of the limits to time resolution and detection efficiency of ultrafast superconducting single photon detectors.
The proposed research effort will yield novel experiments: the realization of scalable indistinguishable quantum dot sources by frequency locking single quantum dots to atomic transitions, the demonstration of new selection rules in semiconductor nanostructures to couple photon polarization to the electron spin only, the development of ultrafast and high efficiency single photon and single plasmon detectors and their implementation in two photon interference and quantum plasmonics experiments.
To carry out the work, multidisciplinary efforts where nanofabrication, quantum optics, semiconductor and superconductor physics will be merged to demonstrate the scalability of quantum dots for quantum information processing, providing crucial new knowledge in single photon optics at the nanoscale. The impact of the project will be important and far reaching as it will address fundamental questions related to the scalability of quantum indistinguishability of remote nanostructures.","1500000","2013-04-01","2018-03-31"
"NARESCO","Novel paradigms for massively parallel nanophotonic information processing","Peter Bienstman","UNIVERSITEIT GENT","In this project we will develop nanophotonic reservoir computing as a novel paradigm for massively parallel information processing. Reservoir computing is a recently proposed methodology from the field of machine learning and neural networks which has been used successfully in several pattern classification problems, like speech and image recognition. However, it has so far mainly been used in a software implementation which limits its speed and power efficiency. Photonics could provide an excellent platform for such a hardware implementation, because of the presence of unique non-linear dynamics in photonics components due to the interplay of photons and electrons, and because light also has a phase in addition to an amplitude, which provides for an important additional degree of freedom as opposed to a purely electronic hardware implementation. Our aim is to bring together a multidisciplinary team of specialists in photonics and machine learning to make this vision of massively parallel information processing using nanophotonics a reality. We will achieve these aims by constructing a set of prototypes of ever increasing complexity which will be able to tackle ever more complex tasks. There is clear potential for these techniques to perform information processing that is beyond the limit of today&apos;s conventional computing processing power: high-throughput massively parallel classification problems, like e.g. processing radar data for road safety, or real time analysis of the data streams generated by the Large Hadron Collider.","1260000","2010-01-01","2015-12-31"
"NATCAT","Emulating Nature through Asymmetric Catalysis","Martin Derwyn Smith","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","We aim to establish a multidisciplinary research program inspired by nature s ability to selectively and specifically control the formation of complex materials, in order to develop new catalytic reactions, generate new materials and delineate new synthetic strategies.  The performance of enzymes - complex catalysts perfected though millions years of evolution - offer ideals of selectivity and specificity that synthetic chemistry can aspire to.  This proposal aims to establish a multidisciplinary approach to develop new practical and predictable catalytic methods based upon and inspired by natural catalysts.  We aim to challenge preconceptions about the field of organic synthesis to focus on the development of asymmetric electrocyclization processes, an entire class of reactions for which there is no general solution.  We have delineated an ion-pairing approach to the only catalytic asymmetric thermal 6À electrocyclic process and we aim to build on the insight provided by this result to develop a general approach to the control of electrocyclic processes.  We also aim to investigate the development of unnatural folded materials that plagiarize some of the features of enzymes  - such as positive cooperativity between non-covalent interactions   to generate more efficient asymmetric catalysts.  This multidisciplinary approach focuses on the design and synthesis of new folding backbones, investigation of their folding propensities and the evolution of catalytic function.  This research program will lead to the development and understanding of new tools essential for the assembly of complex molecules with biological, material or structural value.","1499993","2011-03-01","2016-02-29"
"NATURALE","Bio-Inspired Materials for Sensing and Regenerative Medicine","Molly Morag Stevens","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","Materials with nanometre-scale dimensions have unique functional properties that can lead to novel engineering systems with highly useful characteristics. Most traditional approaches to synthesis of nanoscale materials, unlike those in biology, require stringent conditions and often produce toxic byproducts. Within biology itself, biomaterials are highly organized from the molecular to the nanoscale, with intricate architectures that allow for optimum functionality. The focus for this proposal on bio-inspired materials is two-fold. In the first instance I aim to rationally design biologically responsive peptides to control the assembly and dis-assembly of bio-inorganic nanostructures and develop fundamental enabling technologies with applications in bio-sensing. The second focus is on exploiting our understanding of the natural biological nanostructures found in the complex extracellular matrix of tissues in order to engineer synthetic biomimetic nanostructures for improved cell growth and tissue regeneration. Outcomes will include greater fundamental understanding of cell-matrix interactions and cell differentiation as well as longer-term clinical impacts. I have begun to establish a creative research team with many developing international links and a record of timely high quality research. If successful with this proposal I will be able to manage my group to its full potential and to expand its influence and vision. The proposed research involves development of important new international collaborations in the basic sciences and is highly multidisciplinary in nature encompassing elements of engineering, biology, chemistry and physics and ranging from high-resolution techniques of surface analysis to peptide design and cell biology.","1643021","2008-10-01","2014-06-30"
"NBO","Novel Biomimetic Organocatalysts","Stephen Connon","THE PROVOST, FELLOWS, FOUNDATION SCHOLARS & THE OTHER MEMBERS OF BOARD OF THE COLLEGE OF THE HOLY & UNDIVIDED TRINITY OF QUEEN ELIZABETH NEAR DUBLIN","The primary aim of this project is to substantially expand the frontiers of current benchmark organocatalytic technology by the design, preparation and evaluation of the first class of thiol-based nucleophilic catalyst capable of emulating the action of NAD+-dependant enzymes such as aldehyde dehydrogenases, which promote the chemoselective oxidation/reduction of aldehyde substrates under mild conditions in aqueous media. The proposed artificial enzymes are designed biomimetically (in the true sense of the word) – only careful examination of the core enzyme competencies, modes-of-action and active sites has guided the design process. One of the key issues which this proposal addresses is the inherent difficulty associated with the design of artificial cofactor-dependent enzymes due to the requirement for a) efficient recognition by the catalyst of both the substrate and the cofactor, and b) the exertion of control over their encounter in the active site. We intend to tackle this challenge by covalently attaching groups functionally equivalent to the catalytically active residues of aldehyde dehydrogenases to a rigid NAD+ analogue in a manner which allows for their synergistic and biomimetic cooperation. Structure determination/mechanistic studies and the application of these new catalysts in a range of oxidations/reductions (we envisage that this project will result in the first organocatalyst able to demonstrably promote either - depending on the reaction conditions), (dynamic) kinetic resolutions, desymmetrisations, and ligations) will be undertaken and the development of catalytic processes for reactions currently outside the scope of any catalytic methodology is a clear long-term goal. We also wish to pursue the application of this potentially groundbreaking nucleophilic catalytic technology (for which no efficient organocatalysts have been thus far reported) toward the selective synthesis of enantiopure building blocks and pharmaceutically relevant molecules.","1249772","2008-10-01","2014-09-30"
"NCGQG","Noncommutative geometry and quantum groups","Sergiy Neshveyev","UNIVERSITETET I OSLO","""The goal of the project is to make fundamental contributions to the study of quantum groups in the operator algebraic setting. Two main directions it aims to explore are noncommutative differential geometry and boundary theory of quantum random walks.
The idea behind noncommutative geometry is to bring geometric insight to the study of noncommutative algebras and to analyze spaces which are beyond the reach via classical means. It has been particularly successful in the latter, for example, in the study of the spaces of leaves of foliations. Quantum groups supply plenty of examples of noncommutative algebras, but the question how they fit into noncommutative geometry remains complicated. A successful union of these two areas is important for testing ideas of noncommutative geometry and for its development in new directions. One of the main goals of the project is to use the momentum created by our recent work in the area in order to further expand the boundaries of our understanding. Specifically, we are going to study such problems as the local index formula, equivariance of Dirac operators with respect to the dual group action (with an eye towards the Baum-Connes conjecture for discrete quantum groups), construction of Dirac operators on quantum homogeneous spaces, structure of quantized C*-algebras of continuous functions, computation of dual cohomology of compact quantum groups.
The boundary theory of quantum random walks was created around ten years ago. In the recent years there has been a lot of progress on the “measure-theoretic” side of the theory, while the questions largely remain open on the “topological” side. A significant progress in this area can have a great influence on understanding of quantum groups, construction of new examples and development of quantum probability. The main problems we are going to study are boundary convergence of quantum random walks and computation of Martin boundaries.""","1144930","2013-01-01","2017-12-31"
"NCIRW","Non-classical interacting random walks","Martin Paul Wilhelm Zerner","EBERHARD KARLS UNIVERSITAET TUEBINGEN","The proposed project deals with various topics in the area of random walks (RWs) which interact with their own history and/or a possibly non-homogeneous, random environment. Such models arise from physics and other natural sciences as simplified models of complex phenomena and have recently attracted a lot of attention. They include: (a) Self-interacting RWs, i.e. RWs with memory, for example excited RWs, self-avoiding RWs, and RWs arising from two dimensional statistical mechanics models like Lorentz lattice gas models. (b) RWs in random media, for example RWs in random environments (RWRE), RWs on percolation clusters, and RWs in random potentials. Some of the problems to be considered concern recurrence and transience properties, laws of large numbers, ballistic versus sub-ballistic behavior, zero-one laws, scaling properties, other asymptotic properties, the relation between quenched and annealed behavior and monotonicity properties.","500000","2008-10-01","2014-09-30"
"NEBULAR","Novel Blueprints for the Visible-Light-Mediated Assembly of C–N Bonds via Nitrogen Radicals","Daniele LEONORI","THE UNIVERSITY OF MANCHESTER","Nitrogen-containing compounds underpin every aspect of our daily life as they form the structural basis of almost all pharmaceuticals, agrochemicals, food additives and materials. The invention of methods for the formation of bonds between C and N atoms is of strategic importance for the discovery and evolution of molecules with direct implications on the quality of our lives. Despite this fundamental relevance, forming C–N bonds is still a very challenging task.
This proposal aims to deliver transformative advances for the development of new efficient and selective strategies for the synthesis of N-containing molecules by harnessing visible-light as inexpensive and sustainable source of energy. 
I aim to challenge preconceptions about the field of organic chemistry and to develop innovative methods to control, modulate and transform the reactivity of nitrogen-centered radicals (NCRs), an entire class of reactive intermediates for which there are only limited applications in modern organic chemistry. 
My research group has developed two novel visible-light-mediated ways of generating NCRs, and I aim to: (1) Harness and explore the unique features of visible-light-mediated transformations to provide novel multicomponent reactions of NCRs. (2) Establish NCRs as viable partners in asymmetric photoredox catalysis and use them for the easy construction of complex molecules with defined 3D shapes. (3) Merge the visible-light generation of NCRs with organocatalysis to enable novel dual catalytic asymmetric protocols. (4) Combine the NCRs with transition metal catalysis and discover unprecedented transformations that go beyond photoredox catalysis and transition metal catalysis alone.     
This cohesive and innovative approach will develop new tools essential for the assembly of complex molecules with biological, therapeutic and agrochemical properties. Overall this project will transform the way C–N bonds are forged and how N-containing molecules are assembled.","1494777","2018-01-01","2022-12-31"
"NEDFOQ","Non-equilibrium dynamics of quantum fluids in one dimension","Vadim Cheianov","LANCASTER UNIVERSITY","This research proposal addresses non-equilibrium processes occurring in one-dimensional quantum fluids. The interest to this area has surged in recent years due to the rapid development of fabrication and measurement techniques in nanophysics and physics of ultra-cold atomic gases. Nanoelectronics devices (such as quantum point contacts, nanotubes and organic nanowires) and ultracold gases in elongated optical traps are the experimental systems where one-dimensional quantum fluids are encountered. While the main focus of nanoelectronics has always been on the electrical and spin transport, with only limited access to other aspects of non-equilibrium dynamics, the amazing degree of control over atomic systems has transformed the physics of one-dimensional fluids into a rapidly expanding universe of non-equilibrium phenomena. Quantum quenches, explosions and collisions of atomic clouds, diffusion and drift of quantum impurities, motion and decay of solitary waves have been observed and mapped in real time measurements.  The fundamental value of the research in this direction lies in the strongly correlated nature of  one-dimensional  quantum systems, which makes their kinetic theory a largely unexplored territory.  For these systems, the application of traditional tools of the kinetic theory, such as the Boltzmann collision integral and non-linear equations of hydrodinamics meets with serious conceptual difficulties. Indeed, it is usually impossible to represent the low-energy excitations of a one-dimensional system as a collection of weakly interacting quasiparticles. It is also impossible to consistently quantize non-linear hydrodynamcis within the standard framework of perturbative quantum field theory.  The main goal of this project is to develop methods bypassing these difficulties and to formulate a theoretical framework suitable for the description of non-equilibrium phenomena  in one dimension.","679640","2012-01-01","2014-12-31"
"NEDM","The Neutron Electric Dipole Moment: pushing the precision to understand the matter-antimatter asymmetry","Guillaume, Jean PIGNOL","UNIVERSITE GRENOBLE ALPES","The existence of a permanent electric dipole moment (EDM) of the neutron, or any subatomic particle, would have far reaching implications connecting particle physics with cosmology. Time reversal invariance and CP symmetry would be violated. A new fundamental interaction producing the EDM, that is, deforming the charge distribution inside the neutron, could also have generated the matter-antimatter asymmetry in the early Universe. After 60 years of evolution, techniques to measure the neutron EDM are now so evolved that experiments are sensitive to microphysics associated with an energy scale beyond that accessible at the LHC. This situation offers a high likelihood of discovery for the next generation of experiments. In the same time, any improvement in precision is technically challenging. The control of the magnetic field must surpass that of the state of the art of atomic magnetometers. The n2EDM project aims at improving the precision by an order of magnitude or more. Systematic effects need to be controlled at an unprecedented level. In particular, the use of a mercury co-magnetometer based on the precession of 199Hg spins induces a set of subtle false effects due to the relativistic motional field. 
I propose to initiate a comprehensive program to master these systematic effects beyond the current research program. In particular, the proposed project includes a precise determination of the 199Hg magnetic moment with a precision of 0.1 ppm. To this end, I will attempt a novel approach: combining mercury and 4He magnetometry in the same cell. As a by-product, this will also produce an improved determination of the neutron magnetic moment, a quantity of interest for metrology. The cross-check I propose will prove that all disturbances on the neutron or mercury spins are mastered at the sub-ppm level, a decisive step in the quest for the neutron EDM.","1498840","2017-04-01","2022-03-31"
"NEFERTITI","NEar FiEld cosmology: Re-Tracing Invisible TImes","stefania SALVADORI","UNIVERSITA DEGLI STUDI DI FIRENZE","The goal of NEFERTITI is to make a major step forward in our understanding of the first stars and galaxies by catching the stellar fossils from the early Universe in our Galactic neighborhood. To move beyond the state-of-the-art and study many of these precious fossils, I will adopt a novel approach that integrates theoretical and observational research and that will allow me to fully exploit: i) the huge data-flow from upcoming stellar surveys, ii) my cosmological models, which uniquely link Local data and early cosmic star-formation.  

The first stars profoundly influenced the primordial Universe, affecting subsequent stellar generations and the build-up of the first galaxies. In spite of extraordinary progress in theoretical modeling and observational techniques little is known about their properties, not even their typical mass. A direct exploration of their formation epochs is a tremendous challenge. Even JWST will not see the faint dwarf galaxies where the first stars formed more than 13 billion years ago.

In the Local Group, the living relics of the first stars can be directly observed and used to re-trace the chemical evolution and star-formation of the gas during those “invisible” times. Yet, these early Universe survivors are very rare and difficult to catch. In the present era of wide and deep Local surveys, such as DES, Gaia-ESO, and WEAVE, the total number of stars observed is dramatically increasing. Combining semi-numerical models with radiative transfer codes, I will fully exploit these novel data flow to catch the local stellar fossils and to: 

1) constrain the mass distribution of the first stars,
2) uncover the physical processes driving the build-up of the first galaxies. 

NEFERTITI will link Near and Far-field cosmology, give new insights into the formation of the Local Group, guide the interpretation of data from future surveys, and pave the way for the exploitation of new generation spectrographs on the E-ELT (MOSAIC, HIRES).","1180813","2019-05-01","2024-04-30"
"NEMSQED","Electromechanical quantum coherent systems","Mika Antero Sillanpää","AALTO KORKEAKOULUSAATIO SR","At a low temperature, nearly macroscopic quantum states can be sustained in superconducting (SC) Josephson junctions. Recently, these superconducting qubits have been coupled to electromagnetic resonators, in a manner analogous to cavity Quantum Electro Dynamics (QED) which describes the interaction between atoms and quantized oscillation modes in the quantum limit. On the other hand, there is yet no experimental evidence of a mode of a mechanical oscillator, such as that of a miniaturized vibrating string, to be chilled down to its quantum ground state. The main part of the proposal involves the use the coupling of Nanomechanical Resonators (NR) to SC qubits employed as artificial atoms in order to address the quantum-classical interface in mechanical motion. Similarly as the SC qubit can exchange quanta with electrical oscillators, it can, in principle, communicate with mechanical modes. The research will begin with demonstrating this kind of electromechanical interaction. In order to tackle experimental surprises, I plan on launching two parallel paths, one with a charge qubit, the other using a phase qubit. The formidable main goal is to experimentally reach the quantum ground state of a mechanical mode. I will investigate the following routes: Make a 1 GHz frequency NR, corresponding to 50 mK, which will reach the ground state at accessible temperatures. On the other hand, I propose to side-band cool a lower-frequency NR via the attached SC qubit. Near the quantum limit, I will start taking advantage of the NR as a building block of electromechanical quantum information. I also propose to push the QED setup of SC qubits coupled to electrical cavities towards more and more complicated states in order to test quantum mechanics in the nearly classical limit.","1373000","2010-01-01","2014-12-31"
"NEPA","Non-Equilibrium Protein Assembly: from Building Blocks to Biological Machines","Andela SARIC","UNIVERSITY COLLEGE LONDON","A key challenge in biological and soft-matter physics is to identify the principles that govern the organisation and functionality in non-equilibrium systems. Living systems are by definition out of equilibrium and a constant energy input is required to assemble and disassemble the molecular machinery of life. Only out of equilibrium, can proteins assemble to form functional sub-cellular structures, bind cells into dynamic tissues, and form complex biological machines. Our understanding of the physical mechanisms underlying robust protein assembly in driven systems is far from complete. Here I propose to develop a computer-simulation based framework to discover the physical principles of non-equilibrium protein assembly in biological or biomimetic systems. I will focus on systems where chemical gradients and active mechanical forces control protein assembly pathways and morphologies, and in which protein assembly far from equilibrium performs mechanical work. The particular case studies that I will investigate include mechanosensitive protein channels, fibrils of mechanical proteins, and active elastic filaments that remodel cells. As I aim to uncover generic design rules, my simulation model will only retain essential information on the shape and interaction of the assembling proteins needed to capture the complexity of the assembly. Using such minimal models, the simulations will be able to reach experimentally relevant time and length-scales, and will make quantitative predictions, which will be validated against data obtained by my experimental colleagues. The proposed programme will deliver an in-depth understanding of the molecular mechanisms that control the emergence of function in protein assemblies driven far from equilibrium. This knowledge should enable us to program or reprogram assembly phenomena in living organisms, and will provide principles that will guide the design and control of functional biomimetic assemblies and bio-inspired nano-machines.","1424574","2019-10-01","2024-09-30"
"NESS","Listening to the Future: Next-generation Sound Synthesis 
through Simulation","Stefan Bilbao","THE UNIVERSITY OF EDINBURGH","This proposal is concerned with simulation-based approaches to sound synthesis, in the interest of generating very high quality synthetic sound of a natural acoustic character—partly to emulate real instruments, but also to explore classes of sounds which cannot be produced using conventional synthesis methods, or acoustic instruments. A further goal is to introduce such physical modelling synthesis methods definitively into the world of electronic music, virtual environments, and to the greater public.

Target systems to be studied and simulated include: highly nonlinear acoustic systems (brass instruments, and percussion based on plate and shell vibration; electromechanical instruments; full 3D acoustic spaces; embedding of instruments within 3D spaces in order to achieve fully virtual and spatialized audio; and finally modular connections of systems in order to allow the eventual user, a composer, an instrument design environment. Such complex systems, including strong nonlinear effects, have never before seen a rigorous exploration from a numerical synthesis perspective. This proposed project is of an interdisciplinary nature, and rooted in music, numerical analysis, time-domain simulation, and high-performance computing.

Work will be carried out at various levels: a) theoretical work and time domain algorithm design, with special attention paid to the appropriate choice of model, efficiency and real time operation, and various issues critical in audio, including: adequate perceptual rendering of system responses at audio sample rates; aliasing; robust algorithm design, ensuring numerical stability under highly nonlinear conditions; and modular constructions. b) large-scale parallel implementations on multicore processors and general purpose graphics processing units (GPGPUs), and c) experimental testing through collaborative work with established composers of electronic music, leading to performances original multichanel and fully synthetic music.","1477477","2012-01-01","2016-12-31"
"NetMoDEzyme","Network models for the computational design of proficient enzymes","Silvia Osuna Oliveras","UNIVERSITAT DE GIRONA","Billions of years of evolution have made enzymes superb catalysts capable of accelerating reactions by several orders of magnitude. The underlying physical principles of their extraordinary catalytic power still remains highly debated, which makes the alteration of natural enzyme activities towards synthetically useful targets a tremendous challenge for modern chemical biology. The routine design of enzymes will, however, have large socio-economic benefits, as because of the enzymatic advantages the production costs of many drugs will be reduced and will allow industries to use environmentally friendly alternatives. The goal of this project is to make the routine design of proficient enzymes possible. Current computational and experimental approaches are able to confer natural enzymes new functionalities but are economically unviable and the catalytic efficiencies lag far behind their natural counterparts. The groundbreaking nature of NetMoDEzyme relies on the application of network models to reduce the complexity of the enzyme design paradigm and completely reformulate previous computational design approaches. The new protocol proposed accurately characterizes the enzyme conformational dynamics and customizes the included mutations by exploiting the correlated movement of the enzyme active site residues with distal regions. The guidelines for mutation are withdrawn from the costly directed evolution experimental technique, and the most proficient enzymes are easily identified via chemoinformatic models. The new strategy will be applied to develop proficient enzymes for the synthesis of enantiomerically pure β-blocker drugs for treating cardiovascular problems at a reduced cost. The experimental assays of our computational predictions will finally elucidate the potential of this genuinely new approach for mimicking Nature’s rules of evolution.","1445588","2016-05-01","2021-04-30"
"NetVolution","Evolving Internet Routing: 
A Paradigm Shift to Foster Innovation","Christos-Xenofon Dimitropoulos","FOUNDATION FOR RESEARCH AND TECHNOLOGY HELLAS","Although the Internet is a great technological achievement, more than 40 years after its creation some of its original security and reliability problems remain unsolved. The root cause of these problems is the rigidity of the Internet architecture or in other words the Internet ossification problem, i.e., the basic architectural components of the Internet are set to stone and cannot be changed. The most ossified component of the Internet architecture is the inter-domain routing system.

In this project, our goal is to address this challenge and to introduce a new Internet routing architecture that 1) enables innovation at the inter-domain level, 2) is backward-compatible with the present Internet architecture, and 3) provides concrete economic incentives for adopting it. We propose a new Internet routing paradigm based on a novel techno-economic framework, which exploits emerging technologies and meets these three goals. Our novel idea is that the combination of routing control logic outsourcing with Software Defined Networking (SDN) principles enables to innovate at the inter-domain level and therefore has the potential for a major break-through in the architecture of the Internet routing system. SDN is a rapidly emerging new computer networking architecture that makes the routing control plane of a network programmable. Based on our framework, we propose to design, build, and verify a better inter-domain routing system, which solves fundamental security, reliability, and manageability problems of the Internet architecture. Our work will be organized in four core topics 1) build a mutli-domain centralized routing control platform, 2) improve the reliability and security of the current inter-domain routing system, 3) design techniques for resolving tussles between competing network domains, 4) introduce advanced network monitoring and security techniques that intelligently correlate data from multiple domain to diagnose routing outages and attacks.","1410600","2014-01-01","2018-12-31"
"NEURAMORPH","Dynamics of Amorphous Semiconductors: Intrinsic Nature and Application in Neuromorphic Hardware","Martin Stefan Salinga","RHEINISCH-WESTFAELISCHE TECHNISCHE HOCHSCHULE AACHEN","After decades of perfecting the established way of computing, it is now evident that the fundamental logic of today’s computers will prevent them from ever reaching the efficiency of neural networks as found in nature. Neuromorphic hardware promises a leap forward by following the inherent working principles of biological neural networks. In very-large-scale integrated neuromorphic circuits incorporating an immense number of artificial neurons, the even much larger number of synapses poses the challenge of imitating especially the synaptic functionality in a most compact way. Over the last years, various memristive devices have been proposed to represent the weight of a synapse, determining how well electrical spikes are transmitted from one neuron to another. Existing attempts to achieve spike-timing-dependent plasticity, however, possess inherent problems.
The NEURAMORPH project aims to develop a simple and compact circuit element to regulate the access to the memristive device for weight modifications. The dynamics of electrical excitability intrinsic to the employed amorphous semiconductors will naturally be able to mimic spike-timing-dependent plasticity. For full control over the properties of these synaptic access elements, a fundamental understanding of the relaxation processes in such amorphous materials is imperative. To this end, amorphization conditions will be systematically varied over a wide-range to create very distinct amorphous states. As a measure for relaxation the temporal evolution of their electrical properties will then be investigated. Based on experimental results for a variety of materials, molecular dynamics simulations will be employed to elucidate the relationship between elemental composition, structural dynamics and changing electrical excitability. Finally, as proof of concept, a prototype of a neuromorphic chip will be developed incorporating the new kind of synaptic device.","1499468","2015-10-01","2021-09-30"
"NEURODIAM","High density full diamond cortical implant for long life time implantation","Lionel ROUSSEAU","CHAMBRE DE COMMERCE ET D'INDUSTRIE DE REGION PARIS ILE-DE-FRANCE","Implantable neuroprosthetic devices offer the promise of restoring neurological functions to disabled individuals. Tests demonstrated that an array of microelectrodes implanted in cortex allows to record activity of the brain and to induce a movement on prosthetic limbs or electrical stimulations restore some visual sensations. For these applications the life time and stability of the electrodes are critical features for the reliable operation of any implantable neuronal device. It’s also necessary to have high density implant with small electrodes to cover a large surface of the cortex to have access of neuronal code. A reliable packaging for long term implantable devices are in titanium or glass but not suitable in the case of ECoG (ElectroCorticoGraphy) implant. Indeed, it is necessary to achieve a polymer implant as a core material, to follow the topology of the brain surface. But in long term the polymer swells and moisture penetrates the implant and degrades its performances therefore reducing the lifetime. The goal of NEURODIAM project is to address two major challenges: - increase the lifetime of implant by a specific packaging, - reduce the size of the electrodes to be equivalent to the neurones size (10 µm) without degradation of noise and consequently increase the electrode density for a fine mapping of the cortex. To avoid performance drift of the implant, a new packaging solution completely hermetic will be developed based on the last developments of micro and nano structuration of diamond layer that combines conductive and intrinsic synthetic diamond. Fast ageing tests will be settled to demonstrate the viability of this diamond technology. In-vitro and in-vivo assessment will be performed to demonstrate the efficiency of these implants for recording and stimulation of neuronal tissue. 

This project will produce high performance diamond based technology that can be later used for various implants dedicated to fundamental studies in neurosciences.","1499865","2018-05-01","2023-04-30"
"NeuroLang","Accelerating Neuroscience Research by Unifying Knowledge Representation and Analysis Through a Domain Specific Language","Demian WASSERMANN","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","Neuroscience is at an inflection point. The 150-year old cortical specialization paradigm, in which cortical brain areas have a distinct set of functions, is experiencing an unprecedented momentum with over 1000 articles being published every year. However, this paradigm is reaching its limits.  Recent studies show that current approaches to atlas brain areas, like relative location, cellular population type, or connectivity, are not enough on their own to characterize a cortical area and its function unequivocally. This hinders the reproducibility and advancement of neuroscience. 

Neuroscience is thus in dire need of a universal standard to specify neuroanatomy and function: a novel formal language allowing neuroscientists to simultaneously specify tissue characteristics, relative location, known function and connectional topology for the unequivocal identification of a given brain region. 

The vision of NeuroLang is that a unified formal language for neuroanatomy will boost our understanding of the brain. By defining brain regions, networks, and cognitive tasks through a set of formal criteria, researchers will be able to synthesize and integrate data within and across diverse studies. NeuroLang will accelerate the development of neuroscience by providing a way to evaluate anatomical specificity, test current theories, and develop new hypotheses.

NeuroLang will lead to a new generation of computational tools for neuroscience research. In doing so, we will be shedding a novel light onto neurological research and possibly disease treatment and palliative care. Our project complements current developments in large multimodal studies across different databases. This project will bring the power of Domain Specific Languages to neuroscience research, driving the field towards a new paradigm articulating classical neuroanatomy with current statistical and machine learning-based approaches.","1497045","2018-03-01","2023-02-28"
"NEUROP","Neuromorphic processors: event-based VLSI models of cortical circuits for brain-inspired computation","Giacomo Indiveri","UNIVERSITAT ZURICH","""Brains are remarkable computing devices which clearly outperform conventional architectures in real-world tasks. Computational neuroscience has made tremendous progress in uncovering the key principles by which neural systems carry out computation, and ICTs have advanced to a point where it is possible to integrate almost as many transistors in a VLSI system as neurons in a brain. Yet, we are still unable to develop artificial neural systems with basic computing abilities able to parallel even simple insect brains.
We have recently demonstrated how it is possible to implement large-scale artificial neural networks and real-time sensory motor systems in VLSI technology, exploiting the physics of silicon to reproduce the biophysics of neural systems. But the main bottleneck is in the understanding of how to use these systems to perform general purpose computation. Progress in this domain can be achieved only by pursuing a fully integrated multi-disciplinary approach.  We propose to combine neuroscience, mathematics, computer-science, and engineering to develop a theoretical formalism and its supporting technology for designing spike-based general purpose """"neuromorphic processors"""", as distributed multi-chip neuromorphic VLSI systems, and for programming them to learn to produce desired computations autonomously. We will study the properties of neural circuits in the neocortex, model their coding strategies and spike-driven learning mechanisms using biophysically realistic spiking neural networks, and implement them using hybrid analog digital VLSI circuits.
By interfacing these systems to silicon retinas, cochleas and autonomous robotic platforms we will build embodied neuromorphic processors able to carry out event-based computations in real-world behavioral tasks.""","1494023","2011-03-01","2017-02-28"
"NEUTRINO","Nonlinear Fourier Transforms in Action","Sander WAHLS","TECHNISCHE UNIVERSITEIT DELFT","Nonlinear effects are ubiquitous in many modern technology fields such as photonics and hydromechanical engineering. The standard method to deal with nonlinear effects is to use linear approximations because they are simple to work with. Engineers typically consider nonlinear effects a nuisance that cannot be dealt with in exact terms. This intuition is deeply engrained in the collective subconscious, but it is not always correct -- not all nonlinearities are equally bad. Many practically important nonlinear systems can be approached using so-called nonlinear Fourier transforms, which offer simple closed-form descriptions for nonlinear phenomena that are difficult to work with in the conventional time- or frequency-domain, similar to how the conventional Fourier transform simplifies the analysis of linear systems. Today, almost fifty years after the discovery of the first nonlinear Fourier transform by Gardner et al., nonlinear Fourier transforms have been studied intensively in mathematics and physics. However, despite many potential applications, they have not yet found widespread use in engineering. The lack of efficient numerical algorithms for the computation of nonlinear Fourier transforms similar to the celebrated fast Fourier transform is the major roadblock. I have recently been able to present the first “fast nonlinear Fourier transforms”, but most of this work so far was concerned with signals that obey vanishing boundary conditions. This is the simplest, but not the practically most relevant case. Periodic and Dirichlet boundary conditions occur frequently in practical applications, but their treatment is much more difficult. I aim to push nonlinear Fourier transform into engineering practice by developing fast nonlinear Fourier transforms for two prototypical applications, fiber-optic communications and water-wave data analysis.","1337907","2017-03-01","2022-02-28"
"neutrinoSNO+","Probing fundamental properties of the neutrino at the SNO+ Experiment","Jeanne Rachel Wilson","QUEEN MARY UNIVERSITY OF LONDON","I propose a comprehensive programme of research on SNO+, a multi-purpose
neutrino experiment that has the capacity to push forward the frontier of our knowledge in both neutrino and solar physics by addressing a wide range of physics topics. There are three main goals:
A) To extend our understanding of neutrino oscillations by studying the suppression of low energy solar electron neutrino flux components.
B) To address discrepancies in solar models by publishing the world's first measurement of neutrino fluxes from the CNO-cycle interactions in the Sun.
And C) To contribute to the search for neutrino-less double beta decay, the so-called 'golden channel' for testing the fundamental nature of the neutrino and the absolute neutrino mass scale.

The neutrino survival probabilities and CNO spectra will be extracted simultaneously in a novel approach to the solar analysis that will capitalize on theoretical correlations between the different flux components. Similar techniques will be applied to the double beta analysis allowing for a fully correlated treatment of all backgrounds and systematic uncertainties.

Given the huge potential impact of these measurements, it is imperative that we maximise the physics reach of the SNO+ experiment and ensure the credibility of all results through detailed calibration and modelling to attain a complete understanding of the detector response to both the neutrino signals and inevitable background contributions. In addition to the above analysis goals, this proposal focuses on two key areas - a detailed charcterisation of the detector optical response through calibration measurements and detailed simulations and the development of an electron calibration source to confirm our
understanding of the detector response to electron signals across a broad energy range. Both of these unique contributions should significantly enhance the accuracy and credibility of all SNO+ physics measurements.","1345472","2011-11-01","2017-06-30"
"New-Poetry","New Advances through the boundaries of Poisson Geometry","Marius Crainic","UNIVERSITEIT UTRECHT","This project proposes new research directions that originate in the field of Poisson Geometry and which reach out towards other fields of Differential Geometry and Topology. It can also be seen as the development of a new field- Poisson Topology, the birth of which is clearly predicted by the  recent results of the PI on stability of symplectic leaves.
Aims:
1. solving some of the most fundamental open problems in Poisson Geometry.
2. breaking the current boundaries of Poisson Geometry and bringing it at the forefront of the interplay between other fields in geometry (Foliation Theory, Symplectic Geometry etc).
Methods/tools:
1. build on the breakthrough results of the PI (and his collaborators) such as the one on the integrability of Lie algebroids or the geometric approach to Conn-Weinstein theorem.
2. new tools in Poisson Geometry such as Nonlinear Functional Analysis or the use of the fundamental ideas of Cartan that were not yet exploited in Poisson Geometry (G-structures, Exterior Differential Systems, etc ).
New directions: I propose several interacting directions/subprojects, each one of independent interest. For example:
- study of local invariants in Poisson Geometry (a wide-open problem) based on PI's work and the use of ideas from G-structures.
- a new unified approach to stability  theories, such as Mather's theory or  Nijenhuis-Richardson's
(apparently unrelated!).  Poisson Geometry plays an unifying role. We expect new fundamental results in Poisson Topology and related fields (including moduli spaces of flat connections).
- the study of global aspects of Poisson Geometry. E.g. the existence of codimension one Poisson structures on spheres (the 5-dimensional sphere was settled only last year!). Recall that the similar problem in Foliation Theory served as a driving force for the field (and will be used here). Global aspects will also take us to the world of Symplectic Topology- a high risk/high return journey that has never been taken before","1100000","2011-09-01","2016-08-31"
"new-ppd-environments","First-principles global MHD disc simulations: Defining planet-forming environments in early solar systems","Oliver Lothar Gressel","","The aim of this ambitious research project is to produce the most realistic computer simulations of gaseous protoplanetary accretion discs to date, and thereby define in an assertive way the environment that shapes the assembly and early evolution of planetary systems.
In their role as planet nurseries, protoplanetary discs are of key interest to planet formation theory. Their dynamical, radiative and thermodynamic properties critically define the environment for embedded solids: dust grains, pebbles and planetesimals. In short, the building blocks of planet formation. The discs’ dynamics and structure in turn depend critically on the influence of magnetic fields that couple to tenuously ionised and low-density regions. Being comparatively cold and dense, the ionisation state of the disc plasma is dominated by external far-UV, X-Ray, and cosmic-ray radiation, leading to a layered vertical structure – with turbulent, magnetised surface layers and a magnetically-decoupled midplane. This classic ‘dead-zone’ picture is now turned upside-down by previously ignored micro-physical effects. For instance, ambipolar diffusion is predicted to dominate in the tenuous hot corona of the disc. It is expected that parts of the disc will thus be stabilised and a magneto-centrifugal wind will be launched. This has so far only been studied in very simplified local models that are affected by fundamental limitations.
Our understanding of the structure of protoplanetary discs is about to undergo a dramatic shift, and my proposed research is at the forefront of this development. My recent successful work at the interface between MHD dynamics and planet formation theory makes me ideally skilled to lead a research group in this endeavour and to communicate advancements to a wide audience of theoreticians in planet formation. Our ambitious global simulations will furthermore provide realistic templates to interpret new observations made with the ALMA telescope array.","1392763","2015-06-01","2020-05-31"
"NEW_FUN","New era of printed paper electronics based on advanced functional cellulose","Luis Miguel Nunes Pereira","NOVA ID FCT - ASSOCIACAO PARA A INOVACAO E DESENVOLVIMENTO DA FCT","Fully recyclable and low cost electronic goods are still far from reality. My interest is in creating environmental friendly advanced functional materials and processes able to result in new class of paper based electronic products. This represents a reborn of the paper millenary industry for a plethora of low cost, recyclable and disposable electronics, putting Europe in the front line of a new era of consumer electronics.
While the vision of the proposal is a very ambitious one, my ground-breaking research work to date related with oxide based transistors on paper (from which I am one of the co-inventors) has contributed to the basic technological breakthroughs needed to create the key elements to establish a new era of paper electronics. Field effect transistors (FETs), memory and CMOS devices, with excellent electronic performance and using paper as substrate and dielectric have resulted from my recent work. What I am proposing now is to reinvent the concept of paper electronics. In NEW_FUN I want to develop a completely new and disruptive approach where functionalized cellulose fibers will be used not only as dielectric but also as semiconductor and conductor able to coexist in a multilayer paper structure. That is, assembling paper that can have different functionalities locally, on each face or even along its entire thickness/bulk. This way issues such as failure under bending, mechanical robustness and stability can be minimized. Doing so, electronic and electrochemical devices can be produced not only on paper but also from paper. The outputs of NEW_FUN will open the door to turn paper into a real electronic material making possible disposable/recyclable electronic products, such as smart labels/packages (e.g. food and medicine industry), sensors for air quality control (car, house and industry environments); disposable electronic devices such as bio-detection platforms, lab-on-paper systems, among others.","1429719","2015-09-01","2020-08-31"
"NewAve","New avenues towards solving the dark matter puzzle","Kai Schmidt-Hoberg","STIFTUNG DEUTSCHES ELEKTRONEN-SYNCHROTRON DESY","It is now firmly established that most of the matter in the Universe is in the form of the mysterious dark matter, contributing more than 80% to the total amount of matter. However, despite tremendous theoretical and experimental efforts over the past few decades, dark matter remains elusive and one of the great unknowns until today. To identify the nature of dark matter is evidently of fundamental importance and one of the top priorities in science today. The quest for dark matter is inherently multi disciplinary with strong roots in particle physics, astrophysics and cosmology, providing profound connections between these different disciplines.
This project aims at exploring new avenues towards solving the dark matter puzzle, with a particular focus on a few select groundbreaking topics. These are centered around (i) theoretical dark matter model building, (ii) the study of new collider signatures, (iii) developing new techniques for the comparison and interpretation of direct detection experiments and (iv) identifying astrophysical probes which constrain or give evidence for dark matter self-interactions.
Given the impressive increase in sensitivity of upcoming dark matter experiments as well as the upcoming high energy run of the Large Hadron Collider, there is no doubt that the era of data has begun for dark matter searches and that we can expect putative signals rather than exclusion limits for the near future. It is therefore extremely important to bring together different fields and exploit the complementarity of different search strategies to maximise the amount of information gained from a successful detection. This inherently multi disciplinary approach is at the heart of the current project, which can rely on a well established network of collaborators and will bring together excellent young physicists with different backgrounds to form a small but well structured research group which will significantly advance dark matter phenomenology in Europe.","1214250","2015-06-01","2020-05-31"
"NEWDARK","New Directions in Dark Matter Phenomenology at the TeV scale","Marco Cirelli","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Dark Matter constitutes about 80% of the total matter of the Universe, yet almost nothing is known of its nature: despite the huge experimental and theoretical efforts of the last decades, its true identity is yet to be determined. The recent years and the next few years, however, see several experimental exploratory techniques approaching for the first time the TeV scale, in a multi-faceted attack to the problem: the Large Hadron Collider at CERN in particle physics, the PAMELA and AMS-02 satellites in charged cosmic ray astronomy and the FERMI telescope in gamma ray astronomy. Since general theoretical arguments lead to believe that Dark Matter is a particle inherently related to the TeV scale, the stakes are high of being finally close to the physics that holds the key of the puzzle.
The NewDark project aims at exploring selected new directions in Dark Matter phenomenology, in a multi-disciplinary approach that has its roots in theoretical particle physics and cosmology but constantly looks at astrophysical observations and experimental particle physics results, making the most of the bi-directional interactions. The ultimate goal of the project, as part of the effort at the global scale, is the identification of the nature of the Dark Matter and the exploration of its full phenomenology.
The project is organized around five main themes of Dark Matter research: theory model building, collider signatures, direct detection, indirect detection and astrophysical/cosmological implications. For each one of these, some selected groundbreaking objectives are identified. The emphasis is on new, non-traditional directions, building on the experience gained by the community in studying more traditional avenues and applying it to the new scenarios.
The project requires funds to build up a small but structured multi-disciplinary research team (hiring 4 young post-docs with diverse expertise) and allow it to work on this frontier of astroparticle physics.","1462200","2012-10-01","2018-09-30"
"NEWHEAVYFERMION","Novel materials and extreme conditions to open new frontiers in heavy fermion physics","Dai Aoki","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","The objective of this project is to explore novel phenomena of heavy fermion systems. The focus will be on low temperature novel properties such as quantum criticality, unconventional superconductivity and multipole ordering, which will leads to new horizon not only of heavy fermion physics, but also of material science. We will concentrate on: (1) new materials and high quality single crystals, (2) precise temperature-pressure-field (T,P,H) phase diagrams, (3) quantum singularities and Fermiology, (4) the mechanism of unconventional superconductivity including ferromagnetic superconductor, (5) field-induced phenomena.
To reach our targets, we will first attempt to grow many new compounds based on U, Ce, Yb and other rare earth elements with a careful choice of target, using various techniques. Very high quality single crystals can be a breakthrough in this field of research, in particular for unconventional superconductivity. Then, we will measure their low temperature properties with various experimental techniques under extreme conditions, namely low temperature, high field, high pressure. Activities of material growth and studies of their properties will be coordinated in order to provide rapid a feedback. This work will be comforted by  theoretical work. To carry out specific experiments, we will develop a new AC calorimetry system under extreme conditions and a de Haas-van Alphen (dHvA) measurement system. With this experimental method, we aim to directly observe the heavy electronic state. This is a major issue to clarify the possible Fermi surface instability at quantum singularities. The high quality samples will be supplied to other groups in order to extend our macroscopic and microscopic experimental multi approach.","1500000","2010-11-01","2015-10-31"
"NEWIRES","Next Generation Semiconductor Nanowires","Kimberly Thelander","LUNDS UNIVERSITET","Semiconductor nanowires composed of III-V materials have enormous potential to add new functionality to electronics and optical applications. However, integration of these promising structures into applications is severely limited by the current near-universal reliance on gold nanoparticles as seeds for nanowire fabrication. Although highly controlled fabrication is achieved, this metal is entirely incompatible with the Si-based electronics industry. It also presents limitations for the extension of nanowire research towards novel materials not existing in bulk. To date, exploration of alternatives has been limited to selective-area and self-seeded processes, both of which have major limitations in terms of size and morphology control, potential to combine materials, and crystal structure tuning. There is also very little understanding of precisely why gold has proven so successful for nanowire growth, and which alternatives may yield comparable or better results. The aim of this project will be to explore alternative nanoparticle seed materials to go beyond the use of gold in III-V nanowire fabrication. This will be achieved using a unique and recently developed capability for aerosol-phase fabrication of highly controlled nanoparticles directly integrated with conventional nanowire fabrication equipment. The primary goal will be to deepen the understanding of the nanowire fabrication process, and the specific advantages (and limitations) of gold as a seed material, in order to develop and optimize alternatives. The use of a wide variety of seed particle materials in nanowire fabrication will greatly broaden the variety of novel structures that can be fabricated. The results will also transform the nanowire fabrication research field, in order to develop important connections between nanowire research and the semiconductor industry, and to greatly improve the viability of nanowire integration into future devices.","1496246","2013-09-01","2018-08-31"
"NEWNANOSPEC","New tools for nanoscale optical spectroscopy -
Functional imaging of single nanostructures using antennas","Achim Hartschuh","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","Optical microscopy forms the basis of most of the natural sciences. Besides the direct visualization of objects hidden to the unaided human eye, optical spectroscopy – or in other words “colour vision”- is of prime importance providing information on electronic and vibronic properties. In addition, experiments using ultrafast laser pulses provide the highest possible temporal resolution enabling real-time observations of photo-induced processes. Conventional microscopy, however, suffers from diffraction resulting in limited spatial resolution of about 300 nm and low signal levels.

The aim of this proposal is to develop novel spectroscopic tools with sub-diffraction resolution. Our approach is based on the localization and enhancement of light-matter interactions using optical antennas. We have shown that antenna-enhanced microscopy provides 10 nm resolution combined with enormous signal amplification and now envision new techniques that extend existing schemes into the femtosecond time-domain with further improved image contrast.
Semiconductor nanowires and carbon nanotubes possess unique properties crucial to many areas of technology including communications, alternative energy and the biological sciences. At present, there is a significant lack of understanding regarding the physics of these materials. For example, the correlation between local atomic structure and the resulting optical and functional properties.

We will first address fundamental scientific questions arising from highly localized optical probing and explore new phenomena including antenna-enhanced single photon emission and energy transfer. Using our newly developed tools, we will study functional properties of single nanostructures and demonstrate antenna-enhanced light-detection and generation.

In summary, our work will lead to fundamentally new optical tools providing unprecedented insights into nanostructures and will substantially advance our understanding of light-matter interactions.","1488077","2011-10-01","2016-09-30"
"NEWNET","New Approaches to Network Design","Fabrizio Grandoni","SCUOLA UNIVERSITARIA PROFESSIONALE DELLA SVIZZERA ITALIANA","""Networks pervade every aspect of nowadays life. This is one of the reasons why their design, management, and analysis is one of the most active areas of theoretical and empirical research in Computer Science and Operations Research. The main goal of this project is to increase our theoretical understanding of networks, with a special focus on faster exact exponential-time algorithms and more accurate polynomial-time approximation algorithms for NP-hard network design problems. We will consider classic, challenging open problems in the literature, as well as new, exciting problems arising from the applications. These problems will be addressed with the most advanced algorithmic and analytical tools, including our recently developed techniques: iterative randomized rounding, core detouring, and randomized dissection.

A second, ambitious goal of this project is to stimulate the interaction and cross-fertilization between exact and approximation algorithms. This might open new research horizons.""","1122199","2012-01-01","2016-12-31"
"NewNGR","New frontiers in numerical general relativity","Pau Figueras","QUEEN MARY UNIVERSITY OF LONDON","In recent years general relativity (GR) has become an increasingly important new tool in areas of physics beyond its traditional playground in astrophysics. The main motivation for this comes from the AdS/CFT correspondence which conjectures an equivalence between gravity in anti-de Sitter (AdS) spaces and certain conformal field theories (CFT’s). Via this correspondence, GR now plays a key role in improving our understanding of non-gravitational physics at strong coupling.

The AdS/CFT correspondence naturally leads to the study of GR in dimensions greater than four and/or in AdS spaces. Our current understanding of GR in these new settings is rather limited but it has been realized that the physics of gravity can be significantly different than in the 4d asymptotically flat case. Moreover, to access these new gravitational phenomena numerical methods have been and will be essential. However, the use of numerical GR beyond the traditional 4d asymptotically flat case is still in its infancy. The goal of this project is to improve our understanding of GR in higher dimensions and/or AdS spaces using numerical techniques. To achieve this goal, we will focus on the study of the following topics:

1. Develop stable codes for doing numerical GR in AdS and higher dimensions. We will use numerical GR and the AdS/CFT correspondence to study out of equilibrium phenomena in strongly coupled CFT’s. We will also use numerical GR to understand the endpoint of the various black hole instabilities and thereby address long standing conjectures in GR.

2. New types of stationary black holes. We will use numerical GR to numerically construct new types of black holes in higher dimensions and in AdS, with novel topologies and fewer symmetries than the known ones. We shall apply them to the study of equilibrium configurations in strongly coupled gauge theories at finite temperature.","1284525","2015-09-01","2020-08-31"
"NEWPHYSICSHPC","Unraveling new physics on high-performance computers","Andreas Juettner","UNIVERSITY OF SOUTHAMPTON","Quarks are bound together by the strong nuclear force as described by QCD. Due to confinement quarks and gluons are not detected in experiments but particles which are complicated bound states. Simulations allow for relating the bound state properties to those of the underlying quarks. The calculation is performed by constructing a discrete four dimensional space-time lattice and then solving the QCD equations of motion on high performance computers (e.g. graphics cards cluster or IBM BG/Q at Edinburgh)

New physics will be discovered in terms of discrepancies between Standard Model (SM) predictions and experimental measurements.

A hint for a discrepancy between theory and experiment and therefore new physics exists for the anomalous magnetic moment of the muon. I will implement a new approach to its computation which will provide reliable predictions from first principles and which will substantiate or rebut the apparent tension. Also, my newly developed method for analytically predicting contributions (quark-disconnected diagrams) to the muon anomalous moment which are very hard to compute numerically will be extended to other processes relevant for understanding non-perturbative physics  (e.g. K->pi pi) and for SM-tests (neutron EDM).

The LHCb experiment at CERN, Switzerland, has recently started taking data for processes that are particularly sensitive to new physics. To interpret the experimental data one needs theory-predictions that can only be provided by lattice QCD. Here properties of flavor-changing neutral current decays of particles containing one b-quark and one light quark will be computed.

Next to a large scale simulation of K->pi decays, algorithms will be developed and cut-off effects computed analytically in order to reduce the uncertainty in the lattice computation of Vus, an element of the CKM-matrix.

An UV-fixed point in the non-linear sigma model will be searched with lattice simulations on graphics cards.","977571","2012-05-01","2018-04-30"
"NewPhysLat","Search for new physics through lattice simulations","Antonin PORTELLI","THE UNIVERSITY OF EDINBURGH","Despite its monumental success, we have reason to think that the Standard Model of particle physics is an effective description of a more fundamental theory. In order to maximise the chances of success of experiments to pinpoint the breakdown of this theory, it is crucial to provide precise, ab-initio theoretical predictions to compare it with. One of the main challenges in producing these predictions is to reliably take account of the non-perturbative, confining phase of the strong interaction. So far, the most efficient way to achieve that is to use numerical lattice simulations. In this proposal, I focus on theoretical quantities involved in the search for new physics and propose an ambitious lattice simulation programme to determine them precisely. Firstly, I propose to predict rare kaon decays amplitudes. These decays are extremely rare in the Standard Model and are expected to be sensitive to new physics. Secondly, I propose to determine how to include isospin breaking effects in the calculation of the anomalous magnetic moment of the muon and meson leptonic and semi-leptonic decay rates. Including these effects is a highly non-trivial task which is necessary to push the theoretical precision of these observables beyond the percent level in order to provide a higher constraint on the Standard Model. Thirdly, I propose to explore holographic cosmology, an ambitious and innovative alternative to ΛCDM, the ""standard model"" of cosmology. Finally, all these projects will directly contribute to the development of Grid, the emerging world-leading software solution for lattice simulations. All these projects are strongly pushing the boundaries of the application of lattice simulations and the results will be confronted with experimental measurements within the next five years. This proposal focuses on supporting world-leading particle physics experiments and I will deliver high-impact results which have the potential to uncover new physics beyond the Standard Model.","1499981","2017-10-01","2022-09-30"
"NEWSILICON","Low-valent silicon complexes:Transition metal-like catalysts","Tsuyoshi Kato","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""This project principally concerns the use of silicon (the second most abundant element in the Earth’s crust) as catalyst instead of transition metals. Although the importance of its abundance is well recognized in material chemistry, as it can be seen by the vast world production of silicon based materials (polymers, rubber, semiconductors), real success in chemistry with such a vision has never been achieved. This is probably due to the lack of appropriate stable silicon species with particular and highly modulable electronic properties which can be applied for various catalytic systems. We propose, in this project, the development of new silicon species with a transition metal like behavior. The success of this project should open a new wide research domain in chemistry and could change the vision of catalysis.""","1433725","2012-09-01","2017-08-31"
"NEWSPIN","New Frontiers in Spintronics","Rembertus Abraham Duine","UNIVERSITEIT UTRECHT","The aim of this theory proposal is to develop the research field of spintronics in three new directions - i) antiferromagnetic metals, ii) helimagnets, and iii) ultracold quantum gases - unified by the fact that it is a priori clear that new concepts have to be developed in understanding their spintronics phenomena. The central scientific challenge is understanding transport of a nonconserved quantity, i.e., spin, and its nonconserved current, i.e., the spin current. The proposal capitalizes in part on the PI’s experience with both spintronics and cold atoms to cross-fertilize these sub-disciplines of condensed-matter physics. i) The first focus of the proposal, motivated by experimental follow-ups of the PI’s pioneering theory work, is to theoretically study current-driven magnetization dynamics in antiferromagnetic metals. These materials are very promising for applications in ultrahigh-density information storage technology. ii) The second focus is to study the influence of current on the magnetic state of a helimagnet. The dynamics of the magnetization spiral in a helimagnet can be viewed as motion of a series of domain walls. In addition to its intrinsic fundamental interest this study will therefore shed light on the ongoing issues in current-driven domain wall motion, such as intrinsic versus extrinsic pinning of domains, and the role of intrinsic spin-orbit coupling. iii) The third and last focus of the proposal is to study analogues of spintronics phenomena with cold atoms, exploiting the well-understood microscopic description of these systems to quantum engineer model systems for spintronics, as well as their possibilities to go beyond conventional electronic condensed-matter physics. In particular the prospect for spin currents to be carried by bosonic particles opens up new research directions. This study develops new trends in spin-dependent transport phenomena and current-induced order-parameter dynamics.","876000","2008-09-01","2013-08-31"
"NEWTON","NEw Windown inTO Earth's iNterior","Manuele FACCENDA","UNIVERSITA DEGLI STUDI DI PADOVA","Comprehensive seismic programs undertaken in the past few years, combined with emerging new numerical technologies now provide the potential, for the first time, to explore in detail the Earth’s interior. However, such an integrated approach is currently not contemplated, which produces physical inconsistencies among the different studies that strongly bias our understanding of the Earth’s internal structure and dynamics. Of particular concern are nowadays apparent thermo-petrological anomalies in tomographic images that are generated by the unaccounted-for anisotropic structure of the mantle and that are commonly confused with real thermo-petrological features. Given the diffuse mantle seismic anisotropy, apparent thermo-petrological anomalies contaminate most tomographic models against which tectono-magmatic models are validated, representing a critical issue for the present-day window.
Here we aim to develop a new methodology that combines state-of-the-art geodynamic modelling and seismological methods. The new methodology will allow building robust anisotropic tomographic models that will exploit anisotropy predictions from petrological-thermomechanical modelling to decompose velocity anomalies into isotropic (true thermo-petrological) and anisotropic (mechanically-induced) components. 
As a major outcome, we expect to provide a new, geodynamically and seismologically constrained perspective of the current deep structure and tectono-magmatic evolution of different tectonic settings. This new methodology will be applied to the Mediterranean and the Cascadia subduction zone where, despite the abundant seismological observations, large uncertainties about the subsurface structure and tectono-magmatic evolution persist.
Furthermore, we plan to develop a new inversion technique for seismic anisotropy, and release an open source, sophisticated code for mantle fabric modelling, which will allow coupling geodynamic and seismological modelling in other tectonic settings.","1466030","2018-03-01","2023-02-28"
"NEXCENTRIC","Next-generation on-chip supercontinuum light sources based on graphene-enabled extreme nonlinear optics","Nathalie Vermeulen","VRIJE UNIVERSITEIT BRUSSEL","With this ERC project I want to induce a paradigm shift in the development of integrated nonlinear optical devices. Nonlinear optics, the scientific discipline in which nonlinear light-matter interactions are studied, has been a very active area of research ever since the invention of the laser in 1960. Although this scientific branch has great application potential when implemented in on-chip optical waveguides, its promise for the development of widely usable integrated optical devices has not yet been fulfilled. The state-of-the-art of integrated nonlinear optical devices indeed does not comply with the requirements for widespread deployment as these devices rely on non-standard waveguide designs, large on-chip foot prints and/or impractical pump lasers.
Therefore, I propose in this project to eliminate the issues of the state-of-the-art devices by introducing novel material and device physics. More specifically, my goal is to exploit extreme, but practically unexplored, nonlinear optical properties of graphene-covered silicon waveguides to develop next-generation near-infrared-pumped nonlinear supercontinuum light sources. These will truly be “next-generation” sources as they will rely on standard waveguide design, ultra-compact foot prints and practical near-infrared pump lasers, while exhibiting unprecedented performances. The concrete objectives of my project are to theoretically study, model, fabricate and experimentally demonstrate three novel graphene-on-silicon-based nonlinear optical devices that rely on three different nonlinear optical effects, and the on-chip cascading of these novel devices to create the targeted “next-generation” near-infrared-pumped supercontinuum sources with up to four emission bands. Based on my theoretical and experimental research experience with nonlinearities in waveguides and my preliminary modeling results supporting the feasibility of these objectives, I believe that, with this ERC starting grant, I will be able to carry out this original “high-gain/high-risk” project. By doing so, I will introduce a paradigm shift in the development of integrated nonlinear optical devices enabling them to fulfill their long-awaited promise, and at the same time initiate a new era in the research on graphene and its nonlinear optical applications.","1477980","2013-10-01","2018-09-30"
"NEXT","Neutron-rich, EXotic, heavy nuclei produced in multi-nucleon Transfer reactions","Julia EVEN","RIJKSUNIVERSITEIT GRONINGEN","The heaviest element which has been found in nature is uranium with 92 protons. So far, the elements up to atomic number 118 (oganesson) have been discovered in the laboratory. All transuranium elements are radioactive and their production rates decrease with increasing number of protons. An Island of Stability, where the nuclei have relatively long half-lives, is predicted at the neutron number 182 and, depending on the theoretical model, at the proton number 114, 120 or 126. Current experimental techniques do not allow to go so far to the neutron-rich side close to the Island of Stability.
The observation of gravitational waves as well as electromagnetic waves originating from a neutron star merger has been published on October 16, 2017 and is a first proof of the nucleosynthesis of heavy elements in the r-process. It still remains an open question if superheavy nuclei have been formed in our universe. To answer these questions, we need insight into the nuclear properties of the heaviest elements and how these properties evolve when one moves toward to the neutron-rich side on the nuclear chart.
In the NEXT project, I will set out to discover new, Neutron-rich, EXotic heavy nuclei using multi-nucleon Transfer reactions. I will measure their masses and, thus, pin down the ground state properties of these nuclei. These studies provide insight into the evolution of nuclear shells in the heavy element region. Furthermore, I will measure the fission half-lives of these isotopes. In order to realize the NEXT project, I will built a novel spectrometer, which is a combination of a solenoid separator and Multi-Reflection Time-of-Flight Mass Spectrometer.
The broad experience in heavy element research and mass measurements that I have acquired over the years, and the unique infrastructure at my home institute that houses the AGOR accelerator, makes it so that I am ideally placed to start and lead the NEXT project.","1670323","2019-05-01","2024-04-30"
"NEXTPHASE","NEXT generation of microwave PHotonic systems for AeroSpace Engineering","Yanne Chembo Kouomou","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Aerospace and communication engineering technologies are in constant need of microwaves with extremely high spectral purity and stability. Unfortunately, the generation of such ultra-pure microwaves with compact, versatile and transportable sources is still a very complex challenge. In aerospace engineering, ultra-stable quartz oscillators are overwhelmingly dominant as key components for both navigation and detection systems. However, it is unanimously recognized today that their frequency stability performance is reaching its floor, and will not improve significantly anymore. In the search for an alternative standard for the next generation of ultra-pure microwave sources in aerospace technology, we propose the exploration of an elegant and promising solution relying on optical resonators with ultra-high Q factors (Q ~ 1E10). In these quasi-perfectly shaped cavities, nonlinear effects are significantly enhanced and microwave generation is performed through the extraction of the intermodal frequency. This approach has several advantages over existing or other prospective methods: conceptual simplicity, higher robustness, smaller power consumption, longer lifetime, immunity to interferences, very compact volume, frequency versatility, easy chip integration, as well as a strong potential for integrating the mainstream of standard photonic components for both microwave and lightwave technologies. Our ambition in the NextPhase project is to significantly outperform quartz oscillators and demonstrate performances comparable to cryogenic sapphire oscillators, with a compact (< 100 cm3), versatile (up to at least 200 GHz) and ultra-stable (Allan variance ~ 1E-15 at 1 s; phase noise floor < -160 dBc/Hz)  microwave photonic generator. We also expect our work to open new opportunities of research in optical communications (photonic components for full-optical processing, carrier synthesis), as well as in fundamental aspects of condensed matter and quantum physics.","1384628","2011-11-01","2016-10-31"
"NGHCS","NGHCS: Creating the Next-Generation Mobile Human-Centered Systems","Kalogeraki","ATHENS UNIVERSITY OF ECONOMICS AND BUSINESS - RESEARCH CENTER","Advances in sensor networking and the availability of every day, low-cost sensor enabled devices has led to integrating sensors to instrument the physical world in a variety of economically vital sectors of agriculture, transportation, healthcare, critical infrastructures and emergency response. At the same time, social computing is now undergoing a major revolution: social networks, as exemplified by Twitter or Facebook, have significantly changed the way humans interact with one another. We are now entering a new era where people and systems are becoming increasingly integrated and this development is effectively leading us to large-scale mobile human-centered systems. Our goal is to develop a comprehensive framework to simplify the development of mobile human-centered systems, as well as make them predictable and reliable. Our work has the following research thrusts: First, we develop techniques for dealing efficiently with dynamic unpredictable factors that such complex systems face, including dynamic workloads, unpredictable occurrence of events, real-time demands of applications, as well as user changes and urban dynamics. To achieve this, we will investigate the use of mathematical models to control the behavior of the applications in the absence of perfect system models and a priori information on load and human usage patterns. Second, we will develop the foundations needed to meet the end-to-end timeliness and reliability demands for the range of distributed systems that we will consider by developing novel techniques at different layers of the distributed environment and studying the tradeoffs involved. Third, we will develop general techniques to push computation and data storage as much as possible to the mobile devices, and to integrate participatory sensing and crowdsourcing techniques. The outcome of the proposed work is expected to have significant impact on a wide variety of distributed systems application domains.","960000","2013-03-01","2019-02-28"
"NICEDROPS","Precise and smart nanoengineered surfaces: Impact resistance, icephobicity and dropwise condensation","Manish Kumar TIWARI","UNIVERSITY COLLEGE LONDON","Water freezing (icing) and condensation are ubiquitous in our life. Preventing undesirable icing on surfaces with minimal energy and chemical use, and improving the efficiency of condensation heat exchangers has broad societal value. Thus, I aim to use fundamental insights to offer energy-efficient solutions for undesirable ice formation and promoting dropwise condensation using novel and robust nanoengineered surfaces. My objectives are:

 i) to realise thermodynamically guided metallic surfaces with precise (<10 nm) morphology and controlled superficial stiffness for energy-efficient icing prevention and sustaining dropwise flow condensation
ii) to rationally intercalate polymers and/or suspensions into surface nanotextures and exploit nanomechanics in order to enable robust and smart nanoengineered surfaces for high speed impact, abrasion and chemical resistance; stable icephobicity (delaying freezing); and sustained dropwise condensation.
iii) to develop new fundamental insights to: a) prevent icing due to high speed (~100 m/s) supercooled droplet/ice crystal impact; b) realise icephobicity down to -30 degrees Celsius; c) minimise ice-surface adhesion; and d) sustain dropwise condensation at high (50-100 m/s) vapour speeds.  

The proposal emphasis on energy efficiency is aligned with the EU's 20/20/20 Strategic Energy Technology (SET) Plan. To exemplify their salient impact, the proposed smart nanoengineered surfaces offer a passive solution for airplane icing (and related accidents) and will delay evaporator icing on air source heat pumps and refrigerators, thereby helping to lower the energy use in buildings and cold storages. The latter are tied to the global food storage and distribution challenges. Similarly, sustained dropwise condensation will make condensers in process industry and steam power plants compact and efficient. Optimally, only ~1 micron of the surface depth will require treatment – this will minimize chemical use and promote sustainability.","1908624","2017-03-01","2022-02-28"
"NIGOCAT","Nature-Inspired Gold Catalytic Tools","Cristina Nevado","UNIVERSITAT ZURICH","The study of biologically relevant processes heavily relays on “small molecules”. Thus, the demand for novel chemical probes is of highest importance not only for chemistry, but also for closely related disciplines such as biology, medicine or material science. As the construction of complex molecular architectures from chemical building blocks still remains a far-from-routine task, the development of methodologies to increase the control over chemical reactivity and achieve molecular complexity with higher levels of efficiency has become one of the frontier challenges of chemistry in the 21st century.
NIGOCAT aims to substantially contribute towards this goal. The general objective of this proposal is the design, synthesis and application in catalysis of novel, nature-inspired gold(I) and gold(III)-catalytic tools able to mimic nature´s efficiency and exquisite taste for the synthesis and stereoselective functionalization of “small molecules”. The proposed research tackles three main challenges faced by current synthetic methods: 1. Efficient generation of structural complexity; 2. Selective C-H bond functionalization; 3. High levels of stereocontrol in asymmetric catalysis.
We aim to streamline the construction of molecular complexity based on modular, unprecedented multi-center gold factories. Our hypothesis is that the assembly of different reactive sites within a single catalyst will provide an increased level of efficiency in gold-orchestrated catalytic cascades from simple starting materials, thus mimicking the way nature assembles its complex primary metabolites. Second, we aim to tackle the flexible, selective functionalization of C-H bonds using novel metaloenzyme-inspired ligands on gold. Third, we aim to develop novel gold peptide-based catalytic systems as general tools able to provide high levels of absolute stereocontrol in gold catalysis.","1500000","2012-10-01","2018-09-30"
"NINA","Nitride-based nanostructured novel thermoelectric thin-film materials","Per Daniel Eklund","LINKOPINGS UNIVERSITET","My recent discovery of the anomalously high thermoelectric power factor of ScN thin films demonstrates that unexpected thermoelectric materials can be found among the early transition-metal and rare-earth nitrides. Corroborated by first-principles calculations, we have well-founded hypotheses that these properties stem from nitrogen vacancies, dopants, and alloying, which introduce controllable sharp features with a large slope at the Fermi level, causing a drastically increased Seebeck coefficient. In-depth fundamental studies are needed to enable property tuning and materials design in these systems, to timely exploit my discovery and break new ground.

The project concerns fundamental, primarily experimental, studies on scandium nitride-based and related single-phase and nanostructured films. The overall goal is to understand the complex correlations between electronic, thermal and thermoelectric properties and structural features such as layering, orientation, epitaxy, dopants and lattice defects. Ab initio calculations of band structures, mixing thermodynamics, and properties are integrated with the experimental activities. Novel mechanisms are proposed for drastic reduction of the thermal conductivity with retained high power factor. This will be realized by intentionally introduced secondary phases and artificial nanolaminates; the layering causing discontinuities in the phonon distribution and thus reducing thermal conductivity.

My expertise in thin-film processing and advanced materials characterization places me in a unique position to pursue this novel high-gain approach to thermoelectrics, and an ERC starting grant will be essential in achieving critical mass and consolidating an internationally leading research platform. The scientific impact and vision is in pioneering an understanding of a novel class of thermoelectric materials with potential for thermoelectric devices for widespread use in environmentally friendly energy applications.","1499976","2013-10-01","2018-09-30"
"NIRG","New paradigms for InfraRed modifications of Gravity","Cédric Jean André Marc Deffayet","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""Our proposal addresses theoretical and phenomenological properties of large distance (“Infra-Red”, IR in the following) modifications of the gravitational interaction. Such modifications are motivated by two main reasons: firstly, to find alternative explanations to the presence of dark matter or dark energy in cosmology; secondly, to better understand the currently well accepted cosmological model, disentangling there what does from what does not depend on the large distance dynamics of gravity and extracting as much as possible new information on gravity from the latest cosmological observations. For the second goal, it matters to have at hand alternatives to the standard cosmological model based on general relativity, to serve as benchmarks. Very recently, new ideas have been proposed to modified gravity in the IR. First, a large class of scalar-tensor theories featuring the “k-mouflaging” of the scalar has been proposed and partly classified. Second, new kinds of massive gravities which might be devoid of the standard pathologies of those models have been discovered. Third, models of non local gravity have been proposed with many interesting features. In this proposal, we intend to better understand those constructions, in which the works of the applicant played a major role, and whose properties are largely unexplored. As transversal goals, we also intend to propose new ways to modify gravity in the IR, as well as to develop schemes to tests IR modifications of gravity against cosmological and gravitational data. The project will be lead by the applicant, four postdocs and two students.""","1471296","2013-07-01","2018-06-30"
"NLPRO","Natural Language Programming: Turning Text into Executable Code","Reut Tsarfaty","THE OPEN UNIVERSITY","Can we program computers in our native tongue? This idea, termed natural language programming, has attracted attention almost since the inception of computers themselves.  
From the point of view of software engineering (SE), efforts to program in natural language (NL) have relied thus far on controlled natural languages (CNL) -- small unambiguous fragments of English with strict grammars and limited expressivity. Is it possible to replace CNLs with truly natural, human language?  
From the point of view of natural language processing (NLP), current technology successfully extracts static information from NL texts. However, human-like NL understanding goes far beyond such extraction -- it requires dynamic interpretation processes which affect, and are affected by, the environment, update states and lead to action. So, is it possible to endow computers with this kind of dynamic NL understanding?  
These two questions are fundamental to SE and NLP, respectively, and addressing each requires a huge leap forward in the respective field. In this proposal I argue that the solutions to these seemingly separate challenges are actually closely intertwined, and that one community's challenge is the other community's stepping stone for a huge leap, and vice versa. Specifically, I propose to view executable programs in SE as semantic structures in NLP, and use them as the basis for broad-coverage dynamic semantic parsing.
My ambitious, cross-disciplinary goal is to develop a new NL compiler based on this novel approach to NL semantics. The NL compiler will accept an NL description as input and return an executable system as output. Moreover, it will continuously improve its NL understanding capacity via online learning that will feed on verification, simulation, synthesis or user feedback. Such dynamic, ever-improving, NL compilers will have vast applications in AI, SE, robotics and cognitive computing and will fundamentally change the way humans and computers interact.","1449375","2016-08-01","2022-07-31"
"NMU-LIPIDS","Biomimetic Lipid Structures on Nano- and Microfluidic Platforms","Petra Stephanie Dittrich","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","The projects aim at the formation, manipulation, and analysis of three-dimensional lipid membrane structures on micro- and nano-structured platforms. The goal is to develop a novel methodology to design and create simple artificial cells and cell organelles, bio-hybrid cells, and bio-mimicking membrane networks, which could be an entirely novel tool for cell analysis, and promises fascinating prospects for cell manipulation, biotechnology, pharmacy and material sciences. The basis of the projects is formed by an unconventional concept that involves two current cutting-edge fabrication technologies, i.e. the so-called top-down and bottom-up approaches. The combination of the two approaches, with respect to both engineering methods and biological applications, opens the door to overcome current limitations in the creation of complex soft matter objects in micro- and nanometre dimension. The key method is a recently developed micro-extrusion process. It relies, on the one hand, on the ability of the lipid molecules to self-assemble (“bottom-up”). On the other hand, photolithography processes (“top-down”) are utilized to fabricate microchips, in which shape transformation, handling and analysis of the lipid structures are performed. The proposed engineering process will enable, for the first time, to precisely design composition, size and morphology of complex membrane structures. It will provide the requirements to design an artificial cell of reasonable complexity (“bottom-up”). One main emphasis is the creation of unique bio-hybrid systems, in which artificial membrane structures are connected to living cells, or in which natural membranes of cells are integrated within artificial systems (“top-down”). This highly interdisciplinary study will further include fundamental studies on membrane properties, engineering aspects to generate novel soft-matter devices, and the development of analytical methods and lipid sensors based on micro- and nanostructured chips.","1941000","2008-07-01","2014-06-30"
"NOBUGS","Toward Zero-Defect Software Through Automatic Cooperative Self-Improvement","George Candea","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","""This proposal advocates a fundamentally new approach to achieving software quality: remove the distinction between software use and software testing -- enable programs to accumulate experience from each one of their executions, and leverage this experience toward self-improvement of the software. My hypothesis is that every program execution has information by-products that, if suitably captured and aggregated, can substantially speed up the process of testing programs and proving them correct.  Software is being executed billions of times around the world, with the corresponding information going to waste.  At the same time, traditional software testing tries to simulate a small subset of real-world conditions and executions.  I propose instead viewing every execution of a program as a test run, and the aggregation of executions across the lifetime of all copies of that program as one gigantic test suite.

I propose the study of techniques and formalisms for automatically recouping the information that is lost during everyday software use, aggregating it, and automatically turning it into tests and proofs; techniques to use these tests and proofs to automatically correct the behavior of programs; and techniques for automatically steering programs into exploring behaviors for which information is lacking.  All these techniques will be embodied in a platform, called BeeNet, that implements a massively distributed learning process which turns execution by-products into a collective experience that leads to higher quality software.  This is a radical new way of exploiting the vast (but today completely wasted) information that results from program execution.

I will investigate these questions with an integrated approach that combines thorough theoretical studies with practical application to real-world software, employing the perspectives of three different research communities: operating systems, programming languages, and software verification.""","1334977","2012-02-01","2018-01-31"
"NODAL","Nodal Lines","Igor Wigman","KING'S COLLEGE LONDON","""First observed by the physicist and musician Ernst Chladni in the 18th century, the nodal lines
(also referred to as the Chladni Plates or Chladni Modes) appear in many problems in engineering, physics and natural sciences. Nodal lines describe sets that remain stationary during membrane vibrations, hence their importance in such diverse areas as musical instruments industry, mechanical structures, earthquake study and other fields. My proposed research aims at the nodal patterns and question arising from them with mathematical rigour.

So far, the nodal structures have been mainly addressed in the physics literature, whose statement are lacking the mathematical precision; most of their results are based on numerical experiments and heuristic computations rather than analytic methods typical for mathematics. In his seminal paper, Michael Berry (1977) suggested that the behaviour of the deterministic nodal patterns corresponding to the high frequency vibration on generic membranes is universal, and may be """"miraculously"""" explained by a random ensemble of monochromatic waves. Extensive numerical experiments confirm Berry's predictions, however no rigorous statement is known (or even formulated) to date.

In this research I propose to investigate the nodal structures in depth arising for various random ensembles. These kind of questions, very natural, especially in light of the proposed random models, were studied empirically in physics literature, and in the last few years analytically in the mathematics literature, mainly by Nazarov and Sodin, and the PI in various collaborations. The questions arising are of fundamental importance in mathematical physics, probability theory, mathematical analysis, and, as was recently discovered, number theory. The proposed research aims at rigorously answering some of the related open questions.""","966361","2014-02-01","2019-01-31"
"NOLEPRO","Nonlinear Eigenproblems for Data Analysis","Matthias Hein","UNIVERSITAT DES SAARLANDES","In machine learning and exploratory data analysis, the major goal is the development
of solutions for the automatic and efficient extraction of knowledge from data. This
ability is key for further progress in science and engineering. A large class of
data analysis methods is based on linear eigenproblems. While linear eigenproblems are
well studied, and a large part of numerical linear algebra is dedicated to the efficient
calculation of eigenvectors of all kinds of structured matrices, they are limited in their
modeling capabilities. Important properties like robustness against outliers
and sparsity of the eigenvectors are impossible to realize. In turn, we have shown recently
that many problems in data analysis can be naturally formulated as nonlinear eigenproblems.

In order to use the rich structure of nonlinear eigenproblems with an ease
similar to that of linear eigenproblems, a major goal of this proposal is to develop a general
framework for the computation of nonlinear eigenvectors. Furthermore, the great potential of nonlinear eigenproblems will be explored in various application areas. As the scope of nonlinear eigenproblems goes far beyond data analysis, this project will have major impact not only in machine learning and its use in computer vision, bioinformatics, and information retrieval, but also in other areas of the natural sciences.","1271992","2012-10-01","2017-09-30"
"NOMAD","Nanoscale Magnetization Dynamics","Pietro Gambardella","FUNDACIO INSTITUT CATALA DE NANOCIENCIA I NANOTECNOLOGIA","The aim of NOMAD is to develop frontier approaches to control the magnetodynamic properties of nanometer-sized molecular and metallic elements. The first part of the project recognizes the importance of molecular materials for future technologies based on magnetoelectronic devices. It addresses the stabilization of the magnetic moment of individual molecules beyond their intrinsic limits (slow timescale). Moreover, the construction of spin-sensitive probes with spatial atomic-resolution and a dynamic range extending up to the GHz regime is proposed. These shall be used to characterize magnetodynamic phenomena of individual molecules and metal particles in a nanoscopic environment (fast timescale). The second part relates to the control of magnetic relaxation and coercivity in nanoscale metallic particles. Electric-field manipulation of ferromagnetism has been proven in dilute magnetic semiconductors at temperatures below 50 K. Here, the aim is to demonstrate and optimize electric field-induced changes of the magnetic anisotropy energy in metal layers and nanoparticles embedded in a double tunnel junction, providing a direct or indirect (transition-driven) handle to their magnetic dynamics at room temperature. Metal-based materials constitute the mainstay of present magnetic technology; their electric-field actuation would lead to simpler and power-saving devices that process magnetic information using electrical signals.","1517779","2008-09-01","2013-08-31"
"NONABVD","Nonadiabaticity in Biomolecular Vibrational Dynamics","Benjamin FINGERHUT","FORSCHUNGSVERBUND BERLIN EV","This ERC Starting Grant 2018 aims at the fundamental understanding of ultrafast biomolecular vibrational dynamics in the mid-IR/THz region and respective impact of nonadiabatic effects in dipolar liquids, within nano-confined environments and in the vicinity of biological interfaces. The understanding of these processes via underlying interactions is of fundamental importance with applications covering microscopic descriptions of elementary proton transfer reactions, mechanisms of energy dissipation upon vibrational excitation and solvation dynamics in biological relevant crowded environments. In particular knowledge on anisotropy of ultrafast vibrational energy relaxation together with information about distinguished intra- or inter-molecular acceptor modes, is scarce. As such the ERC Starting Grant 2018 transfers the paradigm of nonadiabatic relaxation, that has proven tremendous predictive power for descriptions of ultrafast electronic relaxation, to the low energy mid-IR/THz domain of biomolecular vibrational (energy relaxation) dynamics. As such the approach provides a description of microscopic phenomena like structural fluctuations, vibrational lifetimes and dissipation of excess energy. The proposed nonadiabatic approach to vibrational dynamics fully accounts for the strong impact of the fluctuating environment and will facilitate a concise theoretical descriptions of proton solvation structure, dynamics and transport within the confinement imposed by proton transport channel proteins. The investigation of proton mobility within reverse micelles will further facilitate the understating of proton structural diffusion within nanoscopic volumes. Such interfacial processes in the vicinity of biological membranes and proton translocation within transmembrane proteins are highly relevant as microscopic foundation of cell respiration driven by the gradient of proton concentration across membranes.","1486805","2019-01-01","2023-12-31"
"NONARCOMP","From complex to non-archimedean geometry","Charles Favre","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""Complex geometry is the study of manifolds that are defined over the complex numbers. Non-archimedean geometry is concerned with analytic spaces over
fields endowed with a norm that satisfies the strong triangular inequality.
The aim of this proposal is to explore the interactions between these seemingly different geometries
with  special emphasis on analytic and dynamical problems.


We specifically plan to develop pluripotential theory over non-archimedean fields. This includes the search  for analogs of the celebrated Yau's theorem. In a more local setting, we shall also look for possible applications of non-archimedean techniques to the """"Openness Conjecture"""" on the structure of singularities of plurisubharmonic functions.

A second axis of research concerns the problem of growth of degrees of iterates of complex rational maps in arbitrary dimensions. We especially aim at extending to arbitrary dimensions the successful non-archimedean techniques that are already available for surfaces.

Finally we want to  investigate the geometry of parameter spaces of complex dynamical systems
acting on the Riemann sphere using  non-archimedean methods.  This requires the development of the  bifurcation theory of non-archimedean rational maps.""","787233","2012-10-01","2017-09-30"
"NONCONTACTULTRASONIC","Non-contact ultrasonics: new methods for large and small scale measurements","Rachel Sian Edwards","THE UNIVERSITY OF WARWICK","Ultrasonic measurements have a wide range of applications, from non-destructive testing (NDT) on bridges, pipeline and railway track, to measurements of elastic constants of single crystals, whose properties are key to the development of new magnetoelastic devices. Traditional techniques require the transducers to be in physical contact with the sample, and non-contact ultrasonics (NCU) is attracting more interest. Improvements to NDT through the use of NCU are leading to new applications in hostile environments, such as measurements at high temperatures or of moving samples, helped by the lack of need for couplant. This proposal covers applications of great importance which could benefit from the use of NCU. The problem of partially closed cracking in metals will first be investigated. Stress corrosion cracking can cause catastrophic damage to pipeline, and there is no current technique which can reliably detect and characterise it. The physical phenomena associated with transmission of the ultrasound will be probed to develop new methods of detecting defects using laser ultrasonics and electromagnetic acoustic transducers. The project will investigate and explain the behaviour of the ultrasonic interactions with defects in the vicinity of cracks, including non-linear behaviour around a partially closed crack. Later work will use ultrasound for elastic and magneto-elastic measurements of single crystals. These measurements are able to quickly and inexpensively identify interesting and important new systems, saving the time and resources which may have been used to investigate samples using neutron studies. New NCU techniques will be developed, capable of detecting magnetic phase changes through variations in the coupling efficiency. The benefits of combining the results from standard measurements with those from novel electromagnetic NCU techniques will be studied. The research will facilitate collaboration with groups within the Department and internationally.","1599810","2008-09-01","2014-08-31"
"NonCovRegioSiteCat","Harnessing Non-Covalent Interactions for Control of Regioselectivity and Site-Selectivity in Catalysis","Robert James PHIPPS","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Nature has been employing attractive non-covalent interactions for billions of years to enable the chemical synthesis machinery of life that is enzymatic catalysis. In comparison, synthetic chemists have only very recently started to employ the very strongest, hydrogen bonds and ion pairs, to control selectivity in synthetic routes. This has occurred predominantly in the rapidly growing of enantioselective organocatalysis and has quickly had a huge impact. The vision of this grant is to take these insights and apply them to control of two more important aspects of selectivity facing the synthetic chemist – regioselectivity (positional selectivity within a particular functional group) and site-selectivity (within a wider molecule). These selectivity aspects are particularly relevant due to the increasing number of methods for functionalisation of C-H bonds, in which the overarching challenge is obtaining selectivity for one in the presence of many. 
My plan to achieve this will be divided into three main parts:
WP1. We will combine reactive and versatile transition metals with bespoke ligands which will interact with a common functional group in the substrate via a key non-covalent interaction. The resulting functionalisation of the substrate will be rendered pseudointramolecular, permitting regioselectivity or site-selectivity to be controlled through judicious catalyst design.
WP2. We will develop novel catalytic strategies to control the selectivity of intermolecular radical reactions. In radical chemistry, whilst reactivity is often high, low selectivity is often the limiting factor and as such often the most selective radical reactions are intramolecular ones. By intramolecularising radical reactions by the use of temporary non-covalent interactions we will solve outstanding problems in aromatic and aliphatic radical C-H functionalisation. 
WP3. The methods developed above will be applied to the late-stage functionalisation of pharmaceutically relevant molecules","1499756","2018-01-01","2022-12-31"
"NONEQ.STEEL","Controlling Non-Equilibrium in Steels","Maria Jesus Santofimia Navarro","TECHNISCHE UNIVERSITEIT DELFT","Stronger and more ductile steels are increasingly demanded for advanced applications. Latest investigations show that nanostructured steels formed by non-equilibrium phases increasing strength, such as martensite and bainite, and enhancing strain hardening, such as austenite, fulfil these demands with outstanding performance.
In the last few years, I have observed that non-equilibrium phases strongly affect each other’s formation and stability, with effects on the kinetics of the microstructure development. Thus, I theoretically and experimentally proved that carbon enrichment of austenite, essential for its stability at room temperature, occurs at a high rate via diffusion from martensite. Moreover, I showed that martensite triggers bainite formation, which significantly increases bainite kinetics. I believe that these interactions between non-equilibrium phases constitute a revolutionary tool for the development of nanostructured steels in the future.
This project addresses a new concept to create novel nanostructured steels in which the microstructure development is controlled by interactions between non-equilibrium phases. This innovative idea opens an unprecedented approach for the design of metallic alloys. Since interactions between phases affect each other’s formation and stability, the project focus on the fundamental study of nucleation and growth of non-equilibrium phases as well as on the analysis of interactions. Investigations will combine the integrated application of advanced experimental techniques with atomic and micro scale analysis of structures by simulations. The project continues with the local analysis of the effect of non-equilibrium phases on the mechanical properties of the steels. The identification and explanations of mechanisms will allow the creation of new nanostructured steels based on non-equilibrium phases’ interactions.","1482011","2012-10-01","2018-03-31"
"NoTape","Measuring with no tape","Søren HAUBERG","DANMARKS TEKNISKE UNIVERSITET","Society generates increasing amounts of data, which is both a resource and a challenge. The data reveal new insights that may potentially improve our livelihood, but their quantity renders such insights difficult to find. Machine learning techniques sift through the data looking for statistical patterns of interest to a given task. Due to an exponential growth in available data, these techniques enable us to automate difficult decisions, such as those needed for personalized medicine and self-driving cars.

NoTape note that machine learning techniques depend on a distance measure to determine which data points are similar and which are not. As this measure is difficult to choose, NoTape develop methods for estimating an optimal distance measure directly from data. Empirical evidence suggest that the optimal distance measure in one region of data space need not coincide with the optimal measure in another region, i.e.that the distance measure should locally adapt to the data. Local adaptability imply that the distance measure itself will be sensitive to noise in the data, and therefore should be described as a random variable. NoTape estimate distance measures as random Riemannian metrics and perform statistical data analysis accordingly. The notion of statistical computations with respect to an uncertain locally adaptive distance measure is uncharted territory, which need new algorithms for numerical integration and for solving differential equations.

As a guiding example, we estimate statistical models that reflect human perception. As perception processes are not fully understood, an optimal distance measure cannot be precisely estimated and the uncertainty of NoTape is needed.

The geometric nature of the developed methods ensure that attained models are interpretable by humans, which contrast current locally adaptive techniques. As society automate more decisions, interpretability is increasing important to ensure that the machine learning system can be trusted.","1463805","2017-12-01","2022-11-30"
"NovAnI","Indentification and optimisation of novel anti-infective agents using multiple hit-identification strategies","Anna Katharina Herta HIRSCH","HELMHOLTZ-ZENTRUM FUR INFEKTIONSFORSCHUNG GMBH","Given the rapid emergence of anti-infective resistance, drugs with a novel mode of action are urgently needed. Because of an exhaustion of existing strategies, a low return on investment and the fact that anti-infectives are difficult to develop (e.g., crossing the peculiar cell wall of Mycobacterium tuberculosis), promising un(der)explored targets and unconventional hit-identification strategies are needed.

I have selected three anti-infective targets based on their biochemical context for which few or no small-molecule inhibitors are known:

1) The antimalarial and antituberculotic drug target DXS is part of a unique biosynthetic pathway for pathogens that is absent in humans, thereby circumventing selectivity issues. Both diseases are a serious health threat with around 1.9 million deaths per year. 
2) Energy-coupling factor transporters are essential vitamin importers for pathogens such as Staphylococcus aureus, the causative agent of methicillin-resistant Staphylococcus aureus (MRSA) infections.
3) The DNA polymerase sliding clamp DnaN has polymerase and DNA repair activities and is an excellent drug target for the development of antibacterial agents against Gram-negative and –positive bacteria given the low incidence of resistance development.

I will address these targets, employing a unique combination of potentially synergistic hit-identification strategies that take into account protein flexibility, provide access to novel scaffolds and give me a cutting edge for the development of novel anti-infectives. 

This ERC proposal builds on my experience with the first two targets and provides an excellent platform for the new target DnaN. My expertise in synthetic organic and medicinal chemistry and established hit-identification strategies together with my collaborations with protein crystallographers, biochemists and pharmacologists place me in an excellent position for not only achieving the goals of this interdisciplinary proposal but also going beyond it.","1499367","2018-02-01","2023-01-31"
"NOVELNOBI","Novel Nanoengineered Optoelectronic Biointerfaces","Sedat Nizamoglu","KOC UNIVERSITY","Interfacing with neural tissues is an important scientific goal to understand cellular processes and to combat nervous-system related diseases. Nanotechnology has a significant potential for the development of new neural interfaces. The atomic-level design and control of the nanostructures for neural interfacing can revolutionize the junction between neurons and nanomaterials. In this project, we propose a totally new approach for understanding fundamental requirements and from this knowledge designing customised nanomaterials with optimised characteristics. These will be used to develop and demonstrate unconventional neural interfaces that are ultimately designed, controlled and constructed at the nanoscale. Hence, the key objectives of this proposal are: (1) to use quantum mechanics in a new way to control and explore the neural photostimulation mechanism, (2) to explore, design and synthesize new biocompatible colloidal nanocrystals for neural photostimulation, to overcome the limitations in terms of toxic material contents (e.g., cadmium, lead, mercury, etc.), (3) to demonstrate novel biocompatible neural interfaces with exciton and quantum funnels, and plasmonic nanostructures for enhanced spectral sensitivity and dynamic range. This new approach from quantum mechanical design to nanocrystal assembly will enable exploring, tuning and controlling the underlying physical mechanisms of neural photostimulation. Furthermore, the biocompatible nanomaterials will result in a more reliable nanobiojunction. The funnel and plasmon structures will lead to unprecedented spectral sensitivities and dynamic ranges that are far beyond the state-of-the-art optoelectronic interfaces. The project is therefore expected to have high impact and may herald a new paradigm in neural interfacing. NOVELNOBI is expected to attract significant attention of researchers from diverse fields such as photonics, nanomaterials, photomedicine and neuroscience.","1500000","2015-07-01","2020-06-30"
"NOVIB","The Nonlinear Tuned Vibration Absorber","Gaetan Kerschen","UNIVERSITE DE LIEGE","""Even after more than one century of flight, both civil and military aircraft are still plagued by major vibration problems. A well-known example is the external-store induced flutter of the F-16 fighter aircraft. Such dynamical phenomena, commonly known as aeroelastic instabilities, result from the transfer of energy from the free stream to the structure and can lead to limit cycle oscillations, a phenomenon with no linear counterpart. Since nonlinear dynamical systems theory is not yet mature, the inherently nonlinear nature of these oscillations renders their mitigation a particularly difficult problem. The only practical solution to date is to limit aircraft flight envelope to regions where these instabilities are not expected to occur, as verified by intensive and expensive flight campaigns. This limitation results in a severe decrease in both aircraft efficiency and performance.

At the heart of this project is a fundamental change in paradigm: although nonlinearity is usually seen as an enemy, I propose to control - and even suppress - aeroelastic instability through the intentional use of nonlinearity. This approach has the potential to bring about a major change in aircraft design and will be achieved thanks to the development of the nonlinear tuned vibration absorber, a new, rigorous nonlinear counterpart of the linear tuned vibration absorber. This work represents a number of significant challenges, because the novel functionalities brought by the intentional use of nonlinearity can be accompanied by adverse nonlinear dynamical effects. The successful mitigation of these unwanted nonlinear effects will be a major objective of our proposed research; it will require achieving both theoretical and technical advances to make it possible. A specific effort will be made to demonstrate experimentally the theoretical findings of this research with extensive wind tunnel testing and practical implementation of the nonlinear tuned vibration absorber.
Finally, nonlinear instabilities such as limit cycle oscillations can be found in a number of non-aircraft applications including in bridges, automotive disc brakes and machine tools. The nonlinear tuned vibration absorber could also find uses in resolving problems in these applications, thus ensuring the generic character of the project.""","1316440","2012-09-01","2017-08-31"
"NOWIRE","Network Coding for Wireless Networks","Christina Fragouli","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Our goal is to develop fundamentally new architectures for wireless networks that offer the convenience of wireless communication while achieving the performance, predictability and security of wired networks. The wireless channel is inherently a shared medium characterized by limited resources and complex signal interactions between transmitted signals. The question we address is how do we transmit information over wireless and how do we exploit the wireless channel properties to share its resources. Ours is a fundamentally different approach to existing strategies, that builds on new physical and packet layer sharing and cooperation paradigms that we have been working on, to extract the optimal throughput and reliability performance from the wireless medium. These are recent breakthroughs in (i) network coding and (ii) wireless cooperation. Network coding is a new area bringing a novel paradigm for network information flow that enables cooperation at a packet level to optimally share the network resources. Deployment of the first network coding ideas in wireless have already indicated benefits as large as a factor of ten in terms of throughput. Complex signal interactions caused by the inherent broadcast nature of wireless channels, is traditionally viewed as an impediment to be mitigated. Recently it has been demonstrated that one can utilize interference to develop cooperation at the wireless signal level (physical layer) for arbitrary wireless networks. This can give significant capacity advantages over techniques that mitigate interference. Both these ideas can radically affect the way information is communicated, stored and collected, and can revolutionize the design of future wireless networks. In this project we plan to addess several fundamental questions that develop on these themes. We take a complete view of these ideas by not only developing the underlying theory but also through validation on wireless testbeds.","1771520","2009-09-01","2014-08-31"
"NPFLAVOUR","The Flavour of New Physics","Luca Silvestrini","ISTITUTO NAZIONALE DI FISICA NUCLEARE","The Standard Model (SM) of electroweak and strong interactions, supplemented by neutrino masses, provides an extremely successful description of all available experimental data in elementary particle physics. However, the basic origin of electroweak and flavour symmetry breaking remains largely unknown, as well as the mechanism stabilizing the electroweak scale. In the coming years, direct searches at LHC will shed light on electroweak symmetry breaking and probe new physics models with new particles up to the TeV scale. At the same time, an impressive amount of data in the flavour sector will be collected at dedicated experiments such as LHCb, Super B-factories, MEG, NA62 and others. This upcoming experimental information sets the stage for the present project, which aims at: i) providing the theoretical tools needed to fully exploit experimental data in the flavour sector, implementing state-of-the art calculations in a consistent framework within the SM or in any New Physics (NP) model discovered by (or not ruled out by) direct searches; ii) determining the flavour structure of the effective Lagrangian that describes energies up to the TeV scale and above, combining direct searches with flavour data; iii) searching for a fundamental mechanism of flavour symmetry breaking that can justify the flavour structure of TeV-scale physics. This fundamental mechanism should address both the SM flavour puzzle, i.e. the origin of masses and mixings of quarks and leptons, and the NP flavour puzzle, i.e. the mechanism protecting TeV-scale NP from causing large deviations from the SM predictions in the flavour observables we have measured so far. The main support requested to the ERC is for hiring six experienced researchers, the rest of the funds are for optimizing the effectiveness of the team and the research
environment.","1258920","2011-11-01","2017-10-31"
"NPRGGLASS","Non Perturbative Renormalization Group Theory of Glassy Systems","Giulio Biroli","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","""Glassy systems are central in several fields from statistical mechanics and soft matter to material sciences and biophysics and they appear even in completely different areas of science such as information theory, computer science, agent-based models and game theory.
The aim of this project is to develop a new, possibly groundbreaking, approach to glassy systems based on the non-perturbative renormalization group (NPRG) formalism. Modern theoretical approaches to glassy systems suffer from severe limitations; it is not clear whether and how one can improve them, and their current status is far from providing a coherent and satisfactory theory.  For reasons detailed below, I believe that the NPRG approach is the long-sought theoretical framework to tackle the glass problem and that it will eventually lead to its solution. I will focus on the problem of the glass transition and the physics of glass-forming liquids. I expect that the progress we will make in this direction will also be instrumental also for other glassy systems such as spin glasses, quantum glasses and jamming systems.""","1010800","2011-11-01","2017-10-31"
"NSECPROBE","Probing quantum fluctuations of single electronic channels in model interacting systems","Carles Oriol Altimiras Martin","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","The fluctuation-dissipation theorem is a prominent milestone in Physics: It links the dissipative response of a physical system to its fluctuations, and provides a microscopic understanding of macroscopic irreversibility. Recent theoretical advances that have generalized the original fluctuation-dissipation theorem to non-linear quantum systems even far from equilibrium, ask for an experimental test, which is the aim of the project. We will measure the current fluctuations and dissipative response of driven quantum systems whose non-linearity arises from strong interactions. We will exploit the flexibility offered by nano-patterned high purity 2D electron gases in order to realize single electron channels in different regimes: 1/ interacting strongly with a single electromagnetic mode (Dynamical Coulomb Blockade of a quantum point contact), 2/ interacting with a single magnetic impurity (Kondo effect in quantum dots), 3/ driving the 2D gas in the fractional quantum Hall effect where current is carried by strongly correlated 1D channels prototypical of Luttinger liquids. Last, we will address a fundamental issue raised in the early days of quantum mechanics: how long does it take for a particle to cross a classically forbidden barrier? While Wigner-Smith’s theorem links the issue to the density fluctuations within the barrier, the fluctuation-dissipation theorem links it further to a quantum relaxation resistance. A full investigation of fluctuation-dissipation relations including quantum effects requires measurements at frequencies hf>k_BT. With the available dilution refrigeration techniques it implies measuring in the few GHz range. Since quantum conductors have an impedance h/e^2~25.8 kohm much larger than the 50ohm impedance of microwave components, new microwave methods able to deal with large impedance values will be developed. They will be based on the extension to finite magnetic field of the wide-band impedance matching methods recently developed by the PI.","1500000","2015-05-01","2020-04-30"
"NU-CLEUS","Exploring coherent neutrino-nucleus scattering with gram-scale cryogenic calorimeters","Raimund STRAUSS","TECHNISCHE UNIVERSITAET MUENCHEN","ν-cleus will be a new multi-purpose table-top experiment aimed at the first exploration of coherent neutrino-nucleus scattering (CNNS) at a nuclear power reactor. Our novel detector technology will achieve an unprecedentedly high sensitivity to new physics within and beyond the Standard Model of Particle Physics, with an enormous discovery potential. The new method is not only complementary to competing approaches, but superior in terms of performance, cost and size. 

The ultra-low threshold character of my experiment will allow a determination of the Weinberg angle at MeV-scale momentum transfers and the first direct search for eV-scale sterile neutrinos via CNNS. We will significantly improve the sensitivity for a neutrino magnetic dipole moment, unravel anomalies in the reactor antineutrino spectrum and test new models for exotic neutral currents. 

My research on gram-scale cryogenic calorimeters (gramCCs) has resulted in a recent breakthrough: we achieved the world-best energy threshold for nuclear-recoils of 19.7eV, one order of magnitude lower than for previous detectors. I propose to operate gramCCs within a fiducial-volume cryogenic detector. This completely new detector concept is suited for an above-ground operation of excellent performance while backgrounds are significantly suppressed. Located at a nuclear power reactor ν-cleus will achieve a signal-to-background ratio of ~10^3 - a unique situation in neutrino physics. This will enable a rapid discovery of CNNS within a few weeks. 

ν-cleus will have enormous impact on modern physics and future technologies. It will be a prototype for next-generation, high-precision solar neutrino experiments and guarantees a technological spin-off for reactor safeguards and non-proliferation measures. With this ERC grant I will set up a high-class research team with world-leading expertise in cryogenic detectors and low-background techniques, which will ensure Europe’s role as a pioneer in this new field.","1642500","2019-04-01","2024-03-31"
"NUCLEAREFT","Nuclear Physics from Quantum Chromodynamics","Evgeny Epelbaum","RUHR-UNIVERSITAET BOCHUM","Explaining low-energy nuclear structure from Quantum Chromodynamics, the
underlying theory of the strong interaction, is one of the major
challenges in contemporary theoretical nuclear and particle physics.
What is needed is, on the one hand, a detailed quantitative understanding of the
interaction between baryons, the relevant effective degrees
of freedom for the problem at hand, based on Quantum Chromodynamics. On the
other hand, a microscopic description of strongly interacting baryons requires
reliable methods to deal with the quantum mechanical few- and many-body problems.
The proposed research addresses both of the two challenges aiming to
achieve a precise, quantitative description of nuclear forces and the
properties of light nuclei and hyper-nuclei firmly rooted in the symmetries of
Quantum Chromodynamics. These goals will be reached by using analytical
methods based on chiral effective field theory combined with large-scale
numerical simulations on high-performance computers.","1165864","2011-01-01","2015-12-31"
"NUCLEOPOLY","DNA Block Copolymers: New Architectures and Applications","Andreas Herrmann","RIJKSUNIVERSITEIT GRONINGEN","With our contributions to DNA block copolymers (DBCs), we have opened a new field of interdisciplinary research at the intersection of polymer chemistry, biology and nanoscience. Within this proposal, we intend to apply our expertise with linear DBCs to new nucleocopolymer architectures ranging from star polymers to DNA networks. Our efforts will not only explore new covalently-bonded polymer topologies but also extend the range of self-assembled supramolecular structures accessible with DBCs. Current progress in this direction has yielded spherical and rod-like DBC micelles. In this proposal we further envisage membranes and vesicles generated by macromolecular DNA amphiphiles. A special focus will be the manipulation of the permeability of these structures by hybridization and the insertion of channel proteins. A major part of the proposal addresses potential applications of DBC architectures in the fields of nucleic acid detection and drug delivery. We will produce selective and sensitive nucleic acid probes employing DBCs with highly emissive conjugated polymer segments or based on novel fluorogenic DNA-templated reactions. Plans for potential delivery systems include the establishment of a DBC-based technology platform to allow combinatorial testing of micelle structures equipped with improved targeting, drug loading and stealth functions. For this purpose, the DNA shell of the nanoscopic aggregates will be exploited for its biological activity in the context of antisense and small interfering RNA activity as well as immune stimulation. Finally, we will employ DBC micelles as programmable nanoreactors within the complex environment of living cells and even carry out sequence-specific organic transformations induced by the cell s own messenger RNA","1500000","2009-11-01","2014-10-31"
"nuDirections","New Directions in Theoretical Neutrino Physics","Joachim Kopp","EUROPEAN ORGANIZATION FOR NUCLEAR RESEARCH","""Thanks to tremendous advances in terrestrial, astrophysical and cosmological experiments, neutrino physics has again become one of the driving forces of progress in astroparticle physics. The proposed project nuDirections provides the indispensable theoretical counterpart to the rapid experimental developments. Our goal is to investigate from a theoretical point of view a multitude of unexplored phenomena within and beyond the Standard Model of particle physics that are now becoming experimentally accessible in new neutrino experiments. The three main pillars of the project are: (1) Light sterile neutrinos. With hypothetical eV-scale sterile neutrinos coming under intense scrutiny by new experiments, sophisticated global fits will remain a linchpin for the theoretical interpretation of experimental data. We plan to carry out these fits using upgrades of our world-leading numerical codes, and to use our results as guidelines for exploring new theoretical models featuring sterile neutrinos as part of a larger """"hidden sector"""" of particle physics. This includes in particular the unique phenomenology of self-interacting sterile neutrinos. (2) Decoherence effects in dense neutrino gases. As neutrinos propagate, coherence between different mass eigenstates is eventually lost due to their different group velocities. We will demonstrate that decoherence can completely modify neutrino oscillations in dense environments such as supernovae or the early Universe. Mapping the rich phenomenology of decoherence effects in neutrino oscillations thus has the potential to play a game-changing role in the physics of supernova neutrinos. (3) Neutrinos and dark matter. We plan to develop a new mechanism for the production of sterile neutrino dark matter in the early Universe and to play a leading role in the theory and phenomenology of neutrino signals from dark matter annihilation or decay.
""","806600","2015-09-01","2020-08-31"
"NURE","Nuclear Reactions for Neutrinoless Double Beta Decay","Manuela CAVALLARO","ISTITUTO NAZIONALE DI FISICA NUCLEARE","Neutrinoless double beta decay (0νββ) is considered the best potential resource to determine the absolute neutrino mass scale. Moreover, if observed, it will signal that the total lepton number is not conserved and neutrinos are Majorana particles. Presently, this physics case is one of the most important research “beyond the Standard Model” and might guide the way towards a Grand Unified Theory of fundamental interactions.
Since the ββ decay process involves nuclei, its analysis necessarily implies nuclear structure issues. The 0νββ decay rate can be expressed as a product of independent factors: the phase-space factors, the nuclear matrix elements (NME) and a function of the masses of the neutrino species.Thus the knowledge of the NME can give information on the neutrino mass, if the 0νββ decay rate is measured.
The novel idea of NURE is to use nuclear reactions of double charge-exchange (DCE) as a tool to determine the ββ NME. In DCE reactions and ββ decay, the initial and final nuclear states are the same and the transition operators have the same spin-isospin structure. Thus, even if the two processes are mediated by different interactions, the NME are connected and the determination of the DCE cross-sections can give crucial information on ββ matrix elements. 
NURE plans to carry out a campaign of experiments using accelerated beams on different targets candidates for 0νββ decay. The DCE channel will be populated using (18O,18Ne) and (20Ne,20O) reactions by the innovative MAGNEX large acceptance spectrometer, which is unique in the world to measure very suppressed reaction channels at high resolution. The complete net involving the single charge-exchange and multi-step transfers characterized by the same initial and final nuclei will be also measured to study the reaction mechanism. The absolute cross-sections will be extracted. The comparison with microscopic state-of-the-art calculations will give access to the NMEs.","1272000","2017-04-01","2022-03-31"
"NUSIKIMO","Numerical simulations and analysis of kinetic models - Applications to plasma physics and Nanotechnology","Francis Filbet","UNIVERSITE LYON 1 CLAUDE BERNARD","This project is devoted to the mathematical and numerical analysis in statistical physics with a special interest to applications in Plasma Physics and nanotechnology with Micro Electro Mechanical Systems (MEMS). We propose to achieve numerical simulations in plasma physics by fully deterministic methods. Using super-computers, a non stationary collisional plasma can be modelled taking into account Coulombian interactions and self-consistent electromagnetic fields to study different regimes and instabilities. These methods are based on high order and conservative finite volume schemes for the transport and fast multi-grid methods for the treatment of collisions. The first application is the simulation of fast ignition or Inertial Confinement Fusion, which is an important issue in plasma physics. Here, the main difficulty concerns the modelling of collisions of relativistic particles and the development of new algorithms for their treatment. Another part is devoted to the derivation of moments models which require less computational effort but keep the main properties of the initial models. The second application concerns micro and nanotechnologies, which are expected to play a very important role in the development of MEMS. Since the scale of micro flows is often comparable with the molecular mean free path, it is necessary to adopt the point of view of kinetic theory. Then applications of kinetic theory methods to micro flows are becoming very important and an accurate approximation of the Boltzmann equation is a key issue. Even nowadays a deterministic numerical solution of the Boltzmann equation still represents a challenge for scientific computing. Recently, a new class of algorithms based on spectral techniques in the velocity space has been been developed for the trend to equilibrium. The next important step is to treat applications for MEMS in nanotechnology for which the main difficulty is to treat complex geometries and moving boundary problems.","490000","2010-01-01","2014-12-31"
"NWScan","Bottom-up Nanowires as Scanning Multifunctional Sensors","Martino Poggio","UNIVERSITAT BASEL","Advances in growth and fabrication of semiconductor nanostructures have led to both the production of exquisitely sensitive force transducers and the development of solid-state quantum devices. Force transducers, typically monolithic Si cantilevers, are central to techniques such as AFM, and MFM. On the other hand, quantum devices including quantum wells, quantum dots (QDs), and single electron transistors are essential to technologies like lasers, optical detectors, and in experiments on quantum information. These two types of devices have – until now – occupied distinct material systems and have, for the most part, not been combined.

New developments in the growth of inorganic nanowires (NWs), however, are set to change the status quo. Researchers can now grow nanoscale structures from the bottom-up with unprecedented mechanical properties. Unlike traditional top-down cantilevers, which are etched or milled out of a larger block of material, bottom-up structures are assembled unit-by-unit to be almost defect-free on the atomic-scale. This near perfection gives NWs a much smaller mechanical dissipation than their top-down counterparts, while their higher resonance frequencies allow them to couple less strongly to common sources of noise. Meanwhile, layer-by-layer growth of NWs is rapidly developing such that both axial and radial heterostructures have now been realized. Such fine control allows for band-structure engineering and the production of devices including FETs, single photon sources, and QDs. NWs are also attractive hosts for optical emitters as their geometry favors the efficient extraction of photons.

These properties and the fact that a NW can be integrated as the tip of an SPM make NWs extremely promising devices. We propose to develop the use of NWs as scanning multifunctional sensors. We intend to 1) use NW cantilevers as force transducers in high-resolution scanning force microscopy, and 2) use NW quantum devices as scanning sensors.","1480680","2013-11-01","2018-10-31"
"O2RIGIN","From the origin of Earth's volatiles to atmospheric oxygenation","Stephan König","EBERHARD KARLS UNIVERSITAET TUEBINGEN","Aim of this project is to understand the connection between endogenic and exogenic processes of our planet that led to the redox contrast between Earth’s surface and interior. For this purpose the time constraints on atmospheric oxygenation can be refined and for the first time linked with a new approach to Earth’s endogenic processes like plate tectonics, mantle melting, volcanism, continent formation and subduction-related sediment- and crust recycling. These objectives will be achieved by using the unique geochemical capabilities of the selenium (Se) isotope system to unlock the geological record of changing oxygen fugacities in the mantle-crust-atmosphere reservoirs. The power of the Se isotope system lies in its redox sensitivity and in the volatile and highly siderophile/chalcophile character of elemental Se. This links Se to the evolution of other volatiles during key geological processes from Earth formation ca. 4.5 Ga ago until today. The occurrence and behavior of Se is fully controlled by accessory micrometric sulfide minerals in the silicate Earth, which may conserve their original Se isotopic signatures over large geological timescales and can be dated via the 187Re-187Os geochronometer. This offers high resolutions in time and space that are groundbreaking for research on Earth System Oxygenation. Covering Earth geologic history, new high-precision Se isotope data of the sedimentary and representative mantle-derived magmatic rock record from all major plate tectonic settings will be combined with the mineral-scale record of robust and global “time capsules” such as diamond inclusions. Once the evolution into todays dynamic Earth’s Redox System is understood, the investigation will be pushed back in time to Earth’s formation. This involves a reconciliation of the meteoritic and Archean rock and mineral-scale Se isotope record to constrain the origin of volatiles essential for the oceans, generation of an atmosphere and development of life on our planet.","1498353","2015-03-01","2021-02-28"
"OASIS","Organic/inorganic hybrids for solution-processable photonic structures","Natalie Stingelin","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","The realisation that modulated light pulses can be confined over long distances with minimum losses within a structure that comprises a controlled spatial distribution of the refractive index n – as, e.g. in optical fibres – has, without doubt, underpinned the telecommunications revolution witnessed during the 20th century. The refractive index n, quantifying how light propagates in a given medium, has as a consequence become one of the most important materials properties in designing photonics products. The other key information for most optical and photonic applications is to know how much light is absorbed by a material. This is described by the extinction coefficient κ. There is, though, an apparent lack of solution-processable systems of  κ close to 0 (i.e. are transparent) whilst n can be manipulated over a broad window – a bottleneck that has rendered fabrication of a range of optical structures impracticable, if not impossible. Here we address this issue and advance versatile, solution-processable polymer/inorganic hybrids whose refractive index n can be tuned over a wide range without compromising their transparency nor processability.

The programme will develop in three directions: i) the design of novel, solution-processable molecular hybrids; ii) the development of (nano-)fabrication technologies for the deposition and/or patterning of such hybrids; and iii) extension of the range of currently explored photonic crystals to entirely new optical devices.

Hence, we have identified a clear need for new materials with increased optical functionalities, and novel concepts and approaches that will allow simple fabrication of structures to light. Key objective for the proposed programme thus is to advance new hybrids, develop a deeper understanding of key structure-property interrelationships of inorganic/organic hybrids and develop novel photonic architectures.","1242478","2011-10-01","2016-09-30"
"OBSERVABLESTRING","The Low Energy Limit of String Theory and the Observable World","Mariana Grana","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","The long-standing challenge of string theory, confronting the real world, has become more pressing and at the same time tangible in view of the upcoming LHC. Since the low energy limit of the theory is the main stage where predictions can be compared with experimental data, the goal of this project is to develop a new unified framework to formulate, compute and analyze this limit and its phenomenology. Understanding the low energy limit of string theory at the level where it can be confronted with precision experiments requires two key elements. On one hand one must obtain the full low energy Lagrangians resulting from compactifications from ten to four dimensions. On the other hand, one must analyze the couplings of quarks and leptons, represented by open strings attached to branes.  Attempts to construct four-dimensional effective theories have focused in the past on a particular class of six-dimensional spaces, but my work in the last few years has shown that realistic solutions arise from manifolds whose differential properties are actually much weaker and that these compactifications have an elegant reformulation in terms of a generalized version of Riemannian geometry. I plan to use the formalism of generalized geometry to obtain the full tree level, perturbative and non-perturbative corrections to the 4D LEEL resulting from compactifications on these manifolds, and to study their phenomenology. Obtaining the full LEEL is the key step towards understanding if the world as we see it today comes from a string theory compactification: only full knowledge of the Lagrangian allows us to determine in detail how these manifolds lead to theories having 4D isolated vacua with a tiny positive cosmological constant, and support branes whose gauge theory spectrum and couplings are those of the Standard Model. Furthermore, the LEEL will be compared with the data of tomorrow: masses and couplings of supersymmetric partners, if supersymmetry is found at the LHC.","945000","2011-02-01","2016-09-30"
"OceaNice","Paleoceanography of the Ice-proximal Southern Ocean during Past Warm Climates","Peter BIJL","UNIVERSITEIT UTRECHT","Antarctic ice sheets are destabilizing because Southern Ocean warming causes basal melt. It is unknown how these processes will develop during future climate warming, which creates an inability to project ice sheet melt and thus global sea level rise scenarios into the future. Studying past geologic episodes, during which atmospheric carbon dioxide levels (CO2) were similar to those projected for this century and beyond, is the only way to achieve mechanistic understanding of long-term ice sheet- and ocean dynamics in warm climates. Past ocean-induced ice sheet melt is not resolved because of a paucity of quantitative proxies for past ice-proximal oceanographic conditions: sea ice, upwelling of warm water and latitudinal temperature gradients. This hampers accurate projections of future ice sheet melt and sea level rise.

OceaNice will provide an integral understanding of the role of oceanography in ice sheet behavior during past warm climates, as analogy to the future. I will quantify past sea ice, upwelling of warm water and latitudinal temperature gradients in three steps:
1. Calibrate newly developed dinoflagellate cyst and biomarker proxies for past oceanographic conditions to glacial-interglacial oceanographic changes. This yields quantitative tools for application further back in time.
2. Apply these to two past warm climate states, during which CO2 was comparable to that of the future under strong and moderate fossil fuel emission mitigation scenarios.
3. Interpolate between new reconstructions using high-resolution ocean circulation modelling for circum-Antarctic quantification of past oceanographic conditions, which will be implemented into new ice sheet model simulations.

The groundbreaking new insights will deliver mechanistic understanding and quantitative estimates of ice-proximal oceanographic changes and consequent ice sheet melt during past warm climates, which will finally allow accurate future sea level rise projections given anticipated warming.","1500000","2019-02-01","2024-01-31"
"ODDSUPER","New mechanisms and materials for odd-frequency superconductivity","Annica BLACK-SCHAFFER","UPPSALA UNIVERSITET","Odd-frequency superconductivity is a very unique superconducting state that is odd in time or, equivalently, frequency, which is opposite to the ordinary behavior of superconductivity. It has been realized to be the absolute key to understand the surprising physics of superconductor-ferromagnet (SF) structures and has also enabled the whole emerging field of superconducting spintronics. This project will discover and explore entirely new mechanisms and materials for odd-frequency superconductivity, to both generate a much deeper understanding of superconductivity and open for entirely new functionalities. Importantly, it will generalize and apply my initial discoveries of two new odd-frequency mechanisms, present in bulk multiband superconductors and in hybrid structures between topological insulators and conventional superconductors, respectively. In both cases odd-frequency superconductivity is generated without any need for ferromagnets or interfaces, completely different from the situation in SF structures. The result will be a significant expansion of the concept and importance of odd-frequency superconductivity to a very wide class of materials, ranging from multiband, bilayer, and nanoscale superconductors to topological superconductors. The project will also establish the connection between topology and odd-frequency pairing, which needs to be addressed in order to understand topological superconductors, as well as incorporate new materials and functionality into traditional SF structures. To achieve these goals the project will develop a novel methodological framework for large-scale and fully quantum mechanical studies with atomic level resolution, solving self-consistently for the superconducting state and incorporating quantum transport calculations.","1121660","2018-02-01","2023-01-31"
"ODYCQUENT","Optimal dynamcal control of quantum entanglement","Florian Mintert","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","The present proposal aims at the optimal control of the preparation, distribution and storage
of entangled states of multipartite quantum systems.
We will employ our recently developed observable entanglement measures to dynamically optimize entanglement per se
rather than fidelity with respect to some specified entangled target state.
We will apply these tools to NV-centers and trapped ions in order to devise time-dependent control pulses
(e.g.) laser pulses) that steer these systems into highly entangled states.
In a second branch we will extend our theoretical tools for entanglement control
in order to achieve close-to-perfectly correlated states even in systems with strong decoherence.","1173240","2011-01-01","2016-08-31"
"ODYSSEY","Open dynamics of interacting and disordered quantum systems","John GOOLD","THE PROVOST, FELLOWS, FOUNDATION SCHOLARS & THE OTHER MEMBERS OF BOARD OF THE COLLEGE OF THE HOLY & UNDIVIDED TRINITY OF QUEEN ELIZABETH NEAR DUBLIN","This research proposal focuses on the open quantum system dynamics of disordered and interacting many- body systems coupled to external baths. The dynamics of systems which contain both disorder and interactions are currently under intense theoretical investigation in condensed matter physics due to the discovery of a new phase of matter known as many-body localization. With the experimental realization of such systems in mind, this proposal addresses an essential issue which is to understand how coupling to external degrees of freedom influences dynamics. These systems are intrinsically complex and lie beyond the unitary closed system paradigm, so the research proposed here contains interdisciplinary methodology beyond the mainstream in condensed matter physics ranging from quantum information to quantum optics. The project has three principal objectives each of which would represent a major contribution to the field:

O1. To describe the dynamics of a interacting, disordered many-body systems when coupled to external baths.
O2. To perform a full characterization of spin and energy transport in their non-equilibrium steady state.
O3. To explore the system capabilities as steady state thermal machine from a systematic microscopic perspective.

This will be the first comprehensive study of the open system phenomenology of disordered interacting many-body
systems. It will also allow for the systematic study of energy and spin transport and the exploration of the potential of these systems as steady state thermal machines. In order to successfully carry out the work proposed here, the applicant will build a world class team at Trinity College Dublin. Due to his track record and interdisciplinary background in many-body physics, quantum information and statistical mechanics combined with his personal drive and ambition the applicant is in a formidable position to successfully undertake this task with the platform provided by this ERC Starting Grant.","1333325","2018-07-01","2023-06-30"
"OldCO2NewArchives","CO2 reconstruction over the last 100 Myr from novel geological archives","James Rae","THE UNIVERSITY COURT OF THE UNIVERSITY OF ST ANDREWS","CO2 exerts a major control on Earth’s environment, including ocean acidity and global climate.  Human carbon emissions have elevated CO2 levels to above 400 ppm, substantially higher than at any time in the 800,000 year ice core record.  If we want to understand how Earth’s environment and climate will respond to a high CO2 world, we need to look deeper into the geological past.  This project provides a novel way to reconstruct ocean pH and atmospheric CO2 levels over the last 100 Myr.  This will allow us to fathom the fundamental mechanisms governing Earth’s environmental evolution, and improve predictions of environmental response to CO2 change in the future.
 
Atmospheric CO2 and ocean pH are closely coupled, because CO2 is acidic and is readily exchanged between the ocean and atmosphere. If ocean pH is known, we can place strong constraints on atmospheric CO2. Thanks to recent developments in geochemistry, it is possible to reconstruct changes in ocean pH using the boron isotope composition (d11B) of fossil shells. The well-studied systematics of this method and its underlying thermodynamic framework provide confidence in its application to the geological record. However calculation of pH from carbonate d11B requires knowledge of the boron isotope composition of past seawater d11Bsw. Here I propose novel strategies and techniques with new or underutilized archives (evaporites, shallow carbonates, and infaunal foraminifera) to constrain this crucial parameter.
 
With d11BSW constrained, new d11B records from benthic foraminifera will provide a 100 Myr record of ocean pH. This benchmark reconstruction will be used to test key hypotheses on major environmental change in the geological record, and to constrain atmospheric CO2 using a state-of-the-art biogeochemical model. These paired data and modelling outcomes will provide a major step forward in our understanding of the fundamental processes regulating Earth’s climate and long-term habitability.","1996784","2019-02-01","2024-01-31"
"OMICON","Organic Mixed Ion and Electron Conductors for High-Energy Batteries","Stefan Freunberger","TECHNISCHE UNIVERSITAET GRAZ","Energy storage is undeniably amongst the greatest societal challenges. Batteries will be key enablers but require major progress. Battery materials that promise a step-change in energy density compared with current Li-ion batteries rely on fundamentally different reactions to store charge, e.g. Si alloying or O2 reduction instead of intercalation. They have in common high volume changes on cycling and poor conductivity. For the active component of a battery electrode to function it must be simultaneously in contact with ionic and electronic pathways to electrolyte and current collector. State-of-the-art conducting additives and binders in the composite electrodes cannot ensure ideal contact for such materials and fail to exploit their full potential.

In this project I directly target these fundamental challenges of high-energy batteries by replacing now used conducting additives and binders with flexible organic mixed ion and electron conductors that follow volume changes to ensure at any stage intimate contact with ions and electrons. This requires progress with the fundamental science of such conductors, for which to achieve we develop and combine synthetic, electroanalytic and spectroscopic methods, aided by theory. Mixed conducting polymer gels, designed for the particular storage material, shall be elaborated for two ultra-high capacity electrodes, the O2 cathode and the Si anode.

The significant advantage, next to intimate contact, is that the packing density of active material can be maximized. This boosts energy stored by total electrode mass and volume by rigorously cutting the amount of non-active materials compared with current approaches. The expected overriding scientific impact includes improved understanding of mixed conductors concerning synthesis, structure, conductivity and their behaviour in the complex battery environment. This opens up new perspectives for the realm of high-capacity battery materials that demand such a breakthrough to succeed.","1494254","2015-04-01","2020-03-31"
"ONEDEGGAM","The search for new physics through precision measurements of the CKM angle gamma","Sneha MALDE","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","There is a strong conviction that the Standard Model of particle physics is only a low energy approximation to a higher energy theory containing new fundamental particles. For example, it is not possible to explain the large asymmetry between the properties of matter and anti-matter that must exist to create the universe within we live, with the Standard Model that can only accommodate asymmetries orders of magnitude smaller. The theoretical framework that describes these asymmetries is the Cabibbo-Maskawa-Kobayashi (CKM) matrix. 
 
Through study the of the differences between the decay of certain types of hadrons containing a beauty quark, and the corresponding anti-particle decays, this project will lead to a precision measurement of a phase commonly called “gamma” which is related to some of the elements of the CKM matrix. 
 
The beauty hadron decay chain involves subsequent decay of charm hadrons, which are not well understood. By understanding this part of the process, in a larger range of decay modes and with significantly enhanced sensitivity than previously possible, the overall understanding of the beauty hadron decay chain is improved. Using the distinctive data collected by the BESIII in China, it is possible to make a number of new measurements that relate to the decay of charm hadrons. With this knowledge it becomes possible to gain considerably more sensitivity from the copious amounts of beauty hadron decays that will be collected by the LHCb experiment at CERN and the Belle II experiment in Japan over the timescale of this project.
 
This new strategy to exploit the synergy between the different experiments means that a global precision of 1° is within reach. This precision has excellent potential to uncover significant discrepancies within the CKM matrix that can only be explained by physics beyond the standard model. This would launch particle physics into the next era of discovery.","1499955","2018-06-01","2023-05-31"
"OOID","The Ocean's Oxygen Isotopes Deciphered: Combining Observations, Experiments and Models","Itay HALEVY","WEIZMANN INSTITUTE OF SCIENCE LTD","The isotopic composition of O in seawater is a fundamental property of Earth's oceans, key to paleoclimate reconstructions and to our understanding of the origin of water on Earth, the water-rock reactions that govern seawater chemistry, and the conditions under which life emerged. Despite more than five decades of research, the geologic history of seawater 18O/16O remains a topic of intense debate. Without exception, well-preserved 18O/16O records from marine precipitates reflect both the minerals' formation temperature, and the isotopic composition of seawater. This duality has prevented unique interpretation of a long-term secular trend, in which 18O/16O in sedimentary rocks (e.g., carbonates, cherts) has increased by ~15 ‰ since the Archean. Here I outline an inter-disciplinary research program to address this fundamental problem, which integrates new geochemical observations, laboratory experiments, and numerical models.

We will generate geologic records of 18O/16O in two previously untapped repositories: iron oxides and iron-bearing authigenic clays. Several characteristics of both, and preliminary results, suggest that these repositories hold the potential to settle the long-standing debate about seawater 18O/16O. We will determine the temperature dependence of mineral-water O isotope fractionation in laboratory experiments and observations of natural systems. We will experimentally test the resistance of these minerals to O isotope exchange under geologically-relevant conditions, with the aim of evaluating the potential for late-stage isotopic resetting. Finally, we will develop models of the marine O isotope cycle, which account for the processes that govern seawater 18O/16O over long timescales, and which will be used to provide a quantitative understanding of the new records. With these new insights, we will explore implications for the geologic history of seawater chemistry, atmospheric composition, climate and biology.","1490596","2018-09-01","2023-08-31"
"OPENGWTRIANGLE","Three ideas in open Gromov-Witten theory","Jake P. Solomon","THE HEBREW UNIVERSITY OF JERUSALEM","The questions motivating symplectic geometry, from classical mechanics to enumerative algebraic geometry, have been studied for centuries.  Many recent advances in the field have stemmed from the theory of J-holomorphic curves, and in particular Gromov-Witten theory.  The past 25 years of research have produced a fairly detailed picture of what can be expected from classical, closed Gromov-Witten theory. However, closed Gromov-Witten theory by itself lacks an interface with Lagrangian submanifolds, one of the fundamental structures of symplectic geometry.  The nascent open Gromov-Witten theory, in which Lagrangian submanifolds enter as boundary conditions for J-holomorphic curves, provides such an interface.
The goal of the proposed research is to broaden and systematize our understanding of open Gromov-Witten theory. My strategy leverages three connections with more established fields of research to uncover new aspects of open Gromov-Witten theory. In return, open Gromov-Witten theory advances the connected fields and reveals links between them. First, the closed and open Gromov-Witten theories are intertwined. Representation theoretic structures in closed Gromov-Witten theory admit mixed open closed extensions. Further, real algebraic geometry gives rise to a large variety of Lagrangian submanifolds providing an important source of intuition for open Gromov Witten theory. In return, open Gromov-Witten theory techniques advance Welschinger's real enumerative geometry. Finally, open Gromov-Witten theory plays a key role in mirror symmetry, a conjectural correspondence between symplectic and complex geometry originating from string theory. In particular, open Gromov-Witten invariants appear in the construction of mirror geometries. Moreover, under mirror symmetry, Lagrangian submanifolds correspond roughly to holomorphic vector bundles. Well understood functionals associated to holomorphic vector bundles go over to open Gromov-Witten invariants.","1249000","2013-10-01","2018-09-30"
"OPREP","Operator Based Representations for Geometry Processing","Mirela Ben-Chen","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","Geometric data is prevalent in many areas of science and technology. From the surface of the brain to the intricate shapes of free-form architecture, complex geometric structures arise in many fields, and problems such as analysis, processing and synthesis of geometric data are of great importance.
One major challenge in tackling such problems is choosing an adequate discrete representation of the geometric data. Traditionally, surface geometric data is treated as an irregularly sampled signal in three-dimensional space, yielding a representation as either a point cloud, or a polygonal mesh. Further analysis and manipulation are done directly on this discrete representation, resulting in algorithms which are often combinatorial, leading to difficult numerical optimization problems. The goal of this research is to explore a fundamentally different approach of representing geometric data through the space of scalar functions defined on it, and representing geometric operations as algebraic manipulations of linear operators acting on such functions. We will investigate the basic theory behind such a representation, addressing questions such as: what are the best function spaces to work with? Which operators can be consistently discretized, leading to discrete theorems analogous to continuous ones? How should multi-scale processing of geometric data be treated in this novel representation? To validate our approach, we will explore how this representation can be leveraged for devising efficient solutions to difficult real-world geometry processing problems, such as numerical simulation of intricate phenomena on curved surfaces, surface correspondence and quadrangular remeshing. By shifting the focus from geometry-centric representations and considering instead shapes through the lens of functional operators, we could potentially lay the ground for a fundamental change in the way that geometric data is treated and understood.","1500000","2017-01-01","2021-12-31"
"OPT4SMART","Distributed Optimization Methods for Smart Cyber-Physical Networks","Giuseppe Notarstefano","UNIVERSITA DEL SALENTO","The combination of embedded electronics and communication capability in almost any mobile or portable device has turned this century into the age of cyber-physical networks. Smart communicating devices with their sensing, computing and control capabilities promise to make our cities, transportation systems, factories and living environments more intelligent, energy-efficient, safe and secure. This extremely complex system has raised a number of new challenges involving ICT disciplines. In particular, a novel peer-to-peer distributed computational model is appearing as a new opportunity in which a service is built-up cooperatively by peers, rather than by a unique provider that knows and owns all data. The interdisciplinary “Optimization Community” is facing this revolution sharing a common need: to find new theories, methodologies and tools to optimize over this complex network system. With this in mind, OPT4SMART has a twofold objective. First, to provide a comprehensive theoretical framework to solve distributed optimization problems over peer-to-peer networks. Second, to develop effective numerical tools, based on this framework, to solve estimation, learning, decision and control problems in cyber-physical networks. To achieve this twofold objective, we will take a systems-theory perspective. Specific problems from these four areas will be abstracted to a common mathematical set-up, and addressed by means of interdisciplinary methodologies arising from a synergic combination of optimization, controls, and graph theories. In particular, OPT4SMART will face the challenge of solving optimization problems under severe communication limitations, very-large-scale problem and data size, and real-time computational constraints. The expected result will be a combination of strong theoretical methods and effective numerical toolboxes available to people in Engineering, Computer Science, Mathematics and other areas, who are facing optimization in cyber-physical networks.","1488750","2015-10-01","2020-09-30"
"OptApprox","Strong Convex Relaxations with Optimal Approximation Guarantees","Ola Nils Anders Svensson","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","One of the most exciting areas of theoretical computer science is to understand the approximability of fundamental optimization problems. A prominent example is the traveling salesman problem, for which a long-standing conjecture states that a certain algorithm gives a better guarantee than current methods can prove. The resolution of this conjecture and of many other fundamental problems is intimately related to an increased understanding of strong convex relaxations.

Although these problems have resisted numerous attempts, recent breakthrough results, in which the PI has played a central role, indicate new research directions with the potential to resolve some of our most exciting open questions.  We propose three research directions  to  revolutionize our understanding of these problems and more generally of the use of convex relaxations in approximation algorithms:

(I)  develop new approaches to analyze and harness the power of existing convex relaxations;

(II) understand the power of automatically generated relaxations; and

(III) prove the optimality of algorithms based on convex relaxations.

The proposed research lies in the frontier of approximation algorithms and optimization with connections to major problems in complexity theory, such as the unique games conjecture. Any progress will be a significant contribution to theoretical computer science and mathematical optimization.","1451052","2014-01-01","2018-12-31"
"OPTIMAX","Optimal Imaging with Present and Future Coherent X-ray Sources","Pierre Thibault","UNIVERSITY OF SOUTHAMPTON","The rapid development of high-brilliance X-ray sources in the last decade has opened the way to extremely powerful imaging modalities. Third generation synchrotron sources and newly built X-ray free-electron lasers offer a very high flux and can produce X-ray beams with excellent coherence properties. Imaging techniques that rely on the coherence of the incoming field can give access to phase contrast signal, known to be much stronger than absorption in the hard X-ray regime, and even make possible lensless imaging, thus offering the potential for very high-resolutions. To make the most of X-ray's high penetration power, it is best to combine these novel imaging approaches with computed tomography to achieve high-resolution, high-sensitivity 3D imaging.

The goal of this project is to break new ground in coherence-based imaging primarily through new developments in data analysis techniques. My group will work on a variety of novel or improved reconstruction approaches that will push the potential of these methods to their limit in resolution, sensitivity and usability, an objective well summarized by the term ``optimal imaging'' in the title of this proposal.

The main sub-projects described in this proposal are: (1) developing, improving and validating ptychographic reconstruction methods; (2) combining tomography and ptychography, and applying it to life and materials science samples; (3) developing efficient and robust tomographic reconstruction algorithms for grating interferometer imaging; and (4) implementing efficient data-analysis pipe-lines for future X-ray free-electron laser experiments.","1498918","2011-11-01","2017-10-31"
"OPTIMLIGHTHARVEST","Large Scale Architectures with Nanometric Structured Interfaces for Charge Separation, Transport and Interception","Roie Yerushalmi","THE HEBREW UNIVERSITY OF JERUSALEM","This research is aimed at developing new architectures at the molecular, nanometric, and macroscopic scales for the design and study of light induced charge transport using synthetic systems. The strategic objective is to establish a comprehensive approach for constructing nanometric scale hybrid structures that will enable us to tune the required physical, chemical, and electrical properties across scales required for efficient harvesting of light energy in a rigorous manner for enhancing our capabilities and basic understanding of light harvesting processes. We will form nanometric architectures featuring molecular diversity and functionality with nanometric gaps coupled to scaffolds capable of electrical transport. The nanometric architectures will be formed via simple yet powerful methods relying on sophisticated use of nanostructure surface chemistry and material properties while minimizing the application of top-down fabrication methods and will be studied at the single building block level as well as at array level. Meticulous study of the light induced charge separation and transport at the nanometric scale using single nanostructure building blocks as well as the collective dynamics of large scale arrays will be addressed with an emphasis on understanding charge dynamics at interfaces. The research activity will utilize unique nanostructure assembly methods and post-growth manipulation of the chemical composition developed during my research.

Achieving our fundamental goals is expected to lead to new insights and capabilities relating to the harvesting of light energy and converting it to electrical energy and to significantly advance our ability to utilize light energy for photocatalysis.","1427000","2011-03-01","2016-02-29"
"OPTINT","OPTINT: Optimization-based Design of Interactive Technologies","Otmar Dieter HILLIGES","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","As technology moves further away from the desktop setting, it is becoming increasingly clear that conventional interfaces (i.e., mice and keyboards) are no longer adequate means for interaction and that the traditional computing paradigm will be replaced or complemented by new forms of interaction such as wearable computing, head-worn displays, AR and VR. 

The main goal of OPTINT is to lay the foundations of design and implementation of 21st century interactive technologies. While contemporary user interface (UI) design techniques have been developed for the era of the PC, modern user interfaces are much more diverse and have to be designed for challenging contexts such as embedded and wearable computers, augmented and virtual reality (AR/VR), the Internet of the Things (IoT) and intelligent robotics. Furthermore, instead of flat, rectangular 2D devices, things will be flexible and of custom shape; instead of mass-produced we’ll use 3D printing to customize all types of technology and end-user designed interactive objects are becoming a reality. Designing in such context requires expertise in a large and diverse set of domains ranging from hardware-level sensor design all the way to user experience aspects. These requirements go largely beyond traditional UI design techniques, calling for next generation tools that can integrate all of them in a unified manner. Embracing these challenges, I argue for a novel approach to the design of interactive devices, leveraging optimization algorithms allowing the designer to focus on user experience instead of having to worry about technical details. OPTINT will deliver an optimization-based framework and a set of easy to use tools that allow user-experience (UX) designers and domain experts to develop a broad range of interactive technologies.","1500000","2017-02-01","2022-01-31"
"OptiQ-CanDo","Hybrid Optical Interferometry for Quantitative Cancer Cell Diagnosis","Natan Tzvi Shaked","TEL AVIV UNIVERSITY","A major challenge in the field of optical imaging of live cells is to achieve label-free but still fully quantitative measurements, which afford high-resolution morphological and mechanical mapping at the single cell level. In particular, developing efficient, non-subjective, quantitative optical imaging technologies for cancer cell diagnosis is a challenging task. The ground-breaking goal of this research project is to establish a robust experimental toolbox for label-free optical diagnosis and monitoring of live cancer cells in-vitro and their potential of metastasis. Optical interferometry is able to provide a platform for imaging live cells quantitatively without the risk of effects caused by using external contrast agents. 
By overcoming critical technological barriers, I suggest novel hybrid optical interferometric approaches that provide a powerful nano-sensing tool for label-free quantitative measurement of cancer cells. This will be obtained by recording the dynamic quantitative, three-dimensional sub-nanometric structural and mechanical characterization of live cancer cells in different stages. For this aim, I will develop a novel low-noise broadband, common-path, off-axis interferometric system for sub-nanometric physical thickness and mechanical mapping of live cells in thousands of frames per second. Additionally, I will develop rapid tomographic approach for fully capturing the cell three-dimensional refractive-index distribution, as a tool to characterize cancer progression. Interferometry will be combined with multi-trap holographic optical tweezers and dielectrophoresis to enable complete cell manipulations including full rotation, imaging of non-adherent cells, and mechanical measurement validation. New set of interferometry-based quantitative parameters will be developed to enable characterization of cellular transformations, and used to characterize cancer cells with different metastasis potential, for cell lines and for circulating tumor cells.","1916250","2016-07-01","2021-06-30"
"OPTISTIM","Patterned optical activation of retinal ganglion cells","Shy Shoham","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","Direct activation of retinal ganglion cells is a promising approach for treating blindness resulting from degenerative diseases of the outer retina. Current neuro-retinal stimulation interfaces being developed use electrical currents from micro-electrode arrays, but future systems could use light patterns to create spatio-temporally complex activity patterns. We will develop, optimize and test different concepts allowing patterned optical stimulation of large populations of retinal ganglion cells based on emerging methods for light-based neuro-stimulation, including glutamate uncaging and ectopic expression of the light-sensitive ion channel Channelrhodopsin II. A series of in vitro and in vivo experiments will examine basic questions regarding the efficacy and safety of this general approach. Our experiments will define the major engineering requirements and constraints towards the development of a light-based approach for restoring vision in individuals with outer-retinal degenerative diseases.","1600000","2008-08-01","2013-07-31"
"OPTNANO","Quantum optics in nanostructures","Stephanie Reich","FREIE UNIVERSITAET BERLIN","Nanomaterials are intriguing structures for quantum optics. Their color depends on their size and shape; they are very selective in the wavelengths they absorb and emit. Although nanostructures have been used to color windows and surfaces since the Middle Ages, we lack the understanding how size, shape, and microscopic structure control the optical properties of nanomaterials. In this project, we plan to develop a fundamental description of quantum optics in one-dimensional nanosystems. Core concepts will be quantum confinement and electron interactions when carriers are forced into a small space. The proposed work will focus on carbon nanotubes as a model nanosystem. The tubes show pronounced confinement effects; they emit and absorb light in the near infrared and visible. We will measure optical transitions, quantum cross sections, and electron interaction using luminescence, Raman scattering, and photoconductivity. The optical properties will be tailored by selecting specific tube types and changing the tube environment. A description of optical processes is incomplete without considering defects in real nanostructures. We will develop techniques to study and introduce imperfections. Their optical signatures and their effect on light emission will be determined on individual tubes. The experiments will be complemented by materials modeling. We will describe confinement effects and Coulomb interaction in semiempirical calculations of nanotube light absorption. The knowledge gained on carbon nanotubes will be applied to predict and study the optical properties of other one-dimensional systems. The goal is to obtain a robust and transferable model of quantum optics in nanostructures. This project will also advance characterization of nanomaterials by optical spectroscopy and applications of nanotubes as light detectors and emitters. We plan to develop tools for nanotube population analysis (tube type) and to test carbon tubes as wavelength-selective photodetectors","1097820","2008-08-01","2013-07-31"
"OptoDNPcontrol","Optically controlled carrier and Nuclear spintronics: towards nano-scale memory and imaging applications","Bernhard Urbaszek","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Carrier spin states in semiconductor nano-structures can be manipulated with fast optical pulses via the optical selection rules. The electron and hole spins in quantum dots interact strongly with the nuclear spins in the host material via the hyperfine interaction. This allows a new, versatile approach to nuclear spintronics, namely applying fast optical initialisation to carrier states and subsequent transfer via dynamic nuclear polarisation (DNP) of the spin information onto long-lived nuclear spin states, with promising applications in quantum information science and novel nuclear magnetic resonance (NMR) techniques.
This project aims to develop new, efficient optical pumping schemes to maximise DNP by going beyond the established Overhauser effects, investigating the possibility of self-polarization and phase transitions of the nuclear spin ensemble. An innovating aspect of this proposal is to use valence state engineering to tailor the highly anisotropic dipolar interaction between nuclei and holes, which can lead to novel, non-colinear hyperfine coupling.
The next innovation proposed is the development of an all-optical technique AONMR that does not require any radiofrequency (rf) coil set-up capable to control mesoscopic spin ensembles. Contrary to standard NMR techniques based on the generation of macroscopic rf-fields, AONMR can address the nuclear spins in one single nano-object via resonant laser excitation.
A further important target is to use quantum dots and other carrier localisation centres as efficient sources of DNP generation and to carry out a detailed study of the diffusion of DNP throughout the sample and finally across the sample surface, varying key sample (chemical composition, strain, substrate orientation) and experimental parameters such as temperature and applied external fields. These experiments are a feasibility study for using hyperpolarized compound semiconductors for increasing the sensitivity in Magnetic Resonance Imaging (MRI).","1495482","2013-02-01","2018-01-31"
"OPTOMECH","Theory of optomechanical circuits","Florian Kai Marquardt","FRIEDRICH-ALEXANDER-UNIVERSITAET ERLANGEN NUERNBERG","The interaction between light and mechanical motion in nanostructures has become a research topic with significant impact and promise recently. This rapidly developing area at the intersection between nanophysics and quantum optics is also known as “cavity optomechanics”. Fundamental investigations in quantum physics and possible applications like ultrasensitive detection of small displacements, forces and masses drive this field. By now, the basic features have been demonstrated in various experiments worldwide during the past five years. These include displacement detection with precisions down to the standard quantum limit, nonlinear dynamics in optomechanical self-oscillations, and cavity-assisted optomechanical laser-cooling of vibrational modes. The concepts involved are general enough to be applicable to a large variety of different setups, extending to variants such as nanomechanical resonators in superconducting microwave circuits and clouds of cold atoms.

It is now time to put these basic elements together and investigate the design of structures containing multiple interacting optical and mechanical modes. These could be used to form optomechanical “circuits” or “arrays”. Recently demonstrated nanofabricated photonic-phononic crystal structures provide one essential platform in which to realize these ideas. On the applied side, integrated optomechanical circuits might combine several functions, such as detection, amplification and general signal processing, or contribute to quantum information processing by converting information to and from the light field. On the fundamental side, arrays of optomechanical elements could be used to study the collective many-body dynamics (both classical and quantum) of these novel nonequilibrium systems. We propose to explore theoretically these possibilities, providing a guide-line for experiments and thereby unlocking the potential of such devices.","1499000","2011-11-01","2016-10-31"
"OptoQMol","Optical Quantum Control of Magnetic Molecules","Lapo Bogani","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","A revolution is underway, as molecular magnets are establishing a fundamental link between spintronics, molecular electronics and quantum computation. On the other hand, we know almost nothing on how a magnetic molecule is affected by electrons flowing through it or by the excitation of a molecular group. OptoQMol will investigate these uncharted waters by developing innovative, ultra-clean methods that will provide information inaccessible to established procedures. This will allow an unprecedented study of the interplay of electronic and spin degrees of freedom in magnetic molecules and of its possible use for quantum logic.

OptoQMol is a strongly multidisciplinary project, and makes use of an innovative mix of chemical and physical methods to overcome present experimental limitations, both in terms of time resolution and cleanliness. Instead of placing a magnetic molecule between bulk electrodes, we will directly grow photoactive groups on the molecule, so that electrons will flow through or close to the spin centers after a light pulse. This affords an ultra-clean system that can be studied in bulk, with a perfectly defined geometry of the magnetic and electronic elements. We will then combine optical and electron paramagnetic resonance techniques with ns time resolution, so as to observe the effect of electron flow on the spins in real time and measure the spin quantum coherence. Eventually we will use these innovative methods to control the interactions among spins and perform quantum logic operations.

The success of OptoQMol will answer two fundamental questions: How do molecular spins interact with flowing electrons? How can we use electronic excitations to perform quantum logic operations between multiple electron spins? The results will open a totally new area of experimental and theoretical investigation. Moreover they will redefine the limits and possibilities of molecular spintronics and allow quantum logic operations among multiple electron spins.","1498300","2014-01-01","2018-12-31"
"OPTRASTOCH","Optimal Transport and Stochastic Dynamics","Jan Maas","INSTITUTE OF SCIENCE AND TECHNOLOGYAUSTRIA","Many important properties of stochastic processes are deeply connected with the underlying geometric structure. The crucial quantity in many applications is a lower bound on the Ricci curvature, which yields powerful applications to concentration of measure, isoperimetry, and convergence to equilibrium.
Since many important processes are defined in discrete, infinite-dimensional, or singular spaces, major research activity has been devoted to developing a theory of Ricci curvature beyond the classical
Riemannian setting. This led to the powerful theories of Bakry-Émery and Lott-Sturm-Villani, which have been extremely successful in the analysis of geodesic spaces and diffusion processes. Building on our recent work, we will develop a wide research program that allows us to significantly enlarge the scope of these ideas.
A) Firstly, we develop a comprehensive theory of curvature-dimension for discrete spaces based on geodesic convexity of entropy functionals along discrete optimal transport. Promising first results suggest that the theory initiated by the PI provides the appropriate framework for obtaining many powerful results from geometric analysis in the discrete setting.
B) Secondly, we analyse discrete stochastic dynamics using methods from optimal transport.
We focus on non-reversible Markov processes, which requires a significant extension of the existing gradient flow theory, and develop new methods for proving convergence of discrete stochastic dynamics.
C) Thirdly, we develop an optimal transport approach to the analysis of quantum Markov processes. We will perform a thorough investigation of noncommutative optimal transport, we aim for geometric and functional inequalities in quantum probability, and apply the results to the analysis
of quantum Markov processes.
The project extends the scope of optimal transport methods significantly and makes a fundamental contribution to the conceptual understanding of discrete curvature.","1074590","2017-02-01","2022-01-31"
"ORANOS","Organic Nanospintronics","Machiel Pieter De Jong","UNIVERSITEIT TWENTE","Research on organic spintronics, which is especially promising due to the prospects of exceptionally long spin lifetimes in organic semiconductors, is currently in a critical phase. After a very successful starting period, progress is now being hampered, largely due to a limited understanding of the critical interfacial properties involved. The mechanisms behind the observed spin valve effects remain poorly understood, and unambiguous evidence of spin polarized transport (other than tunnelling) in organic semiconductors is still lacking. The properties of the hybrid inorganic/organic interfaces are of paramount importance for the device behaviour, and are key to solving the puzzle associated with the physics behind the observed magnetoresistance effects. So far a direct link between device characteristics and interfacial properties remains elusive. In this proposal, we pursue a reliable way to address these crucial issues and to guarantee the progress needed to take the field to the next level.

As a first main objective, we will combine interface preparation and characterization with in-situ device fabrication and testing, using a single dedicated UHV setup equipped with all necessary tools, i.e. spin (and angle) resolved photoelectron spectroscopy, LEED and STM, and magnetotransport, to establish a direct link between interfacial properties and device characteristics. The second main objective is to develop new device architectures, optimized for electrical probing of spin polarization in organic semiconductors. These joined efforts form a highly challenging task, since they require a strongly interdisciplinary approach, combining different complementary expertises. The PI is in a truly unique position to carry out this timely project successfully, having a strong and well documented background in 1) electron spectroscopic characterization techniques of electronic and magnetic properties of hybrid interfaces and 2) spintronic devices and spin polarized transport.","1494640","2012-03-01","2017-02-28"
"ORCA","Optical Responses Controlled by DNA Assembly","Tim Liedl","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","Artificially constructed materials can be designed to shape the propagation of light and can thus exhibit optical characteristics that are not found in nature. With such metamaterials, remarkable optical applications such as cloaking of objects, sensing of molecular environments or the fabrication of perfect lenses that are not bound by optical resolution limits could be realised. However, for metamaterials to operate at visible wavelengths they have to be structured in three dimensions with nanometre precision which currently poses an enormous barrier to their fabrication. By using molecular self-assembly based on the self-recognizing properties of sequence-programmable DNA strands, this barrier will be overcome. After having pioneered the 3D DNA origami method and the creation of DNA-based metamaterials, I propose the following new paths of research:
i) Metamaterials that are switchable in electric or magnetic fields and operate at visible or near infrared wavelengths will be designed and produced by DNA self-assembly for the first time. The hypothesis that materials with strong chirality show negative refraction will be tested and optical resonators with dimensions below 100 nm will be generated.
ii) The light-shaping characteristics of metal particle helices will be used to detect organic molecules. As most organic molecules are chiral and can be considered as chiral arrangements of multiple dipole elements, it is expected that the organic dipoles couple to the plasmonic dipoles of the metal helices. This in turn will induce changes in the optical activity of the material. In a parallel approach, organic molecules will be used to induce conformational changes in DNA-supported particle assemblies, which will then be detected in their optical response. Both of these fundamentally new detection schemes will allow extremely sensitive detection of biomolecules at visible wavelengths.","1433840","2013-12-01","2018-11-30"
"ORDERin1D","Order in one dimension: Functional hybrids of chirality-sorted carbon nanotubes","Sofie Cambré","UNIVERSITEIT ANTWERPEN","The hollow structure of carbon nanotubes (CNTs) with a wide range of diameters forms an ideal one-dimensional host system to study restricted diameter-dependent molecular transport and to achieve unique polar molecular order. For the ORDERin1D project, I will capitalize on my recent breakthroughs in the processing, filling, chiral sorting and high-resolution spectroscopic characterization of empty and filled CNTs, aiming for a diameter-dependent characterization of the filling with various molecules, which will pave the way for the rational design of ultraselective filtermembranes, sensors, nanofluidic devices and nanohybrids with unseen control over the structural order at the molecular scale. In particular, I recently found that dipolar molecules naturally align head-to-tail into a polar array inside the CNTs, after which their molecular directional properties such as their dipole moment and second-order nonlinear optical responses add up coherently, groundbreaking for the development of nanophotonics applications.","1499425","2016-05-01","2021-04-30"
"OrFuNCo","Organic Functionalisation of N2 Using Metal-Main Group and Metal-Metal Cooperativity","Antoine Louis Rene SIMONNEAU","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""Activation and transformation of dinitrogen (N2) into other nitrogen-containing compounds is a challenge for chemists due to the inertness of this molecule. The well-established Haber-Bosch process allows transformation of dinitrogen into ammonia (NH3) at the industrial scale through heterogeneous catalysis, but it is an energy-demanding process (1-2% of the World's annual energy consumption). The ammonia thus produced is almost totally converted into more value-added chemicals. Similarly, in Nature, the nitrogenase enzymes are able to convert N2 into NH3 catalytically, spending a high amount of energy to produce a molecule which is subsequently transformed into amino-acids or nucleotides. At a time where energy savings have become a major issue, alternatives to the Haber-Bosch process are desirable. Improving ammonia synthesis still prevails in current dinitrogen chemistry, despite the relative lack of utility of this molecule. Conversely, a catalytic process affording a nitrogen-containing product directly from N2 does not exist yet, and remains a highly attractive, though challenging, goal. Given this context, the PI proposes to investigate novel molecular chemical tools capable of direct conversion of N2 into nitrogen-containing organic compounds under mild conditions, while approaching the catalysis problem from a new direction. Two unprecedented strategies relying on the symbiotic reactivity of two partners towards dinitrogen will be detailed herein. In the first one, metal-boron frustrated Lewis pairs (FLPs) will help activate and functionalise N2, thus unlocking the thus far missing FLP chemistry of this small molecule. In the second one, it is proposed to explore the virgin territory of N2's cycloaddition reactivity, thanks to bimetallic cooperativity. By the careful examination of stoichiometric reactions considered as key steps of putative catalytic cycles, tackling the """"Holy Grail"""" of catalysis will be facilitated.
""","1499640","2018-03-01","2023-02-28"
"ORGA-NAUT","Exploring Chemical Reactivity with Organocatalysis","Paolo Melchiorre","FUNDACIO PRIVADA INSTITUT CATALA D'INVESTIGACIO QUIMICA","The proposed research seeks to redefine the synthetic potential of a fundamental organic transformation: the functionalisation of carbonyl compounds. Today, synthetic chemists can address even the most daunting of challenges connected with the asymmetric catalytic functionalisation of carbonyl compounds at their alpha and beta positions. In contrast, there are no known general strategies for the corresponding direct, catalytic and enantioselective transformation to a carbonyl group at the gamma position.
By developing innovative methodologies to effectively address this issue, I will provide a novel reactivity framework for exploring unprecedented transformations. This would strengthen the chemistry toolbox to better face the challenges of modern organic chemistry.
I will proceed by combining asymmetric aminocatalysis (a versatile chemical strategy whose potential has not yet been fully explored) with photoredox catalysis driven by visible light. This will provide access to open-shell radical species, which participate in bond constructions that are unavailable using amine organocatalysis alone. Since electron-deficient radicals are known to rapidly react with pi-rich olefins to forge even the most elusive C-C bonds, mild and catalytic approaches to accessing these reaction manifolds offer desirable opportunities for designing new gamma-functionalisations of carbonyl compounds. Developing an innovative system based on a chiral organic catalyst that efficiently harnesses the energy of solar radiation is in line with the European approach to attaining Sustainable Chemistry, one of the central scientific goals of the 21st Century.
This proposal challenges the current paradigms for stereoselective functionalisation by providing a template for directly functionalising unmodified carbonyl compounds at their gamma positions, expanding the way chemists think about making chiral molecules","1500000","2011-11-01","2016-10-31"
"ORGANITRA","Transport of phosphorylated compounds across lipid bilayers by supramolecular receptors","Elisabeth VAN DIJK","UNIVERSITE LIBRE DE BRUXELLES","This ORGANITRA project addresses transmembrane transport. Lipid bilayer membranes not only define the borders of cells and their compartments but are also implicated in metabolic processes and signal transduction. Membranes function as impermeable barriers for ionic and hydrophilic species which can only cross the membrane with the aid of dedicated membrane proteins.
For biotechnological and biophysical applications, the development of anion carriers that can bind an anion and transport it across the lipid bilayer could be of great relevance. In this project, synthetic anion receptors will be developed to bind biologically relevant organic phosphorylated compounds, like nucleotides. These receptors will then be used to transport these organophosphates across membranes.
The receptors will be synthesised by dynamic combinatorial chemistry. Building blocks containing urea or thiourea groups, for efficient phosphate binding, will be connected to multi-armed scaffolds by hydrazone groups. The dynamic character of these bonds will be used to identify efficient receptors from libraries of compounds, using different phosphorylated compounds as templates. With this approach, selective receptors for different nucleotides and related compounds can be obtained.
The transport performance of the receptors will be evaluated with newly developed assays. Liposomes will be used as model systems and transport will be monitored by fluorescence spectroscopy using the quenching of the emission of an encapsulated phosphate sensitive dye. Additionally, the mechanism of the transport processes will be elucidated by fluorescence and 1H and 31P NMR spectroscopies.
Transmembrane carriers for phosphorylated compounds will make it possible to selectively introduce nucleotides into liposomes and cells, opening the way to fuel enzymes with adenosine triphosphate (ATP) in liposomes as biotechnological nanoreactors and to study nucleotide-dependent biochemical processes in cells.","1485274","2019-01-01","2023-12-31"
"ORGELNANOCARBMATER","A Universal Supramolecular Approach toward Organic Electronic Materials and Nanostructured Carbonaceous Materials from Molecular Precursors","Holger Frauenrath","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Research in novel energy sources, efficient energy storage, sustainable chemical technology, and smaller microelectronic devices with interfaces for biological systems are among the current challenges in science and technology. Carbonaceous materials and organic electronic materials which speak the language of biomaterials will play a central role in the search for possible solutions. We aim to develop a universal supramolecular approach for their preparation and propose to develop synthetic pathways toward conjugated oligomers carrying hydrogen-bonded substituents, such as oligopeptide-polymer conjugates. These substituents serve as a supramolecular motif promoting the aggregation of the molecular precursors into single crystals, thin films, or soluble one-dimensional nanostructures. The obtained ordered phases or nanostructures from conjugated molecules themselves are highly interesting candidates for applications in photovoltaic, light-emitting, or semiconducting devices. Related nanostructures from oligo(phenylene)s or oligo(ethynylene)s will serve as reactive molecular precursors for a conversion into soluble graphene ribbon nanostructures. Finally, this approach will be extended toward the preparation of carbonaceous materials from amphiphilic oligo(ethynylene)s as energy-rich molecular precursors under preservation of the mesoscopic morphology, surface chemistry, and carbon microstructure. The obtained materials are highly interesting with respect to ion or hydrogen storage, and transition-metal-free catalysis. Hence, this research project aims to combine synthetic organic chemistry, supramolecular chemistry, and materials science in order to both deliver novel materials and improve our understanding in utilizing supramolecular-synthetic methods in their preparation.","1700000","2009-09-01","2014-08-31"
"ORGZEO","Organic Zeolites","Arne Thomas","TECHNISCHE UNIVERSITAT BERLIN","This ERC-StG proposal, OrgZeo, outlines a strategy for the synthesis, processing and application of charged, porous, covalent organic net- or frameworks, denoted “organic zeolites”. In analogy to their inorganic counterparts, organic zeolites should fulfil the following criteria: a) the material will possess small (nm-range) and accessible pores and consequently very high surface areas and pore volumes and b) the ionic group will be a supporting part of the network, while the counterions, needed to balance the net-charge will on the other hand be mobile and thus exchangeable. However, in contrast to common inorganic zeolites c) the entire or at least main parts of the material will be composed out of covalently bound organic matter, justifying the naming “organic zeolite”. The organic character of the samples will allow introduction of permanent and reversible, positive or negative network charges.
The target of this proposal is the introduction of the next generation of microporous materials combining the best properties of zeolites, MOFs, microporous polymers and polyelectrolytes. It is envisaged that the special architecture and chemical structure of the organic zeolites will further add entirely novel properties to these materials.","1500000","2012-01-01","2016-12-31"
"ORIGIN","The Origin of Solar Activity","Markus Roth","","""Solar activity impacts the near-Earth space environment and the Earth’s climate. It is caused by a magnetic field which varies over an 11-year cycle, the origin of which remained so far an unsolved puzzle for astrophysics. It is assumed that self-excited dynamos generate a complex, large-scale magnetic field in shear zones in the solar interior. Observational evidence for the physical conditions in these regions as well as for the large-scale flow components in the solar convection zone is marginal at best, but is urgently needed to explain the structure and evolution of the magnetic field.
The core of this proposal is to gain insight in the processes that are of major relevance for the solar dynamo by novel and improved methods of helioseismology. The analyses go far beyond the state-of-the-art and are based on highly resolved velocity measurements on the Sun from NASA’s milestone missions SOHO and SDO as well as the instruments of the GONG network. Combining the advantages of innovative local helioseismology methods with an unconventional approach of global helioseismology will fully exploit the unique properties of deeply penetrating seismic waves on the Sun. In this way unprecedented knowledge about the key processes involved in the deep seated origin of solar activity will be gained. By a paradigm change in global helioseismology, this includes for the first time full information on the structure of the meridional circulation and the magnetic field throughout the Sun. Moreover, three-dimensional views on the tachocline region in 200Mm depth and highly resolved seismic maps of the flow and sound speed variations in the sub-surface layers of the Sun present a highly innovative approach to understand the causes for solar activity on short time-scales. The results obtained will be important for other disciplines, e.g. space weather applications to protect technological systems in space and on Earth, as well as predicting the influence of the Sun on Earth’s climate.""","1486800","2012-09-01","2017-08-31"
"OSIRIS","Organic Semiconductors Interfaced with Biological Environments","Natalie Renuka BANERJI","UNIVERSITAET BERN","Transducing information to and from biological environments is essential for bioresearch, neuroscience and healthcare. There has been recent focus on using organic semiconductors to interface the living world, since their structural similarity to bio-macromolecules strongly favours their biological integration. Either water-soluble conjugated polyelectrolytes are dissolved in the biological medium, or solid-state organic thin films are incorporated into bioelectronic devices. Proof-of-concept of versatile applications has been demonstrated – sensing, neural stimulation, transduction of brain activity, and photo-stimulation of cells. However, progress in the organic biosensing and bioelectronics field is limited by poor understanding of the underlying fundamental working principles. Given the complexity of the disordered, hybrid solid-liquid systems of interest, gaining mechanistic knowledge presents a considerable scientific challenge. The objective of OSIRIS is to overcome this challenge with a high-end spectroscopic approach, at present essentially missing from the field. We will address: 1) The nature of the interface at molecular and macroscopic level (assembly of polyelectrolytes with bio-molecules, interfacial properties of immersed organic thin films). 2) How the optoelectronics of organic semiconductors are affected upon exposure to aqueous environments containing electrolytes, biomolecules and cells. 3) How information is transduced across the interface (optical signals, thermal effects, charge transfer, electric fields, interplay of electronic/ionic transport). Via spectroscopy, we will target relevant optoelectronic processes with ultrafast time-resolution, structurally characterize the solid-liquid interface using non-linear sum-frequency generation, exploit Stark shifts related to interfacial fields, determine nanoscale charge mobility using terahertz spectroscopy in attenuated total reflection geometry, and simultaneously measure ionic transport.","1498275","2017-08-01","2022-07-31"
"OTEGS","Organic Thermoelectric Generators","Xavier Dominique Etienne Crispin","LINKOPINGS UNIVERSITET","At the moment, there is no viable technology to produce electricity from natural heat sources (T<200°C) and from 50% of the waste heat (electricity production, industries, buildings and transports) stored in large volume of warm fluids (T<200°C). To extract heat from large volumes of fluids, the thermoelectric generators would need to cover large areas in new designed heat exchangers. To develop into a viable technology platform, thermoelectric devices must be fabricated on large areas via low-cost processes. But no thermoelectric material exists for this purpose.
Recently, the applicant has discovered that the low-cost conducting polymer poly(ethylene dioxythiophene) possesses a figure-of-merit ZT=0.25 at room temperature. Conducting polymers can be processed from solution, they are flexible and possess an intrinsic low thermal conductivity. This combination of unique properties motivate further investigations to reveal the true potential of organic materials for thermoelectric applications: this is the essence of this project.
My goal is to organize an interdisciplinary team of researchers focused on the characterization, understanding, design and fabrication of p- and n-doped organic-based thermoelectric materials; and the demonstration of those materials in organic thermoelectric generators (OTEGs). Firstly, we will create the first generation of efficient organic thermoelectric materials with ZT> 0.8 at room temperature: (i) by optimizing not only the power factor but also the thermal conductivity; (ii) by demonstrating that a large power factor is obtained in inorganic-organic nanocomposites. Secondly, we will optimize thermoelectrochemical cells by considering various types of electrolytes.
The research activities proposed are at the cutting edge in material sciences and involve chemical synthesis, interface studies, thermal physics, electrical, electrochemical and structural characterization, device physics. The project is held at Linköping University holding a world leading research in polymer electronics.","1453690","2013-04-01","2018-03-31"
"OXIDESYNERGY","Understanding the Atomic Scale Synergies of Catalytically Active Nanoclusters on Metal Oxide Surfaces","Jeppe Vang Lauritsen","AARHUS UNIVERSITET","The research theme concerns the application of new experimental methods for atomic-scale characterization of model catalysts based on insulating metal oxides with the goal of exploring the potential for designing new and efficient heterogeneous catalysts by enhanced control of the catalyst structure at the atomic level. This objective will be achieved by a carefully integrated sequence of synthesis, characterization, and reactivity measurements of model catalysts based on insulating metal oxides. The project aims in detail at resolving some pertinent support synergies and size-effects, which have been revealed in catalytic systems. A core challenge and advance, which sets the project apart from previous research, is the application of high-resolution non contact Atomic Force Microscopy (nc-AFM), which is the only available tool that can resolve the atomic structure of insulator surfaces and the morphology of supported nanoclusters. I will combine my proven experience with atom-resolved imaging using nc-AFM with novel methods for synthesizing and analyzing model catalysts, to provide groundbreaking new atomistic insight. A crucial aspect will be the ability to relate nc-AFM observations to actual catalytic properties, and this will be achieved by using complementary surface spectroscopies and reaction measurements performed at real high pressure conditions. I firmly believe that this research strategy can provide the key insight to a significantly better understanding of the numerous catalytic systems based on insulating metal oxides, and this project will enable me to set up a unique world-class experimental facility for such studies.","1050000","2009-10-01","2014-09-30"
"OXTOP","Low-dimensional topology in Oxford","Andras Imre Juhasz","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","This project aims to build a group that brings together experts in gauge-theoretic, geometric, and group-theoretic techniques. It consists of 4 branches. 

1. Cobordism maps in knot Floer homology (HFK). Defined by the PI, these should yield invariants of surfaces in 4-manifolds. Hence, they could be used to bound the 4-ball genus and the unknotting number, providing a tool for finding a counterexample to the smooth 4-dimensional Poincaré conjecture, and to decide whether a given slice knot bounds a ribbon surface. The cobordism maps seem to yield a spectral sequence from Khovanov homology to HFK. An important biological application is an obstruction for two links to be related by a band surgery. 

2. TQFTs. We use our classification of (2+1)-dimensional TQFTs in terms of GNF*-algebras and MCG representations to find new examples of such TQFTs. First, we simplify the algebraic structure, then determine when a GNF*-algebra corresponds to a (1+1+1)-dimensional TQFT. This would allow us to find a (2+1)-dimensional TQFT that is not (1+1+1)-dimensional. 

3. Heegaard Floer (HF) homology and geometrization. There are currently few links known between Floer-theoretic invariants of 3-manifolds and the geometric structures they admit. We propose to study the Floer homology of arithmetic 3-manifolds. These are often L-spaces; the question is when this happens, and whether the HF correction terms contain any number-theoretic information. The next step is studying the relationship between HF and the Thurston geometries, and then gluing along tori via bordered Floer homology. An important step is to understand the behaviour of HF under covering maps.  

4. The Fox conjecture. This states that the absolute values of the coefficients of the Alexander polynomial of an alternating knot form a unimodal sequence. We propose a strategy for attacking this conjecture via the graph-theoretic description of the Alexander polynomial due to Kálmán, and the test of log-concavity of Huh.","1497422","2016-05-01","2021-04-30"
"OXYEVOL","Atmospheric oxygen as a driver of plant evolution over the past 400 million years","Jennifer Claire Mcelwain","UNIVERSITY COLLEGE DUBLIN, NATIONAL UNIVERSITY OF IRELAND, DUBLIN","""The evolution of complex organisms over one billion years ago is intimately linked with a rise in atmospheric oxygen levels (O2) over a critical threshold that would support essential metabolic processes. Over the past 500 million years O2 has varied between lows of 10% to  highs of 35%, compared with current ambient levels of ~21%.Critical events in animal evolutionary history have been linked with shifts in atmospheric O2 such as the origination and radiation of mammals and selective extinction of many land vertebrate groups, at three of the five great mass extinction boundaries. The potential role of O2 as a driver of plant evolution has been almost completely overlooked, despite evidence from space science which shows that sub-ambient O2 can negatively impact all aspects of plant reproduction, phloem loading and photosynthesis. This proposal will address this severe gap in our knowledge of the importance of O2 in shaping patterns in plant evolution, by investigating the role of long-term trends in atmospheric O2 on the timing of major reproductive and vegetative innovations in the plant fossil record. This palaeobotanical approach utilizing the plant fossil record will be coupled with a series of highly novel ‘atmospheric miniworld’ experiments where representative plant taxa from all three major reproductive grades will be subjected to the atmospheric O2:CO2 conditions into which they likely originated and diversified. We will address whether tipping points in the ecological dominance of different evolutionary groups of land plants (angiosperms/ gymnosperms/ pteridophytes) were driven by shifts in prevailing atmospheric O2 content. We will achieve these objectives by conducting controlled competition experiments incorporating all three reproductive grades in miniworlds with differing atmospheric O2:CO2 ratios.""","1584013","2012-02-01","2017-01-31"
"OXYGEN","Quantifying the evolution of Earth's atmosphere with novel isotope systems and modelling","Mark Claire","THE UNIVERSITY COURT OF THE UNIVERSITY OF ST ANDREWS","Atmospheric oxygen is fundamental to life as we know it, but its concentration has changed dramatically over Earth’s 4.5 billion year history.  An amazing qualitative story has emerged, in which Earth’s atmosphere was devoid of free oxygen for the first 2 billion years of planetary history, with two significant increases in concentration at ~2.4 and ~0.55 billion years ago.  Both oxygenation events were accompanied by extreme climatic effects – the “snowball earth” episodes – and paved the way for massive reorganization of biogeochemical cycles such as the Cambrian radiation of macroscopic life. Despite these profound influences on the Earth system, we currently lack fundamental quantitative constraints on Earth’s atmospheric evolution. I am poised to add substantial quantitative rigor to Earth’s atmospheric history, by constraining the concentrations of important gases (e.g., O2, O3, CO2, CH4, organic haze) in ancient atmospheres to unprecedented accuracy. I will accomplish this via an innovative interdisciplinary program focused on the unusual mass-independent isotope fractionations observed in sedimentary rocks containing sulfur and oxygen. These signals are direct remnants of ancient atmospheric chemistry, and contain far more information than can currently be interpreted. This project combines novel experimental and methodological approaches with state-of-the-art numerical modelling to significantly advance our ability to decipher the isotope records. A unique “early Earth” UV lamp coupled to a custom-built photocell will enable direct production of isotope signals under Earth-like conditions, with time-dependent sampling. Groundbreaking analytical methodologies will vastly increase the global geochemical database. The experimental results and data will provide ground-truth for next-generation atmospheric models that will constrain atmospheric composition and its feedbacks with the Earth-biosphere-climate system during key points in our planetary history.","1767455","2016-06-01","2021-05-31"
"p-TYPE","Transparent p-type semiconductors for efficient solar energy capture, conversion and storage.","Elizabeth GIBSON","UNIVERSITY OF NEWCASTLE UPON TYNE","This proposal will develop new transparent p-type semiconductors that will make dye-sensitized solar cells (DSC) a vastly more efficient and a realistic prospect for carbon-free energy generation worldwide. Two key challenges will be addressed: (1) a means of converting NIR radiation to increase the amount of sunlight utilised from 35% to over 70%; (2) a means of storing the energy. Almost all the research in the field is based on dye or “perovskite” sensitized TiO2 (n-type) solar cells, which are limited by their poor spectral response in the red-NIR. pTYPE approaches the problem differently: tandem DSCs will be developed which combine a n-type and a p-type DSC in a single p/n device. This increases the theoretical efficiency from 33% to 43% by extending the spectral response without sacrificing the voltage. The device will be modified with catalysts to convert H2O or CO2 and sunlight into fuel without using sacrificial reagents that limit the efficiency of current systems. An efficient tandem DSC has not yet been developed because p-type DSCs are much less efficient than n-type cells. As an independent Royal Society Dorothy Hodgkin fellow I increased the photocurrent by developing new dyes. This project will exploit this breakthrough by increasing the voltage, which is currently limited by the NiO semiconductor conventionally used. I will rapidly synthesise libraries of alternative p-type semiconductors; select promising candidates based on key criteria which can be measured on a single sample within minutes: transparency and dye adsorption (for high light harvesting efficiency by the dye), conductivity (for high charge collection efficiency) and valence band potential (for high voltage); assemble the new materials in tandem DSCs. As one of the few researchers experienced in preparing, characterising and optimising each aspect of this photoelectrochemical system, I aim to match the efficiency from TiO2 with p-type DSCs to obtain tandem efficiencies above 20%.","1499840","2017-01-01","2022-12-31"
"P-WIND","New light on the gamma-ray sky: unveiling cosmic-ray accelerators in the Milky Way and their relation to pulsar wind nebulae","Marianne Lemoine Ép.Goumard","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Gamma-ray Astronomy pinpoints celestial high energy particle accelerators and may reveal the origin of the cosmic-rays, a century after their discovery. Now is a time of extraordinary opportunity.  Cherenkov telescopes have opened up a new domain and more than 70 very-high energy gamma-ray sources have been detected above 100 GeV, especially by the European experiments H.E.S.S. and MAGIC. NASA's Fermi Large Area Telescope, devoted to the study of the gamma-ray sky between 20 MeV and 300 GeV, was launched in June 2008 and has published the positions of 1500 previously unknown gamma-ray sources spread across the sky.
However, among all the sources detected by satellite and Cherenkov telescopes, hundreds of Galactic gamma-ray sources have no obvious counterpart at optical, radio, or X-ray wavelengths. What are these sources ? What role do they play in the Galaxy's energy budget ? Many of them must be pulsars or nebulae powered by pulsars.
In this project, I propose to use my expertise in both TeV and GeV gamma-ray analysis together with the excellent links of our team with radio observatories to identify the nature of these sources, focusing on pulsars and pulsar wind nebulae as primary candidates. I further propose to use the theoretical models of these cosmic accelerators that I have developed in the past both to enhance the search, and to interpret the results. The range of competences required for the proposed research project is very large and difficult to gather in one single team: pulsar timing, experience with data analysis of extended sources and theoretical know-how in pulsar wind nebulae and high energy phenomena. The P-WIND team would therefore be unique in gamma-ray Astronomy.","592680","2011-01-01","2013-12-31"
"P2PMODELS","Decentralized Blockchain-based Organizations for Bootstrapping the Collaborative Economy","Samer Hassan Collado","UNIVERSIDAD COMPLUTENSE DE MADRID","The Collaborative Economy (CE) is rapidly expanding through new forms of Internet labor and commerce, from Wikipedia to Kickstarter and Airbnb. However, it suffers from 3 main challenges: (1) Infrastructure: centralized surveillance that the central hubs of information exercise over their users, (2) Governance: disempowered communities which do not have any decision-making influence over the platform, and (3) Economy: concentration of profits in a few major players who do not proportionally redistribute them to the contributors.
How can CE software platforms be implemented for solving these challenges? P2PMODELS explores a new way of building CE software platforms harnessing the blockchain, an emerging technology that enables autonomous agent-mediated organizations, in order to (1) provide a software framework to build decentralized infrastructure for Collaborative Economy organizations that do not depend on central authorities, (2) enable democratic-by-design models of governance for communities, by encoding rules directly into the software platform, and (3) enable fairer value distribution models, thus improving the economic sustainability of both CE contributors and organizations. 
Together, these 3 objectives will bootstrap the emergence of a new generation of self-governed and more economically sustainable peer-to-peer CE communities. The interdisciplinary nature of P2PMODELS will open a new research field around agent-mediated organizations for collaborative communities and their self-enforcing rules for automatic governance and economic rewarding. Bringing this proposal to life requires a funding scheme compatible with a high-risk/high-gain vision to finance a fully dedicated and highly motivated research team with multidisciplinary skills.","1498855","2018-01-01","2022-12-31"
"PAAL","Practical Approximation Algorithms","Piotr Sankowski","UNIWERSYTET WARSZAWSKI","The goal of this proposal is the development and study of practical approximation algorithms. We will base our study on
theoretical models that can describe requirements for algorithms that make them practically efficient. We plan to develop an
efficient and useful programming library of approximation algorithms.
Our research on approximation algorithms will be concentrated on two main topics:
- multi-problem optimization, when the solution has to be composed out of different problems that need to interact,
- interplay between regular and random structure of network that could allow construction of good approximation algorithms.
The above concepts try to capture the notion of effective algorithms. It has to be underlined that they were not studied before.
The practical importance of these problems will be verified by the accompanying work on generic programming concepts
for approximation algorithms. These concepts will form the basis of universal library that will include Web algorithms and
algorithms for physical applications.","1000000","2010-11-01","2015-10-31"
"PAC","Proofs and Computation","Eliyahu Ben Sasson","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","The project described in this proposal studies formal proofs and their interaction with computation. The study of propositional proofs is connected to a spectrum of problems in our field, starting with the meta-mathematical quest to explain our failure to understand computation and make progress on the basic questions haunting our field (such as P vs. NP), and ending with the industry-driven quest for better algorithms for solving instances of the satisfiability problem. In a seemingly different direction, the recent introduction of magical probabilistically checkable proofs (PCPs) has opened new horizons in computer science, ranging from a deeper understanding of approximation algorithms and their limits to the construction of super-efficient protocols for the verification of proofs and computations. We suggest to study proofs and computation with three main objectives. First, to construct better SAT solvers via a better understanding of propositional proof systems. Second, to expand the range of applications of PCPs and transform them from the purely theoretical objects that they currently are to practical and accessible formats for use in all settings where proofs are encountered. Third, to expand our theoretical understanding of the intrinsic limits of proofs, with an eye towards explaining why we are unable to make significant progress on central questions in computational complexity. We believe this project can bridge across different regions of computer science such as SAT solving and proof complexity, theory and practice, propositional proofs and probabilistically checkable ones. And its expected impact will start on the theoretical mathematical level that forms the foundation of computer science and percolate to more practical areas of our field.","1743676","2009-12-01","2015-09-30"
"PACE","Precedents for Algal Adaptation to Atmospheric CO2: New indicators for eukaryotic algal response to the last 60 million years of CO2 variation","Heather Marie Stoll","UNIVERSIDAD DE OVIEDO","Evolution of marine algae over the last 60 million years has resulted in a fundamental change in the efficiency of biological carbon pump and shift from communities dominated by calcifying algae (like coccolithophorids) to siliceous diatoms and major size class changes among these groups. The inferred shift in atmospheric CO2 over this time period has been suggested as an important selective pressure on some of these responses, including diatom adaptation to lower atmospheric CO2 concentrations via use of the C4 photosynthetic pathway, and trends towards smaller coccolithophorid cell sizes in response to greater C limitation. If current trends continue, future changes in atmospheric CO2 from anthropogenic activities are likely to reach levels last seen in the Eocene by the end of the next century; such changes will also be accompanied by ocean acidification and changes in stratification. Evidence suggests that modern calcifying algae and diatoms may employ a range of carbon acquisition strategies (such as active carbon concentrating mechanisms) according to the pH and carbon speciation of the seawater in which they live. However calcifying populations from 60 million years ago apparently had a single or less diverse array of carbon acquisition strategies. In this project we thus seek to 1) to identify and calibrate novel fossil indicators for adaptation and evolution in carbon acquisition strategies in eukaryotic algae in response to past changes in the carbon cycle and atmospheric CO2, and 2) apply these indicators to establish the nature and timing of changes in carbon acquisition strategies by algae over the past 60 million years.","1774875","2009-12-01","2015-11-30"
"PACOMANEDIA","Partially Coherent Many-Body Nonequilibrium Dynamics for Information Applications","Sougato Bose","UNIVERSITY COLLEGE LONDON","I propose to investigate two closely connected themes which aim to exploit the full potential of quantum mechanics in information technology. Both the themes concern the exploitation of the nonequilibrium dynamics of many strongly coupled quantum systems which is recently becoming feasible to observe in a plethora of engineered systems. As one broad objective, I plan to examine automata made from a multiple quantum units such as nanomagnets for transporting bits and performing classical (Boolean) reversible logic. In a similar vein, coding of bits in domains of engineered quantum many-body systems and their exploitation for Boolean computing will be explored, as well as examine the quantum nonequilibrium dynamics of a processor which combines transport and processing together. The open nature of the constituent quantum systems will be an integral part of our calculations which will be set in a regime where dissipation (decay of energy from the system) is not significant, though dephasing (loss  of quantum coherence) may be substantial. I foresee the advantage of such automata in highly energy efficient and fast computation whose speed is set by the couplings of the quantum many-body system. The second broad objective seeks to overcome a formidable obstacle in the physical implementation of quantum computation, namely the high control demanded on every quantum bit and their interactions with other quantum bits. I plan to offer and investigate an alternative paradigm where the information is processed by harnessing the minimally controlled dynamics of quantum many-body systems. In this context, I will look both at general questions such as to whether a network of interacting spins can serve as an automata for running an entire quantum algorithm, whether magnon wavepackets can be used like photons for linear optics-type quantum computation, as well as the realization of such ideas in a variety of available quantum many-body systems.","1245078","2012-10-01","2017-09-30"
"PAGAP","Periods in Algebraic Geometry and Physics","Francis Clement Sais Brown","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Periods are the integrals of algebraic differential forms over domains defined by polynomial inequalities, and are ubiquitous in mathematics and physics. One of the simplest classes of periods are given by multiple zeta values, which are the periods of moduli spaces M_{0,n} of curves of genus zero. They have recently undergone a huge revival of interest, and occur in number theory, the theory of mixed Tate motives, knot invariants, quantum groups, deformation quantization and many more branches of mathematics and physics.
Remarkably, it has been observed experimentally that Feynman amplitudes in quantum field theories typically evaluate numerically to multiple zeta values and polylogarithms (which are the iterated integrals on M_{0,n}), and a huge amount of effort is presently devoted to computations of such amplitudes in order to provide predictions for particle collider experiments. A deeper understanding of the reason for the appearance of the same mathematical objects in algebraic geometry and physics is essential to streamline these computations, and ultimately tackle the outstanding problems in particle physics.
The proposal has two parts: firstly to undertake a systematic study of the periods and iterated integrals on higher genus moduli spaces M_{g,n} and related varieties, and secondly to relate these fundamental mathematical objects to quantum field theories, bringing to bear modern techniques from algebraic geometry, Hodge theory, and motives to this emerging interdisciplinary area. Part of this would involve the implementation (with the assistance of future postdoc. team members) of an algorithm for the evaluation of Feynman diagrams which is due to the author and goes several orders beyond what has previously been possible, in order eventually to deduce concrete predictions for the Large Hadron Collider.","1068540","2010-11-01","2015-10-31"
"PAMDORA","Planetary accretion and migration in discs over all ages","Bertram BITSCH","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","The goal of this ERC proposal is to make significant progress in our understanding how planetary systems form in protoplanetary discs. In this ambitious research programme I will answer these three key questions:

How does the dust size distribution affect the evolution of ice lines and initial formation location of planetesimals? How do growing planets migrate in protoplanetary discs? How does the disc evolution affect the formation and composition of planetary systems?

I will tackle these questions using a combination of novel ideas and computer simulations in which I will model the three before mentioned connected key stages of planet formation. The disc evolution model will incorporate grain growth and drift with self-consistent temperature structure calculations. The planet migration simulations will map the migration rates from small planets all the way to giant gap opening planets in these discs. Finally, I will combine these topics and compute the assembly of whole planetary systems from multiple small bodies in gas discs to full grown solar systems. Additionally, I will track the chemical composition and evolution of the growing bodies.

These self-consistent models of the formation process from planetary embryos all the way to full planetary systems will be the first of their kind and will shed light on the origin of the variety of planetary systems featuring terrestrial planets, super-Earths, ice and/or gas giants. By incorporating the chemical composition of planets during their formation into my model, I can not only compare the orbital elements to observations, but also their compositions, where observations of the atmospheres of hot Jupiters already exist and future observations of super-Earths will reveal their atmospheric and bulk composition (e.g. through the PLATO space mission), further constraining planet formation theories.","1491909","2018-01-01","2022-12-31"
"PANAMA","Probabilistic Automated Numerical Analysis in Machine learning and Artificial intelligence","Philipp HENNIG","EBERHARD KARLS UNIVERSITAET TUEBINGEN","Numerical tasks - integration, linear algebra, optimization, the solution of differential equations - form the computational basis of machine intelligence. Currently, human designers pick methods for these tasks from toolboxes. The generic algorithms assembled in such collections tend to be inefficient on any specific task, and can be unsafe when used incorrectly on problems they were not designed for. Research in numerical methods thus carries carries the potential for groundbreaking advancements in the performance and quality of AI. 

Project PANAMA will develop a framework within which numerical methods can be constructed in an increasingly automated fashion; and within which numerical methods can assess their own suitability, and adapt both model and computations to the task, at runtime. The key tenet is that numerical methods, since they perform tractable computations to estimate a latent quantity, can themselves be interpreted explicitly as active inference agents; thus concepts from machine learning can be translated to the numerical domain. Groundwork for this paradigm - probabilistic numerics - has recently been developed into a rigorous mathematical framework by the PI and others. The proposed research will simultaneously deliver new general theory for the computations of learning machines, and concrete new algorithms for core areas of machine learning. In doing so, Project PANAMA will improve the efficiency and safety of artificial intelligence, addressing scientific, technological and societal challenges affecting Europeans today.","1450000","2018-03-01","2023-02-28"
"PaPaAlg","Pareto-Optimal Parameterized Algorithms","Daniel LOKSHTANOV","UNIVERSITETET I BERGEN","In this project we revise the foundations of parameterized complexity, a modern multi-variate approach to algorithm design. The underlying question of every algorithmic paradigm is ``what is the best algorithm?'' When the running time of algorithms is measured in terms of only one variable, it is easy to compare which one is the fastest. However, when the running time depends on more than one variable, as is the case for parameterized complexity:

**It is not clear what a ``fastest possible algorithm'' really means.**

The previous formalizations of what a fastest possible parameterized algorithm means are one-dimensional, contrary to the core philosophy of parameterized complexity. These one-dimensional approaches to a multi-dimensional algorithmic paradigm unavoidably miss the most efficient algorithms, and ultimately fail to solve instances that we could have solved. 

We propose the first truly multi-dimensional framework for comparing the running times of parameterized algorithms. Our new definitions are based on the notion of Pareto-optimality from economics. The new approach encompasses all existing paradigms for comparing parameterized algorithms, opens up a whole new world of research directions in parameterized complexity, and reveals new fundamental questions about parameterized problems that were considered well-understood.
In this project we will develop powerful algorithmic and complexity theoretic tools to answer these research questions. The successful completion of this project will take parameterized complexity far beyond the state of the art, make parameterized algorithms more relevant for practical applications, and significantly advance adjacent subfields of theoretical computer science and mathematics.","1499557","2017-02-01","2022-01-31"
"PARAMIR","Investigating micro-RNA Dynamics using Paramagnetic NMR Spectroscopy","Loic SALMON","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Over the last decades, new discoveries have revealed an ever-increasing diversity of RNA functions, profoundly modifying the conceptual framework of molecular biology. This is the case of micro-RNAs (miRNA), a fascinating class of small non-coding RNA that play essential roles in RNA induced gene silencing and are estimated to target up to 60% of protein-coding genes in humans.
RNA functional diversity is often triggered by conformational changes, so capturing the dynamics of these molecules is key to a precise understanding of their function, which in turn is essential to control their activity. Nuclear Magnetic Resonance (NMR) spectroscopy is extremely well suited to investigate dynamical processes. However, the sparsity of measureable NMR data in a RNA sample represents a major bottleneck, preventing so far an accurate description of RNA conformational fluctuations.
This project aims to overcome this barrier by developing paramagnetic NMR for RNA. By chemically modifying an RNA, I will introduce paramagnetic tags to strategic positions so as to acquire NMR data from otherwise silent substrate. Adequate computational and analytical models will be developed to decode the experimental data into an atomic-level description of dynamics.
These goals require a leap forward with respect to today’s approaches. I propose to achieve this by combining innovative sample preparation strategies and NMR experiments, high magnetic fields, and MD simulations. With these methods, I will enable the determination of the dynamic landscape of let-7, the first miRNA discovered in humans, involved in cell proliferation and differentiation and oncogenesis.
This project will yield a broadly applicable method for the structural and dynamic characterization of RNA with unprecedented details. This knowledge will improve our fundamental biochemical and biophysical conception of RNA, opening new avenues for bioengineering and establishing the bases for rational RNA-oriented drug discovery.","1633500","2019-02-01","2024-01-31"
"PARAMTIGHT","Parameterized complexity and the search for tight complexity results","Dániel Marx","MAGYAR TUDOMANYOS AKADEMIA SZAMITASTECHNIKAI ES AUTOMATIZALASI KUTATOINTEZET","The joint goal of theoretical research in algorithms and
computational complexity is to discover all the relevant algorithmic techniques
in a problem domain and prove the optimality of these techniques.
We propose that the search for such tight results should be done
by a combined exploration of the dimensions running time, quality
of solution, and generality.  Furthermore, the theory of parameterized complexity
provides a framework for this exploration.

Parameterized complexity is a theory whose goal is to
produce efficient algorithms for hard combinatorial problems using
a multi-dimensional analysis of the running time. Instead of
expressing the running time as a function of the input size only
(as it is done in classical complexity theory), parameterized
complexity tries to find algorithms whose running time is
polynomial in the input size, but exponential in one or more
well-defined parameters of the input instance.

The first objective of the project is to go beyond the
state of the art in the complexity and algorithmic aspects of
parameterized complexity in order to turn it into a theory
producing tight optimality results. With such theory at hand, we
can start the exploration of other dimensions and obtain tight
optimality results in a larger context. Our is goal is being able
to prove in a wide range of settings that we understand all the
algorithmic ideas and their optimality.","1150000","2012-01-01","2017-06-30"
"PARAPPROX","Parameterized Approximation","Saket Saurabh","UNIVERSITETET I BERGEN","""The main goal of this project is to lay the foundations of  a ``non-polynomial time theory of approximation"""" -- the  Parameterized Approximation  for  NP-hard optimization problems.   A combination that will use the salient features  of  Approximation Algorithms and
Parameterized Complexity. In the former, one relaxes the requirement of finding an optimum solution. In the latter, one relaxes the requirement of  finishing in polynomial time by restricting the
combinatorial explosion in the running time to a parameter that for reasonable inputs is much smaller than the input size. This project will explore the following fundamental question:

Approximation Algorithms + Parameterized Complexity=?

New techniques  will be developed that will simultaneously utilize the notions of relaxed time complexity and accuracy and thereby make problems for which both these approaches have failed  independently, tractable. It is however conceivable  that for some problems even this combined approach may not succeed. But in those situations we will glean valuable insight into the reasons  for failure.  In parallel to algorithmic studies,  an intractability theory  will be
developed which will provide the theoretical framework to specify the extent to which this approach might work.  Thus,  on one hand the project will give rise to  algorithms that will have impact beyond the boundaries of computer science and on the other  hand it will lead to a complexity theory that will go beyond the established  notions of intractability.  Both these aspects of my project  are groundbreaking -- the new theory will transcend our current ideas of
efficient approximation and thereby raise the state of the art  to a new level.""","1690000","2013-01-01","2017-12-31"
"PARATOP","New paradigms for correlated quantum matter:Hierarchical topology, Kondo topological metals, and deep learning","Titus NEUPERT","UNIVERSITAT ZURICH","Discovering, classifying and understanding phases of quantum matter is a core goal of condensed matter physics. Next to the notion of symmetry breaking phases, the concept of topological phases of matter is a prevailing theme of recent research. Topological phases are envisioned for various applications due to their universal and robust properties, such as protected conducting boundary modes, and provoke fundamental questions about the nature of many-body quantum states by providing the basis for exotic quasiparticles.

In this ERC research project, I propose several new topological phases and novel numerical approaches for studying and classifying the most sought-after topological phases of matter. Concretely, I propose the concept of three-dimensional hierarchical topological insulators, which, in contrast to the known topological phases, do not posses gapless surface, but protected gapless edge modes. Moreover, I plan to study topological metals arising in strongly correlated Kondo systems, going beyond the current paradigm of considering topological metals that arise in the absence of electronic correlations. Furthermore, I propose to make the analogous step for topological superconductors, which have been studied as free models to search for Majorana quasiparticles: For the first time, I want to explore strongly interacting systems that realize the more powerful parafermion quasiparticles with numerical techniques. Finally, in a cross-disciplinary and exploratory sub-project, I will employ methods of deep neural networks to classify strongly correlated quantum phases using supervised learning combined with a technique called deep dreaming.  

Each of these sub-projects has the potential to make a paradigm-changing contribution to the study of strongly correlated and topological states of quantum matter and the combination of them allows to take advantage of synergy effects and a balance between high-risk and definitely feasible key developments.","1362401","2018-01-01","2022-12-31"
"PariTorMod","P-adic Arithmetic Geometry, Torsion Classes, and Modularity","Ana CARAIANI","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","The overall theme of the proposal is the interplay between p-adic arithmetic geometry and the Langlands correspondence for number fields. At the heart of the Langlands program lies reciprocity, which connects Galois representations to automorphic forms. Recently, new developments in p-adic arithmetic geometry, such as the theory of perfectoid spaces, have had a transformative effect on the field. This proposal would establish a research group that will develop and exploit novel techniques, that will allow us to move significantly beyond the state of art. I intend to make fundamental progress on three major interlinked problems.

Torsion in the cohomology of Shimura varieties: in joint work with Scholze, I proved a strong vanishing result for torsion in the cohomology of compact unitary Shimura varieties. In work in progress, we have extended this to many non-compact cases. To obtain a complete picture, I propose to develop new techniques using point-counting and the trace formula and combine them with ingredients from arithmetic geometry.

Local-global compatibility is essential for establishing new instances of Langlands reciprocity. I will use the results on Shimura varieties described above to prove local-global compatibility for torsion in the cohomology of locally symmetric spaces for general linear groups over CM fields. This is one of the fundamental questions in the field. Solving it will require progress on a diverse set of problems in representation theory and integral p-adic Hodge theory.

The Fontaine–Mazur conjecture is the most general reciprocity conjecture. Very little is known outside the case of two-dimensional representations of the absolute Galois group of the rational numbers, which relies crucially on a connection to p-adic local Langlands. I will attack the Fontaine–Mazur conjecture for imaginary quadratic fields. Some crucial inputs will come from the first two projects above.","1469805","2019-01-01","2023-12-31"
"PartonicNucleus","Understanding the Quark and Gluon Structure of the Nucleus","Raphael DUPRE","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The representation of the nucleus as an aggregate of protons and neutrons has been quite successful to describe nuclear properties in the past. However, it is now the time to understand the nuclear structure in terms of quarks and gluons (i.e. the partons). We have known for more than 30 years that the quark distribution deviates by up to 20% from the standard model of nuclear physics. With time, most explanations of this phenomenon have come to fail and this major nuclear effect remains today a mystery, but clearly tells us that a description of the nucleus in which protons and neutrons are not affected by their surrounding medium is incomplete. I propose here to use several recent developments in detection technologies and in hadron physics theory to perform new experiments that will unravel the deeper structure of the atomic nucleus. The first measurement will give the 3D tomography of the nucleus in terms of quarks and gluons. Second, I lay out a strategy to measure transverse momentum dependent parton distribution functions in cold nuclear matter and show how
it can help understand the gluon saturation scale, i.e. the onset of non linear behavior in the nuclear gluon structure. Third, I propose to measure reactions, in which we detect nuclear remnants, to link the nucleon and quark dynamics of the nucleus together. The proposed measurements necessitate the development of a dedicated nuclear low energy recoil tracker (ALERT), that I propose to develop and build in the IPN Orsay laboratory at the Paris-Saclay University (France). This detector will be used at the recently upgraded electron accelerator of Jefferson Lab (USA). This facility offers a unique setup with the most intense multi-GeV electron beam in the world. Together, these three unique measurements form a comprehensive program to decisively advance our understanding of the nuclear structure in terms of quarks and gluons.","1405881","2018-10-01","2023-09-30"
"PASCAL","Probabilistic And Statistical methods for Cosmological AppLications","Domenico Marinucci","UNIVERSITA DEGLI STUDI DI ROMA TOR VERGATA","This is an interdisciplinary project at the interface between Mathematical Statistics, Probability and Cosmological Applications. The principal focus is on the harmonic analysis for isotropic, mean square continuous spherical random fields, with a view to applications to Cosmic Microwave Background radiation data analysis. We will focus in particular on the following issues:
1) the characterizations of isotropy in harmonic space, the analysis of higher order polyspectra and their applications to non-Gaussianity analysis;
2)  the constructions of spherical needlets/wavelets and their stochastic properties;
3) the analysis of random sections of spin fiber bundles on the sphere, as motivated by the analysis of CMB polarization and weak gravitational lensing
4) adaptive density estimation for directional data, as motivated by Cosmic Rays data analysis.","1193000","2011-11-01","2016-10-31"
"PASSME","Air-sea gas exchange - PArameterization of the Sea-Surface Microlayer Effect","Oliver Wurl","CARL VON OSSIETZKY UNIVERSITAET OLDENBURG","The Earth’s oceans absorb about 11 billion tonnes of carbon dioxide (CO2) each year, about 25% of all anthropogenic CO2. The oceans are huge reservoirs of CO2, and a better understanding on how the oceans absorb CO2 is critical for predicting climate change. The sea-surface microlayer (SML), the aqueous boundary layer between the ocean and atmosphere, plays an important role in the exchange of gases between the ocean and atmosphere. The effects of the SML on air-sea gas exchange have been widely ignored by past and current research efforts due to uncertainties to what extent the SML covers the oceans.  However, we recently reported the ubiquitous coverage of the oceans with SML, which pushes the SML into a new and wider context that is relevant to many ocean and climate sciences.

I propose experiments at multiple scales, i.e. in laboratory tanks, wind wave tunnel, mesocosm and during a long-term field study. I propose a systematic field study measuring air-sea CO2 fluxes and mapping chemical, biological and physical properties of the SML. With the experiments on smaller scales, such measurements will allow for the first time (i) to define new parameters controlling gas fluxes, (ii) to quantify short-time and seasonal variability, (iii) to define global proxies for the effects of the SML, and (iv) to develop and apply a new parameterization for the correction of global CO2 flux data. For the first time, biogeochemical processes relevant to carbon cycling are investigated on the ocean’s surface at an interfacial level. Furthermore, I aim to reconstruct the natural composition of the SML in a wind-wave tunnel to study its ability to modify the ocean’s surface at well-defined wind regimes.

The results from the proposed studies can form the basis for an improvement of current assessments of CO2 fluxes, and oceanic uptake rates. A better understanding in the oceanic uptake of atmospheric CO2 is critical in predicting climate trends and establishing policies.","1485797","2014-04-01","2019-03-31"
"PATHWISE","Pathwise methods and stochastic calculus in the path towards understanding high-dimensional phenomena","Ronen ELDAN","WEIZMANN INSTITUTE OF SCIENCE LTD","Concepts from the theory of high-dimensional phenomena play a role in several areas of mathematics, statistics and computer science. Many results in this theory rely on tools and ideas originating in adjacent fields, such as transportation of measure, semigroup theory and potential theory. In recent years, a new symbiosis with the theory of stochastic calculus is emerging.

In a few recent works, by developing a novel approach of pathwise analysis, my coauthors and I managed to make progress in several central high-dimensional problems. This emerging method relies on the introduction of a stochastic process which allows one to associate quantities and properties related to the high-dimensional object of interest to corresponding notions in stochastic calculus, thus making the former tractable through the analysis of the latter. 

We propose to extend this approach towards several long-standing open problems in high dimensional probability and geometry. First, we aim to explore the role of convexity in concentration inequalities, focusing on three central conjectures regarding the distribution of mass on high dimensional convex bodies: the Kannan-Lov'asz-Simonovits (KLS) conjecture, the variance conjecture and the hyperplane conjecture as well as emerging connections with quantitative central limit theorems, entropic jumps and stability bounds for the Brunn-Minkowski inequality. Second, we are interested in dimension-free inequalities in Gaussian space and on the Boolean hypercube: isoperimetric and noise-stability inequalities and robustness thereof, transportation-entropy and concentration inequalities, regularization properties of the heat-kernel and L_1 versions of hypercontractivity. Finally, we are interested in developing new methods for the analysis of Gibbs distributions with a mean-field behavior, related to the new theory of nonlinear large deviations, and towards questions regarding interacting particle systems and the analysis of large networks.","1308188","2019-01-01","2023-12-31"
"PBM - FIMBSE","Partial Behaviour Modelling: A Foundation for Incremental and Iterative Model-Based Software Engineering","Sebastian Uchitel","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","Software systems are amenable to analysis through the construction of behaviour models. This corresponds to the traditional engineering approach to construction of complex systems. The advantage of using behaviour models to describe systems is that they are cheaper to develop than the actual system. Consequently, they can be analysed and mechanically checked for properties in order to detect design errors early in the development process and allow cheaper fixes. Although behaviour modelling and analysis has been shown to be successful in uncovering subtle requirements and design errors, adoption by practitioners has been slow. One of the reasons for this is that traditional approaches to behaviour models are required to be complete descriptions of the system behaviour up to some level of abstraction, i.e., the transition system is assumed to completely describe the system behaviour with respect to a fixed alphabet of actions. This completeness assumption is limiting in the context of software development process best practices which include iterative development, adoption of use-case and scenario-based techniques and viewpoint- or stakeholder-based analysis; practices which require modelling and analysis in the presence of partial information about system behaviour. This aim of this project is to shift the focus from traditional behaviour models to partial behaviour models – operational descriptions that are capable of distinguishing known behaviour (both required and proscribed) from unknown behaviour that is yet to be elicited. Our overall aim is to develop the foundations, techniques and tools that will enable the automated construction of partial behaviour models from multiple sources of partial specifications, the provision of early feedback through automated partial behaviour model analysis, and the support incremental, iterative elaboration of behaviour models.","1366450","2008-09-01","2014-05-31"
"pcCell","Physicochemical principles of efficient information processing in biological cells","Frank Noe","FREIE UNIVERSITAET BERLIN","Biological cellular function relies on the coordination of many information processing steps in which specific biomolecular complexes are formed. But how can proteins and ligands find their targets in extremely crowded cellular membranes or cytosol? Here, I propose that intracellular signal processing depends upon the spatiotemporal order of molecules arising from “dynamical sorting”, i.e. no stable structure may exist at any time and yet the molecules might be ordered at all times. I propose to investigate the existence and the physicochemical driving forces of such dynamical sorting mechanisms in selected neuronal synaptic membrane proteins that must be tightly coordinated during neurotransmission and recovery. To this end, massive atomistic and coarse-grained molecular simulations will be combined with statistical mechanical theory, producing predictions to be experimentally validated through our collaborators.
The main methodological challenge of this proposal is the infamous sampling problem: Even with immense computational effort, unbiased molecular dynamics trajectory lengths are at most microseconds for the protein systems considered here. This is insufficient to calculate relevant states, probabilities, and rates for these systems. Based on recent mathematical and computational groundwork developed by us and others, I propose to develop enhanced sampling methods that will yield an effective speedup of at least 4 orders of magnitude over current simulations. This will allow protein-ligand and protein-protein interactions to be sampled efficiently using atomistic models, thus having far reaching impact on the field.
The proposed project is at the forefront of an interdisciplinary field, spanning traditional areas such as physical chemistry, computer science, mathematics, and biology. The project claims fundamental advances in both simulation techniques and in the understanding of the physicochemical principles that govern the functionality of the cell","1397328","2013-01-01","2017-12-31"
"PCG","The Elementary Theory of Partially Commutative Groups","Ilya Kazachkov","UNIVERSIDAD DEL PAIS VASCO/ EUSKAL HERRIKO UNIBERTSITATEA","""The solution of Tarski's problems on the first-order theory of free
groups has uncovered deep connections between Model Theory, Geometry
and Group Theory and served as a nexus and  motivation for many
classical results in Geometric Group Theory and Theoretical Computer
Science.

Just as the Tarski problems connected the theory of free groups with
the geometry of trees, our goal is to point at a new direction in
Group Theory and develop appropriate generalisations of the techniques
and results whose nature is based on the geometry of higher
dimensional counterparts of trees and interplays with the theory of
partially commutative groups, notably the theory of groups acting on
real cubings.

We then shall apply these tools to approach fundamental questions in
the model theory of partially commutative groups: classify finitely
generated groups elementarily equivalent to a given partially
commutative group and prove decidability and stability of their
first-order theory.""","1021217","2014-09-01","2019-08-31"
"PDF4BSM","Parton Distributions in the Higgs Boson Era","Juan Rojo Chacon","STICHTING VU","With the recent discovery of a Higgs-like particle at the Large Hadron Collider (LHC), particle physics has entered a completely new era. The central goal for high energy physics in the following years will be the detailed determination of the properties of this new particle, in particular checking its consistency with the Standard Model Higgs boson hypothesis, and to further explore the highest energy domain in search for further new physics, like supersymmetry or extra dimensions, closely related to the Higgs-like boson properties and to dark matter and dark energy studies. It is thus of paramount importance to be able to provide accurate theoretical predictions for signal and background processes both for Higgs production and for hypothetical  new particles, in order to optimize both the characterization of cross sections, couplings and branching fractions, but also to maxime the LHC discovery potential. A crucial ingredient of these theoretical predictions for an hadron collider as the LHC are the Parton Distribution Functions (PDFs) of the proton. This project aims to fully exploit the LHC potential to achieve the ultimate experimental and theoretical precision in the determination of PDFs to make essential contributions to our understanding of the structure of the nucleon, in particular in the regions more relevant for Higgs and BSM physics searches at the LH, the match between PDFs and NLO Monte Carlo event generators, a crucial tool for accurate exclusive event description at the LHC, and to  propose new avenues in New Physics searches from precision LHC measurements, where PDFs are often the dominant systematic uncertainties.","1330502","2014-07-01","2019-06-30"
"PEBBLE2PLANET","From pebbles to planets: towards new horizons in the formation of planets","Anders Johansen","LUNDS UNIVERSITET","""The goal of this ERC Starting Grant proposal is to make significant advances in our understanding of how planetesimals and gas giant planets form. I propose an ambitious research programme dedicated to answering three key questions at the frontier of planet formation theory: - How do mm-sized particles grow past the bouncing barrier? - What is the Initial Mass Function of planetesimals? - How do the cores of gas giants form and evolve?} I will address these questions using a combination of novel ideas and computer simulations to model three critical stages of planet formation: 1) the growth of pebbles into rocks and boulders by coagulation and vapour condensation, 2) the gravitational collapse of clumps of rocks and boulders into planetesimals with an array of sizes, and 3) the long term growth of planetesimals as they grow to become cores of gas giants by accreting pebbles embedded in the gas. These investigations will form an important theoretical foundation for understanding the next generation of observations of protoplanetary disc pebbles, planetesimal belts, and planetary systems. The self-consistent models for the formation of planets resulting from this proposal will shed light on the spatial distribution of pebbles in gas discs around young stars (observable with the ALMA telescopes), on the initial state of planetesimal belts (crucial for understanding the evolution of debris discs observable with JWST and the asteroid and Kuiper belts), and on the formation and evolution of the wealth of exoplanetary systems detected in the near future (by astrometry with the Gaia satellite, by ground-based radial velocity surveys, and by direct imaging with E-ELT).""","1332467","2012-01-01","2016-12-31"
"PECS","Powerful and Efficient EUV Coherent Light Sources","Jens Limpert","FRIEDRICH-SCHILLER-UNIVERSITAT JENA","The interest in and hence the need for coherent short-wavelength (EUV spectral range) laser sources is rapidly increasing. Potentially, such sources will allow for novel approaches in fundamental science, metrology, imaging, spectroscopy and might even enable new lithographic production techniques. One example is Atto-Science, one of the groundbreaking topics in physical science of the next decade. High harmonic generation (HHG) in noble gases is considered as the most suitable technique to generate spatially coherent EUV light. Unfortunately, the conversion efficiency of HHG is rather small. Furthermore, conventional ultra-short pulse laser sources required for HHG are limited in average power due to thermo-optical problems, therefore, the resulting EUV radiation is characterized by an extremely low number of photons per unit time (average power). Consequently, all the applications of EUV radiation suffer from the lack of powerful coherent light sources in this interesting spectral range, which results extremely long integration or processing times. Ultimately this lack of power make current EUV sources be considered as laboratory curiosities without any relevance for real world applications. The goal of the proposed project is to investigate novel methods to increase the efficiency and the average output power of HHG based EUV light sources. Of outmost importance is the development of efficient, compact and powerful (&gt;3 kW average power) high peak power (&gt;100 MW) ultra-short pulse (&lt;200 fs) laser systems to drive the HHG process. Fiber based amplifiers have the potential to fulfil this parameter range in a compact und ultra-stable manner allowing the generation of EUV radiation outside a specially protected laboratory environment. In summary, the goal of the proposed project is the development of efficient and powerful tailored, meaning application-oriented, EUV light sources.","1450000","2009-11-01","2013-10-31"
"PEDAL","Plasmonic Enhancement and Directionality of Emission for Advanced Luminescent Solar Devices","Sarah Josephine Mc Cormack","THE PROVOST, FELLOWS, FOUNDATION SCHOLARS & THE OTHER MEMBERS OF BOARD OF THE COLLEGE OF THE HOLY & UNDIVIDED TRINITY OF QUEEN ELIZABETH NEAR DUBLIN","Applying photovoltaic (PV) panels to buildings is an important application for wider PV deployment and to achieving our 20% Renewable Energy EU targets by 2020.  PEDAL will develop a disruptive PV technology where record increases in efficiency are achieved and costs dramatically reduced; 
(1) Diffuse solar radiation will be captured to produce higher efficiencies with concentration ratios over 3 in plasmonically enhanced luminescent solar concentrators (PLSC).  Current LSC efficiency achieved is 7.1%, [1].  This proposal will boost efficiency utilising metal nanoparticles (MNP) tuned to luminescent material type in LSCs, to induce plasmonic enhancement of emission (PI and team have achieved 53% emission enhancement).  MNP will be aligned to enable directional emission within the LSC (being patented by PI and team).  These are both huge steps in the reduction of loss mechanisms within the device and towards major increases in efficiency.
(2) Plasmonically enhanced luminescent downshifting thin-films (PLDS) will be tailored to increase efficiency of solar cells independent of material composition. MNP will be used, where the plasmonic resonance will be tailored to the luminescent species to downshift UV. MNP will be aligned to enable directional emission within the PLDS layer, reducing losses enabling dramatic increases in a layer adaptable to all solar cells.
(3) These novel systems will be designed, up-scaled and a building integrated component fabricated, with the ability not only to generate power but with options for demand side management. 
Previous work has been limited by quantum efficiency of luminescent species, with this breakthrough in both the use of MNP for plasmonic emission enhancement and alignment inducing directionality of emission,  will lead to efficiencies of both PLSC and PLDS being radically improved.  PEDAL is a project based on new phenomena that will allow far reaching technological impacts in solar energy conversion and lighting.","1447410","2015-04-01","2020-03-31"
"PEP-PRO-RNA","Peptide-derived bioavailable macrocycles as inhibitors of protein-RNA and protein-protein interactions","Tom Grossmann","STICHTING VU","The objective of this proposal is the elucidation of general principles for the design of bioavailable peptide-derived macrocyclic compounds and their use for the development of inhibitors of protein‒protein (PPI) and protein‒RNA interactions (PRI). Over the last decade, drug discovery faced the problem of decreasing success rates which is mainly caused by the fact that numerous novel biological targets are reluctant to classic small molecule modulation. In particular, that holds true for PPIs and PRIs. Approaches that allow the modulation of these interactions provide access to therapeutic agents targeting crucial biological processes that have been considered undruggable so far.  Herein, I propose the use of irregularly structured peptide binding epitopes as starting point for the design of bioactive macrocycles. In a two-step process high target affinity and bioavailability are installed:
1) Peptide macrocyclization for the stabilization of the irregular bioactive secondary structure
2) Evolution of the cyclic peptide into a bioavailable macrocyclic compound
Using a well-characterized model system developed in my lab, initial design principles will be elucidated. These principles are subsequently used and refined for the development of macrocyclic PPI and PRI inhibitors. The protein‒protein and protein‒RNA complexes selected as targets are of therapeutic interest and corresponding inhibitors hold the potential to be pursued in subsequent drug discovery campaigns.","1499269","2016-03-01","2021-02-28"
"PEQEM","Photonics for engineered quantum enhanced measurement","Jonathan MATTHEWS","UNIVERSITY OF BRISTOL","Advances in measurement always lead to dramatic advances in science and in technology. Our society is now heavily dependent on the sensors that permeate environmental monitoring, security, healthcare and commerce. This is quantified by the global sensing market worth rising from $110 billion in 2015 to $124 billion in 2016, and is predicted to continue to rise to $240 billion by 2022. Now, our rapidly growing understanding of how to control quantum systems vastly expands both the potential performance and application for measurement and sensing using quantum-enhanced techniques. But these techniques will only efficiently find disruptive use once they are engineered for robustness, deliver desired operational parameters and are shown to work in a platform that can be mass-produced.

This project adopts an engineering approach to the disciplines of photonic quantum enhanced sensing and squeezed light quantum optics. We will develop integrated photonics that are tailored to enable miniature, deployable and ultimately low cost sensors that exceed the state of the art through (i) exploitation of the quantum mechanics of light and by (ii) developing the requisite high performance of components in an integrated photonics platform. The methodology is to combine quantum optics of Kerr-nonlinear materials that generate squeezed light and quantum state detection with photonic device engineering. We will benchmark device performance using quantum metrology techniques. By the end of this project, we will have developed all-integrated squeezed light generation and detection technology, that provides enhanced sensors for absorption and phase measurements beyond the shot noise limit --- the hard limit that bounds performance of state of the art “classical” sensors. Applications include next generation quantum metrology experiments, measurement of photo-sensitive samples, precise characterization of photonic components and trace gas detection.","1497890","2019-01-01","2023-12-31"
"PERCENT","Percolating Entanglement and Quantum Information Resources through Quantum Networks","Antonio Acín","FUNDACIO INSTITUT DE CIENCIES FOTONIQUES","Quantum communication networks consist of several nodes that are connected by quantum channels. By exchanging quantum particles, the nodes share quantum correlations, also know as entanglement. Essential for the future development of quantum communication is to understand the design of efficient protocols for the distribution of entanglement between arbitrarily distant nodes. The main objective of the present proposal is to construct the theory of entanglement distribution through quantum networks. At present, very little is known about this fundamental problem, namely about which properties of a quantum network are required to be able to establish entanglement over large distances. Very recently, we have proved that the distribution of entanglement through quantum networks defines a new type of critical phenomenon, an entanglement phase transition called entanglement percolation. These surprising effects do not appear in the standard repeater configuration previously considered. Crucial for the construction of these examples is the use of concepts already known in statistical mechanics, such as percolation. Our scope is to go far beyond these proof-of principle examples and derive the general theoretical framework describing entanglement percolation, exploiting the connection between statistical concepts and entanglement theory. The obtained framework will also be applied to other information resources, such as secret bits. Then, the ultimate aim of the project is to provide a global picture of the distribution of quantum information resources over realistic quantum communication networks.","699600","2008-11-01","2013-12-31"
"PERDY","Perceptually-Driven Optimizations of Graphics Content for Novel Displays","Piotr Didyk","UNIVERSITA DELLA SVIZZERA ITALIANA","Displays play a vital role in many professional and personal activities. They are a crucial interface between a user and the digital world in tasks involving visualization and interaction with digital data. The abilities of new display technologies regarding reproduction of important visual cues, such as binocular disparity, accommodation, or motion parallax, outperform the capabilities of methods for optimizing graphics content to match the requirements of particular hardware designs. This leads to a poor visual quality and massive computational overhead, which hamper the adoption of novel displays. I argue that there are significant gaps between hardware, computational techniques, and understanding of human perception, which prevents taking full advantage of these technologies.

To overcome these limitations, I and my team will combine hardware, computation, and perception into a unique platform where the capabilities of displays and quality requirements are represented in a shared space. The basis for our project will be in-depth understanding of human perception. Our experiments will focus on three aspects: (1) investigation of perceptual limits across a wide field of view, (2) involving all visual cues, and (3) establishing optimal trade-offs between different quality aspects. We will build efficient computational models that will predict perceived quality and enable perceptual optimizations to drive new content adaptation techniques.

This project will contribute display-specific perceptual optimizations of graphics content to match the requirements of human perception. It will address the key aspects of portable devices such as energy efficiency and visual quality. Our experiments and modeling of human perception will provide crucial insights into new hardware developments. The contributions will be necessary for development and standardization of new, high-quality display devices which will not only improve existing applications but also enable new ones.","1497302","2019-02-01","2024-01-31"
"PETA-CARB","Rapid Permafrost Thaw in a Warming Arctic and Impacts on the Soil Organic Carbon Pool","Guido Grosse","ALFRED-WEGENER-INSTITUT HELMHOLTZ-ZENTRUM FUR POLAR- UND MEERESFORSCHUNG","In a warming Arctic, frozen soil organic carbon (SOC) stored in permafrost will increasingly become vulnerable to thaw and mobilization. Over millennia, permafrost soils accumulated about 1672 Petagram of SOC, about twice the carbon currently in the atmosphere. Rapid permafrost thaw (thermokarst) releases fossil SOC as greenhouse gases, constituting a positive feedback to global warming. However, complex landscape, hydrological, and ecological feedbacks necessitate quantification of landscape scale carbon pools and fluxes in Arctic permafrost regions. A globally important question is whether permafrost soils will turn from a natural carbon sink into a source.
The project combines remote sensing based change detection, mapping, and spatial data analysis for permafrost landscapes, quantitative field studies, and modelling of thermokarst processes to quantify the size and vulnerability of deep permafrost SOC pools to rapid permafrost thaw and resulting impacts. The three research topics are: (1) Systematic measurement of rapid permafrost thaw, (2) Determining deep permafrost SOC stocks and carbon accumulation rates, and (3) Quantification of deep permafrost SOC pools and vulnerability assessment.
The project will provide for the first time quantitative data on rapid permafrost thaw over large regions, provide first-time data on the size of SOC pool components related to thermokarst, substantially enhance previous SOC pool estimates for Yedoma deposits and arctic river deltas, and characterize overall permafrost SOC distribution and vulnerability to thaw. It will answer the question of how climate change affects permafrost SOC pools and how permafrost thaw feeds back to climate.","1786966","2013-11-01","2018-10-31"
"PETADISK","Petascale numerical simulations of protoplanetary disks: setting the stage for planet formation","Sebastien Fromang","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","The main goal of this proposal to the ERC Starting Grant scheme is to make ground-breaking progress in our understanding of the dynamical processes that shape the structure of protoplanetary disks. This will be achieved by performing state-of-the-art high resolution numerical simulations of protoplanetary disks, using novel computing techniques and taking advantage of the future European petascale supercomputers. The project will address the following fundamental questions in accretion disks theory:
- What are the properties of MHD turbulence in protoplanetary disks?
- What are the effects of radiative processes on protoplanetary disks structure?
- What are the consequences of dead zones for protoplanetary disk structure?
In addition, the project will look for potential observational signatures of these processes that might be detected by ALMA. Since planetary systems like our own are believed to emerge from protoplanetary disks, the project will make decisive contributions in describing the structure of the environment in which planetary systems form, the interest of which extends to the entire planet formation community.","1093152","2011-09-01","2016-08-31"
"PETAL","Polarization condEnsation for Telecom AppLications","Julien Fatome","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""The aim of the PETAL project is to provide a radically novel approach to polarization control issues and to transform this parameter into an additional fully exploited asset rather than a problem to be avoided. While current opto-electronic technologies are principally based on complex active-feedback loop control and algorithms, the breakthrough idea of PETAL is to explore a new type of phenomenon based on the unexpected ability of light to self-pull, self-trap and self-stabilize its own polarization state. Based on a nonlinear effect occurring in optical fibers, this all-optical, broadband and quasi-instantaneous polarization condensation phenomenon could find many applications in photonics and open up the path to new exciting researches and horizons.
In this project, PETAL will first focus on proof-of-principle and theoretical/numerical modeling of the polarization condensation phenomenon before implementing this concept in novel and original optical functions for telecommunication applications. In particular, PETAL will report the first experimental observation of an all-optical self-stabilization and control of signal polarization with an error free transmission. PETAL will also show that polarization condensation could provide optical regeneration or detection of polarization multiplexed signals and could be used to implement ideal polarization beam splitter or simplify current coherent receiver. Based on this novel concept, PETAL will also demonstrate new all-optical functions for signal processing such as optical flip-flop memory, isotropic-like span transmission or polarization-based router. Moreover, PETAL aims to go beyond the polarization issues and will generalize this concept to spatial mode multiplexing applications. Finally, miniaturization and multi-implementation of these novel functions will be carried out in a same device so as to report the first field-trial experiment of such a technology.""","1452818","2012-10-01","2017-09-30"
"PETALO","A positron emission tomography apparatus based on liquid xenon with time of flight applications","Paola FERRARIO","FUNDACION DONOSTIA INTERNATIONAL PHYSICS CENTER","This project presents a new technology for detectors used in positron emission tomography (PET), based on liquid xenon instead of current scintillator crystals. The basic element is a liquid xenon scintillating cell, with its size optimized to maximize the number of gammas that interact in the cell. Silicon photomultipliers read out by low power, low noise customized integrated circuits for time of flight applications will be used as sensors.
Xenon is a noble gas which scintillates as response to ionizing radiation. Scintillation is very fast and intense, which results in the possibility of building a PET of good energy and spatial resolution and excellent time resolution. This, in turn, makes possible the measurement of the time-of-flight (TOF), which increases the sensitivity of the detector. Recently, the PI has published a Monte Carlo study of the coincidence resolving time that can be achieved by the PETALO technology, obtaining the promising result of less than 100 ps FWHM, which would be a break-through in the PET scanner field. The low cost of liquid xenon compared to conventional scintillating crystals opens two possible applications: one one hand, a full body PET reducing the cost and with an already better performance than the current technology; on the other hand, a smaller brain scanner, optimized to maximize the improvement in the performance with TOF measurements.
This project will demonstrate the technological and commercial feasibility of the proposed technology. For this purpose, first a set of prototypes with two cells will be built to evaluate the resulting performance of the PETALO technology using different kinds of photosensors (UV light sensitive SiPMs versus conventional ones coated with a wavelength shifter). In a second phase, a full ring of the dimensions of a brain scanner will be built, using the technology that has performed better according to the results of this first phase.","1500000","2018-07-01","2023-06-30"
"PeV-Radio","Digital Radio Detectors for Galactic PeV Particles","Frank Schröder","KARLSRUHER INSTITUT FUER TECHNOLOGIE","The most energetic particles in our Galaxy are accelerated by yet unknown sources to energies much beyond the reach of human-made accelerators such as LHC at CERN. The detection of PeV photons from such a natural Galactic accelerator will be a fundamental breakthrough. For this purpose I propose a digital radio array for air showers at South Pole building on my proven expertise in successfully setting up and managing an antenna array in Siberia. Recently, we have discovered that by using higher radio frequencies than before the energy threshold can be lowered dramatically from 100 PeV to about 1 PeV. The new radio array will significantly enhance the present PeV particle detectors at South Pole in both, accuracy and aperture towards lower elevations. One of the most promising candidates for the origin of cosmic rays, the Galactic Center presently outside of the field of view, will be observable 24/7 with the radio array. The extrapolation of classical TeV observations predicts more than twenty PeV photons to be detected by the radio array within three years. Since the radio array is sensitive simultaneously to cosmic photons and charged particles from all directions of the sky, the search for any photon sources can be done in parallel to cosmic-ray physics with unprecedented accuracy and exposure in the energy range of 1 PeV to 1 EeV. Thus, this radio array will create highest impact in astroparticle physics by the following scientific objectives all targeting the most energetic particles in our Galaxy: PeV photons and their correlation with sources of neutrinos and charged cosmic rays, mass separation of cosmic rays, search for mass-dependent anisotropies, particle physics beyond the reach of LHC. This timely proposal is a unique chance for European leadership in this novel technique. It provides the chance for scientific breakthrough by detection of the first PeV photons ever, and by the discovery of natural accelerators of multi-PeV particles.","1629541","2019-02-01","2024-01-31"
"PFPMWC","Probing fundamental physics with multi-wavelength cosmology","Michael Brown","THE UNIVERSITY OF MANCHESTER","""Recent measurements of the Cosmic Microwave Background (CMB) combined with the large-scale distribution of galaxies have defined a standard cosmological model. This model is a remarkable fit to the observations but also raises profound questions - we do not know how the initial conditions were imprinted in the early Universe, nor do we know the nature of the dark energy. In this proposal, I request funds to set up a new cosmology data analysis team at the University of Manchester to tackle these issues. Over the next five years, my team will pursue a program of work intended to have a large impact on two hugely important fields of observational cosmology which are uniquely suited to answering these questions --- the polarisation of the microwave background and weak gravitational lensing. The former is the most powerful way to probe the early Universe while the latter is potentially the most sensitive probe of dark energy.  Building on the innovative methods I developed for the QUaD experiment, I will apply new analysis techniques to mitigate systematics and maximise the science return from current and future CMB polarisation experiments including the Planck satellite, the ground-based QUIJOTE experiment and phase 2 of the ground-based QUIET experiment. In the field of weak lensing, I will perform pioneering radio lensing analyses with forthcoming instruments including the Square Kilometre Array pathfinders, e-MERLIN and MeerKAT. One particularly novel idea which I will develop is the use of polarisation information to reduce noise and to minimise contamination from the intrinsic alignment of galaxies in radio lensing analyses.  The research described in this proposal will allow my team to establish an international leadership position in both CMB polarisation and radio weak lensing research in advance of a possible CMB polarisation satellite mission and the commissioning of the Square Kilometre Array radio telescope towards the latter part of this decade.""","1424269","2012-01-01","2016-12-31"
"PHAROS","Guiding Light through Disorder in Adaptive Photonic Resonator Arrays","Allard Pieter Mosk","UNIVERSITEIT UTRECHT","Planar photonic crystals are dielectric nanostructures that are pursued worldwide as a platform for integrated nanophotonic circuits. Such circuits will process signals coded in light and will consist of thousands of basic components such as resonant nanocavities. At present, unavoidable nanometer-scale disorder makes such large-scale integration impossible. Disorder causes the resonances of the nanocavities to shift randomly, resulting in Anderson localization, an interference effect that blocks the propagation of light. Anderson localization – predicted in 1958 by Nobel Prize winner Philip Anderson – is an intriguing scientific phenomenon as well as a serious threat to applications.

I propose to create adaptive nanophotonic systems. In these systems, I will use a spatially modulated light beam to modify the resonance frequency of each individual nanocavity. After adaptive tuning, the spatially structured light exactly counteracts the disorder and guides signals safely through the nanophotonic circuit. Effectively the signals will propagate in a perfect nanophotonic structure. As a second main innovation, I will employ an ultrafast structured light beam to write new, ordered and functional patterns into the circuit. This transformational technology will enable applications wherein optical circuits become fully programmable. The circuit will be modified dynamically in less time than that needed for a photon to pass through it. Spatial light modulators will enable us to address and control thousands of individual nanophotonic components.

Our dynamic and adaptive nanophotonic system will enable new technology, such as dynamically tunable delay lines, and open up new regimes of light propagation: the crossover regime of Anderson localization, ultraslow light that propagates scarcely faster than sound, and dynamic light propagation where the time dependence of the nanostructure drastically influences the flow of light.","1496400","2011-10-01","2017-01-31"
"PHDVIRTA","Physically-based Virtual Acoustics","Kalle Tapio Lokki","AALTO KORKEAKOULUSAATIO SR","The objective of the project is to find new methods for quality evaluation and modeling of room acoustics. Room acoustics has been studied over 100 years, but, e.g., the relation between objective attributes and subjective measures is not fully understood yet. This project will develop novel methods to simulate and auralize sound propagation in rooms, in particular in concert halls. The research is divided into three main topics. First, authentic auralization with physically-based room acoustics modeling methods will be studied. The recently introduced acoustic radiance transfer method is developed further to handle complex reflections from surfaces as well as diffraction. The second research topic is quality evaluation of concert hall acoustics. Novel algorithms will be developed for spatial sound analysis of a large impulse response database. Live recordings will be analyzed to find new objective quality measures. Quality assessments will also be performed subjectively with sensory evaluation methods borrowed from food industry. The third topic is related to augmented reality audio technology, which reveals the potential and richness of emerging technologies, giving a scenario of the possible future personalized mobile audio communications. The results of the project will be widely applicable in the academia, but also in every day life of people all over the world. The new knowledge in room acoustics will help to build acoustically better concert halls and public places such as libraries, shopping malls, etc. The augmented reality audio applications will help and enrich communication between humans. The concert hall acoustics research has great potential to find novel objective and subjective quality metrics. They also help in creation of authentic auralization, which will be one of the main tools for consultants in design, and in particular when explaining design results to architects, clients, and public audience.","880224","2008-07-01","2013-06-30"
"PHELIX","Photo-Engineered Helices in Chiral Liquid Crystals","Nathalie Helene Katsonis","UNIVERSITEIT TWENTE","Supramolecular helices are a striking expression of chirality which is found at every level of biological materials, from plant cell walls to bones. Helical biomaterials formed out of equilibrium display multiple length scales, adaptation of structure to function and responsiveness to changing environments, a unique set of features that constitutes a fascinating source of inspiration for materials science. However, matching the complexity of these biological architectures by rational design of synthetic systems remains a major contemporary challenge. The aim of this project is to develop sophisticated helical materials with responsive architectures that are of interest in optical communication, energy management, photonic materials and mechanical actuation.
The innovative and versatile approach proposed here consists in using light i) to engineer the period, handedness and orientation of the cholesteric helix, and ii) to stabilise the structures formed out of equilibrium by in-situ formation of polymer networks. Three tasks will run concurrently:
Task 1: Stimuli-responsive infrared super-reflectors
Task 2: Dynamic templates for long range ordering of nano-objects
Task 3: Photomechanical actuation of helicoids and spiral ribbons
“Phelix” will yield complex systems that reach beyond the state of the art in stimuli-responsive materials, push the frontiers of research on supramolecular helices and shed new light on transmission of chirality across length scales. Ultimately, the omnipresence of helical structures in nature means that biomedical applications could be envisioned also. The proposal builds on my recent investigations on light-responsive helices in cholesteric liquid crystals. I have demonstrated the expertise in liquid crystals, photochemistry and microscopy required for this research and my leadership experience ensures its success.","1496400","2012-11-01","2017-10-31"
"Pho-T-Lyze","Photonic Terahertz Signal Analyzers","Sascha PREU","TECHNISCHE UNIVERSITAT DARMSTADT","Spectrum analysis and vector network analysis are enabling technologies for component development throughout the microwave and millimeter wave band. Due to the lack of affordable electronics for frequencies above 100 GHz, vector network analyzers (VNAs) have to use frequency extenders to reach into the THz frequency band (100 GHz-10 THz). The bandwidth of frequency extended electronic systems is restricted to about 50%. Several extender setups have to be used for larger spans, requiring realignment and tedious recalibration. Further, extenders become increasingly expensive the higher the THz frequency and are, therefore, barley used. Electronic spectrum analyzers face similar problems as VNAs. Only a few examples of highly expensive photonic, pulsed frequency comb-based systems have been demonstrated. Affordable, large bandwidth commercial THz metrology tools are missing so far. 
This proposal aims for the development of photonic THz characterization tools based on telecom-wavelength compatible photomixing technology to satisfy this need:
1.) Photonic vector network analysers (PVNAs) with extreme frequency coverage will be realized by two approaches:
   a) A planar, on-chip, and broadband dielectric waveguide topology with integrated photomixers for the realization of a continuous-wave (CW) two-port PVNA, covering at least 100 GHz to 1.1 THz in a single setup.
   b) A pulsed, free space photonic two-port VNA for frequency extension towards 5 THz.
2.) CW photonic THz spectrum analyzers (PSAs) with a frequency coverage of at least 50 GHz -1.1 THz and a simple extension towards 2.7 THz. This system will be realized both free space and on-chip by using a photonic sweep oscillator that is mixed with the signal to be investigated and down-converted by a room-temperature operating THz detector. 
These systems will provide a solid basis for THz component development. The long term goal beyond this proposal is a competence center for THz device engineering.","1500000","2017-06-01","2022-05-31"
"PHOCONA","Photonics in Flatland: Band Structure Engineering of 2D Excitons in Fluorescent Colloidal Nanomaterials","Iwan Moreels","UNIVERSITEIT GENT","In PHOCONA we aim to develop a new class of highly fluorescent 2D and quasi-2D colloidal nanomaterials for solution-processed coherent light sources and ultrafast single-photon emitters. The 2D excitons are created in suspended semiconductor nanoplates with a thickness below 5 nm, and transition-metal dichalcogenide monolayers. Prepared by colloidal chemistry and liquid-phase exfoliation, respectively, they take advantage of both quantum confinement through their nanoscale dimensions, as well as dielectric confinement via the dielectric mismatch between nanomaterials and surrounding matrix. Via smart nanocrystal design, more specifically by exploiting novel synthesis methods that include asymmetric in-plane confinement potentials, anisotropic crystal structures, and lattice strain-induced band structure modifications in core/shell heterostructures,  the nanomaterials will be shaped toward efficient multiexciton and stimulated emission, as well as stable, blinking-free single-photon emission. The intrinsic 2D exciton properties will be further modified toward ultrafast exciton recombination by coupling the nanomaterials to small mode-volume plasmonic nanocavities, hereby placing them in a local photon density-of-states that will lead to a strong Purcell enhancement. The realization of cost-effective, energy-efficient and highly flexible light emitters, which can be synthesized and processed via methods that are scalable to large areas and volumes, forms an important milestone in the ongoing development of optical and quantum communications technology, as well as new lighting and display applications.","1427625","2017-10-01","2022-09-30"
"PHODIR","PHOtonic-based full DIgital Radar","Antonella Bogoni","CONSORZIO NAZIONALE INTERUNIVERSITARIO PER LE TELECOMUNICAZIONI","PHODIR project aims to study, design and realize a full digital transceiver radar demonstrator based on photonic technology both for signal generation and for RF received signal processing. Hybrid technologies merging second generation optical systems and conventional radar architecture could be the answer to issues deriving from electronic devices poor performances such high SFDR (Spurious Free Dynamic Range) and high phase noise level that are nowadays impeding the construction of a fully digital radar transceiver. Starting from a conventional radar architecture, the generation of high frequency signals in the optical domain at the transmitter section and the use of ultra-high bit rate and short width optical pulse train for RF received signal sampling are the main solutions we are going to investigate in the project to overcome problems related to electronic devices. The innovative research aspects of the proposal are: -Definition of a new full digital radar transceiver architecture based on photonic technology -Development and realization of new electro-optic second generation devices -New parallel signal processing algorithms. The most important benefits arising from the project are: -Electro-optic system integration -New technological development in the design of advanced photonic devices -New technological/scientific development in the design of high performance radar CNIT can provide an high level of experience in the field of photonic devices design and implementation and radar system analysis and design. Theoretical support and experimental test-bed thanks to instrumentations and competences in very high-bit rate Gb/s Optical Time Division Multiplexing Systems and techniques for ultra-fast optical sampling are also provided by CNIT.","1600000","2009-12-01","2013-11-30"
"PHONOMETA","Frontiers in Phononics: Parity-Time Symmetric Phononic Metamaterials","Johan CHRISTENSEN","UNIVERSIDAD CARLOS III DE MADRID","The boost experienced by acoustic and elastic (phononic) metamaterial research during the past years has been driven by the ability to sculpture the flow of sound waves at will. Thanks to recent developments at the frontiers of phononic metamaterials it can be identified that active phononic control is at the cutting edge of the current research on phononic metamaterials. Introducing piezoelectric semiconductors as a material platform to discover new avenues in wave physics will have the potential to open horizons of opportunities in science of acoustic wave control. Electrically biased piezoelectric semiconductors are non-reciprocal by nature, produce mechanical gain and are highly tunable. 

The aim is to explore novel properties of sound and the ability to design Parity-Time (PT) symmetric systems that define a consistent unitary extension of quantum mechanics. Through cunningly contrived piezoelectric media sculpturing balanced loss and gain units, these structures have neither parity symmetry nor time-reversal symmetry, but are nevertheless symmetric in the product of both. PHONOMETA is inspired and driven by these common notions of quantum mechanics that I wish to translate into classical acoustics with unprecedented knowledge for the case of sound. 

I expect that the successful realization of PHONOMETA has the potential to revolutionize acoustics in our daily life. Environmental and ambient noise stem from multiple scattering and reflections of sound in our surrounding. The extraordinary properties of PT acoustic metamaterials have the groundbreaking potential to push forward physical acoustics with new paradigms to design tunable diode-like behaviour with zero reflections, which is applicable for noise pollution mitigation. Also I anticipate to impact the progress on invisibility cloaks by introducing PT symmetry based acoustic stealth coatings for hiding submarines.","1325158","2016-12-01","2021-11-30"
"PHONUIT","Phononic Circuits: manipulation and coherent control of phonons","Ilaria ZARDO","UNIVERSITAT BASEL","In the last decades, the power to control photons and electrons paved the way for extraordinary technological developments in electronic and optoelectronic applications. The same degree of control is still lacking with quantized lattice vibrations, i.e. phonons. Phonons are the carriers of heat and sound. The understanding and ability to manipulate phonons as quantum particles in solids enable the control of coherent phonon transport, which is of fundamental interest and could also be exploited in applications. Logic operations can be realized with the manipulation of phonons both in their coherent and incoherent form in order to switch, amplify, and route signals, and to store information. If brought to a mature level, phononic devices can become complementary to the conventional electronics, opening new opportunities.
I envision to realize each part of this technology exploiting phonons and to bring them together in an integrated circuit on chip: a phononic integrated circuit. The objective of the proposal is:
A: the realization of coherent phonon source and detector;
B: the realization of phonon computation with the use of thermal logic gates;
C: the realization of phonon based quantum and thermal memories.
To this end it is crucial to engineer nanoscale heterostructures with suitable interfaces, and to engineer the phonon spectrum and the interface thermal resistance. Phonons will be launched, probed and manipulated with a combination of pump-probe experiments and resistive thermal measurements on chip.
The proposed research will be of great relevance for fundamental research as well as for technological applications in the field of sound and thermal management.","1488388","2018-01-01","2022-12-31"
"PHOTO-EM","Solar cells at the nanoscale: imaging active photoelectrodes in the transmission electron microscope","Caterina Ducati","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The exploitation of renewable sources of energy is one of the biggest challenges of our time, with wide ranging implications in both Science and Society. The new generation of dye-sensitized solar cells and hybrid polymer-inorganic solar cells represents one of the most exciting developments in this field. These promising devices based on photoactive nanomaterials can be produced at low cost, but they have an overall power conversion efficiency of 10-12%, attributed to short charge carrier recombination times and diffusion lengths. If we hope to improve this performance we must learn how the solar cells behave at the nanoscale, under realistic working conditions.
To achieve this I propose to study photovoltaic materials in the transmission electron microscope, under photon irradiation. The three main areas to pursue are: a) In situ illumination technique development, b) Study of physical properties of solar cells, c) Theoretical interpretation of the spectroscopy results. The work plan of this ERC project will follow different strands in parallel, so that we can explore this novel field more efficiently.
Our in situ illumination technique will be exported to a new monochromated and aberration corrected transmission electron microscope with very high spatial and energy resolution. The ultimate challenge is to provide maps of the electronic properties and photovoltaic behaviour of a solar cell, in particular to evaluate –on the atomic level- the effect of grain boundaries and surfaces on the performance of the device.
We will study both dye-sensitized and bulk heterojunction solar cells, starting from the individual nanostructured components, with the aim of producing working cross-section devices to be mounted and operated inside the electron microscope. Efficient data processing and theoretical interpretation of the microscopy results will be essential to the success of this process, so we will build capabilities in these areas to support and guide the experimental work.
The team I want to lead in this scientific mission is ideally composed of a postdoctoral research assistant and two PhD students. The postdoc will take care of technique development and theoretical aspects, while the students will concentrate on the study of materials and devices.","1381541","2010-12-01","2015-11-30"
"photocatH2ode","Gathering organic and hybrid photovoltaics with artificial photosynthesis for Photo-Electro-Chemical production of hydrogen","Vincent Marius Lucien Artero","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","The future of energy supply depends on innovative breakthroughs regarding the design of efficient systems for the conversion and storage of solar energy. The production of H2 through direct light-driven water-splitting in a Photo-Electro-Chemical (PEC) cell, appears as a promising solution. However such cells need to respond to three main characteristics: sustainability, cost-effectiveness and stability. Fulfilling these requirements raise important scientific questions regarding the elaboration and combination of the best materials able to harvest light and catalyse H2 and O2 evolution.
The objective of this project is to design an operating photocathode based on Earth abundant elements for PEC H2 production, answering therefore the sustainability and cost issues. The novelty relies on the approach gathering organic and hybrid photovoltaics with artificial photosynthesis to design new materials and architectures: I will combine and immobilize molecular photosensitizers with bioinspired catalysts on an electrode thanks to electronic junctions. This will allow (i) optimizing light-driven charge separation, (ii) driving electrons from the electrode to the catalyst, (iii) and limiting charge recombination processes.
The project is divided into four tasks. The two first tasks are focused on the elaboration of new photoelectrode architectures: In task 1, I propose to engineer a H2-evolving electrode thanks to donor-acceptor dyes immobilized on p-type semi-conductors. In task 2, I propose to implement organic photovoltaics materials in a H2-evolving electrode. The third task focuses on the elaboration of new catalysts, incorporating redox-active  (non-innocent) ligands in order to systematically bias electron transfer towards the catalyst. These new catalysts will be implemented on the new photoelectrode architectures.
The last task focuses on the ultimate assembly of a PEC cell and on the performance assessments at all steps of the project (photocathodes and full cell).","1500000","2012-12-01","2017-11-30"
"PHOTOCHROMES","Photochromic Systems for Solid State Molecular Electronic Devices and Light-Activated Cancer Drugs","Joakim Andréasson","CHALMERS TEKNISKA HOEGSKOLA AB","Photochromic molecules, or photochromes, can be reversibly isomerized between two thermally stable forms by exposure to light of different wavelengths. Upon isomerization, properties such as excitation energies, redox properties, charge distribution, and structure experience significant changes. These changes can be harnessed to switch “on” or “off” the action of a variety of photophysical processes in the photochromic constructs, e.g., energy and electron transfer. Until now, the focus of my research has been to show proof of principle for a large selection of molecule-based photonically controlled logic devices (solution based) with the functional basis in the switching of the transfer processes mentioned above. Now, I wish to extend the study to include experiments in the solid state, e.g., polymer matrices. Taking the step into doing solid state chemistry is not only a prerequisite for any real-world application. It will also allow for experiments that cannot be performed in fluid solution, such as aligning molecules in a stretched film for chemistry with polarized light, and immobilization of molecules for selective addressing in a three-dimensional array of volume elements. Furthermore, I intend to investigate the possibility to photonically control the membrane penetrating and the DNA-binding abilities of photochromes, aiming at, in a long-term perspective, light-activated cancer drugs. Due to the fact that both the structure and the charge distribution of a photochrome may change drastically upon isomerization, one of the two isomeric forms is often suitable for penetrating a membrane. Inside the membrane, e.g., in a cell, the photochrome can be photo-isomerized to a structure with high affinity for strong binding to DNA. Upon binding, transcription is inhibited and the cell dies. If desired, pH-sensitivity and two-photon processes could be used to further increase the selectivity in addressing very specific regions of the body, such as a tumor.","1000000","2008-09-01","2013-08-31"
"PhotoMutant","Rational Design of Photoreceptor Mutants with Desired Photochemical Properties","Igor Schapiro","THE HEBREW UNIVERSITY OF JERUSALEM","From a technological viewpoint photoreceptor proteins, the light-sensitive proteins involved in the sensing and response to light in a variety of organisms, represent biological light converters. Hence they are successfully utilized in a number of technological applications, e.g. the green-fluorescent protein used to visualize spatial and temporal information in cells. However, despite the ground-breaking nature of this utilization in life science and other disciplines, the attempts to design a photoreceptor for a particular application by protein mutation remains an open challenge. This is exactly the scope of my research proposal: the application of multi-scale modelling for the systematic design of biological photoreceptor mutants.

With this target in mind I will study representatives of two prominent photoreceptor proteins subfamilies which are of towering interest to experimentalists: proteorhodopsins and cyanobacteriochromes. Computer models of these proteins will be constructed using accurate multi-scale modeling. Their excitation energies and other properties (e.g. excited-state reactivity and efficiency) will be calculated using multireference methods that were shown to have an accuracy of <3 kcal/mol. The insights gained from simulations of the wild-type proteins will provide the basis for proposing mutations with altered photochemical properties: in essence to predict absorption and emission spectra, excited-state lifetime and quantum yields. 

This research requires interactions across the disciplines, as the best candidates will be synthesized and characterized experimentally by collaborators. The outcome of these experiments will provide feedback to improve both the properties of the mutants and the simulation methodology. Ultimately this high-risk/high gain project should derive a comprehensive understanding that would result in novel biotechnological applications, e.g. optogenetic tools, fluorescent probes and biosensors.","1397475","2016-02-01","2021-01-31"
"PHOTOSI","Silicon nanocrystals coated by photoactive molecules: a new class of organic-inorganic hybrid materials for solar energy conversion","Paola Ceroni","ALMA MATER STUDIORUM - UNIVERSITA DI BOLOGNA","Silicon nanocrystals (SiNCs) have gained much attention in the last few years because of their remarkable optical and electronic properties, compared to bulk silicon. These unique properties are due to quantum confinement effects and are thus strongly dependent on the nanocrystal size, shape, surface functionalization and presence of defects.
The aim of the present project is the coupling of SiNCs with photo- and electroactive molecules or multicomponent systems, like dendrons, to build up a new class of hybrid materials to be employed in the field of light-to-electrical energy conversion (solar cells).
SiNCs possess several advantages with respect to more commonly employed, quantum dots, which usually contain toxic and rare metals like lead, cadmium, indium, selenium: a) silicon is abundant, easily available and essentially non toxic; b) silicon can form covalent bonds with carbon, thereby offering the possibility of integrating inorganic and organic components in a robust structure; c) absorption and emission can be tuned across the entire visible spectrum from a single material, upon changing the nanocrystal dimension.
This project will address the understanding of the fundamental photophysical and electrochemical properties of SiNCs, and their electronic interactions with the functional coating units. Taking advantage of the acquired knowledge, the project will then be devoted to the implementation of these hybrid materials as light-harvesting and charge transport components in photoelectrochemical cells. PhotoSi is expected to lead to solar cells with high efficiency (superior electronic properties of the hybrid material), low cost (the amount of the nanostructured material is significantly reduced compared to conventional Si cells), and low environmental impact (Si is essentially non toxic, and new less-energy demanding synthetic methodologies will be explored).","1182606","2012-01-01","2017-12-31"
"PhotoSmart","Photo-switching of smart surfaces for integrated biosensors","Martina Gerken","CHRISTIAN-ALBRECHTS-UNIVERSITAET  ZU KIEL","Smart surfaces with switchable properties hold great promise for future integrated sensors. Azobenzene molecules have been demonstrated to switch reversibly between the trans and cis isomer with picosecond time constants, when triggered with an external light source. Due to the different molecular geometries and electronic properties of the isomers, these may be used as molecular switches for realizing smart surfaces. The objective of this research proposal is to establish methods for integrating photo-switchable smart surfaces into miniaturized sensors. For efficient switching this requires on-chip light sources providing sufficient intensity at the location of the molecular switch. Ultraviolet and blue organic light emitting diodes will be integrated monolithically onto dielectric substrates with a periodically nanostructured high refractive index layer. This slab photonic crystal allows for resonant excitation of the molecular switches. Two types of smart surfaces will be studied. First, the reversible switching of wettability between hydrophilic and hydrophobic will be investigated, which is of particular importance for reconfigurable microfluidic chips. Second, the switchable surface adsorption of biomaterials is targeted. The periodic switching of the binding sites between an active and an inactive state will cause a periodic measurement signal. This allows for the use of lock-in techniques with superior signal-to-noise ratio and for subtraction of the background at same position. Combining both types of smart surfaces promises reconfigurable, multifunctional, highly-selective future integrated biosensors. The final goal of the proposed project is to demonstrate for the first time an integrated microsystem with smart surfaces switched by on-chip light sources for spatial and temporal control of the surface wettability as well as control of binding sites for biomolecules.","1499878","2013-07-01","2018-06-30"
"PHOTOTUNE","Tunable Photonic Structures via Photomechanical Actuation","Arri Priimägi","TAMPEREEN KORKEAKOULUSAATIO SR","The next frontier in photonics is to achieve dynamic and externally tunable materials that allow for real-time, on-demand control over optical responses. Light is in many ways an ideal stimulus for achieving such control, and PHOTOTUNE aims at devising a comprehensive toolbox for the fabrication of light-tunable solid-state photonic structures. We harness light to control light, by making use of photoactuable liquid-crystal elastomers, which display large light-induced deformations through coupling between anisotropic liquid-crystal order and elasticity brought about by the polymer network.

We will take liquid-crystal elastomers into a new context by intertwining photomechanics and photonics. Specifically, PHOTOTUNE is built around the following two objectives:

(i) Tunable photonic bandgaps and lasing in photoactuable layered structures: The aim is to take photomechanical materials into the scale of optical wavelengths and utilize them in thickness-tunable liquid-crystal elastomer films. Such films will be further integrated into layered structures to obtain photonic crystals and multilayer distributed feedback lasers whose properties can be tuned by light.

(ii) Photomechanical control over plasmonic enhancement on nanostructured elastomeric substrates: Fabrication of metal nanostructures on substrates that can contract and expand in response to light comprises a perfect, yet previously unexplored, nanophotonic platform with light-tunable lattice parameters. We will apply such tunable photoelastomeric substrates for surface-enhanced Raman scattering and phototunable nonlinear plasmonics. 

We expect to present a wholly new technological toolbox for tunable optical components and sensing platforms and beyond: The horizons of PHOTOTUNE are as far-reaching as in studying distance-dependent physical phenomena, controlling the speed of light in periodic structures, and designing actively-tunable optical metamaterials.","1486400","2016-05-01","2021-04-30"
"PhotUntangle","Rendering the opaque transparent: Untangling light with bespoke optical transforms to see through scattering environments","David PHILLIPS","THE UNIVERSITY OF EXETER","When light propagates through an opaque material, such as living tissue or a multi-mode optical fibre, it fragments and scatters multiple times. The emergent wavefront no longer forms an image because the spatial information it carries has been scrambled. Reversing this scattering offers the prospect of using visible light for high-resolution imaging of structures deep inside the human body in a safe, non-ionising way. It has recently been shown that this light scattering can be characterised and inverted. Yet arbitrary spatial mode inverters that can unscramble hundreds of light modes simultaneously to efficiently reform an image do not currently exist. The aim of this project is to understand how to design and build them.
I will pioneer the use of focused lasers to write intricate nano-structures directly into glass. The key advancement will be to overcome extreme fabrication tolerances by employing a fluid design approach, whereby the design will be modified during the fabrication process. In parallel, I will develop dynamic transformers, capable of rapidly reprogrammable optical transformations. Further, I will create new computational techniques to overcome residual levels of crosstalk, and develop new ultra-fast scattering characterisation methods based on compressed sensing. This project will advance our fundamental understanding of how to control optical scattering in complex media. Key aims are to:
- Understand how to design a new class of optical elements that can perform efficient spatial mode transforms on demand.
- Build both passive spatial mode transformers to manipulate hundreds of modes simultaneously, and active transformers that can perform dynamically reconfigurable transformations at video-rates.
- Apply this technology to unscramble light that has propagated through a moving multi-mode optical fibre in real-time, pushing towards ultra-thin micro-endoscopy, and explore an array of applications to next generation imaging systems and beyond.","1790105","2018-11-01","2023-10-31"
"PHOXY","Phosphorus dynamics in low-oxygen marine systems: quantifying the nutrient-climate connection in Earth’s past, present and future","Caroline Slomp","UNIVERSITEIT UTRECHT","Phosphorus (P) is a key and often limiting nutrient for phytoplankton in the ocean. A strong positive feedback exists between marine P availability, primary production and ocean anoxia: increased production leads to ocean anoxia, which, in turn, decreases the burial efficiency of P in sediments and therefore increases the availability of P and production in the ocean. This feedback likely plays an important role in the present-day expansion of low-oxygen waters (“dead zones”) in coastal systems worldwide. Moreover, it contributed to the development of global scale anoxia in ancient oceans. Critically, however, the responsible mechanisms for the changes in P burial in anoxic sediments are poorly understood because of the lack of chemical tools to directly characterize sediment P. I propose to develop new methods to quantify and reconstruct P dynamics in low-oxygen marine systems and the link with carbon cycling in Earth’s present and past. These methods are based on the novel application of state-of-the-art geochemical analysis techniques to determine the burial forms of mineral-P within their spatial context in modern sediments. The new analysis techniques include nano-scale secondary ion mass spectrometry (nanoSIMS), synchotron-based scanning transmission X-ray microscopy (STXM) and laser ablation-inductively coupled plasma-mass spectrometry (LA-ICP-MS). I will use the knowledge obtained for modern sediments to interpret sediment records of P for periods of rapid and extreme climate change in Earth’s history. Using various biogeochemical models developed in my research group, I will elucidate and quantify the role of variations in the marine P cycle in the development of low-oxygen conditions and climate change. This information is crucial for our ability to predict the consequences of anthropogenically-enhanced inputs of nutrients to the oceans combined with global warming.","1498000","2012-01-01","2016-12-31"
"PhyMorph","Unravelling the physical basis of morphogenesis in plants","Arezki Boudaoud","ECOLE NORMALE SUPERIEURE DE LYON","Morphogenesis is the remarkable process by which a developing organism acquires its shape. While molecular and genetic studies have been highly successful in explaining the cellular basis of development and the role of biochemical gradients in coordinating cell fate, understanding morphogenesis remains a central challenge for both biophysics and developmental biology. Indeed, shape is imposed by structural elements, so that an investigation of morphogenesis must address how these elements are controlled at the cell level, and how the mechanical properties of these elements lead to specific growth patterns. Using plants as model systems, we will tackle the following questions:

i.   Does the genetic identity of a cell correspond to a mechanical identity?
ii.  Do the mechanical properties of the different cell domains predict shape changes?
iii. How does the intrinsic stochasticity of cell mechanics and cell growth lead to reproducible shapes?

To do so, we will develop a unique combination of physical and biological approaches. For instance, we will measure simultaneously physical properties and growth in specific cell groups by building a novel tool coupling atomic force microscopy and upright confocal microscopy; we will integrate the data within physical growth models; and we will validate our approaches using genetic and pharmacological alterations of cell mechanics.

In plants, shape is entirely determined by the extracellular matrix (cell walls) and osmotic pressure. From that perspective, plants cells involve fewer mechanical parameters than animal cells and are thus perfectly suited to study the physical basis of morphogenesis. Therefore we propose such a study within the shoot apical meristem of Arabidopsis thaliana, a small population of stem cells that orchestrates the aerial architecture of the plant.

This work will unravel the physical basis of morphogenesis and shed light on how stochastic cell behaviour can lead to robust shapes.","1401023","2012-10-01","2017-09-30"
"PHYS.LSS","Cosmological Physics with future large-scale structure surveys","Licia Verde","UNIVERSITAT DE BARCELONA","Future, large galaxy surveys (such as BOSS, DES, LSST, EUCLID, ADEPT etc.) will cover of the order of 10000 square degrees on the sky, with the primary science goal to unravel the nature of the physics responsible for the current accelerated expansion of the universe. This acceleration likely involves new physics which could imply ether a modification of our understanding of particles and fields (if the acceleration is caused by a new negative pressure-component) or a change in our understanding of space and time (by modifying Einstein&apos;s General Relativity laws). The unprecedented and exquisite data provided by these surveys will make possible also other interesting science with implications for fundamental physics (e.g., inflation, neutrino properties) and astrophysics (e.g., biasing, galaxy formation). The success of future large-scale galaxy surveys evidently requires a correct interpretation of their data. The current proposal, which benefits from the interaction of Cosmology, astrophysics and particle physics, aims at building up a set of robust tools to maximize the physics extracted from large-scale structure data. Such an interplay is mandatory to ensure a suitable modeling of the observables and a meaningful comparison with the theoretical predictions. The PI is involved with surveys such as BOSS, ADEPT and LSST and for the past year has been leading a working group with the goal of bringing together particle physicists and cosmology to better understand dark energy. The methods developed in the proposal presented here are expected to be used by the international community involved in future surveys. This would imply a big step for Spanish groups joining or even leading future Cosmology or Astro-particle physics projects.","1395000","2009-11-01","2015-10-31"
"PhysProt","Determining Physical Properties of Heterogeneous Protein Complexes in Small Volumes","Tuomas Pertti Jonathan Knowles","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The objective of this proposal is to probe in aqueous solution protein complexes which are both heterogeneous and possess highly variable stoichiometries. The study of heterogeneous protein systems by conventional means is very challenging since most current biophysical methods perform best for pure solutions of isolated components - yet proteins exert in the majority of cases their biological functionality through forming complexes. We propose in this application that the key to study such systems is to operate in much smaller volumes than in conventional biophysical experiments. We will use microfluidics to obtain information about the physical properties of protein complexes in real time through quantitative micron-scale measurements of mass transport of molecular species under the action of diffusion and electric or centrifugal fields. Furthermore, by working in small volumes, we will study nucleation phenomena inherent to many protein self-assembly phenomena on the level of single nucleation events by segregating individual nuclei into spatially distinct compartments. Modern microfabrication techniques that allow for the manipulation of liquids on the picolitre scales required for this project are available and will be exploited, but the potential of this technology to define experimentally highly heterogeneous protein complexes in terms of their key fundamental physical properties, such as the hydrodynamic radius, charge and mass, and shed light on the physical basis of protein self-assembly, have remained unexploited. Using this approach, we will explore biological problems of fundamental and practical importance characterised by heterogeneity, including functional chaperone complexes, formation and detection of amyloid oligomers and studies of complex biomolecular mixtures. This programme will deliver fundamentally new approaches to study heterogeneous protein complexes and will shed light on the physical principles that govern protein self-assembly.","1499895","2014-02-01","2019-01-31"
"PHYTOCHANGE","New approaches to assess the responses of phytoplankton to Global Change","Bjoern Christian Rost","ALFRED-WEGENER-INSTITUT HELMHOLTZ-ZENTRUM FUR POLAR- UND MEERESFORSCHUNG","Phytoplankton are responsible for a major part of global primary production due to the immensity of the marine realm and are heavily implicated in global biosphere equilibriums by driving elemental chemistry in surface oceans, exporting massive amounts of C to sediments and influencing ocean-atmosphere gas exchange. Climate change will alter the marine environment within the next 100 years. Increasing atmospheric CO2 has already caused higher aquatic pCO2 levels and lower pH (ocean acidification) and rising temperature will impact ocean stratification, and hence light and nutrient conditions. Phytoplankton will be affected by these Earth system transformations in many ways, altering the complex balance of biogeochemical cycles and climate feedback mechanisms. Prediction of how phytoplankton may respond at the cellular and ecosystem levels is a key challenge in global change research. The proposed project will investigate physiological reactions of 3 important phytoplankton groups (diatoms, coccolithophores, cyanobacteria) to environmental factors which will be affected by global change (pCO2/pH, light, nutrients). Using an innovative combination of cutting-edge mass-spectrometric and fluorometric techniques, a suite of in vivo assays will be applied in lab and field experiments to develop a process-based understanding of cellular responses. Specific biogeochemical issues will be addressed since diatoms are the main drivers of vertical organic C fluxes, coccolithophores regulate ocean alkalinity through calcification, and N2-fixing cyanobacteria control availability of reactive N. These are relevant in different marine zones, from Southern Ocean to equatorial oligotrophic waters. Data will significantly improve understanding of key processes in phytoplankton and will be exploited in multidisciplinary contexts ranging from molecular to ecological processes and, through cellular and ecosystem models, to predictions of marine biosphere responses to future global change","1399984","2008-06-01","2013-05-31"
"PICO","Pico: no more passwords","Francesco Stajano","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Passwords, passphrases and PINs have become a usability disaster. Even though they are convenient for implementers, they have been over-exploited, and are now increasingly unmanageable for end users, as well as insecure. The demands placed on users (passwords that are unguessable, all different, regularly changed and never written down) are no longer reasonable now that each person has to manage dozens of passwords. This project will develop and evaluate an alternative design based on a hardware token called Pico that relieves the user from having to remember passwords and PINs. Besides relieving the user from memorization efforts, the Pico solution scales to thousands of credentials, provides ``continuous authentication'' and is resistant to brute force guessing, dictionary attacks, phishing and keylogging. To promote adoption and interoperability, the Pico design has not been patented. The Principal Investigator has been invited to speak about Pico in three continents (including at USENIX Security 2011) since releasing the first draft of his design paper.","1350000","2013-02-01","2017-12-31"
"PICOMAT","Picometer scale insight and manipulation of novel materials","Jannik Christian Meyer","UNIVERSITAT WIEN","""The recent years have witnessed an explosive growth in the numbers of new materials with fascinating properties and high application potential. Two-dimensional materials are at the focus of interest, in particular graphene but also two-dimensional niobium diselenide, molybdenum disulfide, hexagonal boron nitride, mono-layer bismuth strontium calcium copper oxide, and a variety of other layered materials. This proposal builds on my recognized expertise and experience in the areas of atomically resolved studies and atomic level manipulation of new materials using electron beams in a transmission electron microscope. Instead of observing random, beam-driven or contamination-induced variations in image sequences,  I plan to carry out targeted, controlled experiments to study atomic scale modifications in real time. I will establish new experimental approaches to study the properties of low-dimensional systems, light-element- and radiation-sensitive samples. The first key objective is controlled in-situ manipulation, via imposing chemical modifications that are locally activated by the electron beam and directly followed in real time. The second and strongly interlinked objective is to alleviate the effects of radiation damage by different new approaches (beyond low-voltage imaging), by making use of new statistical methods that exploit the multiplicity of identical configurations.  I aim to transfer very recent developments for low-dose imaging from structural biology to the case of point defect configurations in a crystalline material, to allow the identification of atomic configurations that are currently not accessible as they do not withstand the electron dose that would be needed for their identification.  Overall, this project will provide fundamental new insights to the science and applications of some of today's most promising new materials, new routes to tailor their properties, and methodological advances that will reach well beyond our target materials.""","1468279","2013-08-01","2018-07-31"
"PICSEN","Propagative and Internal Coherence in Semiconductor Nanostructures","Jacek Kasprzak","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""This project concerns the field of coherent, nonlinear, ultrafast light-matter interaction on a quantum level in solids. It proposes to experimentally explore limits of: i) internal coherence of an individual emitter; ii) radiative coupling between pairs of emitters. A potential long term application of this work could be envisaged, as one can expect that individual emitters could serve as qubits for implementations of optically controlled quantum information processing in solids. As individual emitters we will employ excitons in semiconductors: either bound to impurities or confined in quantum dots. Firstly, by embedding the latter into upright photonic nanowires, that are now available in the team, we will amplify the collection of their coherent optical response by nearly four orders of magnitude as compared to the current state-of-art. This will provide an unprecedented access to their coherent as well as dephasing interaction with phonons. It will also enable retrieval of their n-wave mixing responses to scrutinize coherent couplings within an individual emitter. The second objective is the demonstration of an efficient, controllable and non-local coherent coupling mechanism between distant emitters, which is a prerequisite for the construction of quantum logic gates and networks. Here, such a radiative coupling will be demonstrated and manipulated using resonant emitters embedded into in-plane one-dimensional waveguides, which permit virtually unattenuated propagation of coherence. The internal and propagative coherence of individuals and radiatively coupled pairs will be explored using beyond-the-state-of-the-art methods of coherent nonlinear spectroscopy. Specifically, we will develop a spatially-resolved heterodyne spectral interferometry combined with ultrafast pulse-shaping. The proposed advanced methodology of this ERC project can be associated with techniques developed in other domains, like nuclear magnetic resonance and astrophysics instrumentation.""","1499708","2012-12-01","2017-11-30"
"PiHOMER","Pioneering Heterogeneous Organometallic-Mediated Electrocatalytic Reactions","Gregory George Wildgoose","UNIVERSITY OF EAST ANGLIA","This project pioneers the use of Heterogeneous Organometallic Mediated Electrocatalytic Reactions (HOMER) and our understanding thereof. Examples of electron transfer activation of C-H, C-Halide bonds and cycloaddition reactions of unactivated olefins are sought that are catalysed by appropriate combinations of electrochemically generated odd-electron (17-electron) organometallic “radical” complexes and 7-electron perhalogenated arylborane Lewis acidic radical species. The electrochemistry of a series of Group VI-IX transition metal half-sandwich complexes, empirically “Cp’M(CO)n” (where n=2 or 3 in order to satisfy the metal centre’s demand for an 18-electron configuration; Cp’ = η5-C5H4CO2R, where R = Me) will be explored both in the solution phase and covalently attached to an electrode surface. These chemically modified “HOMER platforms” will be exposed to a wide variety of hydrocarbon feedstocks during electrolysis, and evidence of  C-H/C-halide bond activation sought. Optimization of the HOMER catalysts will be attempted by systematically varying the ligands within the complex.This includes a study of the electrochemistry of Group IV to VIII transition metal complexes bearing at least 1 borylcyclopentadienyl ligand that incorporates a Lewis acid group into the HOMER catalyst.
Novel perchlorinated arylboranes, (C6F5)3-nB(C6Cl5)n n=1-3 “BArCl” recently reported by the PI and co-workers will be examined for their activity in Frustrated Lewis Pair (FLP) activation of small molecules. An “electrochemical-FLP” (e-FLP) system will be pioneered by electrochemically oxidising the [BArClH]- products of FLP activation of H2 to produce [BArClH]• intermediates -  a source of “H•”. The activation of small molecules via the e-FLP concept will be explored, and combined with suitable HOMER catalysts in an effort to generate e-FLP-HOMER systems capable of undergoing an electrocatalytic analogue of the Fischer-Tropsch reaction.","1327116","2012-08-01","2017-07-31"
"PINNACLE","Perovskite Nanocrystal-Nanoreactors for Enhanced Light Emission","Alexander URBAN","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","The unprecedented advancement of halide perovskite photovoltaics and light-emission applications has far outpaced the basic scientific research necessary to understand this fascinating yet perplexing material and to optimize material quality and device integration. Additionally, the intricate nature of the perovskite is susceptible to degradation from environmental stress, such as moisture and heat, currently deterring commercialization.

This research project will realize a novel synthesis for perovskite nanocrystals (NCs) by means of block copolymer nanoreactors. These will enable an unprecedented control over size and dimensionality of the NCs into the quantum-confinement regime. Using these NCs, we will determine the fundamental optical, electrical, and phononic properties of perovskite, mainly by means of temperature-controlled transient optical spectroscopy. 

Elucidation of the degradation mechanisms through controlled subjecting to external stress, will lead to strategies for designing the nanoreactor to shield the NCs, mitigating these effects. Additionally, we will investigate the high mobility of (halide) ions in perovskites, and likewise design the polymeric nanoreactor to deter ion migration and enable NC implementation into existing optoelectronic applications and currently unattainable architectures, such as hetero-structures and exciton funnels.

We will create stable, high-quality NC-films, enabling the formation of multilayers for exciton funnelling by means of Förster resonance energy transfer (FRET). We will highlight the NC potential by integrating them into LEDs of various architectures, by demonstrating low-threshold ASE and realizing unprecedented perovskite-laser geometries, e.g. vertical cavity surface emitting lasers (VCSELs) and plasmonic nanopatch lasers.
PINNACLE will greatly further the understanding of halide perovskites, benefitting the research community, and lead to novel optoelectronic devices and exciting new applications.","1498188","2018-03-01","2023-02-28"
"PIONEER","Peri-Ocularly Navigated Exteroceptive Snake Robot for Novel Retinal Interventions","Christos BERGELES","KING'S COLLEGE LONDON","Intraocular treatments require manipulation of structures with dimensions comparable to hand tremor. The demanded dexterity, coupled with reduced haptic and depth perception, calls for robotic assistance. 

Despite notable benefits, existing robots are not clinically disruptive but follow well-trodden intervention protocols with significant limitations, e.g. lack of flexibility at the scleral incision and limited manipulation bandwidth, as to avoid scleral, lens, and retinal damage. Robotics also does not obviate the prerequisite of the risky, cataract-inducing vitrectomy, which may cause retinal detachment (RD) or sight loss. 

Novel interventions like stem-cell delivery pose yet further challenges. Apart from removing healthy vitreous, they require millimetre-long retinal tears, lifting the retinal membrane, and injecting a stem-cell suspension or sheet. Current robots facilitate manipulations but conceivably neither enable alternative approaches nor reduce retinal-tear-induced risks.

PIONEER, the proposed snake robot, can disrupt clinical protocol by navigating peri-ocularly and around the orbital muscles to suprachoroidally reach the retina. Revolutionizing existing robot paradigms, PIONEER innovates both scientifically and technically. 

Optimal robot compliance will ensure force-adaptive peri-ocular steering conforming to the eye’s exterior. A tactile sleeve with micro-sensors will provide exteroceptive force sensing and shape estimation. Enhanced navigation, fusing optical coherence tomography with on-line vessel detection from novel tip-mounted probes, will ensure safe guidance to avoid vessels through imposed virtual fixtures and path planning. No vitrectomy will be required and the posterior scleral incision will leave the retinal membrane intact, minimising RD risk. 

PIONEER can be an enabler of emerging stem-cell interventions and futuristic procedures like drug-implant insertion and nerve interfacing, currently at human-dexterity limits or impossible.","1500000","2017-04-01","2022-03-31"
"PLACQED","Plasmonic cavity quantum electrodynamics with diamond-based quantum systems","Mete Atature","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","This proposal aims to realize physical systems for the realization of plasmonic cavity quantum electrodynamics using optically active diamond-based quantum systems such as atomic impurities. Color centers in diamond provide a suitable test bed for applications of quantum information processing, as well as selected spin-spin interactions. While there are hundreds of known color centers in diamond, but only one (Nitrogen vacancy) is studied extensively. We will study optical properties and identify energy levels of alternative color centers both naturally occurring and artificially implanted, potential candidates being Ni, Si, or Fe impurities. We will in parallel study solid-state-based cavity QED with light confinement at sub-wavelength scale. Using metal nanostructures and plasmons, we aim at achieving individual or ensemble strongly coupled emitter-cavity systems. Further, we will study how sub-wavelength structures of a medium alter the material-based properties, so the optical fields can experience exotic media with negative refractive indices.","1712342","2008-08-01","2013-07-31"
"PLANETARYSYSTEMS","Planets - The Solar System and Beyond","Re'em Sari","THE HEBREW UNIVERSITY OF JERUSALEM","The discovery of the first extra solar planet, merely twelve years ago, ushered an explosive growth in our knowledge of planetary systems. Extrasolar planets have been detected with ever-smaller masses and today Earth analogs orbiting other stars are on the discovery horizon! Observations of disks around young stars reveal the initial conditions for planet formation while detections of debris disks probe post formation stages. Closer to home, exploration of the Kuiper Belt provides new clues on planet migration and on the intermediate stages of planetary accretion. Some discoveries, like extrasolar planets with short orbital periods and high eccentricities, have led to a complete overhaul of previously accepted planet formation theories. The increasing wealth of observations creates a unique opportunity to answer fundamental questions pertaining to planets and planetary systems. The relevant objects include on one hand giant extrasolar planets, a thousand times more massive than Earth, and on the other hand rocky and icy Kuiper Belt Objects, a millionth of the Earth mass. The physical processes vary from the resonant interaction of giant extrasolar planets with gas disks to collisions of solid bodies in the outer solar system. Still, much of the underlying physics, especially orbital dynamics, is common. We propose, therefore, an innovative program of integrated studies of the above subjects. A unique aspect of my group’s approach is utilizing the common physics for a synergic treatment of these traditionally separated topics. By answering open questions in dynamics, investigating the inner workings of planetesimal coagulation and interpreting the properties of extrasolar planets we will make significant breakthroughs in the understanding of planet formation and its possible outcomes. This will illuminate our place in the universe and will guide farther searches of planets. Our exploration is at the beginning of a long voyage seeking life around nearby stars.","1000000","2008-06-01","2013-05-31"
"PLANETOGENESIS","Building the next generation of planet formation models: protoplanetary disks, internal structure, and formation of planetary systems","Yann Alibert","UNIVERSITAET BERN","The discovery of extra-solar planetary systems with properties so different from those of our own Solar System has overturned our theoretical understanding of how planets and planetary systems form. Indeed, planet formation models have to link observations of two classes of objects: Protoplanetary disk, whose structure and early evolution provide the initial conditions of planets formation, and actual detected planets. The observational knowledge of these two classes of objects will see in the near future dramatic improvements, with three major breakthroughs: 1) high angular resolution observations will tightly constrain the structure and early evolution of protoplanetary disks, 2) direct observation of extrasolar planets will allow to understand their internal structure as well as their formation process, and 3) detection of very low mass extrasolar planets will constrain the mass function of planets and planetary systems, down to the terrestrial planet regime The goal of this project is to develop a theoretical understanding of planet formation that quantitatively stands up to these observational confrontations. For this, we will build on the basis of first generation planet formation models developed at the time the PI was assistant at the Physikalisches Institute of the University of Berne. The PI, a PhD student, and a Postdoc will conduct three inter-related sub-projects linked to the three breakthroughs mentioned above: A) improving the disk part of planet formation models, B) determining the internal structure of forming planets, including the effects of accretion shocks and envelope pollution by infalling planetesimals, and calculating their early evolution, and C) building planetary system formation models, including both gas giant and low mass rocky planets.","1395323","2010-02-01","2015-11-30"
"PLASMAPOR","""Plasma penetration into porous materials for biomedical, textile and filtration applications.""","Rino Achiel Morent","UNIVERSITEIT GENT","""My group will explore the undeveloped field of penetration of non-thermal plasma into porous structures. Porous materials are an exciting class of materials with a wide range of applications. However, given the narrow dimensions of the porous network, modifying in a homogeneous way an entire porous material is a challenging task.

This project is based on the use of non-thermal atmospheric pressure plasmas for an effective internal surface modification of 3D porous structures. To make plasma technology reach this desired level of controlled penetration into porous structures, a far better understanding of the penetration of chemical active species into porous structures is required. Therefore, my project envisages a thorough study of the interactions between a non-thermal plasma and a second phase, the second phase being a porous substrate. Through diagnostics of the process-relevant plasma parameters and a quantitative analysis of the plasma-induced effects, the knowledge on the physics and chemistry of such hybrid plasma systems will be enhanced and, in most cases, newly founded.

My group will start exploring this exciting field by focussing on three cornerstone research lines. Firstly, I will develop new plasma reactor concepts enabling effective plasma penetration. Secondly, these newly developed plasma reactors will be employed for the internal surface modification of porous biodegradable polyester scaffolds used in tissue engineering. Thirdly, besides the development of biomedical implants, the possibilities for the design of functional porous textiles and advanced filter materials will also be explored. Realisation of these three cornerstones would result in a major breakthrough in their specific field which makes this proposal inherently a relatively high risk/very high gain proposal.

I therefore strongly believe that my research program will open a whole new window of opportunities for porous materials with a large impact on science and society.""","1518800","2012-06-01","2017-05-31"
"PLASMATS","Plasma-assisted development and functionalization of electrospun mats for tissue engineering purposes","Nathalie Marie-Thérèse De Geyter","UNIVERSITEIT GENT","""In this project, I will explore the unique combination of two fascinating research themes: electrospinning and plasma technology. Electrospun nanofibrous matrices (so-called mats) are an exciting class of materials with a wide range of possible applications. Nevertheless, the development and functionalization of these electrospun materials remain very challenging tasks.

Atmospheric pressure plasma technology will be utilized by my research group to create advanced biodegradable electrospun mats with unprecedented functionality and performance. To realise such a major breakthrough, plasma technology will be implemented in different steps of the manufacturing process: pre-electrospinning and post-electrospinning.

My group will focus on four cornerstone research lines, which have been carefully chosen so that all critical issues one could encounter in creating advanced biodegradable electrospun mats are tackled. Research cornerstone A aims to develop biodegradable electrospun mats with appropriate bulk properties, while in research cornerstone B pre-electrospinning polymer solutions will be exposed to non-thermal atmospheric plasmas. This will be realized by probing unexplored concepts such as discharges created inside polymer solutions. In a third cornerstone C, an in-depth study of the interactions between an atmospheric pressure plasma and an electrospun mat will be carried out. Finally, the last cornerstone D will focus on plasma-assisted surface modification of biodegradable electrospun mats for tissue engineering purposes.

Realization of these four cornerstones would result in a major breakthrough in their specific field which makes this proposal inherently a relatively high risk/very high gain proposal. I therefore strongly believe that this research program will open a whole new window of opportunities for electrospun materials with a large impact on science and society.""","1391100","2014-02-01","2019-01-31"
"PLASMECS","NanoPlasmoMechanical Systems","Silvan SCHMID","TECHNISCHE UNIVERSITAET WIEN","""With their unparalleled mass and force sensitivities, nanomechanical resonators have the potential to considerably improve existing sensor technology. However, one major obstacle still stands in the way of their practical use: The efficient transduction (actuation & detection) of the vibrational motion of such tiny structures. Localized plasmon resonances ""focus"" optical fields below the diffraction limit of light and present a powerful new method to optically transduce the vibrational motion of nanomechanical structures.
The objective of this project is to establish for the first time a complete plasmonic transduction in novel NanoPlasmoMechanical Systems (NaPlaMS). This new method is easy to implement and enables the freespace addressability and efficient transduction of mesoscopic (sub-wavelength) plasmonic pillar arrays. I will explore the ground-breaking new properties of NaPlaMS pillar arrays in three mutually supporting subprojects (SP). SP1 studies fundamental aspects of plasmomechanics by integrating nanoplasmonic antennas of various geometry and materials on highly force sensitive string resonators. These devices allow the unique optical and mechanical study of i) plasmonic quantum tunneling and ii) optical forces between plasmonic nanostructures of various shapes and materials. SP2 will make use of the strong plasmomechanical light-interaction of the high frequency NaPlaMS pillars for the development of next generation reconfigurable metamaterial for optic modulation. Compared to state-of-the-art bulky and powerhungry modulators, NaPlaMS modulators will be low-power and sub-wavelength-size as required for future optic telecommunication and consumer products. SP3 utilizes the exceptional mass sensitivity of NaPlaMS pillar arrays to create unique mass sensors. The goal is to create a sensor for native & neutral protein mass spectrometry to provide a revolutionary small and cheap tool for proteomics, which will accelerate the development of protein drugs.""","1497550","2016-11-01","2021-10-31"
"PLASMHACAT","Plasmonics-based Energy Harvesting for Catalysis","Hiroshi Ujii","KATHOLIEKE UNIVERSITEIT LEUVEN","Many critical photochemical and photophysical processes, from photosynthesis in plants, to photocatalytic reactions, and to generation of electricity in solar cells, depend on an efficient light-matter interaction. In order to increase, for example, the efficiency of photocatalysis, the interaction of the photocatalyst with light has to be increased. This project will pursue two lines of investigation in order to achieve this.  Firstly, the concept of light-harvesting will be exploited. Light energy can be harvested by collecting, directing and concentrating it at a reaction center, in a fashion that mimics that used by plants. Secondly, for specific types of catalysis such as noble metal nano-particle (NP) based catalysis, the plasmon light field at the metal NPs can potentially be used to enable a more efficient light-matter interaction. The applicant proposes to combine both approaches, to create a plasmonic antenna to funnel light to a reaction center, whilst at the same time using the plasmons generated as an efficient reaction field in catalysis. The outcome will make it possible to drastically increase activities of (photo)catalysts, enabling their efficient operation under sunlight or even in weak room light conditions. For this, the project firstly adevelops novel photo-induced synthesis for metal NPs, both in solution and at surfaces, as well as at arranging the NPs in effective antennae. Secondly, microscopy modes will be developed/implemented that allow monitoring the growth of the NPs in situ, that allow checking the quality of the arrays and that allow in situ monitoring of catalytic test reactions. These knowledge will be applied to ‘real world’ (photo)catalysts (gold NP catalysis and TiO2, respectively). This project will thus result in new light-induced synthesis and fabrication methods of NPs; in new and/or improved microscopy modes and spectroscopic schemes in order to study the relationship between plasmonic properties and chemical reactions.","1499120","2011-10-01","2016-09-30"
"PLASMIC","Plasmonically-enhanced III-V nanowire lasers on silicon for integrated communications","Kirsten Emilie Moselund","IBM RESEARCH GMBH","The ambition of PLASMIC is to address the bottleneck caused by electrical interconnects and develop on-chip optical interconnect solutions based on plasmonically-enhanced nanoscale emitters.
Nanoscale photonic components are desirable for on-chip communications because of density, speed and because reducing the size of the cavity might reduce the lasing threshold. Conventional photonics are limited in scale by the diffraction-limit to dimensions of half of the wavelength of light in the material. This limit does not apply to plasmonics, an optical mode that exists at the interface between a metal and a dielectric. Thus, they have a great potential for applications where down-scaling and confinement are primordial.
One of the barriers for applying plasmonics is the large losses associated with the metals. Thus in PLASMIC alternative plasmonic metals will be investigated based on their potential for tuning, VLSI compatibility, deposition methods and achieving lower optical losses in the near-IR. I will focus on highly doped semiconductors, metal nitrides, as well as multi-layers and compounds to form new plasmonic materials. Specifically, I will evaluate the use of the field-effect to achieve the semiconductor-metal transition to tune the plasma frequency.
New pioneering device concepts for plasmonic-photonic emitters on a silicon platform integrated with passive silicon photonic waveguides will be developed. To implement the gain medium for the lasers, I will exploit a novel nanowire (NW) integration approach: Template-Assisted Epitaxy. The unique advantages make it possible to grow III-V NWs on any orientation of silicon and aligned to lithographic features.
The devices will be based on a hybrid cavity formed between the NW and a Si waveguide with gratings to provide feedback. My team and I will explore dimensional scaling potential as well as the energy efficiency of plasmonic and photonic devices operating both in a lasing as well as in a subthreshold operation mode.","1941750","2016-04-01","2021-03-31"
"PLASMOLIGHT","NEW FRONTIERS IN PLASMON OPTICS: FROM NANOCHEMISTRY TO QUANTUM OPTICS","Romain Quidant","FUNDACIO INSTITUT DE CIENCIES FOTONIQUES","At this point in time where plasmon optics has become a mature field of research, we propose here to create new bridges with other scientific disciplines in which the optical properties of plasmonic nanostructures could successfully address major roadblocks.  The proposed scientific project consists of two independent parts, in which plasmonics is combined with Nanochemistry and Quantum optics, respectively.

First, we will investigate how plasmonics could contribute to control with nanometer accuracy the deposition of a wide range of molecules or other nano-objects at a surface pre-patterned with noble metal nanostructures.  Our approach is foreseen to overpass some of the major limitations of existing methods by combining parallel patterning over large areas with a resolution down to 10nm. Beyond demonstrating the feasibility of this novel approach, we propose to exploit it to increase the sensitivity of bio-chemical plasmonic sensing and surface enhanced Raman scattering.

The second part of the project will study the use of the recent concept of plasmon nano-optical tweezers to develop a novel integrated quantum platform. The developed platform will be tested for applications to quantum simulation.","1146496","2011-04-01","2015-08-31"
"PLASMOPT","Ultrahigh-Intensity Plasma Optics","Fabien Quéré","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","Ultraintense and ultrashort light pulses have a huge potential for applications, such as the production of very compact particle accelerators. Exploiting this potential requires pushing the characteristics of lasers beyond their present state-of-the-art performances. However, the laser technology used so far is approaching its limits, in particular because of the optical breakdown of conventional optical media. Overcoming these limits requires finding radically new approaches for optics at ultrahigh laser intensities. The idea of this proposal consists in developing optical elements based on plasmas, i.e. plasma optics . Since plasmas are already ionized, they can sustain electromagnetic fields of extremely large amplitude. They can thus be exploited to produce several key optical elements needed to manipulate e.g. shorten, convert in frequency, or even amplify- existing ultraintense lasers. To this end, two main physical processes are exploited: laser-excited Langmuir waves, and the Doppler effect associated to the relativistic motion of plasmas in ultraintense laser fields. This project would contribute to the conception of a system consisting in a chain of several plasma optics, placed at the output of a table-top laser, which would deliver few-optical-cycle long PetaWatt-class near-visible light pulses, as well as Terawatt-class attosecond pulses in the soft x-ray range. Such light sources would open exciting perspectives in Science and Technology. More fundamentally, this project will exploit the coherent light emission induced during relativistic laser-plasma interaction as a fine probe of the ultrafast plasma dynamic. This new type of diagnostic should lead to significant progresses in the understanding of laser-plasma interaction at extreme laser intensities.","1140000","2010-01-01","2014-12-31"
"PLEASE","""PLEASE: Projections, Learning, and Sparsity for Efficient data-processing""","Remi Gribonval","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","""Sparse models are at the core of many research domains where the large amount and high-dimensionality of digital data requires concise data descriptions for efficient information processing. A flagship application of sparsity is compressed sensing, which exploits sparsity for data acquisition using limited resources. Besides sparsity, a key pillar of compressed sensing is the use of random low-dimensional projections.
The standard principle of general sparse and redundant representations is to rely on overcomplete dictionaries of prototype signals called atoms. The foundational vision of this proposal is that the efficient deployment of sparse models for large-scale data is only possible if supported by a new generation of efficient sparse models, beyond dictionaries, which must encompass computational efficiency as well as the ability to provide sparse and structured data representations.
Further, I believe that the true impact of compressed sensing has been to unearth an extremely powerful yet counter-intuitive tool: random projections, which open new avenues in machine learning. I envision applications to data sizes and volumes of collections that cannot be handled by today’s technologies.
A particular challenge is to adapt the models to the data by learning from a training corpus. In line with the frontier research on sparse decomposition algorithms, I will focus on obtaining provably good, yet computationally efficient algorithms for learning sparse models from collections of training data, with a geometric insight on the reasons for their success.
My research program is expected to impact the whole data processing chain, from the analog level (data acquisition) to high level processing (mining, searching), where sparsity has been identified as a key factor to address the “curse of dimensionality”. Moreover, the theoretical and algorithmic framework I will develop will be directly applied to targeted audiovisual and biomedical applications.""","1493537","2012-01-01","2016-12-31"
"Plio-ESS","Pliocene Constraints on Earth System Sensitivity","Alan Michael Haywood","UNIVERSITY OF LEEDS","The magnitude of long-term global temperature rise due to an increasing concentration of carbon dioxide (CO2) in the atmosphere is a question of relevance to policy makers and society. Previous studies have addressed this issue on the basis of the equilibrium response of the climate system due to fast feedbacks such as clouds and sea ice-albedo, often referred to as Climate Sensitivity. Plio-ESS will use the new concept of Earth System Sensitivity that additionally includes slow feedbacks such as those derived from changes in the major ice sheets and vegetation distribution. This has the potential to revolutionise the scientific debate on anthropogenic emissions of greenhouse gases and climate stabilisation targets. The aim of the project is to produce a robust estimate of the Earth System Sensitivity using the last interval in Earth history when CO2 was at modern or near future levels – the mid-Pliocene Warm Period. Using a combined modelling and geological data approach, Plio-ESS will integrate reconstructions of mid-Pliocene vegetation and ice sheets into climate and Earth system models. In this context Plio-ESS will push the frontier of palaeoclimatology by using state-of-the-art models which will enable the importance of resolution, improved model physics and the inclusion of additional Earth System components on model estimates of Earth System Sensitivity to be identified. Ensembles of experiments exploring the plausible range in model boundary conditions and physics will also quantify the uncertainty on estimates of Earth System Sensitivity. The outcome of the project will be a rigorous estimate of Earth System Sensitivity, which can be used by climate scientists and policy makers in defining stabilisation targets for greenhouse gas emissions and global temperatures to avoid dangerous levels of climate change.","1419968","2011-12-01","2016-11-30"
"PLIOPROX","New proxies to quantify continental climate development during the Pliocene","Johannes Weijers","UNIVERSITEIT UTRECHT","Atmospheric CO2 concentrations have risen rapidly since pre-industrial times and our current climate is not yet in equilibrium with this; it will change. To obtain insight in the type and magnitude of this change and to validate climate models used to project these changes, we need to look back at past climates. The most recent time in Earth history with CO2 levels that were similar to today is the Pliocene. The Pliocene thus provides a unique window into a world that exhibited many of the climate characteristics that we might experience. These are documented by proxies locked into sedimentary archives, especially marine sediments. It remains a challenge for palaeoclimatologists, however, to quantify past terrestrial temperatures. I have recently developed a novel proxy for quantitative annual mean air temperature reconstruction, which is based on the distribution of membrane lipids synthesised by soil bacteria. Upon soil erosion these molecules are transported to the marine realm where they become part of the marine sedimentary archive.

The PlioProx project aims at a quantitative reconstruction of continental temperatures and latitudinal temperature gradients for the Pliocene. This will be achieved by applying this new palaeothermometer to high resolution marine sediment records near river outflows to generate river-basin integrated records of continental air temperature. This approach also allows for a direct comparison to reconstructed sea surface temperatures. Using globally distributed sediment records, latitudinal temperature gradients will be constructed which will be compared to moisture transport and rainout, reconstructed using stable hydrogen isotopes from plant wax lipids. Results will provide vital new insights in climate evolution on land under elevated atmospheric CO2 concentrations. It will also contribute to improving the next generation earth system models that are used to predict future climate.","1500000","2012-10-01","2017-09-30"
"PlusOne","An ultracold gas plus one ion: advancing Quantum Simulations of in- and out-of-equilibrium many-body physics","Carlo Sias","ISTITUTO NAZIONALE DI RICERCA METROLOGICA","The concept of a localized single impurity in a many-body system is at the base of some of the most celebrated problems in condensed matter. The aim of the PlusOne project is to realize the physical paradigm of a single localized impurity in a many-body system to advance quantum simulation of in- and out-of equilibrium many-body physics. Our quantum simulator will consist of a degenerate gas of fermions as a many-body system, with a single trapped ion playing the role of the impurity. The novel design of our atom-ion hybrid system surpasses all the limitations that prevent current systems from reaching full control of atom-ion interactions because it is energetically closed. Using this system, we will characterize atom-ion collisions in the so-far unexplored ultracold regime.
We will use the single trapped ion to induce non-equilibrium dynamics in the many-body system by quenching the atom-ion interactions. This process will cause an entanglement between the many-body dynamics and the ion’s internal state, enabling us to detect the many-body evolution by performing quantum tomography on the ion. 
By these means, we will observe the emergence of the Anderson Orthogonality Catastrophe for the first time in the time domain, and investigate the universality of this phenomenon. 
Additionally, we will explore the thermodynamics of a system out of equilibrium by measuring the work distribution of a non-equilibrium transformation, and testing the seminal Tasaki-Crooks fluctuation relation for the first time in a many-body system in the quantum regime. 
Finally, we will use the single trapped ion as a single atom probe and as a density- and time- correlation detector in a system of atoms loaded in an optical lattice. This achievement will significantly improve current methods for probing many-body physics with ultracold atoms.
Our groundbreaking system will hence inaugurate concrete and decisive advances in the quantum simulation of many-body physics with quantum gases.","1496250","2015-05-01","2020-04-30"
"PNICTEYES","Using extreme magnetic field microscopy to visualize correlated electron materials","Isabel Guillamón Gómez","UNIVERSIDAD AUTONOMA DE MADRID","Strong electronic correlations often produce intertwined phases where multiple length scales coexist. These produce spatially varying electronic properties containing unique insight on the many-body effects that determine the emergence of novel collective behavior. Addressing the problem of electron correlations requires powerful microscopes probing electronic properties down to atomic scale.
A major challenge in electron correlated materials is to understand the emergence of high critical temperature (HTc) superconductivity. Fe-based superconductivity offers ultra-pure materials easily tunable through relevant phases emerging from electron correlations (antiferromagnetism, nematicity and superconductivity), providing a tremendous opportunity to unveil the microscopic pairing mechanism behind HTc superconductivity.
High magnetic fields are needed to disentangle the electronic correlations, because they enable comparison between normal and superconducting phases and unveil quantum critical behavior and vortex physics. Traditional research under very high magnetic fields uses macroscopic measurements of the spatially averaged magnetic and electronic properties.
The goal of PNICTEYES project is to combine very high magnetic fields with scanning tunneling microscopy (STM) to visualize spatial electronic heterogeneity in Fe-based superconductors. The microscopes developed within this project will operate up to 22 T using superconducting coils in-house and above 30 T using resistive and hybrid magnets at international high magnetic field facilities. Implementing novel spectroscopic methods, such as Landau level spectroscopy, we will disentangle the electronic correlations behind the microscopic mechanism of HTc superconductivity in Fe-based superconductors.
The success of this project will provide new insights in fundamentals of HTc superconductivity and first enable ultra-high magnetic field STM opening innovative opportunities in other fields as graphene or magnetism.","1704375","2016-03-01","2021-02-28"
"POCYTON","A Novel Detection Scheme to Enable Point of Care Flow Cytometry","Michael Baßler","FRAUNHOFER GESELLSCHAFT ZUR FOERDERUNG DER ANGEWANDTEN FORSCHUNG E.V.","In PoCyton, a revolutionary concept for the detection zone of a flow cytometer is proposed. Flow cytometers are fluorescence-based cell counters and as such are indispensable instruments in clinical and biomedical research. Over the last four decades, despite gradual technical improvements in the constituent components, the detection principle has virtually remained unchanged. Fluorescently tagged cells in suspension are made to flow through a narrow focal excitation area and then detected via the fluorescent pulse emitted by them. The narrow focus imposes restrictions on the flow rate and, as a consequence, on feasible sample volumes. Moreover, the alignment of cell-flow, excitation, and detection requires extreme precision. To this end, expensive, bulky components have to be used, preventing substantial miniaturization of flow cytometry. In PoCyton, the detection zone will be enlarged and superimposed with a pseudo-random pattern leading to a temporally extended, distinctly coded signal recorded for each fluorescing cell. In analogy to spread-signal methods, each cell will be reconstructed from the coded signal by correlation techniques. While the precision in spatial cell discrimination outperforms that of conventional flow cytometry only slightly, the signal-to-noise ratio is enhanced significantly, resulting in a notable improvement in sensitivity. In addition, the enlargement of the detection zone dramatically mitigates alignment issues. In PoCyton, various implementations and extensions towards multi-colour flow cytometry will be studied experimentally to demonstrate their high sample-throughput and miniaturization (lab-on-a-chip) potential. Ultimately, a wider range of flow cytometry methods will thus be made available for routine use in clinical laboratories and medical point-of-care diagnosis, e.g., for cancer treatment. PoCyton is a multi-disciplinary project primarily involving expertise in optics, microfluidics, micro-systems, and signal processing.","1427825","2011-01-01","2015-12-31"
"POLAFLOW","Polariton condensates: from fundamental physics to quantum based devices","Daniele Sanvitto","CONSIGLIO NAZIONALE DELLE RICERCHE","Polaritons are quantum superpositions of light and matter which combine appealing properties of both: the high coherence of photons and the strong interaction (non-linearities) of electrons. With the report of their Bose-Einstein condensation in 2006, they stand as one of the most exciting semiconductor-optical system of today. Given their peculiar character, they encompass different interdisciplinary areas of research which spans from the physics of phase transitions, critical phenomena and strongly-correlated systems (superfuidity, superconductivity, etc.) to various branches of quantum physics (quantum optics, quantum information, etc.), till the possibility of building polariton-based optical logics for implementation of optical circuits; all exciting realms yet to be explored.

The majority of the outstanding findings reported have been realised in structureless samples with no, or random, potential barriers for polariton states. This proposal aims at developing the polariton physics in the presence of designed and controllable potential landscapes which will allow the observation and study of a new series of phenomena related to the system's reduced dimensionality and out-of-equilibrium character.

Strong of several and complementary techniques to realize such potentials in microcavities, both in my institute and in partnership with leading growers worldwide, I will explore three phases of prospective physics in the framework where the polariton flow can be controlled, driven, localised and guided. First, I will study transport and interferometry. Then, these straightforward upgrades on the polariton state-of-the-art will be used to design elementary devices, such as polariton transistors (classical logic) or entangling devices (quantum logic). In a final phase, polariton lattices with controllable attributes will be used to study fundamental quantum phases from the superfluid to the Mott insulator, with prospects of realizing a polariton quantum simulator.","1482600","2012-11-01","2017-10-31"
"POLAR","Polar Molecules: From Ultracold Chemistry to Novel Quantum Phases","Silke Ospelkaus","GOTTFRIED WILHELM LEIBNIZ UNIVERSITAET HANNOVER","The recent realization of ultracold ensembles of polar molecules close to quantum degeneracy  has marked a milestone in atomic and molecular physics and physical chemistry.  Molecules rotate and vibrate and therefore offer many more quantum degrees of freedom than their atomic counterparts. Polar molecules  - with their permanent electric dipole moment   interact via strong long-range and anisotropic interaction, which provides unique opportunities for control of chemical reactions and engineering of novel strongly-correlated quantum many-body systems.
In this research proposal, we plan to develop techniques to control molecular processes such as chemical reactions on the quantum level and experimentally establish polar molecules as novel strongly correlated quantum many-body systems. Key topics will be the strong dipolar interaction and quantum confinement when molecules are forced into restricted geometries. The experimental work will focus on quantum-degenerate gases of bi-alkali polar molecules in optical lattices. We plan to develop techniques to precisely tailor and control the interaction potential between polar molecules by means of external ac and dc electric fields. Together with versatile control over restricted geometries, this will supply us with a unique toolbox to control ultracold chemical reactions and to engineer a wide variety of strongly correlated quantum phases - ranging from phases arising in the context of the  finite-range Hubbard Hamiltonian to self-assembling crystalline structures and chains. Polar molecular quantum gases have the potential to open new scientific frontiers and address long-standing questions for cold controlled chemistry and fundamendal questions in condensed matter physics.","1260000","2011-02-01","2017-01-31"
"PoLiChroM","Superfluidity and ferromagnetism of unequal mass fermions with two- and three-body resonant interactions","Matteo Zaccanti","CONSIGLIO NAZIONALE DELLE RICERCHE","Superfluidity and magnetism characterize a wealth of interacting fermion systems encompassing solid-state, nuclear and quark matter environments. From the interplay of these phenomena, the two following issues have been raised: Can superfluid pairing bear a mismatch in the two Fermi surfaces? Can a homogeneous fermion system become ferromagnetic via a zero-ranged interparticle repulsion?
Despite decades of interdisciplinary investigations, such questions have not gotten undisputed answers so far.
Here, I will experimentally address these problems with a new model system composed of ultracold fermionic Chromium and Lithium atoms with resonant interactions. The two species will mimic electrons of different spins, or quarks of different colours, but exhibiting the high degree of control of an atomic quantum simulator.
In particular, two features make this system stand far beyond any other available one: the peculiar Chromium-Lithium mass ratio enables a resonant control of three-body elastic interactions on top of the usual two-body ones, together with an extraordinary suppression of atom recombination into paired states in the regime of strong interspecies repulsion.
The first property greatly enhances the observability of elusive polarized superfluid regimes, such as the Fulde-Ferrel-Larkin-Ovchinnikov phase, where pairs condense in nonzero momentum states, and the Sarma or “breached pair” phase, where a homogeneous gapless superfluid coexists with unbound particles.
The second makes such mixture a prime platform for the quantum simulation of Stoner’s model for itinerant ferromagnetism, whose study has been denied in nowadays experiments, where pairing instability plagues the formation of sizeable magnetic domains. 
I will use high-resolution imaging of the system and state-of-the-art spectroscopy schemes for disclosing such exotic phases via a thorough investigation of the phase diagrams of Fermi-Fermi mixtures with attractive or repulsive interactions.","1495000","2015-04-01","2020-03-31"
"POLIGHT","Polymer-Inorganic Flexible Nanostructured Films for the Control of Light","Hernan Miguez García","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","The POLIGHT project will focus on the integration of a series of inorganic nanostructured materials possessing photonic or combined photonic and plasmonic properties into polymeric films, providing a significant advance with respect to current state of the art in flexible photonics. These highly adaptable films could act either as passive UV-Vis-NIR selective frequency mirrors or filters, or as matrices for light absorbing or optically active species capable of tailoring their optical response. The goal of this project is two-fold. In one aspect, the aim is to fill a currently existing hole in the field of materials for radiation protection, which is the absence of highly flexible and adaptable films in which selected ranges of the electromagnetic spectrum wavelengths can be sharply blocked or allowed to pass depending on the different foreseen applications. In another, the POLIGHT project seeks to go one step beyond in the integration of absorbing and emitting nanomaterials into simple flexible polymeric matrices by including hierarchically structured photonic lattices that provide fine tuning of the optical properties of these hybrid ensembles. This will be achieved by means of enhanced matter-radiation interactions that result from field localization effects at specific resonant modes. The opportunity arises as a result of the recent development of a series of robust inorganic photonic structures that present interconnected porous networks susceptible of hosting polymers and thus inheriting their mechanical properties.","1497730","2012-12-01","2017-11-30"
"POLYDOT","Control of the Electronic Properties in Hybrid- Quantum Dot/Polymer-Materials for Energy Production","Emilio Palomares","FUNDACIO PRIVADA INSTITUT CATALA D'INVESTIGACIO QUIMICA","The PolyDot project aims to foster necessary progress on frontier research that integrates a number of leading concepts in the field of photoelectrochemistry in association with new concepts from areas such as nanoscience and materials chemistry. As an example, key scientific elements of the PolyDot project are the synthesis of new molecular electronic components, such as semiconducting quantum dots, the design of self-organising functional interfaces through supramolecular interactions and the evaluation of these systems for its potential technological application as light driven energy supplier devices. Thus, the proposal is at the meeting point between supramolecular chemistry, nanostructured inorganic materials science and optoelectronic device physics. It is therefore highly multidisciplinary and involves my research group, which is working in the device physics characterisation and materials science fields. We believe that this project will develop a critical mass of expertise targeting this innovative approach towards solar powered devices allowing Europe to establish a scientific world lead and will also form a secure basis for renewable energy technological exploitation.","1299960","2009-11-01","2014-10-31"
"POLYTE","Polynomial term structure models","Damir Filipovic","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","""The term structure of interest rates plays a central role in the functioning of the interbank market. It also represents a key factor for the valuation and management of long term liabilities, such as pensions. The financial crisis has revealed the multivariate risk nature of the term structure, which includes inflation, credit and liquidity risk, resulting in multiple spread adjusted discount curves. This has generated a strong interest in tractable stochastic models for the movements of the term structure that can match all determining risk factors.

We propose a new class of term structure models based on polynomial factor processes which are defined as jump-diffusions whose generator leaves the space of polynomials of any fixed degree invariant. The moments of their transition distributions are polynomials in the initial state. The coefficients defining this relationship are given as solutions of a system of nested linear ordinary differential equations. As a consequence polynomial processes yield closed form polynomial-rational expressions for the term structure of interest rates. Polynomial processes include affine processes, whose transition functions admit an exponential-affine characteristic function. Affine processes are among the most widely used models in finance to date, but come along with some severe specification limitations. We propose to overcome these shortcomings by studying polynomial processes and polynomial expansion methods achieving a comparable efficiency as Fourier methods in the affine case.

In sum, the objectives of this project are threefold. First, we plan to develop a theory for polynomial processes and entirely explore their statistical properties. This fills a gap in the literature on affine processes in particular. Second, we aim to develop polynomial-rational term structure models addressing the new paradigm of multiple spread adjusted discount curves. Third, we plan to implement and estimate these models using real market data.""","995155","2012-12-01","2017-11-30"
"POMCAPS","Self-organisation at two length-scales: generation and characterisation of porous materials with chemically and physically modified surfaces","Wiebke Drenckhan","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Surfaces play a crucial role in the interaction of a material with its environment. Recent advances in Soft Matter physics reveal the extraordinary properties of surfaces with complex physico-chemical modifications. Of particular interest is the influence of such modifications on the wetting and flow of simple or complex fluids. Despite growing research efforts, a sound understanding and large-scale applications remain out of reach due to the difficulty of creating complex surfaces with satisfying control and cost. In particular, no technique exists to reliably modify surfaces within complex materials, like micro-porous solids.

I therefore propose to develop an original bottom-up approach which relies on the self-organisation of interfacially active agents (polymers, particles) at the interface between two fluids. Using microfluidic techniques and the self-ordering of equal-volume drops under gravity, I will create highly periodic emulsions from these fluids which are stabilised by one type of agent. Solidification of the continuous phase (including the agent) and removal of the discrete phase will lead to the creation of a micro-porous solid with well-defined morphology to which the agent confers the desired surface modification (polymer brush, surface roughness).

In systematically comparing the properties of these porous solids with those of flat modified surfaces, I aim to solidly correlate their surface properties with the resulting wetting/flow properties of simple and complex fluids. Building on this understanding and the acquired technical knowhow I aim to realise two long-sought applications: a supersponge & a liquid spring.

To establish my research group as a world leader in this rapidly evolving and competitive domain at the interface between physics and chemistry, I need to tackle this project at different length scales and levels of complexity with a long term vision. At my career stage only an ERC Starting Grant can provide me with this possibility","1499973","2012-12-01","2017-11-30"
"POPSTAR","Reasoning about Physical properties Of security Protocols with an Application To contactless Systems","Stéphanie Patrica Edith DELAUNE","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The shrinking size of microprocessors as well as the ubiquity of wireless communication have led to the proliferation of portable computing devices with novel security requirements. Whereas traditional security protocols achieve their security goals relying solely on cryptographic primitives like  encryptions and hash functions, the protocols employed to secure these devices establish and rely in addition on properties of the physical world. 
For instance, they may use, as  basic building blocks, protocols for ensuring physical proximity, secure localisation, or secure neighbourhood discovery.

Unfortunately, we often hear about ill-conceived systems, and portable computing devices raise some serious concerns about privacy.To draw meaningful conclusions, the security analysis of these systems has to be done taking into account physical properties, such as transmission delay, network topology, and node positions. This contrasts sharply with standard models used to analyse traditional protocols.

The main objective of the POPSTAR project is to develop foundations and practical tools to analyse modern security protocols that establish and rely on physical properties. In particular, we will:
- devise models and develop techniques to make possible a rigorous analysis of cryptographic protocols that establish and rely on physical properties;
- develop foundations and practical tools for formally verifying security properties, as well as privacy properties that play a prominent role is many applications;
- experiment the developed techniques for analysing the security of modern contactless systems.

The POPSTAR project will significantly advance the use of formal verification to contribute to the security analysis of protocols that rely on physical properties. This project is bold and ambitious, and answers the forthcoming expectation from consumers and citizens for high level of trust and confidence about contactless nomadic devices.","1499750","2017-02-01","2022-01-31"
"PORABEL","Nanopore integrated nanoelectrodes for biomolecular manipulation and sensing","Aleksandra Radenovic","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","In this proposal we aim to address several complex biophysical problems at single molecule level that remained elusive due to the lack of appropriate experimental approach where one could manipulate independently both interacting biomolecules and in the same time measure the strength of their interaction and correlate it with their electronic signature. In particular we are interested in finding out how biopolymer finds, enters and translocates nanopore. Equally intriguing is still unresolved mechanism of phage DNA ejection. We will also investigate how exactly proteins recognize the target binding places on DNA and if  the protein DNA recognition is based on the complementarity of their charge patterns.
To allow addressing those biophysical problems we will develop novel experimental framework by integrating electrodes to the nanopore based force spectroscopy. The proposed strategy will enable two directions of the research: single molecule manipulation and single molecule detection /sensing equally suitable for investigating complex biophysical problems and molecular recognition assays.
By exploiting superior sensing and detection capabilities of our devices, we will investigate following practical applications improved nucleotide detection, selective protein detection and protein charge profiling via nanopore unfolding.
Unique combination of optical manipulation and nanofluidics could lead to new methods of bioanalysis, mechanical characterization and discrimination between specific and non-specific DNA protein interactions. This research proposal combines nanofabrication, optics, nano/microfluidics, electronics, computer programming, and biochemistry","1439840","2010-10-01","2015-09-30"
"PORPHDYN","STRUCTURE AND DYNAMICS OF PORPHYRIN-BASED MATERIALS IN SOLUTION vs. INTERFACES","Emad Flear Aziz Bekhit","HELMHOLTZ-ZENTRUM BERLIN FUR MATERIALIEN UND ENERGIE GMBH","The goal of the proposed project is to perform a systematic study of the electronic structure and the dynamic behaviour of specifically selected metalloporphyrin materials in solution, and at liquid-solid interfaces, with a time resolution covering the range from sub-femtosecond up to picoseconds which is fundamental for their biochemical properties. Metalloporphyrins play a key role in a wide range of processes, such as the light energy conversion in photosynthesis, or in haem proteins performing the transport of small ligands like oxygen. Because of their peculiar optical, electronic and magnetic properties, they are potential candidates for technological applications like in molecular electronics. It is of fundamental importance to gain a detailed understanding of the structural and dynamic properties of these materials under realistic conditions (i.e. ambient pressure, room temperature, and in heterogeneous environments). Soft X-ray spectroscopy is an ideal tool for this purpose, which has recently been implemented by the PI for the investigation of applied materials in the liquid phase, drawing on international recognition of his achievements. Taking advantage from his broad expertise, the PI extended the soft X-ray spectroscopy to biochemical systems in solutions for the first time. Since last year the PI became the youngest group leader at HZB, and Junior Professor at the Freie Universität Berlin. Here, the PI proposes to use the laser pump / soft X-ray probe spectroscopy at first in the sub-picosecond domain. The experiments will extended to femtosecond resolution at the X-ray Free Electron Laser facility. To address sub-femtosecond processes, the systematic scanning of the systems with core-clock spectroscopy is intended. The study will provide fundamental knowledge about the electronic structure and dynamics of metalloporphyrin materials which are essential for different applications, thus helping significantly to enhance their performance.","1499000","2011-09-01","2016-08-31"
"PowAlgDO","Power of Algorithms in Discrete Optimisation","Stanislav ZIVNY","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Convex relaxations, such as linear and semidefinite programming, constitute one of the most powerful techniques for designing efficient algorithms, and have been studied in theoretical computer science, operational research, and applied mathematics. We seek to establish the power convex relaxations through the lens of, and with the extensions of methods designed for, Constraint Satisfaction Problems (CSPs).

Our goal is twofold. First, to provide precise characterisations of the applicability of convex relaxations such as which problems can be solved by linear programming relaxations. Secondly, to derive computational complexity consequences such as for which classes of problems the considered algorithms are optimal in that they solve optimally everything that can be solved in polynomial time. For optimisation problems, we aim to characterise the limits of linear and semidefinite programming relaxations for exact, approximate, and robust solvability. For decision problems, we aim to characterise the limits of local consistency methods, one of the fundamental techniques in artificial intelligence, which strongly relates to linear programming relaxations.

Recent years have seen some remarkable progress on characterising the power of algorithms for a very important type of problems known as non-uniform constraint satisfaction problems and their optimisation variants. The ultimate goal of this project is to develop new techniques and establish novel results on the limits of convex relaxations and local consistency methods in a general setting going beyond the realm of non-uniform CSPs.","1437367","2017-01-01","2021-12-31"
"PP1TOOLS","Development of chemical biology tools for the elucidation of protein phosphatase-1 substrates and druggability","Maja Banks-Köhn","ALBERT-LUDWIGS-UNIVERSITAET FREIBURG","Protein serine/threonine phosphatases (PSTPs) are considered undruggable although they are involved in the most prominent post-translational modifications. This is mainly due to an apparent lack of substrate specificity. One important PSTP is protein phosphatase-1 (PP1), a ubiquitous PSTP that is predicted to catalyze about 1/3rd of Ser and Thr dephosphorylations in eukaryotic cells, counteracting hundreds of kinases. PP1 has broad substrate specificity but is restrained in vivo by numerous PP1-interacting proteins functioning for example as substrate-targeting proteins and forming specific holoenzymes with PP1. PP1 holoenzymes play a role in many different diseases such as cancer (counteracting oncogenic kinases), diabetes (insulin release), Alzheimer’s (dephosphorylation of Tau protein) and HIV (viral translation). Currently, there are no chemical modulators available that target PP1 selectively, except that we recently developed the first compound that selectively activates PP1 in intact cells, leading to rapid dephosphorylation of PP1 substrates. The activator does not act on the most closely related protein phosphatase-2A. This proposal aims to generate and apply tools for the investigation of PP1, in part based on our previously developed activator. The tools include selective, photo- and enzymatically releasable chemical inhibitors and activators and semisynthetic proteins, and they will be applied to study PP1–substrate interactions and help identify the correlating interacting proteins. The proposed research will provide long-sought selective chemical tools to study PP1 by applying new concepts of activator and inhibitor design using peptide and small molecule chemistry to an enzyme class that is difficult to be targeted chemically. This research program will contribute to a much more detailed understanding of PP1 biology, and will open doors to investigate PP1 and its holoenzymes as drug targets.","1164516","2014-01-01","2018-12-31"
"PPOLAH","Predicting Properties of Large Heterogeneous Systems with Optimally-Tuned Range-Separated Hybrid Functionals","Leeor Kronik","WEIZMANN INSTITUTE OF SCIENCE LTD","I propose to develop a broadly applicable, quantitatively reliable, computationally simple approach to the study of large heterogeneous systems, and to apply it to important problems in molecular and organic electronics and photovoltaics. This will be based on a radically different approach to the development and application of density functional theory (DFT) - determining an optimally yet non-empirically tuned system-specific functional, instead of seeking a universally applicable one.

Large heterogeneous systems are vital to several of the most burning challenges facing materials science. Perhaps most notably, this includes materials systems relevant for basic energy sciences, e.g., for photovoltaics or photocatalysis, but also includes, e.g., organic/inorganic interfaces that are crucial for molecular, organic, and hybrid organic/inorganic (opto)electronic systems. Theory and modelling of such systems face many challenges and would benefit greatly from accurate first principles calculations. However, the “work-horse” of such large-scale calculations – DFT – faces multiple, serious challenges when applied to such systems. This includes treating systems with components of different chemical nature, predicting energy level alignment, predicting charge transfer, handling weak interactions, and more. Solving all these problems within conventional DFT is extremely difficult, and even if at all possible the result will likely be too computationally complex for many applications.

Instead, I propose a completely different strategy - sacrifice the quest for an all-purpose functional and focus on per-system physical criteria that can fix system-specific parameters without recourse to empiricism. The additional flexibility would help us gain tremendously in simplicity and applicability without loss of predictive power. I propose a practical scheme based on tunable range-separated hybrid functionals and a plan for its application to a wide range of practical systems.","1500000","2011-09-01","2016-08-31"
"PPP","Protecting and Preserving Human Knowledge for Posterity","Dimitra-Isidora Mema Roussopoulou","ETHNIKO KAI KAPODISTRIAKO PANEPISTIMIO ATHINON","""The amount and variety of content being published online is growing at an exceptional rate.  Online publishing enables content to reach a much larger audience than paper publishing but offers no guarantee of long-term access to the content.  This work investigates techniques for building a large, reliable peer-to-peer system for the preservation of online published material.  The system consists of a large number of low-cost, persistent web caches (peers) that cooperate to detect and repair damage by voting in """"opinion polls"""" on the content of their cached documents. The peers are autonomous and mutually suspicious.  Project activities include 1) investigating defenses against adversaries whose goal is to attack the preservation process; 2) performing a foundational study of the interconnections between identity, trust, and reputation models in peer-to-peer systems; 3) investigating the use of estimates of peer diversity to increase the fault and attack tolerance of peer-to-peer systems; and 4) developing, analyzing, implementing, and testing new protocols that address the high frequency of updates of online government documents, the large volumes of scientific data, and the privacy concerns of sensitive medical data.

This work is being evaluated using a real testbed of over 200 libraries around the world with the support of publishers representing over 2000 titles.  The broader impact of the work is that all electronic material preserved through the system including academic journals, government documents and web articles, and scientific and medical data will remain accessible to generations of citizens for both research and education purposes.""","1032916","2011-10-01","2017-12-31"
"PRECISE-NANO","Atomically precise nanoelectronic materials","Peter Wilhelm Liljeroth","AALTO KORKEAKOULUSAATIO SR","""The current devices used in electronics have reached nanometre dimensions where the precise nature and location of every atom matters. To further drive the development of existing technologies and to test proposals for next generation nanoelectronics, we need tools that allow structural and electronic characterization of materials down to the atomic scale. We need to be able to measure the position and nature of the atoms, as well as the local density of electronic states at a specific energy, the charge distribution, and atomic scale magnetic properties. In addition, we need methods that allow controlled manipulation of the relevant atomic-scale details of the active region of the material, where we could pick and place atoms or molecules and create vacancies at pre-defined locations.

The goal of this project is to develop such tools and apply them to three different materials systems of enormous current interest: nanostructured graphene, molecular networks and topological insulators. Progress towards using these materials in real life electronic applications is currently limited by the restricted experimental handle of their structure at the atomic scale: for example, edge structure in graphene nanostructures and contacts in molecular devices. I will use scanning probe techniques to fabricate nanoelectronic devices of atomically precise structure and explore the possibilities offered by well-defined nanostructures and spatially controlled charge and spin-doping.""","1494448","2012-02-01","2017-01-31"
"PRECISION","Precision measurements to discover new scalar and vector particles","Johannes ALBRECHT","TECHNISCHE UNIVERSITAT DORTMUND","The Standard Model of particle physics successfully describes all known particles and their interactions. However, questions like the nature of dark matter or the hierarchy of masses and couplings of quarks and leptons remain to be understood. Hence, one searches for new phenomena that will lead to a superior theory that can explain these questions. All such theories introduce additional quantum corrections. Decay rates of processes which are strongly suppressed in the Standard Model are highly sensitive to these corrections.

The LHCb experiment at CERN has recorded the world’s largest sample of beauty mesons. In the five years of this proposal, this sample will be enlarged by more than a factor of five. This sets an optimal environment for precision tests for new phenomena in strongly suppressed beauty decays.

This proposal aims to discover new scalar or vector particles in precision measurements of leptonic and semi-leptonic beauty decays. These new particles are not predicted by the Standard Model of particle physics, a potential discovery would mark the most important finding in High Energy Physics of the last decades. Some existing anomalies in flavour data can be interpreted as hints for the particles searched for in this proposal. Two classes of measurements are planned within this proposal: the complete scan of purely leptonic beauty decays which include flavour changing neutral current as well as lepton flavour violating modes. Lepton flavour universality is tested in loop decays through a novel inclusive strategy. All proposed measurements will advance the world’s knowledge significantly and have a large discovery potential.","1498249","2016-12-01","2021-11-30"
"Precision inflation","Precision tests of the inflationary scenario","David Seery","THE UNIVERSITY OF SUSSEX","The Planck survey satellite will constrain the statistics of the primordial density perturbation more stringently than ever before. Present observations require these statistics to be accurately Gaussian to one part in 10^3, but hint at a departure from Gaussianity slightly below this level. Analysis of Planck data will certainly dominate cosmology in the forthcoming decade, but at present we lack the technology to fully exploit information encoded in the nongaussian fraction. Moreover, if Planck confirms the presence of a microwave background bispectrum with amplitude at the level suggested by both WMAP data and probes of the three-dimensional density field, this will be sufficient to rule out the simplest and theoretically-favoured model of inflation. But there is no consensus concerning what should take its place. Instead, we should seek guidance from experiment. In this proposal I outline a tightly focused, coherent and ambitious five-year programme of research aimed at maximizing our understanding of experimental data.

In partnership with two ERC-funded postdoctoral researchers, the programme outlined in this proposal will yield four key science outcomes: first, a robust method, based on partial-wave expansions, with which to rule out microphysical models which are incompatible with observational data. Second, delivery of a precision numerical toolkit enabling the space of inflationary models and their initial conditions to be constrained by observation at an accuracy commensurate with the high-precision Planck data. Third, the ability to extract accurate predictions for observables in the case of multiple-field, non-canonical models. These are of great interest because of their status as effective field theories of ultraviolet-complete unifications of gravity with the Standard Model. Fourth, an understanding of the process by which inflationary nongaussianities can be generated, and a clear analysis of the conclusions which can be drawn from experiment.","1247591","2012-10-01","2017-09-30"
"PRECISIONFLAVOUR","Particle Physics beyond the Energy Frontier","Hanno Jonas Rademacker","UNIVERSITY OF BRISTOL","""The Standard Model of particle physics is incomplete. It fails to address such
fundamental questions as the baryon asymmetry of the universe
(i.e. our own existence), dark matter and dark energy, and gravity;
it has too many free parameters, and suffers from self-consistency
problems (the fine-tuning and hierarchy problem).  Nearly all
alternatives to the Standard Model that address these problems predict the
existence of new, heavy particles.

Flavour physics is sensitive to quantum loops that can be affected
by new particles with masses even beyond those that can be directly
produced at the highest-energy colliders - it allows us to see
beyond the energy frontier. This makes it highly sensitive to
physics beyond the Standard Model  The observed size of the matter-antimatter
asymmetry of the universe proves that additional, undiscovered
sources of charge-parity (CP) violation must exist. CP violation
measurements, which are the domain of flavour physics, hold
therefore particular promise in the search this """"New Physics"""".

We will use powerful new techniques and the unprecedented datasets
of LHCb to extract some of the most important, and currently most
poorly constrained CP violation parameters. We will collaborate
across several international collaborations to further improve our
measurements. We will reach new levels of precision that will
dramatically increase the New Physics reach of the entire flavour physics
approach.""","1399984","2012-10-01","2018-09-30"
"PrecisionNuclei","Strong interactions for precision nuclear physics","Andreas EKSTRÖM","CHALMERS TEKNISKA HOEGSKOLA AB","Nuclear physics is a cornerstone in our scientific endeavour to understand the universe. Indeed, atomic nuclei bring us closer to study both the stellar explosions in the macrocosmos, where the elements are formed, and the fundamental symmetries of the microcosmos. Having access to a a precise description of the interactions between protons and neutrons would provide a key to new knowledge across 20 orders of magnitude; from neutrinos to neutron stars. Despite a century of the finest efforts, a systematic description of strongly interacting matter at low energies is still lacking. Successful theoretical approaches, such as mean-field and shell models, rely on uncontrolled approximations that severely limit their predictive power in regions where the model has not been adjusted.

In this project I will develop a novel methodology to use experimental information from heavy atomic nuclei in the construction of nuclear interactions from chiral effective field theory. I expect this approach to enable me and my team to make precise ab initio predictions of various nuclear observables in a wide mass-range from hydrogen to lead as well as infinite nuclear matter. I will apply Bayesian regression and methods from machine learning to quantify the statistical and systematic uncertainties of the theoretical predictions. The novelty and challenge in this project lies in synthesising (i) the design of nuclear interactions, (ii) ab initio calculations of nuclei, and (iii) statistical inference in the confrontation between theory and experimental data. This alignment of methods, harboured within the same project, will create a clear scientific advantage and allow me to tackle the following big research question: how can atomic nuclei be described in chiral effective field theories of quantum chromo dynamics?","1499085","2018-02-01","2023-01-31"
"PREDATORS","Plate-rate experimental deformation: Aseismic, transient or seismic fault slip","Matt IKARI","UNIVERSITAET BREMEN","Despite many advances in earthquake science, the tendency for faults to host earthquake slip, aseismic slip or slow slip events is far from well understood.  Earthquakes are not yet predictable in a meaningful way, and laboratory observations do not satisfactorily explain many general observations of fault slip.  Existing data has been gathered at slip velocities orders of magnitude faster than plate convergence rates, therefore the fundamental question addressed by the PREDATORS project is how faults slip when driven tectonic rates as they are in nature.  I suggest that laboratory friction experiments conducted at these rates may reveal widespread frictional instability that explains the occurrence of (both fast and slow) earthquakes on plate-boundary faults, and that long-term shear loading driven by slow, plate convergence rates is more representative of interseismic real faults and captures processes which intermediate- to high-velocity experiments cannot. 
The experimental research proposed here utilizes an increasing complexity approach, from existing successful techniques to more innovative measurements using equipment modified to reliably shear at appropriately slow rates and under a wide range of interior Earth conditions.  Rock and mineral standards will be used to establish a basic and widely applicable framework for frictional behaviour, while natural fault samples will be used for site-specific problems. 
This project will provide a comprehensive set of measurements and observations of fault behaviour at realistically slow plate tectonic deformation rates. Combined with existing measurements, this will provide a complete description of rock/sediment friction over the entire possible range of slip velocities.  By comparison with geophysical observations on real faults, these results will help explain current seismicity patterns and other slip phenomena, and predict fault behaviour at locations where sampling and geologic characterization is limited.","1499250","2017-03-01","2022-02-28"
"PredicTOOL","Nanomethods to understand what makes an endogenous protein immunogenic","Mihaela Delcea","UNIVERSITAET GREIFSWALD","One of the major challenges in understanding the complex immune system is the question when and why this defense system attacks endogenous proteins (i.e. self-proteins). Currently, there are no reliable methods to predict what induces such an immune response, despite it has a major impact in medicine by causing autoimmune diseases, and in biotechnology by inducing adverse reactions towards new drugs.
PredicTOOL will concentrate on antibody-mediated immune reactions to endogenous proteins. By high sensitivity nanotechnological tools based on spectroscopic and imaging techniques (e.g. Circular Dichroism Spectroscopy, Single Molecule Force Spectroscopy, Isothermal Titration Calorimetry, Fluorescence Microscopy), PredicTOOL will identify common patterns that characterize the interaction of autoantibodies with their antigens and further interaction with cells of the immune system. We will use well characterized human model diseases (cardiovascular). The proposed technologies will allow applying the rules of classic mechanics to identify the pattern of autoantibody-antigen interactions and will lead to better understand why an endogenous protein induces an immune response.
The project aims to: i) identify patterns expressed in proteins to which autoantibodies bind on a nanometer scale; ii) investigate whether certain mutations or post-translational changes in the proteins facilitate conformational changes leading to expression of such patterns; iii) assess the binding of human autoantibodies to such modified proteins and compare this with the binding to the wild type/native protein; and iv) develop a platform of microstructured arrays to investigate immunogenic proteins. The results of this highly interdisciplinary and translational project will allow to better understand autoimmunity and to develop new ways for prevention and treatment in medicine, and to optimize the production of safer biotherapeutic drugs.

The vision of PredicTOOL is that physics has the potential to substantially change the view on the pathogenesis of autoimmunity. It is highly interdisciplinary and bridges in a translational approach, immunology, physics, biotechnology and medicine. It targets to identify patterns driving antibody-mediated cardiovascular diseases. The project bears two major areas of application. In medicine, it can lead to better understand autoimmunity and to develop new ways for prevention and treatment. In the development of biotherapeutics, it can help to finally produce safer biotherapeutic drugs. The biggest strength of the project is to directly work in the human system (proteins and antibodies) applying state-of-the-art nanotechnological techniques and physical methods. We have already established the proof-of-principle of our approach using PF4/heparin model system, which now allows transferring our findings to other proteins to identify common patterns.","1491250","2015-04-01","2020-03-31"
"PREDMODSIM","Predictive models and simulations in nano- and biomolecular mechanics: a multiscale approach","Marino Arroyo","UNIVERSITAT POLITECNICA DE CATALUNYA","The predictive ability of current simulations of interesting systems in nano- and biomolecular mechanics is questionable due to (1) uncertainties in material behavior of continuum models, (2) severe limitations of atomistic simulations in the computationally accessible length and time scales in relation with the scales of scientific and technological interest, and (3) the limited understanding gained from terabytes of data produced in supercomputing platforms. These difficulties seriously undermine the credibility of computer simulations, as well as their real impact in scientific and technological endeavors. Examples include fundamental challenges in materials science (structure-property relations), molecular biology (sequence-structure-function of proteins), or product engineering (virtual testing for analysis, optimization, control). This proposal addresses three important topics in nano- and biomolecular mechanics, whose full understanding and technological exploitation require predictive models and simulations: (1) Mechanics of carbon nanotubes at engineering scales, (2) Mechanics of fluid membranes in eukaryotic cells and bio-inspired technologies and (3) Local-to-global conformational space exploration and free energy calculations for biomolecules. We follow a multiscale approach, which seeks to incorporate the net effect of the small-scale phenomena described by fundamental models of physics into the coarser (computable) scales at which the system or device operates. In addition to specific impact in these applications, the proposed research is expected to exemplify the potential of multiscale approaches towards predictive and quantitative science and technology, as well as contribute to the credibility and utility of large investments in supercomputing.","1462198","2009-10-01","2014-09-30"
"preQFT","Strategic Predictions for Quantum Field Theories","John Joseph Carrasco","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","Ambitious Questions:

*  How does the relatively calm macroscopic universe survive and emerge from the violent quantum fluctuations of its underlying microphysics? 
*  How do classical notions of space and time emerge from fundamental principles, and what governs their evolution? 

These questions are difficult to answer---perhaps impossible given current ideas and frameworks---but I believe a strategic path forward is to thoroughly understand the quantum predictions of our Yang-Mills and Gravity theories, and unambiguously identify their non-perturbative UV completions. The first step forward, and the goal of this project, is to move towards the trivialization of perturbative calculations.  

Consider the notion of failure-point calculations -- calculations that push modern methods and world-class technologies to their breaking-point. Such calculations, for their very success, engender the chance of cultivating and exploiting previously unappreciated structure. In doing so, such calculations advance the state of the art forward to some degree, dependent on the class of the problems and nature of the solution. With scattering amplitude calculations, we battle against (naive) combinatorial complexity as we go either higher in order of quantum correction ( loop order ), or higher in number of external particles scattering (multiplicity), so our advances must be revolutionary to lift us forward.  Yet I and others have shown that the very complications of generalized gauge freedom promise a potential salvation at least as powerful as the complications that confront us. The potential reward is enormous, a rewriting of perturbative quantum field theory to make these principles manifest and calculation natural, an ambitious but now realistic goal. The path forward is optimized through strategic calculations.","1299958","2015-06-01","2020-05-31"
"PRESEISMIC","Exploring the nucleation of large earthquakes: cascading and unpredictable or slowly driven and forecastable","Zacharie DUPUTEL","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","How do earthquakes begin? Answering this question is essential to understand fault mechanics but also to
determine our ability to forecast large earthquakes. Although it is well established that some events are preceded by foreshocks, contrasting views have been proposed on the nucleation of earthquakes. Do these foreshocks belong to a cascade of random failures leading to the mainshock? Are they triggered by an aseismic nucleation phase in which the fault slips slowly before accelerating to a dynamic, catastrophic rupture? Will we ever be able to monitor and predict the slow onset of earthquakes or are we doomed to observe random, unpredictable cascades of events? We are currently missing a robust tool for quantitative estimation of the proportion of seismic versus aseismic slip during the rupture initiation, cluttering our attempts at understanding what physical mechanisms control the relationship between foreshocks and the onset of large earthquakes.

The current explosion of available near-fault ground-motion observations is an unprecedented opportunity to capture the genesis of earthquakes along active faults. I will develop an entirely new method based a novel data assimilation procedure that will produce probabilistic time-dependent slip models assimilating geodetic, seismic and tsunami datasets. While slow and rapid fault processes are usually studied independently, this unified approach will address the relative contribution of seismic and aseismic deformation.

The first step is the development of a novel probabilistic data assimilation method providing reliable uncertainty estimates and combining multiple data types. The second step is a validation of the method and an application to investigate the onset of recent megathrust earthquakes in Chile and Japan. The third step is the extensive, global use of the algorithm to the continuous monitoring of time-dependent slip along active faults providing an automated detector of the nucleation of earthquakes.","1499545","2019-01-01","2023-12-31"
"PRIMCHEM","Primitive chemistry in planetary atmospheres: From the upper atmosphere down to the surface","Nathalie, Marie Carrasco","UNIVERSITE DE VERSAILLES SAINT-QUENTIN-EN-YVELINES.","The presence of organic compounds was essential to the emergence of life on Earth 3.5 to 3.8 billion years ago. Such compounds may have had several different origins; amongst them the ocean-atmosphere coupled system (the primordial soup theory), or exogenous inputs by meteorites, comets and Interplanetary Dust Particles. 

Titan, the largest moon of Saturn, is the best known observable analogue of the Early Earth. I recently identified a totally new source of prebiotic material for this system: the upper atmosphere. Nucleobases have been highlighted as components of the solid aerosols analogues produced in a reactor mimicking the chemistry that occurs in the upper atmosphere. The specificity of this external layer is that it receives harsh solar UV radiations enabling the chemical activation of molecular nitrogen N2, and involving a nitrogen rich organic chemistry with high prebiotic interest. 

As organic solid aerosols are initiated in the upper atmosphere of Titan, a new question is raised that I will address: what is the evolution of these organic prebiotic seeds when sedimenting down to the surface? Aerosols will indeed undergo the bombardment of charged particles, further UV radiation, and/or coating of condensable species at lower altitudes. I expect possible changes on the aerosols themselves, but also on the budget of the gas phase through emissions of new organic volatiles compounds. The aerosols aging may therefore impact the whole atmospheric system. 

An original methodology will be developed to address this novel issue. The successive aging sequences will be experimentally simulated in chemical reactors combining synchrotron and plasma sources. The interpretation of the experimental results will moreover be supported by a modelling of the processes. This complementary approach will enable to decipher the aerosols evolution in laboratory conditions and to extrapolate the impact on Titan atmospheric system.","1487500","2015-09-01","2021-08-31"
"PRISM","Ice-binding proteins: from antifreeze mechanism to resistant soft materials","Ilja Karina Voets","TECHNISCHE UNIVERSITEIT EINDHOVEN","Crystallization of water into ice is lethal to most organisms and detrimental to many soft materials. Freeze-tolerant fish living in polar seas evolved to tackle this problem with an unusual coping strategy. They produce ‘antifreeze’ proteins that block the growth of nascent ice crystals within a narrow temperature range known as the ‘thermal hysteresis gap’ enabling survival under extreme conditions. Encoding this functionality into synthetic polymers would open up new avenues in biomedicine, agrifood and materials science for e.g. cryopreservation, crop hardiness, ice-templating, dispersion stability, and advanced coatings. Progress requires a profound understanding of the mechanism of non-colligative freezing point depression at the molecular level and allows for efficient strategies for the design and preparation of powerful macromolecular antifreezes.

I propose to unravel how antifreeze proteins work and to build upon these insights to explore effective routes towards ice-binding polymers aiming to make sensitive soft materials freeze-resistant. Within this challenge we first focus on single-molecule experiments to visualize bound proteins and study the strength of the non-covalent interaction with ice. We will study if and when adsorption on ‘foreign’ interfaces and solution assembly impact activity. These fundamental insights will guide our research towards synthetic antifreeze agents with superior functionality to achieve record supercooling in complex environments. This knowledge-based design of polymers with high affinity for crystalline interfaces holds great promise for many areas of science and technology in which crystallization plays a decisive role.","1661605","2015-05-01","2020-04-30"
"PRISTINE","High precision isotopic measurements of heavy elements in extra-terrestrial materials: origin and age of the solar system volatile element depletion","Frédéric, Pierre, Louis Moynier","INSTITUT DE PHYSIQUE DU GLOBE DE PARIS","""The objectives of this proposal, PRISTINE (high PRecision ISotopic measurements of heavy elements in extra-Terrestrial materials: origIN and age of the solar system volatile Element depletion), are to develop new cutting edge high precision isotopic measurements to understand the origin of the Earth, Moon and solar system volatile elements and link their relative depletion in the different planets to their formation mechanism. In addition, the understanding of the origin of the volatile elements will have direct consequences for the understanding of the origin of the Earth’s water. To that end, we will approach the problem from two angles: 1) Develop and use novel stable isotope systems for volatile elements (e.g. Zn, Ga, Cu, and Rb) in terrestrial, lunar and meteoritic materials to constrain the origin of solar system’s volatile element depletion 2) Determine the age of the volatile element depletion by using a novel and original approach: calculate the original Rb/Sr ratio of the Solar Nebula by measuring the isotopic composition of the Sun with respect to Sr via the isotopic composition of solar wind implanted in lunar soil grains.
The stable isotope composition (goal #1) will give us new constraints on the mechanisms (e.g. evaporation following a giant impact or incomplete condensation) that have shaped the abundances of the volatile elements in terrestrial planets, while the timing (goal #2) will be used to differentiate between nebular events (early) from planetary events (late). These new results will have major implications on our understanding of the origin of the Earth and of the Moon, and they will be used to test the giant impact hypothesis of the Moon and the origin of the Earth’s water.""","1487500","2015-04-01","2020-03-31"
"ProbDynDispEq","Probabilistic and Dynamical Study of Nonlinear Dispersive Equations","Oh","THE UNIVERSITY OF EDINBURGH","Nonlinear dispersive partial differential equations (PDEs) appear ubiquitously as models describing wave phenomena in various branches of physics and engineering. Over the last few decades, multilinear harmonic analysis has played a crucial role in the development of the theoretical understanding of the subject. Furthermore, in recent years, a non-deterministic point of view has been incorporated into the study of nonlinear dispersive PDEs, enabling us to study typical behaviour of solutions in a probabilistic manner and go beyond the limit of deterministic analysis.

The main objective of this proposal is to develop novel mathematical ideas and techniques, and make significant progress on some of the central problems related to the nonlinear Schrödinger equations (NLS) and the Korteweg-de Vries equation (KdV) from both the deterministic and probabilistic points of view. In particular, we consider the following long term projects:

1. We will study properties of invariant Gibbs measures for nonlinear Hamiltonian PDEs. One project involves establishing a new connection between the limiting behaviour of the Gibbs measures and the concentration phenomena of finite time blowup solutions. The other project aims to understand the space-time covariance of the Gibbs measures in the weakly nonlinear regime.

2. We will first construct the invariant white noise for the cubic NLS on the circle. Then, we will provide a statistical description of the global-in-time dynamics for the stochastic KdV and stochastic cubic NLS on the circle with additive space-time white noise.

3. We will develop novel analytical techniques and construct the local-in-time dynamics for the cubic NLS on the circle in a low regularity.

4. We will advance the understanding of traveling waves and prove scattering for some energy-critical NLS with non-vanishing boundary conditions.","1007811","2015-03-01","2020-02-29"
"ProDIS","Provenance for Data-Intensive Systems","Daniel Deutch","TEL AVIV UNIVERSITY","In the context of data-intensive systems, data provenance captures the way in which data is used, combined
and manipulated by the system. Provenance information can for instance be used to reveal whether
data was illegitimately used, to reason about hypothetical data modifications, to assess the trustworthiness
of a computation result, or to explain the rationale underlying the computation.
As data-intensive systems constantly grow in use, in complexity and in the size of data they manipulate,
provenance tracking becomes of paramount importance. In its absence, it is next to impossible to follow the
flow of data through the system. This in turn is extremely harmful for the quality of results, for enforcing
policies, and for the public trust in the systems.
Despite important advancements in research on data provenance, and its possible revolutionary impact,
it is unfortunately uncommon for practical data-intensive systems to support provenance tracking. The
goal of the proposed research is to develop models, algorithms and tools that facilitate provenance
tracking for a wide range of data-intensive systems, that can be applied to large-scale data analytics,
allowing to explain and reason about the computation that took place.
Towards this goal, we will address the following main objectives: (1) supporting provenance for modern
data analytics frameworks such as data exploration and data science, (2) overcoming the computational
overhead incurred by provenance tracking, (3) the development of user-friendly, provenance-based analysis
tools and (4) experimental validation based on the development of prototype tools and benchmarks.","1306250","2018-12-01","2023-11-30"
"ProFoundNet","Probabilistic Foundations for Networks","Alexandra Martins da Silva","UNIVERSITY COLLEGE LONDON","In an ever-connected world, increasingly complex network systems play a crucial role in many daily tasks. This results in an acute need for methods/tools that can enable easy control of the network and, at the same time, provide rigorous guarantees about its behavior, performance, and security. Recent years saw the growth of a new software ecosystem -Software-defined networking (SDN)- which advocates a clean and open interface between networking devices and the software that controls them.  Yet, existing SDN languages do not support reasoning about crucial quantitative aspects, such as: ``How much congestion is there?'' or ``Is the network resilient under failure?''. Enabling compositional quantitative reasoning is the major breakthrough needed to fully realize the vision of SDN. 

The central objective of this project is to develop new abstractions for programming of networks, with high-level modular constructs. We will provide rigorous semantic probabilistic foundations, enabling quantitative reasoning. This will serve as a solid platform for program analysis tools where compositional reasoning about complex interactions will be a reality. Our goal will be achieved through an interdisciplinary research effort: using techniques from concurrency and formal methods, areas where akin challenges can be found in the quest to design correct software systems. We will leverage the wealth of recent advances in those areas (some of which from the PI's own research) to networks, and bring awareness and new challenges arising from applications in networking to the other two communities. 

The project will significantly advance the foundations of network programming/verification in new and previously unexplored directions. This line of research will not only result in fundamental theoretical contributions and insights in their own right but will also impact the practice of network programming and lead to new and more powerful techniques for the use of engineers and programmers.","1500000","2016-04-01","2021-03-31"
"PROGRAM-NANO","Programmed Nanostructuration of Organic Materials","David Gonzalez Rodriguez","UNIVERSIDAD AUTONOMA DE MADRID","“Program-Nano” aims at establishing unconventional and versatile strategies towards organic architectures whose size, composition, internal structure, and function can be rationally predesigned and controlled. In a bio-inspired manner, we will “program” functional molecules with the required information to self-assemble into unique, well-defined nanofibers or nanotubes. We want to focus on two main ambitious objectives for the application of such organic nanostructured materials.
1) The design and preparation of optoelectronic devices, such as plastic solar cells, where nanostructured fibers are integrated within the active layers. The major goal is to determine the influence of the molecular organization and the morphology at the nanoscale on the performance of the device, and to try in this way to set new records in device efficiency.
2) The fabrication of plastic nanoporous materials for the separation, storage or catalytic transformation of (bio)molecules in which the size, the shape ratio, and the internal functionalization of the nanopores can be custom-tailored.","1300932","2011-11-01","2016-10-31"
"PROGRESO","Probing General Relativity with Stellar Orbits","Stefan Gillessen","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Of the four fundamental forces in nature, gravity is the least tested one. A uniquely accessible laboratory for gravitational physics is the Galactic Center, hosting the closest massive black hole with 4 million solar masses. Its vicinity and the advent of high-resolution, near-infrared techniques allowed for the observation of ≈ 30 individual stellar orbits in its gravitational field. The stars are ideal test particles for the potential in which they move, and thus the keys to directly probing the gravitational potential are ultra-precise astrometry and radial velocities of the stars.
All data to date can be described by Keplerian orbits around a single central mass. However, relativistic effects will lead to post-Newtonian orbits. These effects will soon be detectable with near-infrared interferometry for faint objects. With the next-generation instrument GRAVITY for the Very Large Telescope Interferometer of ESO the achievable resolution is ≈ 4 mas, and the astrometric accuracy ≈ 10 µas - the scale of the event horizon. This is an improvement of a factor ≈ 20 compared to current, adaptive-optics based data.
PROGRESO aims at detecting the relativistic effects in the stellar orbits around Sgr A*. Also, the emission from hot spots close to the last stable orbit might be used. Scientifically, this will test gravity in a so far unprobed regime of mass and space-time curvature. The spin of the black hole might be determined in this way, and in the most optimistic case test the 'no-hair' theorem. With PROGRESO I would like to start a research group that focuses on interpreting the novel interferometric data of the Galactic Center with yet to be determined instrumental systematics, on exploiting it scientifically and on optimally combining it with existing and future data of the present instruments.
The proposed research group will be hosted at MPE (Garching, Germany), the PI institute for GRAVITY, and therefore have access to commissioning and guaranteed time data.","1187700","2013-04-01","2018-12-31"
"PROHAPTICS","Haptic Signal Processing and Communications","Eckehard Gotz Steinbach","TECHNISCHE UNIVERSITAET MUENCHEN","During the last decade, audio-visual communication has shaped the way humans interact with technical systems or other humans. During the next decade, haptic communication has the potential to further augment human-to-human and human-to-machine interaction. With recent advances in Virtual Reality, Man-Machine Interaction, Telerobotics, Telepresence, and Telemanipulation, the processing and communication of haptic signals are rapidly gaining in relevance and are becoming an enabling technology for many novel fields of application. The objective of this proposal is to investigate fundamental methods and technologies for the efficient processing and communication of haptic signals. We will develop a mathematical model of human haptic perception which will be instrumental in ensuring that introduced distortions stay below human perception thresholds. One of the main goals of this work is to leverage the model of human haptic perception for efficient lossy compression of haptic signal streams. We will study haptic signal processing and communication both from a theoretical point of view but also experimentally by designing, implementing and evaluating haptic interaction testbeds. The performance of the proposed haptic processing and communication methods will be analyzed both objectively and subjectively. With our work plan, we see the opportunity to establish a de facto standard for future haptic data communication.","1489000","2011-01-01","2015-12-31"
"PROMETHEUS","Flame nanoengineering for antibacterial medical devices","Georgios SOTIRIOU","KAROLINSKA INSTITUTET","Engineers in nanotechnology research labs have been quite innovative the last decade in designing nanoscale materials for medicine. However, very few of these exciting discoveries are translated to commercial medical products today. The main reasons for this are two inherent limitations of most nanomanufacture processes: scalability and reproducibility. There is too little knowledge on how well the unique properties associated with nanoparticles are maintained during their large-scale production while often poor reproducibility hinders their successful use. A key goal here is to utilize a nanomanufacture process famous for its scalability and reproducibility, flame aerosol reactors that produce at tons/hr commodity powders, and advance the knowledge for synthesis of complex nanoparticles and their direct integration in medical devices. Our aim is to develop the next generation of antibacterial medical devices to fight antimicrobial resistance, a highly understudied field. Antimicrobial resistance constitutes the most serious public health threat today with estimations to become the leading cause of human deaths in 30 years.
We focus on flame direct nanoparticle deposition on substrates combining nanoparticle production and functional layer deposition in a single-step with close attention to product nanoparticle properties and device assembly, extending beyond the simple commodity powders of the past. Specific targets here are two devices; a) hybrid drug microneedle patch with photothermal nanoparticles to fight life-threatening skin infections from drug-resistant bacteria and b) smart nanocoatings on implants providing both osteogenic and self-triggered antibacterial properties. The engineering approach for the development of antibacterial devices will provide insight into the basic physicochemical principles to assist in commercialization while the outcome of this research will help the fight against antibiotic resistance improving the public health worldwide.","1812500","2018-03-01","2023-02-28"
"PROMISE","Origins of the Molecular Cloud Structure","Jouni Tapani Kainulainen","CHALMERS TEKNISKA HOEGSKOLA AB","Understanding the physical processes that control the life-cycle of the interstellar medium (ISM) is one of the key themes in the astrophysics of galaxies today. This importance originates from the role of the ISM as the birthplace of new stars, and therefore, as an indivisible component of galaxy evolution. Exactly how the conversion of the ISM to stars takes place is intricately linked to how the internal structure of the cold, molecular clouds in the ISM forms and evolves. Despite this pivotal role, our picture of the molecular cloud structure has a fundamental lacking: it is based largely on observations of low-mass molecular clouds. Yet, it is the massive, giant molecular clouds (GMCs) in which most stars form and which impact the ISM of galaxies most. I present a program that will fill this gap and make profound progress in the field. We have developed a new observational technique that provides an unparalleled view of the structure of young GMCs. I also have developed a powerful tool to study the most important structural characteristics of molecular clouds, e.g., the probability distribution of volume densities, which have not been accessible before. With this program, the full potential of these tools will be put into use. We will produce a unique, high-fidelity column density data set for a statistically interesting volume in the Galaxy, including thousands of molecular clouds. The data set will be unmatched in its quality and extent, providing an unprecedented basis for statistical studies. We will then connect this outstanding observational view with state-of-the-art numerical simulations. This approach allows us to address the key question in the field: Which processes drive the structure formation in massive molecular clouds, and how do they do it? Most crucially, we will create a new, observationally constrained framework for the evolution of the molecular cloud structure over the entire mass range of molecular clouds and star formation in the ISM.","1266750","2016-02-01","2021-01-31"
"PROPERTY TESTING","Property testing and sublinear algorithms for languages and combinatorial properties","Eldar Fischer","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","Property testing, an investigation started in [Blum, Luby and Rubinfeld, 1993], [Rubinfeld and Sudan, 1996], and [Goldreich, Goldwasser and Ron, 1996], deals with the following general question: Distinguish, using as few queries as possible, between the case where the input satisfies a certain property, and the case where the input is epsilon-far from this, i.e. the case where there is no way to make the input satisfy the given property even if it is modified in an epsilon fraction of its positions. Ideally the number of queries, i.e. the size of the portion of the input that is read by the (probabilistic) algorithm, depends only on epsilon and does not depend at all on the input length. However, algorithms that read more than a constant amount, as long as it is sublinear in the input size, are also deemed interesting. The related topic of sublinear algorithms concentrate on similar notions of approximation, but with the stronger requirement that the running time (rather than query complexity) that is less than the order of the input size. The purpose of this proposal is to investigate advanced topics in the frontier of property testing, especially with respect to the relation of the easiness of testing to other notions of complexity, and to investigate possible uses of ideas from property testing in other fields of computer science. Particular emphasis will be given to hypergraph-like models, sparse models, and models in which the description of the property in itself is represented as a graph or a combinatorial structure. The latter holds particular promise with regards to applications both inside and outside theoretical CS. Some topics going beyond testing (such as stronger testing notions, and testing-related notions from Probabilistically Checkable Proofs) will also be addressed.","963540","2008-07-01","2013-06-30"
"PROPHET","Simplifying Development and Deployment of High-Performance, Reliable Distributed Systems","Dejan Kostic","KUNGLIGA TEKNISKA HOEGSKOLAN","Distributed systems form the foundation of our society's infrastructure. Unfortunately, they suffer from a number of problems. First, they are time-consuming to develop because it is difficult for the programmer to envision all possible deployment environments and design adaptation mechanisms that will achieve high performance in all scenarios. Second, the code is complex due to the numerous outcomes that have to be accounted for at development time and the need to reimplement state and network models. Third, the distributed systems are unreliable because of the difficulties of programming a system that runs over an asynchronous network and handles all possible failure scenarios. If left unchecked, these problems will keep plaguing existing systems and hinder development of a new generation of distributed services.

We propose a radically new approach to simplifying development and deployment of high-performance, reliable distributed systems. The key insight is in creating a new programming model and architecture that leverages the increases in per-node computational power, bandwidth and storage to achieve this goal. Instead of resolving difficult deployment choices at coding time, the programmer merely specifies the choices and the objectives that should be satisfied. The new runtime then resolves the choices during live execution so as to maximize the objectives. To accomplish this task, the runtime uses a groundbreaking combination of state-space exploration, simulation, behavior prediction, performance modeling, and program steering. In addition, our approach reuses the effort spent in distributed system testing by transmitting a behavior summary to the runtime to further speed up choice resolution.","1450000","2011-02-01","2016-12-31"
"PROSECURE","Provably secure systems: foundations, design, and modularity","Véronique Cortier","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Security protocols are short distributed computer programs dedicated to securing communications on digital networks. They are designed to achieve various goals such as data privacy and data authenticity, even when communication channels are controlled by malicious users. Their increasing penetration in many important applications makes it a very important research challenge to design and establish security properties. In the last decade, formal approaches and automated verification techniques have been successfully applied for detecting potential attacks. However, the security guarantees obtained so far usually hold in a rather abstract model, and are limited to isolated specific protocols analyzed for a few set of specific security properties. Moreover new types of protocols are still emerging in order to face new technological and societal challenges.

The goal of the project is to propose foundations for a careful analysis and design of large classes of up-to-date protocols. Proposing a secure environment for network-based communications has a societal as well as an economical prominent impact.

To achieve this goal, we foresee three main tasks. First, we plan to develop general verification techniques for new classes of protocols that are of primary interest in nowadays life like e-voting protocols, routing protocols or APIs. Second, we will consider the cryptographic part of the primitives
that are used in such protocols (encryption, signatures, ...), obtaining higher security guarantees. Third, we will propose modular results both for the analysis and design of protocols. As a particular outcome, each of the tasks will allow to characterize simple design principles that ease the analysis (thus the security) of protocols and discard families of attacks.","1470000","2011-02-01","2016-01-31"
"PROSECUTOR","Programming Language-Based Security To Rescue","Andreas Sabelfeld","CHALMERS TEKNISKA HOEGSKOLA AB","It is alarming that the society's critical infrastructures are not
fully prepared to meet the challenge of information security.  Modern
computing systems are increasingly extensible, inter-connected, and
mobile. However, exactly these trends make systems more vulnerable to
attacks. A particularly exposed infrastructure is the world-wide web
infrastructure, where allowing the mere possibility of fetching a web
page opens up opportunities for delivering potentially malicious
executable content past current security mechanisms such as
firewalls. A critical challenge is to secure the computing
infrastructures without losing the benefits of the trends.

It is our firm belief that attacks will continue succeeding unless a
fundamental security solution, one that focuses on the security of the
actual applications (code), is devised. To this end, we are convinced
that application-level security can be best enforced, *by
construction*, at the level of programming languages.

ProSecuToR will develop the technology of *programming language-based
security* in order to secure computing infrastructures.
Language-based security is an innovative approach for enforcing
security by construction.  The project will deliver policies and
enforcement mechanisms for protecting who can see and who can modify
sensitive data. Security policies will be expressible by the
programmer at the construction phase. We will devise a policy
framework capable of expressing fine-grained application-level
security policies. We will build practical enforcement mechanisms to
enforce the policies for expressive languages. Enforcement mechanisms
will be fully automatic, preventing dangerous programs from executing
whenever there is a possibility of compromising desired security
properties. The practicality will be demonstrated by building robust
web applications. ProSecuToR is expected to lead to breakthroughs in
*securing web mashups* and *end-to-end web application security*.","1500000","2013-01-01","2017-12-31"
"PROSPER","Design of polymer optical fibre gratings for endoscopic biosensing purposes","Christophe Caucheteur","UNIVERSITE DE MONS","""PROSPER is a multidisciplinary project covering the emerging fields of photonics, bio-chemistry and endoscopy, targeting to contribute to the development of a new class of biochemical optical sensors that would significantly improve the healthcare of the future.
PROSPER will address this objective through the demonstration of biosensors based on a functionalized polymer optical fibre (POF) in which specially-designed refractive-index gratings have been written. Immobilised biomolecular receptors on the grafted fibre surface will allow label-free recognition through the monitoring of wavelength shifts in the grating spectral response. Such biosensors are predicted to exhibit a surrounding refractive index detection limit of 10-6 in real time, which is classical for biodetection. Although generic and able to work in various areas such as environmental monitoring, food analysis, agriculture or security, the proposed biosensors will be targeted for medical diagnostics where they present the most ground-breaking nature. Indeed, unlike bulk structures, they require reduced reaction volumes for ex-vivo measurements and present the advantageous possibility of assaying several parameters simultaneously (e.g. several cancer-associated antigens in one sample). As a result, statistical analysis of these parameters can potentially increase the reliability and reduce the measurement uncertainty of a diagnostic over single-parameter assays. More importantly, the proposed biosensors have the unique potential to be used in-vivo in an endoscope (for this reason POFs are privileged over silica), which would considerably improve the diagnostic. The ultimate target of PROSPER is thus to demonstrate the feasibility of diagnostics outside of laboratory settings. A final prototype consisting of a packaged polymer biosensor will be validated.""","1438368","2011-12-01","2016-11-30"
"ProteinDynamics","Visualizing the Conformational Dynamics of Proteins by Time-Resolved Electron Microscopy","Ulrich Joseph LORENZ","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The function of many proteins involves large-amplitude domain motions that occur on a timescale of microseconds to millisecond. In the absence of tools to directly observe these dynamics, our understanding of the function of proteins is necessarily incomplete and must frequently rely on extrapolation from known static structures. Here, the implementation of real-time imaging of single particle dynamics in liquid phase is proposed with both microsecond time resolution as well as near-atomic spatial resolution. The experimental approach builds on several recent technological advances, namely the advent of Time-Resolved (“Four-dimensional”) Electron Microscopy, graphene liquid cell technology, and direct electron detection cameras, which are combined with established single-particle reconstruction techniques in cryo-Electron Microscopy. Visualizing the conformational dynamics of proteins will fundamentally advance our understanding of these nanoscale machines and has the potential to greatly benefit biomedical applications.","2000000","2018-03-01","2023-02-28"
"PROTEOFOLD","Proteomimetic Foldamers: Towards Future Therapeutics and Designer Enzymes","Andrew John Wilson","UNIVERSITY OF LEEDS","The purpose of this project is to develop a RULE-BASED APPROACH for the design and synthesis of proteomimetics of the alpha-helix and in doing so establish to what extent the structural and functional role of the alpha-helix can be reproduced with non-natural molecules in a PREDICTABLE manner We will focus on developing aromatic oligoamide proteomimetics (compounds that mimic the secondary structure from which they are derived) of one of the dominant secondary structural motifs observed in proteins the alpha-helix. Helices play a key role in mediating many protein-protein interactions, they interact with proteins and contribute residues to the resulting complex that form part of a catalytic site and they operate within the context of an entire protein structure as scaffolding upon which other helices, sheets, turns and coils pack to generate an active 3D structure. We will therefore: (i) develop a general approach for the inhibition of alpha-helix mediated protein-protein interactions, (ii) develop proteomimetics that can bind to an inactive protein and restore catalytic activity (iii) develop proteomimetics that can be covalently incorporated into the primary sequence of a protein without abolishing its function. This will lead to immense opportunities for development of new therapeutics and proteins with new functionality. More significantly, re-engineering nature to the extent of replacing whole segments of protein backbone with non-natural prostheses as proposed here will begin to answer the fundamental question: Is the astonishing structural and functional complexity achieved through precise secondary and tertiary organisation of primary protein structure confined to sequences of alpha-amino acids?","1000000","2010-02-01","2015-01-31"
"PROTINT","Towards a quantitative framework for understanding protein-protein interactions: from specific effects to protein ecology","Bojan Zagrovic","UNIVERSITAT WIEN","Non-covalent protein-protein interactions underlie most of biological activity on the molecular level.  A binding event between two proteins typically consists of two stages: 1) a diffusional, non-specific search of the binding partners for each other, and 2) specific recognition of the compatible contact surfaces followed by complex-formation.  Despite significant progress in studying these processes, a number of open questions remain.  How do partners find each other in the crowded and interaction-rich cellular environment?  What are the exact mechanisms of the specific recognition of binding surfaces?  What is the role of induced fit as opposed to conformational selection in the process?  We propose to utilize atomistic-level and coarse-grained molecular dynamics simulations and advanced computational techniques in close collaboration with experiment to address these questions, with the ultimate goal of developing a unified picture combining both specific and non-specific contributions to protein-protein interactions.  We will focus on several test-cases of broad biological significance, such as the ubiquitin system, to test two central ideas: 1) that protein dynamics is the principal determinant of specific molecular recognition in many systems, and 2) that co-localization, which non-specifically affects the binding process, is a direct consequence of the general physico-chemical properties of the binding partners, irrespective of the features of their binding sites.  Methodologically, we will further develop and utilize distributed computing techniques on the world-wide-web and computation on streaming processors to meet the high demand for computational power, inherent in studying protein interactions in silico.  In our work, we will closely collaborate with experimentalists, ranging from NMR and X-ray crystallography experts to molecular biologists to both validate our simulations and theoretical work as well as assist in interpreting experimental findings.","1495790","2011-09-01","2017-03-31"
"PSC and LMCF","Positive Scalar Curvature and Lagrangian Mean Curvature Flow","Andre Da Silva Graça Arroja Neves","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","The interplay between Geometry and Analysis has been among the most fruitful mathematical ideas in recent years, the most obvious example being Perelman's proof of Poincare' conjecture. The goal of this proposal is to establish a research group that will make significant  progress in the following two distinct problems.

Scalar Curvature: A classical theorem in Riemannian Geometry states that nonnegative scalar curvature metrics which are flat outside a compact set must be Euclidean. The equivalent problem for positive scalar curvature is known as the Min-Oo conjecture and, after being checked in many particular cases, was recently disproven by myself and coauthors. I plan to prove optimal results relating positive scalar curvature and area of minimal surfaces. I also plan to show that these manifolds admit an infinite number of minimal surfaces (Yau's conjecture). My approach consists of studying min-max methods in order to obtain  existence of higher-index minimal surfaces.

Mean curvature flow: An hard open problem consists in determining which Lagrangians in a Calabi-Yau admit a minimal Lagrangian (SLag) in their isotopy class and a complete answer would have a strong impact in Algebraic Geometry and Mirror Symmetry. A well known approach consists in deforming a given Lagrangian in the direction of its mean curvature and hope to show convergence to a SLag. The difficulty with this method is that finite-time singularities can occur.
I plan to study the regularity  theory for this flow and  show singularities have codimension two. This would be the ground stage to continue the flow past the singular time. My approach consists in classifying the possible parabolic blow-ups and find monotone quantities which will rule out non SLag blow-ups.","1100000","2012-01-01","2016-12-31"
"PSINFONI","Particle-Surface Interactions in Near Field Optics: Spin-orbit Effects of Light and Optical/Casimir Forces","Francisco José RODRÍGUEZ FORTUÑO","KING'S COLLEGE LONDON","PSINFONI aims to open new avenues of research in ultrafast routing and polarization synthesis of light in nanophotonics, and in the manipulation, sorting, levitation and trapping of nanoparticles, molecules, or single atoms near novel nanomaterial surfaces. To this end, we will explore the fundamentals and applications of a range of novel nanophotonic phenomena: (i) spin-orbit interactions of light for polarization-controlled optical routing and polarization synthesis at the nanoscale; (ii) repulsive and switchable lateral optical forces on particles near engineered surfaces for optical manipulation and sorting; and (iii) Casimir repulsive and lateral forces for quantum levitation / frictionless nanomaterials. All these diverse phenomena can be studied under the single framework of particle-surface interactions in the near field, greatly diversifying the research outcomes from a single research effort. Knowledge of the full 3D electromagnetic fields in particle-surface systems will form the foundation from which to explore fundamental aspects and limitations of the above mentioned effects, opening new applications in information technologies and new nanomaterials. Proof of principle experimental demonstrations will be performed where possible.","1427361","2017-03-01","2022-02-28"
"PSPC","Provable Security for Physical Cryptography","Krzysztof Pietrzak","INSTITUTE OF SCIENCE AND TECHNOLOGYAUSTRIA","""Modern cryptographic security definitions do not capture real world
adversaries who can attack the algorithm's physical implementation, as
they do not take into account so called side-channel attacks where
the adversary learns information about the internal state of the
cryptosystem during execution, for example by measuring the running
time or the power consumption of a smart-card.

Current research on side-channels security resembles a cat and mouse
game. New attacks are discovered, and then heuristic countermeasures
are proposed to prevent this particular new attacks.  This is
fundamentally different from the """"provable security"""" approach followed
by modern cryptography, where one requires that a cryptosystem is
proven secure against all adversaries in a broad and well-defined
attack scenario. Clearly, this situation is unsatisfactory: what is
provable security good for, if ultimately the security of a
cryptosystem hinges on some ad-hoc side-channel countermeasure?
Despite this, until recently the theory community did not give much
attention to this problem as it was believed that side-channels are a
practical problem, and theory can only be of limited use to prevent
them. But recently results indicate that this view is much too pessimistic.

On a high level, the goal of this project is to bring research on
side-channels from the realm of engineering and security research to
modern cryptography. One aspect of this proposal it to further
investigate the framework of leakage-resilience which adapts the
methodology of provable security to the physical world.  If a
cryptosystem is leakage-resilient, then this implies that its
implementation is secure against every side-channel attack, making
only some mild (basically minimal) assumptions on the underlying
hardware.""","1121206","2010-11-01","2015-10-31"
"PTCC","Phase transitions and computational complexity","Amin Coja-Oghlan","JOHANN WOLFGANG GOETHE-UNIVERSITATFRANKFURT AM MAIN","This is a project in the area of Theoretical Computer Science, 	particularly discrete algorithms and computational complexity. 	Many Constraint Satisfaction Problems (`CSPs') such as Boolean satisfiability or graph coloring are well-known to be NP-hard, i.e., the worst-case computation time to solve these problems is exponential in the size of the problem instance. To illuminate the conceptual origins of the computational hardness of these problems, a major research effort over the past 30 years has been the study of Random instances of CSPs. Over the past decade, motivated by problems in statistical mechanics,  physicists have developed stunningly detailed hypotheses on the structural and conceptual nature of random CSPs, based on ingenious but highly non-rigorous techniques. These hypotheses have led to a new class of Message Passing Algorithms, as well as to evidence that certain natural types of random CSPs may be computationally intractable. The goal of this project is to study these ideas rigorously and comprehensively from the perspective of the theory of computing.","1099575","2011-10-01","2016-09-30"
"PUNCA","Preparing for Unveiling the Nature of the Cosmic Acceleration","Baojiu Li","UNIVERSITY OF DURHAM","In less than a decade, various large cosmological surveys, such as Euclid, DESI and eROSITA, will start collecting data. These aim to bring ground-breaking changes to our understanding of the accelerated cosmic expansion - one of the grand challenges in physics today - by improving the precision in determining key cosmological parameters to percent level and testing the various theoretical models, such as dark energy and non-standard gravity. They will, for the first time, allow General Relativity to be tested to such precision beyond the local Universe. However, such exciting goals can only be achieved if the accuracy of theory predictions is greatly improved to match that of observations.

I propose to tackle this challenge by using state-of-the-art numerical techniques to study the leading theoretical models beyond standard LCDM in unprecedented accuracy, thereby preparing for their tests by 3 most promising cosmological probes - weak lensing, redshift space distortions and galaxy clusters. This numerically very demanding project is made possible by our recent developments of efficient simulation and analysis pipelines for each of these probes. I will build the PUNCA team which has a wide expertise to study constraints by these probes using accurate theoretical predictions from my planed simulations, and which will work closely to assess the power of novel joint constraints. To link model predictions to observations, and to understand critically their systematics, I will make realistic mock galaxy catalogs using simulations of unprecedented resolution and sophisticated galaxy formation models.

The results will have important implications for fully exploiting the potential of upcoming surveys (e.g. Euclid, DESI, of which I am an active member, and eROSITA) in testing models. The pipelines and expertise developed will be useful for analysing real data from those surveys. Given the starting times (2017-20) of the latter, this project (2016-21) is extremely timely.","1499952","2017-03-01","2022-02-28"
"PURPOSE","Opening a new route in solid mechanics: Printed protective structures","Jose Antonio RODRÍGUEZ-MARTÍNEZ","UNIVERSIDAD CARLOS III DE MADRID","Dynamic fragmentation of metals is typically addressed within a statistical framework in which material and geometric flaws limit the energy absorption capacity of protective structures. This project is devised to challenge this idea and establish a new framework which incorporates a deterministic component within the fragmentation mechanisms. 
In order to check the correctness of this new theory, I will develop a comprehensive experimental, analytical and numerical methodology to address 4 canonical fragmentation problems which respond to distinct geometric and loading conditions which make easily identifiable from a mechanical standpoint. For each canonical problem, I will investigate traditionally-machined and 3D-printed specimens manufactured with 4 different engineering metals widely used in aerospace and civilian-security applications. The goal is to elucidate whether at sufficiently high strain rates there may be a transition in the fragmentation mechanisms from defects–controlled to inertia–controlled. If the new statistical-deterministic framework is proven to be valid, defects may not play the major role in the fragmentation at high strain rates. This would bring down the entry barriers that the 3D-printing technology has found in energy absorption applications, thus reducing production transportation and repairing, energetic and economic costs of protective structures without impairing their energy absorption capacity.
It is anticipated that leading this cutting-edge research project will enable me to establish my own research team and help me to achieve career independence in the field of dynamic behaviour of ductile solids.","1497507","2018-03-01","2023-02-28"
"Q-CEOM","Quantum Cavity Electro- and Opto-Mechanics","Albert Schliesser","KOBENHAVNS UNIVERSITET","Nanomechanical oscillators have recently been realised in the quantum regime, by coupling them to a single mode of the electromagnetic field. Platforms using both superconducting microwave circuits and optical cavities have been employed—separately—for this purpose. Based on the PI's extensive contributions to these developments, we propose to explore the intriguing conceptual and experimental prospects of hybrid multimode systems involving microwave, mechanical and optical modes in the quantum regime, thus unifying the fields of quantum cavity optomechanics and electromechanics.
To reach this ambitious goal, an optomechanical system involving two optical modes and one mechanical mode will serve as testbed for quantum conversion and tripartite entanglement protocols. Particular attention will be devoted to the evasion of mechanical thermal noise through noise-resilient schemes, relying, for example, on mechanically dark Bogoliubov modes. This will enable the conservation of quantum coherence in spite of the inevitable coupling of the mechanical device to a thermal environment. The protocols, once established, will be transferred to a hybrid multimode system, consisting of a superconducting microwave resonator, a nanomechanical oscillator, and an optical cavity mode. In this system, we will explore unprecedented opportunities to transduce, entangle and amplify microwave and optical modes through a mechanical device.
The specific implementation proposed here opens new avenues for the ultralow-noise processing of microwave signals, with potential applications in radio astronomy or magnetic resonance imaging. In the quantum sciences, it bears great promise to overcome the dichotomy between superconducting circuit platforms for information processing, and flying optical photons for its communication. More generally, the schemes studied here can serve as a blueprint for mechanical transducers—coupling to spin, charge, and fields alike—in hybrid quantum systems.","1495073","2015-07-01","2020-06-30"
"Q-DIM-SIM","Quantum spin simulators in diamond","Nir BAR-GILL","THE HEBREW UNIVERSITY OF JERUSALEM","Quantum interacting systems are at the forefront of contemporary physics, and pose challenges to our understanding of quantum phases, many-body dynamics, and a variety of condensed matter phenomena. Advances in quantum applications, including quantum computation and metrology, rely on interactions to create entanglement and to improve sensitivity beyond the standard quantum limit. In recent years tremendous effort has been invested in developing precision experimental tools to study and simulate complicated many-body Hamiltonians. So far, such tools have been mostly realized in cold atomic systems, trapped ions and photonic networks. 

I propose a novel experimental approach using Nitrogen-Vacancy (NV) color centers in diamond, superconducting couplers, super-resolution addressing and cryogenic cooling, as a many-body quantum spin simulator. The NV center is a unique spin defect in a robust solid, with remarkable optical properties and a long electronic spin coherence lifetime (∼3 ms at room temperature). We have recently demonstrated that this coherence time can be extended to almost 1 second at low temperature, paving the way for interaction-dominated NV-based experiments. 

The goal of this project is to develop a paradigm of atom-like spin defects in the solid-state as a platform for studying elaborate quantum many-body spin physics (e.g. the Haldane phase in 2D) and quantum information systems (e.g. one-way quantum computing). I intend to combine a low temperature environment with a novel optical super-resolution system and nanofabricated superconducting structures on the diamond surface to produce a unique experimental setup capable of achieving this goal. The ability to engineer and control interacting NV systems in the solid-state diamond lattice has far-reaching applications for studying fundamental problems in many-body physics and in quantum information science.","1500000","2017-01-01","2021-12-31"
"Q-PHOTONICS","Quantum fluids of photons in optically-induced structures","Ofer Firstenberg","WEIZMANN INSTITUTE OF SCIENCE LTD","A variety of classical optical systems exhibiting rich and complex matter-like behavior have been explored in recent years. Unfortunately in the optical regime, photons – the fundamental constituents of light – do not interact strongly with one another, and therefore cannot be used for studying many-body effects. It is only in the extreme regime of quantum nonlinear optics where effective interactions between photons are made strong. In an atomic gas, strong long-range interactions can be achieved by coupling photons to interacting atoms. First experiments have indicated the formation of a two-photon bound state via this mechanism. The main goal of the proposed research is to develop an optical system based on atomic interactions that realizes quantum many-body physics with optical photons subject to a rich variety of model problems.

The proposed method relies on reconfigurable, optically-induced, three-dimensional, structures, which are fully compatible with the underlying atomic process. These structures enable the spatial compression of photons for enhancing the interactions, wave guiding for one-dimensional confinement in long media, and a rich variety of two-dimensional potentials with tunable interactions, from nearly-free photons to various tight-binding models with a controllable level of disorder. Optically-induced structures also offer advantages to optical quantum information, enabling better gate fidelities due to stronger nonlinearities and multimode coupling for processes such as photon routing.

Our method has the potential to realize quantum gases and fluids of interacting photons. We can manipulate the effective mass and the band structure, control the potential landscape, and tune the scattering length in the system from attractive to repulsive. In particular, we intend to study few-photon bound states, quantum solitons, Luttinger liquids of photons, and Wigner crystallization in one and two dimensions.","1500000","2016-06-01","2021-05-31"
"Q-ROOT","Quantum optomechanics at ROOm Temperature","Pierre Thibaud Julien Verlot","UNIVERSITE LYON 1 CLAUDE BERNARD","5 years ago, the field of optomechanics has entered the quantum regime. By doing so, this domain which investigates the reciprocal interactions between light and mechanical motion has overcome the long-standing paradox of Quantum Mechanical effects at the macroscopic scale. Such outstanding achievement relies on the so-called “cavity nano-optomechanical” technology, which combines strongly reduced dimensions with ultra-high optical confinement, enabling very large optomechanical coupling rates at the nanoscale.
In a more fundamental perspective, decreasing the size of optomechanical systems has enabled minimizing the detrimental effects of decoherence, resulting in a quasi-instantaneous collapse of quantum coherence at a macroscopic scale. At present, optomechanical systems seem to have reached their limits at cryogenic temperatures and remain overly sensitive to decoherence at room temperature to display any quantum behaviour. 
The project Q-ROOT proposes a novel cavity optomechanical approach showing such unprecedentedly large coupling rates that it will operate in the quantum regime at room temperature for the first time. Our concept relies on tethering a low-loss nano-optical scatterer at the edge of the lightest possible mechanical device that is a carbon nanotube resonator. This system is expected to outperform the state-of-the-art (including atom–based systems) by orders of magnitude, even at room temperature. Amongst objectives, Q-ROOT notably plans to demonstrate ground-state cooling, strong ponderomotive squeezing, the standard quantum limit, quantum non-demolition of mechanical Fock states, and optomechanical photon blockade at room temperature. Besides very fundamental impact, the unique sensing abilities of the system developed in Q-ROOT will be further utilized in order to perform quantum limited sensing applications at room temperature, paving a generalized use of optomechanics for quantum sensing and information technology at room temperature.","1500000","2018-09-01","2023-08-31"
"Q-SENS2","Quantum-Enhanced Sensors with Single Spins","Paola Cappellaro","LABORATORIO EUROPEO DI SPETTROSCOPIE NON LINEARI","Precision measurements are among the most important applications of quantum physics. Concepts derived from quantum information science have been explored to enhance precision measurements, as entangled states have been recognized to potentially provide sensitivity beyond the classical limit. Recent advances have also enabled the development of new types of controlled quantum systems for the realizations of solid-state qubits. Their use as quantum sensors will enable new capabilities, such as an unprecedented combination of sensitivity and spatial resolution.
Unfortunately, progress towards real-world applications of quantum sensors is currently limited by the fragile nature of quantum superposition states and difficulties in preparation, control and readout of useful quantum states. Q-SEnS2 aims at overcoming these fundamental challenges by developing novel paradigms for quantum enhanced metrology and sensing in three key areas:
1. Entanglement: We will explore novel classes of entangled states that promise to be more easily created and robust against decoherence.
2. Control: We will develop quantum control to enhance device sensitivity to the signal, attain spectral signal resolution, and achieve increased noise immunity of the sensor.
3. Readout: We will investigate quantum enhanced readout techniques to increase measurement efficiency and to reach sensor performance near the Heisenberg limit.
These concepts will be implemented in an experimental platform centered on the Nitrogen- Vacancy (NV) center. The NV center electronic spin can be individually addressed, exploiting optical techniques for polarization and readout and magnetic resonance for its precise manipulation. Thanks to its excellent coherence properties, the NV center has emerged as a remarkable qubit candidate and as a versatile sensor. In Q-SEnS2 we will study NV-based magnetometry, which has the potential to be a transformative technology in fields ranging from medical imaging to materials science","1500000","2014-03-01","2019-02-28"
"QAPPA","Quantifying the atmospheric implications of the solid phase and phase transitions of secondary organic aerosols","Annele Kirsi Katriina Virtanen","ITA-SUOMEN YLIOPISTO","In our ground-breaking paper published in Nature we showed, that the atmospheric Secondary Organic Aerosol (SOA) particles formed in boreal forest can be amorphous solid in their physical phase. Our result has already re-directed the SOA related research. In the several follow-up studies, it has been shown that SOA particles generated in the laboratory chamber from different pre-cursors and in various conditions are amorphous solid.

My ultimate task is to quantify the atmospheric implications of the phase state of SOA particles. Solid phase of the particles implies surface-confined chemistry and kinetic vapour uptake limitations because mass transport (diffusion) of reactants within the aerosol particle bulk becomes the rate limiting step. The diffusivity of the molecules in particle bulk depends on the viscosity of the SOA material. Hence, it would be a scientific break-through, if the kinetic limitations or the viscosity of the SOA particles could be estimated since these factors are a key to quantify the atmospheric implications of amorphous solid phase of the particles.

To achieve the final goal of the research, measurement method development is needed as currently there is no method to quantify the viscosity of the SOA particles, or to study the kinetic limitations and surface-confined chemistry caused by the solid phase of nanometer sized SOA particles. The methodology that will be developed in the proposed study, aims ambitiously to quantify the essential factors affecting the atmospheric processes of the SOA particles. The developed methodology will be use in extensive measurement campaigns performed both in SOA chambers and in atmospheric measurement sites in Europe and in US maximising the global significance of the results gained in this study.

The project enables two scientific breakthroughs: 1) the new methodology applicable in the field of nanoparticle research and 2) the quantified atmospheric implications of the amorphous solid phase of particles.","1499612","2014-02-01","2019-01-31"
"QC&C","Quantum fields and Curvature--Novel Constructive Approach via Operator Product Expansion","Stefan Hollands","UNIVERSITAET LEIPZIG","It was realized already from the beginning that the theory of quantized fields (QFT) does not easily fit into known mathematical structures, and the quest for a satisfactory mathematical foundation continues to-day. Parts of this theory have already been tremendously successful, e.g. in the quantitative description of elementary particles, and ideas from QFT have revolutionized entire fields of mathematics. But the non-perturbative construction of the most important QFT s, namely renormalizable theories in 4d, remains unsolved. The aim of this project is to make a substantial contribution to this quest for the mathematical construction of  such QFT s (on curved manifolds), and the exploration of their mathematical structure. We want to pursue a novel ansatz to achieve this goal. The essence of our novel approach is to focus attention on the algebraic backbone of the theory, which manifests itself in  the so-called  operator-product-expansion.  The study of such algebraic structures related to operator products has already been tremendously useful in the study of conformal field theories in low dimensions, but we here propose that a suitable version of it also has great potential to be used as a constructive tool for the much more complicated quantum gauge theories in four dimensions. It is not expected that an explicit solution can be obtained for such models-especially so in curved space-but the idea is instead to analyze powerful consistency conditions on the quantum field theory arising from the OPE ( associativity conditions ) and to use them to prove that the theory exists in a mathematically rigorous sense. Our approach will be complemented by other powerful and deep mathematical tools that have been developed over the past decades, such as the sophisticated non-perturbative expansions uncovered in the school of  constructive quantum fields theory , Hochschild cohomology, RG-flow equation techniques, microlocal analysis, curvature expansions, and many more.","818099","2011-04-01","2016-03-31"
"QCC","Quantum Communication and Cryptography","Iordanis Kerenidis","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""Quantum Information Processing has the potential to revolutionize the future of information technologies. My long-term vision is a network of quantum and classical devices, where individual agents have the ability to communicate efficiently in a variety of ways with trusted and untrusted parties and securely delegate computational tasks to a number of untrusted large-scale quantum computing servers. In such an interconnected world, the notion of security against malicious adversaries is an imperative. In addition, the interaction between agents must remain efficient and it is important to provide the agents with incentives for honest behaviour. The realization of such a complex network of classical and quantum communication must rely on a solid theoretical foundation that nevertheless is able to foresee and handle the intricacies of real-life implementations.

The targeted breakthrough of this proposal is to set the benchmark, both theoretically as well as experimentally, of some necessary communication components for such a hybrid network. The concrete objectives of our project are to: design novel cryptographic primitives as a powerful quantum mechanical toolkit for the quantum network infrastructure; study quantum communication and games in order to minimize the communication overhead and guarantee the honest behaviour of rational agents; enhance the understanding of the fundamentals of quantum mechanics and of classical complexity theory through the lens of quantum information and cryptography; study the security of cryptographic primitives in realistic conditions and use state-of-the-art photonic systems to implement complex cryptographic primitives.

An ERC Starting Grant will enable me to reach the above objectives through the creation and coordination of an independent quantum cryptography group that will raise the level of competence and competitiveness of the EU and will provide methods and applications essential for the future of information technology.""","980640","2013-05-01","2018-04-30"
"QCDforfuture","QCD for the Future of Particle Physics","Jennifer SMILLIE","THE UNIVERSITY OF EDINBURGH","The momentous discovery of the Higgs boson in 2012 marked the start of a new era in particle physics.  The increase in energy of collisions at the Large Hadron Collider (LHC) this year allows us to probe fundamental physics at an energy
scale which has been out of reach until now.  This presents a challenge to particle theory to keep pace with these developments, and respond to the fact that Standard Model interactions will have different features in this new energy range.  We must understand these differences in order to extract as much information as possible from LHC data, and in particular to identify any signs of new physics.  My framework, High Energy Jets, is the only tool of its kind to include the dominant high-energy corrections to all orders in the strong coupling and these have already been shown to be necessary to describe data at the lower collisions energies of 7 and 8 TeV.  However, these corrections alone are not enough.

My proposed research programme will develop a novel and powerful framework for theoretical predictions based on the lessons learned from LHC Run I.  In particular it will combine the necessary high-energy corrections with state-of-the-art next-to-leading-order (NLO) fixed-order descriptions. A separate objective is to combine the high-energy corrections with the resummation contained in parton shower programs.  This is necessary to describe data in regions where there is both evolution in rapidity and transverse momentum.  The ultimate goal is to combine all three: high-energy corrections, NLO calculation and parton shower.  Separate theoretical objectives will significantly improve our understanding of the underlying theory, which should ultimately enhance our description of data far beyond any current prediction.  This will be the most complete description of quantum chromodynamics at colliders to date, and will be essential for the exploitation of future data from the LHC and beyond.","1438003","2017-01-01","2022-11-30"
"QCDTHERMO","QCD thermodynamics on the lattice","Sándor Katz","EOTVOS LORAND TUDOMANYEGYETEM","Quantum Chromodynamics (QCD) at finite temperature and non-zero density describes phenomena relevant to the early universe and heavy-ion collisions. The applicability of perturbation theory is limited to large temperatures and densities. We plan to use lattice simulations to study QCD thermodynamics. There are different regularizations of QCD on the lattice. The computationally most effective one is the staggered formulation, while Wilson or chiral fermions are theoretically more established. We have to distinguish studies at vanishing baryon densities from the ones concerning non-zero density. At vanishing densities the order of the QCD transition between the hadronic phase and the quark-gluon plasma was studied using staggered fermions. In the physical, continuum limit the transition was found to be a crossover. The transition temperature has also been determined. These studies should be and will be extended using Wilson and chiral fermions. This way the staggered results can be checked. At non-vanishing densities direct lattice simulations are prohibited by the infamous sign problem. Recently the multi-parameter reweighting method was developed to study moderate densities using simulations at zero baryon density. The phase diagram as well as the critical point of QCD was determined using staggered fermions with a single lattice resolution. We plan to extend these studies in two ways. In the first step finer lattices will be studied with staggered fermions and a continuum extrapolation will be attempted. In the second step Wilson and possibly chiral fermions will be used. At large densities where the sign problem is the most severe the density of states method will be used. Based on our experience with PC clusters we will build a new, high performance cluster to achieve these goals. The establishment of a strong new research group certainly will improve the competitivity of the European lattice community.","1300000","2008-07-01","2014-03-31"
"QD-CQED","A quantum dot in a cavity:  A solid state platform for quantum operations","Pascale Francoise Senellart","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""A quantum dot (QD) in a microcavity is an ideal single spin-single photon interface: the spin of a carrier trapped inside a QD can be used as a quantum bit and the coupling to photons can allow remote spin entanglement. A QD in a cavity can also generate single photons or entangled photon pairs, often referred to as flying quantum bit. Controlling the QD spontaneous emission is crucial to ensure optimal coupling of the photon and spin states. The present project relies on a unique and original technology we have developed which allows us to deterministically control the QD-cavity system. With this technique, we can fabricate a large number of identical coupled QD-cavity devices operating either in the weak or strong coupling regime. The potential of the technique has been proven by the fabrication of the brightest source of entangled photon pairs to date (Nature 2010).
The objective of the present project is to build up a platform for basic quantum operations using QDs in cavities. The first aim is to develop highly efficient light emitting devices emitting indistinguishable single photons and entangled photon pairs. The mechanisms leading to quantum decoherence in QD based sources will be investigated. We will also explore a new generation of devices where QDs are coupled to plasmonic nano-antenna. The second objective is to implement basic quantum operations ranging from entanglement purification  to quantum teleportation using QD based sources. The third objective of the project is to control the spin-photon interface. We first aim at demonstrating quantum non-demolition spin measurement through highly sensitive off-resonant Faraday rotation. We then aim at entangling two spins separated by macroscopic distances, using their controlled interaction with photons. This will be obtained either by making a single photon interact with two spin in cavities or by interfering indistinguishable photons emitted by two independent charged QDs.""","1482000","2011-11-01","2016-10-31"
"QD-NOMS","Elementary quantum dot networks enabled by on-chip nano-optomechanical systems","Fei DING","GOTTFRIED WILHELM LEIBNIZ UNIVERSITAET HANNOVER","Is there any limit to the size of a quantum system? How large and how small can it be? Both questions are related to scalability, a most critical issue in quantum technologies. A scalable quantum network, which can be extended almost infinitely, is built by entangling individual quantum systems, e.g. atoms. It will provide thrilling opportunities across a range of intellectual and technical frontiers in quantum information science. Building such a network is however a great challenge, in both physics and engineering.

Often referred to as artificial atoms, semiconductor quantum dots (QDs) are among the most promising single and entangled photon sources to build a photonic quantum network. However there is a longstanding and yet unsolved challenge on scalability, since, unlike real atoms, every QD is different. By engineering individual QDs with an innovative nano-optomechanical system (NOMS), elementary QD networks will be built via scalable interactions of single or entangled photons, in a fashion similar to that of real atoms.
 
The scientific goals are to upscale QD networks with the first demonstrations of (1) indistinguishable entangled photons from different QDs, (2) deterministic entanglement swapping, purification and graph states with multiple QDs (3) deterministic Boson sampling with more than 4 QDs on chip.
 
The technological goals are (1) to downscale the footprint (<50 µm) of individual QD sources with full tunabilities, and to realize (2) arrays (>4×4) of tunable single and entangled photon sources, (3) waveguide integration on III-V/silicon chips, and (4) compact quantum LED demonstrators.
 
QD-NOMS will address the physical and technological challenges in building a solid-state QD-based quantum network. Its success does not only provide a novel toolkit to realize scalable QD systems for cutting-edge fundamental researches but also brings the semiconductor QD based platforms, after a decade of development, to the attention of practical applications.","1774693","2017-01-01","2021-12-31"
"QFTCMPS","Quantum field theory, the variational principle, and continuous matrix product states","Tobias Osborne","GOTTFRIED WILHELM LEIBNIZ UNIVERSITAET HANNOVER","Quantum field theories, quantum systems with an infinite number of degrees of freedom, present the most subtle and complex systems in physics. At the same time, the study of quantum field theory has provided us with some of the most powerful theoretical tools to study many particle quantum systems. Many of the major insights in this theory have been obtained using the powerful technology of perturbation theory, whereas a main source of nonperturbative results has been lattice gauge theory. The variational principle has not met with as much systematic success in explaining these systems due to the dearth of good variational wavefunctions.

In condensed matter physics we have recently witnessed tremendous progress, spurred by developments in quantum information theory, in understanding the properties of physical states of strongly interacting many particle quantum systems. This has culminated in the realisation that the physics of low-dimensional systems is well captured by variational classes known as matrix product states, projected entangled-pair states, and the multiscale entanglement renormalisation ansatz. Very recently a continuum generalisation of the matrix product state variational class has been developed which promises to afford, via the variational principle, new insights into the behaviour of strongly interacting quantum field theory.

The purpose of this proposal is to: (a) understand how to use the variational principle to simulate the dynamics, both imaginary and real-time, of (1+1)- and higher-dimensional quantum field theories within the continuum matrix product state variational class; (b) extend the recent theoretical advances in the field of locally interacting quantum spin systems to study the correlation structure of equilibrium and non-equilibrium quantum fields; and (c) relate the developed formalism to cavity QED and hence develop experimental proposals to simulate strongly interacting quantum field theories with cavity QED systems.","1343219","2011-08-01","2016-07-31"
"QGP","Characterisation of a novel state of matter: The Quark-Gluon Plasma","Andre Mischke","UNIVERSITEIT UTRECHT","I propose to explore the properties of a novel state of matter, the Quark-Gluon Plasma (QGP), created by colliding atomic nuclei at the highest energy ever reached using triggered particle correlations. The QGP is predicted by the fundamental theory of strong interactions and is characterized by an equilibrated system of free quarks and gluons that are the constituents of atomic nuclei. My investigation of the QGP properties will give unique insights into the development of the early universe and the properties of matter under extreme conditions. Among other results, particle correlation measurements have revealed first compelling evidence for the existence of the QGP state. Due to the limited sensitivity of the used probes, the conclusions are to some extent qualitative rather than quantitative. To get a deeper understanding of the mechanisms at work I propose to study heavy-quark correlations and their in-medium modification in collisions of heavy nuclei by combining the information from different detection systems. I have verified the feasibility of this measurement at lower energies. I am currently one of the world’s experts in measuring heavy-quark correlations and I propose to perform such a measurement at the forefront particle accelerator, the Large Hadron Collider, located at the European Laboratory for Particle Physics CERN. My investigation will be done utilizing the dedicated ALICE (A Large Ion Collider Experiment) detector, which is most suited for measurements in heavy-ion collisions. I would like to do my project with one Postdoc and one Ph.D. student during a period of five years. My research team will be embedded in one of the leading institutes in the field of heavy-ion physics which provided a crucial hardware component to the ALICE experiment. My expertise and the outstanding working environment will guarantee high quality in performing my key measurement. The ALICE experiment will be the place of new discoveries.","850000","2008-09-01","2013-08-31"
"QGP-MYSTERY","Demystifying the Quark-Gluon Plasma","Ante BILANDZIC","TECHNISCHE UNIVERSITAET MUENCHEN","The primary objective of this project is to explore the properties of a new state of matter, the Quark-Gluon Plasma (QGP), at the highest energies to date. The QGP consists of asymptotically free quarks and gluons and the evidence for its discovery was announced in experiments analyzing collisions of heavy ions. Previous experiments at lower energies have caused a dramatic change of paradigm in our understanding - measurements at Relativistic Heavy Ion Collider showed that the QGP behaves like a strongly coupled liquid, instead of the weakly interacting gas expected by theorists. The focus of this project is the measurement of the anisotropic flow, which has proven to be the most informative probe for studying the properties of nuclear matter produced in heavy-ion collisions. The analysis methods to be utilized are multiparticle correlation techniques developed by the PI. Currently mainly the first moments (averages) of multiparticle correlations are used. Within the scope of the theoretical subproject we aim to derive analytic expressions for higher order moments of multiparticle correlations by continuing a pioneering work recently initiated by the PI. The here proposed project aims also the measurement of new flow observables, so-called Symmetric Cumulants, recently introduced in the field by PI. The recent restart of Large Hadron Collider at top energy, a record breaking 5.02 TeV center of mass energy, offers a truly unique and a timely opportunity for this project. The main dataset will comprise the heavy-ion collisions collected with ALICE detector at the Large Hadron Collider. Collisions of smaller systems will be scrutinized as well in order to determine the onset of QGP formation. This project offers a unique opportunity to pin down quantitatively the properties of the QGP, beyond the rather qualitative analyses that are currently carried out in the field. It also will impact the fields of high energy physics, nuclear physics, cosmology and hydrodynamics.","1366875","2018-01-01","2022-12-31"
"QGPDYN","Dynamics of the Quark-Gluon Plasma: A Journey into new phases of the Strong Interaction","Vincenzo Greco","UNIVERSITA DEGLI STUDI DI CATANIA","The study of the fundamental theory of the strong interaction, Quantum Chromo Dynamics (QCD), under extreme conditions of temperature and density has captured an increasing interest due to its very rich dynamical content and its relation to the Early Universe physics. Heavy-Ion Collisions (HIC) at ultra-relativistic energy (above ~10 AGeV) provide the possibility to scrutiny the QCD phase diagram reaching energy densities high enough to cause a phase transition of the hadronic matter into a quark-gluon plasma (QGP). The experiments conducted at the Brookhaven National Laboratory have shown that such a state of matter behaves like a nearly  perfect fluid  with a very small shear viscosity, a very high opacity at high transverse momentum pT (>6 GeV) and manifesting evidences for a modification of the hadronization process at intermediate pT (2-6 GeV) . Interestingly and surprisingly enough, also in the heavy quark sector there are hints that viscosity is similarly small. Furthermore some evidence of a new phase of QCD, the color glass condensate (CGC), has been found while its relevance should increase in the upcoming LHC program at CERN. These are the main physics subjects that we deal with in our project.
The main objective of the project is to determine the value of the shear viscosity to entropy ratio h/s of the  perfect fluid   and the implications of a possible primordial CGC phase on the dynamical evolution of the system. The research gains strength from a comprehensive study of the rich phenomenology including both inclusive and more exclusive observables (like the two-three particles correlations triggered by hadronic jets) and from the inclusion of hadronization effects. In addition the study will be conducted considering also the heavy flavor sector where a fluid behaviour similar to the light one is observed. The microscopic origin of such a similarity will be investigated together with its relation to the suppression/regeneration of quarkonia.","655000","2011-05-01","2017-01-31"
"QHC","Quantum Hamiltonian Complexity","Dorit Aharonov","THE HEBREW UNIVERSITY OF JERUSALEM","Quantum computation suggests a revolution in technology and in cryptography, a completely new perspective on the foundations of theoretical computer science, and a different approach to the study of physical systems.

One of the major new developments in quantum computation over the last few years has been the emergence of a new field called ``Quantum Hamiltonian complexity (QHC)'', which sits on the boundary between computational complexity theory and condensed matter physics. This direction investigates computational aspects of physical objects such as ground states and Hamiltonians, using techniques from both physics and theoretical computer science. This direction has already had
an immense impact on both quantum computation and condensed matter physics.

This project aims not only to investigate  fundamental questions in quantum
Hamiltonian complexity as it exists today, such as quantum states generation, tensor network descriptions of quantum states, area laws, and the complexity of Hamiltonians, but also to greatly broaden the scope of this new paradigm, into the study of quantum PCP; into new frontiers in quantum algorithms such as quantum walks, adiabatic algorithms and topology and tensor networks related algorithms; as well as into the study of quantum protocols such as coin flipping, quantum interactive proofs and quantum cryptography and their implications on our understanding of quantum entanglement.","1499900","2012-03-01","2017-11-30"
"QINTERNET","Quantum communication networks","Stephanie Wehner","TECHNISCHE UNIVERSITEIT DELFT","My goal is to overcome the two-most pressing theoretical challenges necessary to build large-scale quantum communication networks: routing and designing protocols that use them to solve useful tasks. In two interconnected projects, I will devise entirely new concepts, models and mathematical methods that take into account the intricacies of real world quantum devices that can operate on only very few quantum bits at a time.

(1) Security: I will prove the security of quantum cryptographic protocols under realistic conditions, and implement them in collaboration with experimentalists. I will develop a general theory and practical tests for the security of multi-party cryptographic primitives using untrusted quantum devices. This is mathematically challenging due to the possibility of entanglement between the devices.

(2) Routing: I will initiate the systematic study of effective routing in a quantum communication network. This is necessary for quantum networks to grow in scale. Quantum entanglement offers very different means of routing messages than is possible in classical networks, and poses genuinely new challenges to computer science. I will design routing protocols in a multi-node quantum network of potentially different physical implementations, i.e., hybrid networks, that will establish a new line of research in my field. 

Quantum networks are still in their infancy, even though quantum communication offers unparalleled advantages that are provably impossible using classical communication. Building a quantum network is an interdisciplinary effort bringing together computer science, physics, and engineering. I am in a unique position in computer science, since I have recently joined QuTech where I have direct access to small quantum devices - bringing me tantalizingly close to seeing such networks realized. As with early classical networks, it is difficult to predict where our journey will end, but my research will join theory and experiment to move forward.","1498725","2016-03-01","2021-02-28"
"QIOS","Quantum Interfaces and Open Systems","Anders Sørensen","KOBENHAVNS UNIVERSITET","""Researchers have strived to obtain control of a variety of different quantum systems, each characterized by their own distinct advantages: quantum optical systems offer excellent isolation from the environment while solid state systems allow for integrated micro-fabricated devices. At the same time nuclear spins in molecules can remain decoupled from the environment even under rather harsh conditions, and this is the basis of NMR experiments. Given these distinct advantages it is very fruitful to investigate hybrid devices merging the advantages of each of the systems. To do this it is essential to develop quantum interfaces to connect the different systems. By their very nature such quantum interfaces exchange information with their environment and are therefore open quantum systems.

In this project I wish to establish a strong theoretical quantum optics group which can guide and inspire the experiments towards breaking new grounds for open quantum systems and making quantum interfaces between distinct physical systems. The objective is to develop concrete proposals for how to experimentally control and exploit the interaction of quantum systems with their surroundings and for how this can be used for quantum interfaces.

The work in this project is particularly relevant for applications in quantum information processing, where the current challenge is to take the field from proof-of-principle demonstrations to truly scalable devices. Such challenge demands new interdisciplinary theoretical ideas for hybrid devices. This proposal addresses several key challenges for quantum information processing: scalable multimode quantum repeaters based on hybrid approaches, entanglement enabled quantum metrology, photonic engineering based on surface plasmons, dissipative preparation of entangled states, and phonon engineering for quantum dots. In addition applications towards nuclear spin cooling to improve NMR experiments as well as ultra cold atoms will be explored.""","1431542","2012-10-01","2017-09-30"
"QIP","Towards a Quantitative Theory of Integer Programming","Daniel DADUSH","STICHTING NEDERLANDSE WETENSCHAPPELIJK ONDERZOEK INSTITUTEN","Integer programming (IP), i.e. linear optimization
with integrality constraints on variables, is one of the most successful
methods for solving large scale optimization problems in practice.  While many
of the base IP problems such as the traveling salesman problem (TSP) or
satisfiability (SAT) are NP-Complete, IPs with tens of thousands of variables are
routinely solved in just a few hours by current state of the art IP solvers. 

The main goal of this proposal is to develop a quantitative theory capable of
explaining when and how well different IP solver techniques will work on a wide
range of instances. Here we will study many of the principal tools used to solve
IPs including branch & bound, the simplex method, cutting planes and rounding
heuristics. Our first direction of study will be to develop parametrized classes
of instances, inspired by the structure of realistic models, on which branch &
bound and the simplex method are provably efficient. The second research
direction will be to develop alternatives to ad hoc rounding heuristics and
cutting plane selection strategies with provable guarantees and provide their
applications to important classes of IPs. Lastly, we will explore the power and
limitations of IP techniques in the context of algorithm design by comparing
them to powerful techniques in theoretical computer science and analyzing their
worst-case performance for solving general integer programs.  While the main
thrust of this proposal is theoretical, it will be complimented by an
experimental component performed in collaboration with well-known experts in
computational IP, both to gain valuable insights on the structure of real-world
instances and to validate the effectiveness newly suggested approaches. The
proposed research is designed to make breakthroughs in our quantitative
understanding of IP techniques, many of which have long resisted theoretical
analysis.","1500000","2019-01-01","2023-12-31"
"QLEDS","Quantum Logic Enabled test of Discrete Symmetries","Christian Ospelkaus","GOTTFRIED WILHELM LEIBNIZ UNIVERSITAET HANNOVER","""This proposal aims to apply ion-trap quantum logic techniques to precision measurements on individual (anti-)protons for fundamental physics tests. In particular, we aim to measure g-factors of single (anti-)protons as a precise test of CPT symmetry. This requires a method to detect single (anti-)proton spin flips. Current efforts based on classical “magnetic bottle” techniques are hurt by the extreme difficulty and slowness of the spin state detection. Discrete and direct state measurement is a prerequisite for inaccuracies below 10-6 and has not been achieved yet.
Towards this end, we will employ a radically different approach and use quantum logic techniques developed by the PI in the NIST ion storage group of D. J. Wineland [Nature 476, 181(2011); Nature 471, 196(2011)]. This will allow us to transfer the (anti-)proton’s spin state to a nearby trapped atomic “logic” ion and subsequently read it out using standard quantum logic detection techniques along the lines of Heinzen and Wineland [PRA 42, 2977; J. Res. NIST 103, 259 (1998)]. The same ideas are also at the root of NIST’s world-record single-ion Al+ frequency standard.
Ultimately, this quantum logic technique will lead to a precise test of CPT symmetry, a fundamental symmetry within the standard model of particle physics, by comparing the proton’s and the antiproton’s g-factor with fast detection and single spin-flip resolution. It thus has the potential to reach inaccuracies below 10-9, exceeding the state-of-the-art for the antiproton g-factor by six orders of magnitude. Such a measurement is urgently needed to complement ongoing tests with electrons and positrons. It is closely intertwined with our desire to understand the observed matter-antimatter imbalance in the universe and to obtain a unified description of matter and interactions. Further, the project will considerably broaden the arsenal of quantum state manipulation techniques in Penning traps and possibly impact high precision mass measurement.""","1619640","2013-09-01","2018-08-31"
"QMULT","Multipartite Quantum Information Theory","Matthias Christandl","KOBENHAVNS UNIVERSITET","Quantum information theory studies the way information is stored, transmitted and processed in quantum devices. Mathematically, quantum information theory extends Shannon's theory of information but differs from it by allowing both for stronger correlations known as entanglement and for non-commutative effects resulting in measurement uncertainty as required by the laws of quantum physics. Entanglement has been shown to be crucial for the advantages offered by quantum communication and computation.

In recent years, researchers have gained a good understanding of quantum information theory involving two parties, for instance in the transmission of quantum bits from a sender to a receiver. Yet the study of quantum protocols for communication tasks involving multiple parties, for instance the joint counting of online votes or the compression of data distributed in a network, is still in its infancy. The reason for this is two-fold: (i) a lack of understanding of entanglement among multiple particles and (ii) the non-commutative nature of quantum theory, two facts that pose major difficulties for the design of multiparty quantum coding schemes.

It is the goal of this research project to overcome these two main obstacles so that a comprehensive theory of quantum information can be developed. Just as the Internet, where a network of many interacting computers has replaced point-to-point communication channels such as phone lines, the future of quantum communication will involve communication among many parties. The multipartite quantum information theory explored in this project is therefore expected to impact not only current experiments but also our future communication infrastructure.","1389581","2013-09-01","2019-05-31"
"QNets","Open Quantum Neural Networks: from Fundamental Concepts to Implementations with Atoms and Photons","Markus MUELLER","SWANSEA UNIVERSITY","Reaching a fundamental understanding of quantum many-body systems and fully harnessing their computational power for information processing is one of today’s greatest scientific challenges. To date, unprecedented research efforts are underway to build quantum devices, which would outperform the most powerful classical computers. At the same time, neural networks are currently revolutionising the handling of large amounts of data, with enormous success in pattern and speech recognition, machine learning, the analysis of ‘big data’ and ‘deep learning’. Driven by the hope of combining massive parallel information processing in neural networks with quantum advantages like computational speedup, there have been various efforts to develop quantum neural networks – without satisfactory answers to date. The overarching goal of this theoretical research programme is to tackle this enormous challenge from a fresh perspective: we will establish and explore a conceptual framework for quantum neural networks and identify quantum optical physical building blocks, based on concepts in the domain of open many-body quantum systems. This ambitious aim will be achieved by interlinking a multitude of scientific areas ranging from atomic physics, quantum optics, quantum engineering and condensed matter physics to quantum information and computer science. This research will not only generate a genuine step change in our fundamental understanding of the ways nature allows for quantum information processing. It will also lay the foundation for quantum neuromorphic engineering of a new generation of quantum neural hardware in state-of-the-art and newly emerging experimental systems of ultra-cold atoms and trapped ions. With my interdisciplinary background in quantum information and quantum engineering, quantum optics and atomic physics, I am in a unique position to successfully realise this research. I will also strongly benefit from the vital scientific environment at Swansea University.","1486439","2019-06-01","2024-05-31"
"QOM","Quantum Optomechanics: quantum foundations and quantum information on the micro- and nanoscale","Markus Aspelmeyer","UNIVERSITAT WIEN","Quantum states of mechanical resonators promise access to completely new experimental regimes of physics: from unprecedented levels of force sensitivity to the generation of macroscopic quantum superpositions of massive objects containing up to 10^20 atoms. This opens up not only exciting possibilities for novel applications but also allows to (re)address fundamental questions of quantum physics, in particular its relation to the classical world. For this reason the preparation and control of mechanical quantum states has long been an enticing but far fetched goal of breakthrough character. With the advent of micro- and nano-mechanics this goal is at the verge of becoming an experimental reality. The last few years have witnessed unprecedented global progress in pushing mechanical systems towards the quantum regime. A thriving interdisciplinary field has emerged that aims to exploit the tremendous potential that lies in the control of mechanical quantum states. The main idea of this proposal is to combine the tools and concepts of quantum optics with micro- and nano-mechanical systems. Such combination provides a unique and powerful approach that allows, with a minimal set of experimental interactions, universal quantum control over mechanical systems via opto-mechanical interactions. The feasibility of the approach has recently been verified by us and by several other groups worldwide in a series of experimental demonstrations of mechanical laser cooling. The main objective of the proposed research is to go significantly beyond the current state-of-the-art and to develop the field of quantum-opto-mechanics to its full extent, both in experiment and theory. This will also increase the European visibility in this highly topical area of research. My professional background in both solid-state physics and quantum optics and quantum information will be of additional help in this highly interdisciplinary endeavour.","1670904","2009-11-01","2014-10-31"
"QPE","Quantum Photonic Engineering","Mark Thompson","UNIVERSITY OF BRISTOL","By harnessing the unique properties of quantum mechanics (superposition and entanglement) to encode, transmit and process information, quantum information science offers significant opportunities to revolutionise information and communication technologies. 

The far-reaching goal of this project is to build quantum technology demonstrators that can outperform conventional technologies in communications and computation. 

For quantum information technologies (QITs) to have as big an impact on society as anticipated, a practical and scalable approach is needed. One promising approach to QITs is the photonics implementation, where single particles of light (photons) are used to encode, transmit and process quantum information – in the form of photonic quantum-bits (qubits). Currently, state-of-the-art experiments are limited to the “few-photon” regime, occupying many metres of space on an optical table, constructed from bulk optical elements, with no routes to scalability and far from outperforming conventional technologies. 

Integrated quantum photonics has recently emerged as a new approach to address these challenges. This research programme will take an engineering approach to QITs and draw upon rapidly growing field of silicon photonics. We will develop a silicon-based quantum technology platform where single-photon sources, circuits and detectors will be integrated into miniature microchip circuits containing thousands of discrete components, enabling breakthroughs in quantum communications and computation, and developing a scalable approach to quantum technologies.

There are no new physics breakthroughs required to achieve the goals of this project, however, there are hard engineering challenges that need to be addressed.","1978060","2015-05-01","2020-04-30"
"QPQV","Quantum plasmas and the quantum vacuum: New vistas in physics","Mattias Marklund","UMEA UNIVERSITET","The quantum vacuum constitutes a highly nontrivial medium, in which complex nonlinear processes, such as pair production and photon splitting, can take place. These processes will yield measurable alterations to classical electromagnetic wave dynamics and laser-matter interactions using the next-generation laser systems. It has been suggested that this could even give rise to self-compression of electromagnetic pulses in vacuum, and therefore produce intensities above the laser limit. This gives the possibility of anti-matter production, light splitting, and light collisions, that could be of importance for testing the invariance properties of the laws of physics. Furthermore, the properties of the quantum vacuum holds the key to a fundamental understanding of highly magnetized stars, the relation of spacetime dynamics to thermodynamics, and could be used to obtain information about e.g. dark matter candidates. Thus, the effects of the quantum vacuum will be noticeable both on a practical level, in future high intensity field experiments and applications, as well as at the level of basic research, providing crucial information about the properties of the laws of physics. The aim of this proposal is manifold. Using high intensity electromagnetic field generation different aspects of the quantum vacuum will be probed. The experimental investigation of the Unruh effect will yield insight into black hole physics and effects of spacetime structure on quantum field theory. The possibility to detect elastic scattering among photons would open up a completely new branch in science and deepen our understanding of the laws of physics. Moreover, using state-of-the-art laser facilities, methods for probing extreme plasmas, where quantum particle dynamics and the nonlinear quantum vacuum are important, will be developed. This holds promising applications as lasers approach entirely new intensity level in the near future.","1000000","2008-08-01","2013-07-31"
"QRGRAPH","Quasirandomness in Graphs and Hypergraphs","Daniela Kuehn","THE UNIVERSITY OF BIRMINGHAM","A structure is called quasirandom if it has a number of properties that one would expect from a random structure with similar parameters. For instance, a graph is quasirandom if its edges are spread evenly over the vertices. This concept has been remarkably useful in many areas, including Number theory, Graph theory and the design of algorithms.

Quasirandomness is a field that is developing very rapidly, but there are many connections and properties that are still unexplored. In my proposal, I will concentrate on 4 important topics where I believe that quasirandomness is crucial to further progress: hypergraph matchings, decompositions of graphs, topological subgraphs as well as sparse graphs and hypergraphs.

As an illustration of a matching problem, consider a group of people and construct a graph by drawing an edge if they like each other - a perfect matching splits the people into teams of 2 which can work together. How and when this can be achieved for teams of 2 is well understood, but not for teams of 3 or more people. This can be formulated as a hypergraph matching problem. I believe that quasirandom decompositions can be used to give quite general sufficient conditions which guarantee a perfect hypergraph matching.

A better understanding of quasirandomness of sparse hypergraphs would have applications e.g. to checking whether a Boolean formula is satisfiable. This is one of the fundamental problems in Theoretical Computer Science.","742749","2010-12-01","2015-11-30"
"QSPINMOTION","Quantum coherence and manipulation of a single flying electron spin","Tristan Aurélien Yan Meunier","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""In quantum nanoelectronics, one of the paradigms is to use quantum mechanics in order to build more efficient nanoprocessors. In this context, the electron spin has been identified as a good degree of freedom to store and to manipulate quantum information efficiently. The defined building block of this quantum computer strategy is called a spin qubit. Towards this goal, intense experimental efforts have been invested in AlGaAs heterostructures where quantum dots with only one electron can be realized. In such a system, all the basic operations of a quantum nanoprocessor have been demonstrated in spin qubits and they constitute a very promising platform to study spin dynamics at the single electron level.
To scale up the spin qubit system, one has to be able to make two distant qubits interacting. The protocol consists in the exchange of a quantum particle between the two qubits. In this respect, one can take advantage of the fact that a single electron can be transported within nanostructures. Understanding how to preserve quantum information stored in the spin of an electron while transferring it between two quantum dot systems is of crucial importance. Recently, the PI has realized a first important step towards this goal, namely the realization of efficient single electron transfer between two distant quantum dots on a timescale faster than the spin decoherence time
Here we propose to give a new dimension to the spin qubit system by investigating quantum coherence and manipulation of a single flying electron spin. Displacing coherently a single electron spin between two distant quantum dots not only represents a viable solution towards entanglement between distant qubits but also opens new ways of manipulating coherently electron spins via spin-orbit interaction. The new knowledge expected from these experiments is likely to have a broad impact extending from quantum spintronics to other areas of nanoelectronics.""","1500000","2013-01-01","2017-12-31"
"QSuperMag","Harnessing Quantum Systems with Superconductivity and Magnetism","Josep Oriol Romero-Isart","UNIVERSITAET INNSBRUCK","QSuperMag aims at using magnetic fields and superconductors to harness quantum degrees of freedom in order to make accessible an unprecedented parameter regime in the fields of quantum micro- and nanomechanical oscillators, quantum simulation with ultracold atoms, and solid-state quantum information processing. The goal is to establish a new paradigm in quantum optics by replacing laser light with magnetic fields, and especially, superconductors.
Laser light has been the ubiquitous tool in the last decades to control and manipulate quantum systems because it is fast, coherent, and can be focused to address individual degrees of freedom. However, the use of lasers poses fundamental limitations, such as heating and decoherence due to scattering and absorption of photons, and a minimum length-scale to achieve coherent control due to the diffraction limit. The main goal of QSuperMag is to circumvent these limitations by using magnetic fields and superconductors to harness quantum systems that are traditionally controlled and addressed by laser light. This will be done by developing new theory and proposing experiments which lie at the interplay between the fields of quantum science and superconductivity.
QSuperMag’s goals are to:
-Propose cutting-edge experiments in the field of quantum micromechanical systems. This will be achieved by exploiting the unique features of our recent proposal for quantum magnetomechanics using magnetically-levitated superconducting microspheres [ORI et al. PRL 109, 11013 (2012)].
-Put forward a magnetic nanolattice  for ultracold atoms in which the distance between lattice sites is of the order of few tens of nanometers. Together with a magnetic toolbox this will place the field of quantum simulation in a radically new scenario.
-Use superconductors to enhance the coupling of remote magnetic dipoles in order to design an all-magnetic quantum information processor in diamond. This will also have relevant technological applications.
￼￼￼￼","1293483","2013-11-01","2018-10-31"
"QuadraComb","Quadratic dispersive resonators for optical frequency comb generation","François Leo","UNIVERSITE LIBRE DE BRUXELLES","Optical frequency combs are made of thousands of equally spaced spectral lines, each an ultra-stable laser in its own right. They act as “spectral rulers” against which unknown optical frequencies can be measured, and they have had a revolutionary impact on numerous fields ranging from the detection of extra-solar planets to precision metrology, winning its inventors a Nobel prize in 2005. Traditionally, frequency combs have been generated by ultrashort pulsed lasers, but in 2007 an important observation changed the research landscape: a continuous-wave laser coupled into a microscopic resonator was shown to spontaneously transform into thousands of comb lines via third-order nonlinear optical effects. I believe that yet another revolution lies at the horizon. Specifically, recent experiments have alluded to the possibility of realizing optical frequency combs purely through second order (quadratic) nonlinear effects. The intrinsic features of the second order nonlinearity hold promise to deliver access to new regions of the electro-magnetic spectrum beyond all conventional frequency comb technologies. But unfortunately, experimental investigations are scarce and the physics that underlie frequency comb formation in quadratic resonators is poorly understood. The goal of the QuadraComb project is to pursue a complete characterization of frequency comb generation in dispersive quadratically nonlinear resonators. I plan to (i) develop theoretical models to describe quadratic frequency combs, and (ii) develop novel platforms for the experimental realization of quadratic frequency combs.","1579213","2018-01-01","2022-12-31"
"QUAERERE","Quantifying aerosol-cloud-climate effects by regime","Johannes Renatus Quaas","UNIVERSITAET LEIPZIG","Global climate change is widely considered one of the main concerns of humankind. However, predictions are highly uncertain, with no substantial improvement since more than three decades. They are hampered by the huge uncertainty  of climate forcing, which is dominated by the uncertainty in anthropogenic aerosol-cloud-climate effects (“aerosol indirect effects”).  The goal of QUAERERE (Latin for researching) is a reliable, observations-based, global quantification of these effects, which would also imply a constraint on climate sensitivity and thus climate predictions.  This goal is now reachable combining recent advances in different disciplines: (i) a decade-long satellite dataset involving retrievals of the relevant quantities is now available, complemented by a complete aerosol dataset from a new reanalysis; (ii) on the basis of high-resolved numerical weather pre­diction models, which include parameterisations of aerosol cycles and cloud-precipitation microphysics, cloud-system resolving simulations at a regional scale are now possible; reliable simulations beyond idealised cases are thus possible. These tools are complemented by comprehensive global climate models and reference ob­servations from ground-based sites.
The problem in aerosol-cloud-climate effects is in its complexity: Various processes counteract each other, and large spatiotemporal variability of clouds buffers the forcing effects. QUAERERE proposes a two-fold “divide-and-conquer” approach to this complex problem: (i) aerosol-cloud-climate effects will be investigated by regime; this allows to circumvent the problem of aerosol-cloud-climate ef­fects being buffered when averaging over different regimes; and (ii) by investigating individual terms con­tributing to the aerosol-cloud-climate effects separately; this allows to analyse individual statistical relation­ship in satellite observations and model results consistently, and to perform model sensitivity studies for cause-effect attribution.","1448160","2012-10-01","2017-09-30"
"QUAHQ","PROBING EXOTIC QUANTUM HALL STATES WITH HEAT QUANTUM TRANSPORT","François PARMENTIER","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Under high magnetic field and at low temperatures, electronic interactions in a two-dimensional electron gas give rise to exotic, strongly correlated many-body quantum Hall states. These states have been proposed for the implementation of new quantum circuits, for instance realizing topologically protected quantum computing. Although exciting, these states remain poorly understood, because the conventional experimental approach for their investigation, dc electron transport, only yields limited information. In particular, electron transport only probes the physics of the current-carrying edge channels of the quantum Hall effect propagating along the edges of the electron gas, leaving the physics of the bulk unexplored. To gain a better understanding of these exotic states and their origin, I propose a new, unconventional approach, based on heat transport measurements, which directly probes the charge-neutral, heat-carrying collective modes characterizing these interactions-induced states. I will focus on the debated ν=0 quantum Hall state of monolayer and bilayer graphene, which is thought to arise from spontaneous spin- and valley- symmetry breakings due to interactions, and on the fractional quantum Hall effect, where the competition between interaction and disorder gives rise to low-energy, heat-carrying neutral modes which have not yet been observed in graphene. Investigating the neutral modes through heat transport will address two important questions regarding these exotic new states: does ν=0 indeed arise from spontaneous symmetry breakings, and what is the origin of the low-energy neutral modes in the fractional quantum Hall effect, particularly in graphene. Furthermore, it will be possible to apply my approach to the investigation of other exotic quantum states in two-dimensions, such as the superfluid excitonic condensate in electron-hole bilayer systems.","1499839","2019-02-01","2024-01-31"
"Quality","From correct to high-quality reactive systems","Orna Kupferman","THE HEBREW UNIVERSITY OF JERUSALEM","Formal verification is the study of algorithms and tools for the development of correct hardware and software designs. Two fundamental problems in formal verification are temporal logic model checking -- given a mathematical model of the system and a temporal-logic formula that specifies the desired behavior of the system, decide whether the model satisfies the formula, and synthesis -- given a temporal-logic formula that specifies the desired behavior, generate a system that satisfies the specification with respect to all environments. Formal verification improves earlier verification methods, which are based on simulation and are thus neither exhaustive nor fully automatic.

Formal verification is Boolean: the system may either satisfy its specification or not satisfy it. The objective of this research is to add a quality measure to the satisfiability of specifications of reactive systems, and to use it in order to formally define and reason about quality of systems and in order to significantly improve the quality of automatically synthesized reactive systems. We plan to do so by developing a theory of multi-valued specification formalisms -- temporal logic and automata, studying the algorithmic aspects of the new formalisms, and suggesting novel applications of multi-valued automata in verification, design, and synthesis of reactive systems.","1498400","2012-01-01","2017-12-31"
"QUANT-DES-CNT","Quantum Design in Carbon Nanotubes","Shahal Ilani","WEIZMANN INSTITUTE OF SCIENCE LTD","Quantum design, the ability to control the microscopic properties of a quantum system, has proven to be an invaluable tool in experimental physics. Carbon nanotubes are an ideal system to implement quantum design in the solid-state; their strongly interacting electrons, unusual spin properties, and unique mechanical qualities make them an excellent platform for studying quantum phenomena in low dimensions. However, for many years this potential has been hindered by the dominance of strong electronic disorder in this
system. Fortunately, a series of recent breakthroughs in making nanotubes free of disorder has dramatically changed this situation, opening up a wide range of opportunities for high-precision experiments in these systems.
In this work I propose to develop a new technology that will enable quantum design experiments in
carbon nanotubes. This technology, which builds on my recent development of ultra-clean electronic devices in nanotubes, will allow us to create nanotube device-architectures that go far beyond those currently available. Specifically, we will be able to control the properties of individual electrons with microscopic precision (~100nm), manipulate their quantum states, and image their individual wavefunctions. This new toolset will be used to study previously unexplored realms in condensed matter physics, ranging from the correlated states-of-matter formed by electrons in one-dimension, to quantum information experiments with multiple electronic spins, and finally to mechanical studies of nanotube resonators in the quantum limit.
These studies will address some of the most fundamental aspects pertaining to the physics of electrons, spins and phonons in low dimensions.","1499940","2011-01-01","2015-12-31"
"QuantGeomLangTFT","The Quantum Geometric Langlands Topological Field Theory","David Andrew Jordan","THE UNIVERSITY OF EDINBURGH","We will use modern techniques in derived algebraic geometry, topological field theory and quantum groups to construct quantizations of character varieties, moduli spaces parameterizing G-bundles with flat connection on a surface.  We will leverage our construction to shine new light on the geometric representation theory of quantum groups and double affine Hecke algebras (DAHA's), and to produce new invariants of knots and 3-manifolds.

Our previous research has uncovered strong evidence for the existence of a novel construction of quantum differential operators -- and their extension to higher genus surfaces -- in terms of a four-dimensional topological field theory, which we have dubbed the Quantum Geometric Langlands (QGL) theory.  By construction, the QGL theory of a surface yields a quantization of its character variety; quantum differential operators form just the first interesting example.  We thus propose the following long-term projects:

1. Build higher genus analogs of DAHA's, equipped with mapping class group actions -- thereby solving a long open problem -- by computing QGL theory of arbitrary surfaces; recover quantum differential operators and the (non-degenerate, spherical) DAHA of G, respectively, from the once-punctured and closed two-torus.
2. Obtain a unified construction of both the quantized A-polynomial and the Oblomkov-Rasmussen-Shende invariants, two celebrated -- and previously unrelated -- conjectural knot invariants which have received a great deal of attention.
3. By studying special features of our construction when the quantization parameter is a root of unity, realize the Verlinde algebra as a module over the DAHA, shedding new light on fundamental results of Cherednik and Witten.
4. Develop genus one, and higher, quantum Springer theory -- a geometric approach to constructing representations of quantum algebras -- with deep connections to rational and elliptic Springer theory, and geometric Langlands program.","1100948","2015-06-01","2020-05-31"
"QUANTHOM","Quantitative methods in stochastic homogenization","Antoine Kenneth Florent Gloria","UNIVERSITE PIERRE ET MARIE CURIE - PARIS 6","This proposal deals with the development of quantitative tools in stochastic homogenization, and their applications to materials science. Three main challenges will be addressed.
First, a complete quantitative theory of stochastic homogenization of linear elliptic equations will be developed starting from results I recently obtained on the subject combining tools originally introduced for statistical physics, such as spectral gap and logarithmic Sobolev inequalities, with elliptic regularity theory. The ultimate goal is to prove a central limit theorem for solutions to elliptic PDEs with random coefficients.
The second challenge consists in developing an adaptive multiscale numerical method for diffusion in inhomogeneous media. Many powerful numerical methods were introduced in the last few years, and analyzed in the case of periodic coefficients. Relying on my recent results on quantitative stochastic homogenization, I have made a sharp numerical analysis of these methods, and introduced more efficient variants, so that the three academic examples of periodic, quasi-periodic, and random stationary diffusion coefficients can be dealt with efficiently. The emphasis of this challenge is put on the adaptivity with respect to the local structure of the diffusion coefficients, in order to deal with more complex examples of interest to practitioners.
The last and larger objective is to make a rigorous connection between the continuum theory of nonlinear elastic materials and polymer-chain physics through stochastic homogenization of nonlinear problems and random graphs. Analytic and numerical preliminary results show the potential of this approach. I plan to derive explicit constitutive laws for rubber from polymer chain properties, using the insight of the first two challenges. This requires a good understanding of polymer physics in addition to qualitative and quantitative stochastic homogenization.","1043172","2014-02-01","2019-08-31"
"QUANTMATT","Dynamics and transport of quantum matter --- exploring the interplay of topology, interactions and localization","Jens Hjörleifur Bárðarson","KUNGLIGA TEKNISKA HOEGSKOLAN","Quantum matter is condensed matter which properties are dominated by the quantum nature of its constituents. The two most fundamental properties of quantum mechanics are interference and entanglement. How do these properties, and their derivatives, show up in an experiment? And how does one control them? These are the fundamental questions addressed in this proposal.

The study is divided into three main parts: many-body localization, topological insulator nanowires, and topological semimetals. Many-body localization is concerned with the interplay of interference and entanglement and is central to questions about quantum thermalization. I aim to understand experimental signatures of many-body localization as well as devising simulation schemes that allow us to conduct numerical experiments on many-body localization for larger system sizes than has been so far possible. The interplay of interference, topology and geometry is the central theme of the topic of topological insulator nanowires. I have in the past theoretically demonstrated the signatures of fundamental quantum phenomena in these systems, including perfectly transmitted mode and Majorana fermions. The major goal of this part of the project is to collaborate closely with experimental groups seeking to verify my past theories, by providing new and more detailed predictions for these systems. This requires to further understand experimental details, develop certain theoretical devices and simulation techniques based on them. The final part on topological semimetals is particularly timely in view of recent experimental realizations of Dirac semimetals and the impending realization of Weyl semimetals, which both can be roughly thought of as 3D analogs of graphene. I seek to understand their unique transport signatures and the interplay of disorder with 3D Dirac fermions. The three parts feed into and from each other both through unified concepts and common methodology.","1500000","2016-01-01","2020-12-31"
"QUANTUMCANDI","Interfacing quantum states in carbon nanotube devices","Alexander Högele","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","Coherent control and sensitive detection of quantum states in condensed matter are among the most topical challenges of modern physics. They drive the development of novel materials, theoretical concepts, and experimental methods to advance our understanding of fundamental laws of quantum mechanics and to create transformative technologies for future applications. During the past decades carbon has emerged as a new material platform to address these challenges: graphene and carbon nanotubes have been created as paradigm systems with exceptional physical properties.

As atomically-thin cylinders carbon nanotubes combine ultra-low mass with extreme mechanical stiffness. This identifies them as perfect candidates for the realization of ultra-high quality mechanical resonators with applications in quantum metrology and sensing. Their crystalline lattice can be made free of nuclear spins by material engineering to ensure ultra-long electron spin coherence times for quantum information processing and coherent spintronics. In addition, semiconducting single-wall carbon nanotubes exhibit optical resonances with unprecedented tunability in color for quantum communication and cryptography. These outstanding material properties form the basis for our scientific research proposal.

Our vision is to realize up-conversion schemes interfacing light with spin, mechanical, and spin-mechanical degrees of freedom in carbon nanotube devices. In particular, we will study spin dynamics in carbon nanotubes with an isotopically engineered nuclear spin lattice and we will suspend individual carbon nanotubes in high-fidelity optical micro-cavities to detect and control mechanical motion down to the quantum ground state. Ultimately, our devices will realize entirely novel regimes of quantum states by hybridizing light with magnetic or mechanical excitations and explore the foundations of emerging technologies at the quantum limit.","1739680","2014-01-01","2018-12-31"
"QUANTUMCRASS","Towards a fully quantum ab initio treatment of chemical reactions at solid surfaces","Angelos Michaelides","UNIVERSITY COLLEGE LONDON","The making and breaking of bonds involving hydrogen atoms at the surfaces of materials plays a major role in nature. For example, the formation and activation of C-H, N-H, and O-H bonds lies at the heart of heterogeneous catalysis and is no less important to other disciplines such as electrochemistry and astrophysics, not to mention the widely discussed “hydrogen economy” of the future. When dealing with hydrogen, quantum nuclear effects - tunnelling and quantum delocalization - can be significant at room temperature and below. Despite this fact, and despite growing economic and environmental incentives to carry out hydrogenation and dehydrogenation reactions at lower temperatures most theoretical studies neglect the role quantum nuclear effects play in such processes. Here, we will address this by developing and applying ab initio path integral techniques for the rigorous treatment of quantum nuclear effects in elementary diffusion and reaction events at solid surfaces. The path integral formalism of quantum mechanics provides a powerful approach for treating quantum nuclear effects and when done with an ab initio determination of the underlying potential energy surface highly accurate predictions can be achieved. This project will begin with ab initio path integral simulations of time independent quantum properties such as addressing the extent of quantum delocalisation of adsorbed hydrogen atoms and hydrogen atoms incorporated in molecules adsorbed on solid surfaces. Following this ab initio centroid molecular dynamics techniques specifically designed for the determination of quantum transition state theory rate constants and mechanisms of elementary reaction and diffusion processes at solid surfaces will be developed. This highly ambitious project will culminate in the fully quantum treatment of several elementary reactions at metal surfaces and in so doing open up a new research frontier: the fully quantum path integral treatment of surface chemistry.","912916","2008-08-01","2012-07-31"
"QuantumNet","A Scalable Quantum Network based on Individual Erbium Ions","Andreas REISERER","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","A future quantum network will consist of quantum processors that are connected by quantum channels, just like conventional computers are wired up to form the Internet. In contrast to classical devices, however, the entanglement and non-local correlations available in a quantum-controlled system facilitate novel fundamental tests of quantum theory and numerous applications in distributed quantum information processing, quantum communication, and precision measurement. While pioneering experiments have demonstrated the entanglement of two quantum nodes separated by up to 1.3 km, accessing the full potential of quantum networks requires scaling of these prototypes to more nodes and larger distances. To this end, a new technology that overcomes the bottlenecks of existing physical systems has to be developed.
Here, I propose to harness the exceptional properties of individual Erbium ions embedded in Yttrium crystals to increase the size of quantum networks via implementation of the seminal quantum repeater proposal, which is one of the most intensively pursued research topics in current quantum science. The key proposed steps to this goal are (I) implementation of a quantum spin-photon interface at a telecommunication wavelength, (II) multiplexing of many quantum bits in one device via frequency-selective addressing, and (III) implementation of remote entanglement swapping and purification to increase the range of quantum-secure communication beyond its current fundamental limit.
These goals have been out of reach for any experimental platform until now. They become feasible by combining the powerful concepts developed in cavity quantum electrodynamics using cold atoms with the exceptional coherence of spins in specific host crystals. Successful implementation will demonstrate the feasibility of quantum networks over global distances, a milestone advancement for quantum communication and quantum science in general.","1477500","2018-03-01","2023-02-28"
"QuantumProbe","A Quantum Non-Demolition Microscope","Artur Stephan Widera","TECHNISCHE UNIVERSITAET KAISERSLAUTERN","QuantumProbe will devise a novel microscope to probe and fully control the intriguing properties of quantum systems, formed by neutral atoms trapped in optical lattices. This includes in particular the possibility of in-depth quantum state characterization as well as engineering arbitrary quantum correlations. The microscope’s achievements will go far beyond standard state-of-the-art detection and manipulation capabilities in both, top-down approaches using large quantum gas systems, and bottom-up approaches with single particles.
For this purpose, single or few well controlled neutral atoms will be immersed as “quantum-probes” into a quantum target system, a Mott-insulating state of another atomic species. Hence, QuantumProbe extends the concept of scanning microscopy to a single atom based coherent microscope, capable of parallel multi-tip operation with more than one probe atom. The fundamental measurement mechanism of this microscope is the entanglement between probe and target atoms, induced by coherent inter-species interaction, and subsequent detection of the probe atom’s state. QuantumProbe will thereby enable local, quantum non-demolition measurements of atom number or spin state as well as local, coherent manipulations such as spin flips and controlled-NOT quantum gates within the Mott-insulator.
QuantumProbe will pave the way for break-throughs in various fields of research and practice: It will introduce local, fundamental quantum gates in scalable many-body systems, highly relevant for implementations of quantum information processing and quantum computing strategies; help elucidating the classification and quantification of multi-particle entanglement, which is even theoretically not fully clear; allow local coherent spin state engineering and read-out, opening the route for studies of quantum magnetism and related quantum simulations with single atom resolution; and devise tools for the studies of impurity physics in a well controlled environment.","1368600","2011-08-01","2016-07-31"
"QUANTUMSUBCYCLE","Ultrafast quantum physics on the sub-cycle time scale","Rupert Huber","UNIVERSITAET REGENSBURG","The physics of condensed matter depends on ultrafast dynamics of its atomic constituents. Femtosecond light pulses have been exploited to monitor these phenomena by stroboscopic means. Yet, the time resolution is limited by the duration of the intensity envelope of the light pulses used. We propose a new class of sub-cycle optics, which harnesses the absolute optical phase and amplitude of ultrashort transients to control condensed matter faster than an oscillation cycle of light. Merging latest terahertz technology with nanooptics, we tailor extreme electric and magnetic near-fields of phase-locked infrared pulses in all four spatio-temporal dimensions. This unprecedented laboratory allows us to pioneer long sought-after non-adiabatic quantum physics of all relevant elementary degrees of freedom: electronic charge and spin as well as photons.
(i) Optical acceleration of electrons in the sub-cycle limit will permit to test yet unobserved key concepts of relativistic quantum transport, such as Zitterbewegung of Dirac fermions and Bloch oscillations in bulk semiconductors.
(ii) We aim to switch the spin direction in magnetic materials by giant magnetic or electric fields, of 10 GV/m and several 10 Tesla, promising record control speeds and unique vistas onto the fastest magnetic elementary processes.
(iii) By advancing the sensitivity of electro-optic sampling to the few-photon level the quantum nature of the oscillating carrier wave will be detected in the time domain. Spontaneous creation of photons out of quantum vacua, reminiscent of Hawking radiation of black holes, may be traced.
The project breaks grounds for basic research, shedding new light onto the foundations of quantum electrodynamics, solid state physics and magnetism, as well as a new kind of field resolved quantum optics.","1494564","2013-04-01","2018-03-31"
"QUANTUMWALKS","Quantum walks in superconducting networks","Nadav Katz","THE HEBREW UNIVERSITY OF JERUSALEM","""I propose to build a general purpose continuous quantum walk platform using superconducting devices (resonators, qubits and SQUIDS). This system will include up to 40 sites and will implement basic quantum simulation algorithms, generalized interferometry and explore the quantum-classical boundary for many-particle entangled systems.
Quantum walks (QW) are a novel scheme for quantum information processing. The core idea is to encode the problem into a network and propagate quantum particles within. The entanglement of the many-body state due to interference between sites of the network brings, at the appropriate time, to a desired answer/observable. Recent implementations with optical photons or trapped ions and atoms have brought this theoretical process to the forefront of fundamental and applied quantum engineering.
In parallel, superconducting devices are experiencing a renaissance due to modern understanding of materials, fundamental physics of superconductivity and fabrication techniques. The coherence times of superconducting qubits have improved by almost 5 (!) orders of magnitude over the past ten years. Recent developments include single microwave sources and detectors, quantum-limited amplifiers, heterodyne techniques for measurement and state tomography.
Building such a network involves significant challenges, both fundamental and technical. On the fundamental level I intend to improve coherence times of our devices by advanced material science characterization, simulation tools and rapid turn-around characterization. My group will build a """"quantum compiler"""" system for designing new layouts, bridging abstract design to implementation. On the technical level we will implement a flip chip bias circuit to overcome site inhomogeneity and for evolving and measuring results. This will be an enabling system for a broad range of quantum information processing applications and fundamental experiments, with unprecedented computational power and flexibility.""","1317560","2013-08-01","2018-07-31"
"QUARKGLUONPLASMACMS","Quark-Gluon Plasma through dilepton studies with the CMS experiment at the Large Hadron Collider","Raphael Granier De Cassagnac","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","This proposal aims at consolidating a Quark-Gluon Plasma research team recently founded by Raphaël Granier de Cassagnac, the Principal Investigator (PI) of this proposal, at the Laboratoire Leprince-Ringuet (LLR).
The PI has a deep experience of heavy-ions physics, working since 9 years in the PHENIX collaboration of the Relativistic Heavy Ion Collider. His recognized activities already propelled him soon after having joined the CMS collaboration at the Large Hadron Collider, as convener of the world-wide Heavy Ions Physics Analysis Group for a term covering the 2010-2011period.
CMS, the Compact Muon Collaboration, is extremely well suited for muon measurements. From di-muon mass spectra we will first measure Z-bosons for the first time in a heavy-ions environment. This provides useful information on quark distribution function in nuclei, and opens the field of Z-jet studies, allowing unbiased studies of jet fragmentation function. We will also measure quarkonia (J/È and Upsilons). Though Upsilons will be novel measurements, J/È have been extensively studied by the PI at RHIC. A larger suppression observed at forward rapidity is still a puzzle, that we will help solving.
We propose to enhance a computing centre (the GRIF Tier-2) to conduct heavy-ions specific data reconstruction, analysis and simulations. We also want to open a new activity: electron reconstruction in CMS heavy-ions environment. This very challenging objective will benefit from LLR highly experienced p+p physicists in electron reconstruction. The access to the dielectron mass spectra will raise the statistics of our signal and provide a crucial cross-check of all studies.
Finally, we want to keep a phenomenological component in the team, so to have all the tools to properly interpret our own results.","1133600","2010-11-01","2015-10-31"
"Quasicrystal","An Optical Quasicrystal for ultracold atoms","Ulrich Walter SCHNEIDER","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","During the last fifteen years, ultracold atoms in optical lattices have emerged as a powerful model system to study the many-body physics of interacting particles in periodic potentials. The main objective of this proposal is to extend this level of control to quasiperiodic potentials by realizing an optical quasicrystal.

Quasicrystals are a novel form of condensed matter that is non-periodic, but long-range ordered. They have first been observed in the 1980s by Dan Shechtman in diffraction experiments. Quasicrystals give rise to a pattern of sharp Bragg peaks, similar to periodic crystals, but with rotational symmetries that are impossible for periodic structures. Their structure was found to be given by aperiodic tilings with more than one unit cell, such as the celebrated Penrose tiling. 
Even though quasicrystals are long-range ordered, many foundational concepts of periodic condensed matter systems such as Blochwaves or Brillouin zones are not applicable. This places them on an interesting middle ground between periodic and disordered systems and highlights their potential for novel many-body physics.

We will first characterize the optical quasicrystal using Kapitza-Dirac diffraction, and then study their unusual transport properties and relaxation dynamics after quantum quenches in the presence of interactions. We will additionally look for interesting novel phases at strong interactions and investigate the topological properties of quasiperiodic potentials. 

Building on my substantial expertise with optical lattices, I thus plan to build a versatile quantum simulator for the physics of quasicrystals by combining a non-periodic optical potential with ultracold Rubidium and Potassium gases.","1499086","2017-01-01","2021-12-31"
"QUASIFT","Quantum Algebraic Structures In Field Theories","Vasily PESTUN","INSTITUT DES HAUTES ETUDES SCIENTIFIQUES","Quantum Field Theory is a universal framework to address quantum physical systems with infinitely many interacting degrees of freedom, applicable both at the level of fundamental interactions, such as the subnuclear physics of quarks and gluons and at the phenomenological level such as the physics of quantum fluids and superconductivity.
    Traditionally, weakly interacting quantum field theory is formulated as a perturbative deformation of the linear theory of freely propagating quantum waves or particles with interactions described by Feynman diagrams. For strongly non-linear quantum field theories the method of Feynman diagrams is not adequate.
    The main goal of this proposal is to develop novel tools and techniques to address strongly non-linear quantum field theories.
    To achieve this goal we will search for hidden algebraic structures in quantum field theories that will lead to efficient algorithms to compute physical observables of interest. In particular we identify non-linear quantum field theories with exactly solvable sectors of physical observables.

In this project we will focus on three objectives:
- build general theory of localization in supersymmetric Yang-Mills theory for arbitrary geometrical backgrounds
- find all realizations of symplectic and supersymplectic completely integrable systems in gauge theories
- construct finite supersymmetric Yang-Mills theory in terms of the algebra of locally supersymmetric loop observables for maximally supersymmetric gauge theory

The realization of the above objectives will uncover hidden quantum algebraic structures and consequently will bring ground-breaking results in our knowledge of quantum field theories and the fundamental interactions.","1498750","2016-09-01","2021-08-31"
"QUASIPERIODIC","Dynamics of quasiperiodic type","Artur Avila Cordeiro De Melo","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","This project is dedicated to the study of two distinct classes of dynamical systems which display a quasiperiodic component.

The first class consists of quasiperiodic cocycles, and we will largely focus on connections with the spectral theory of quasiperiodic Schrodinger operators.  Up to very recently, our understanding had been mostly restricted to situations where the potential would have some clear characteristics of large or small potentials.  In particular, no genuinely global theory had been devised that could go so far as give insight on the phase-transition between large-like and small-like potentials.  With the introduction by the PI of techniques to analyze the parameter dependence of one-frequency potentials which involve much less control of the dynamics of associated cocycles, and the discovery of new regularity features of this dependence, it is now possible to elaborate a precise conjectural global picture, whose proof is one of the major goals of the project.

The second class consists of translation flows on higher genus surfaces.  The Teichmuller flow acts as renormalization in this class, and its chaotic features have permitted a detailed description of the dynamics of typical translation flows.  This project will concentrate on the the development of techniques suitable to the analysis of non-typical families of translation flows, which arise naturally in the context of certain applications, as for rational billiards.  We aim to obtain results regarding the spectral gap for restrictions of the SL(2,R action, the existence of polynomial deviations outside exceptional cases, and the weak mixing property for certain billiards.","1020840","2010-12-01","2015-11-30"
"QUASIRIO","Quantum simulations with trapped Rydberg ions","Markus Thomas Hennrich","STOCKHOLMS UNIVERSITET","This project focuses on the realization and application of trapped Rydberg ions for quantum information processing and quantum simulation. It will bring together two prospective quantum computational systems: trapped ions and Rydberg atoms. Joining them will form a novel quantum system with advantages from both sides.
This approach will open a new path of investigation for quantum computing and simulation and will allow investigation of different physical qualities not yet addressed in the existing systems. In particular, it promises to speed up entangling interactions by three orders of magnitude and to extend the interaction distance by at least a factor of two between neighbouring ions. The higher speed of entangling interactions would allow the execution of more complex quantum algorithms before decoherence destroys the stored quantum information. The increased coupling distance would enable the controlled interaction of neighbouring ions which are trapped individually.  This would allow setting up a quantum computational system formed by a Coulomb crystal or a two-dimensional array of individually trapped ions.
Such qualities make trapped Rydberg ions a powerful alternative approach for scalable quantum information processing. In particular, a string, crystal or two-dimensional array of interacting trapped Rydberg ions can be used for simulations of complex quantum systems intractable by classical computers.","1499955","2012-02-01","2017-01-31"
"QUASOM","Quantifying and modelling pathways of soil organic matter as affected by abiotic factors, microbial dynamics, and transport processes","Markus Reichstein","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Soils play a critical role in the coupled carbon-cycle climate system. However, our scientific understanding of the role of soil biological-physicochemical interactions and of vertical transport for biogeochemical cycles is still limited. Moreover the representation of soil processes in current biosphere models operating at global scale is crude compared to vegetation processes like photosynthesis. Hence, the general aim of this project is to improve our understanding of the key interactions between the biological and the physicochemical components of the soil system that are often not explicitly considered in current experimental and modeling approaches. However, these interactions are likely to influence the biogeochemical cycles for a large part of the terrestrial biosphere and thus have the potential to significantly impact the Earth System as a whole. This will be achieved through an approach that integrates new soil mesocosm experiments, field data from ongoing European projects and soil process modeling. In mesocosm tracer experiments the fate of new and autochthonous soil organic matter will be followed under varying temperature and moisture regimes, explicitly investigating the role of microbiota. This project will test the hypothesis that transfer coefficients between soil organic matter pools, respiration and microbial biomass formation are constant as implemented in current soil organic matter models. Novel soil model structures will be developed that may explicitly account for the role of microbes and transport for soil organic matter dynamics. This will be supported by multiple-constraint model identification techniques, which allows testing and achieving model consistency with several observation types and theory. The soil modules will be incorporated into global terrestrial biosphere models which are coupled and uncoupled to the atmosphere allowing specific model experiments for investigating feedback mechanisms between soil, climate, and vegetation.","946800","2008-09-01","2014-02-28"
"QUASYModo","Symmetric Cryptography in the Post-Quantum World","María NAYA PLASENCIA","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","As years go by, the existence of quantum computers becomes more tangible and the scientific community is already anticipating the enormous consequences of the induced breakthrough in computational power. Cryptology is one of the affected disciplines. Indeed, the current state-of-the-art asymmetric cryptography would become insecure, and we are actively searching for alternatives. Symmetric cryptography, essential for enabling secure communications, seems much less affected at first sight: its biggest known threat is Grover’s algorithm, which allows exhaustive key searches in the square root of the normal complexity. Thus, so far, it is believed that doubling key lengths suffices to maintain an equivalent security in the post-quantum world.
The security of symmetric cryptography is completely based on cryptanalysis: we only gain confidence in the security of a symmetric primitive through extensive and continuous scrutiny. It is therefore not possible to determine whether a symmetric primitive might be secure or not in a post-quantum world without first understanding how a quantum adversary could attack it. Correctly evaluating the security of symmetric primitives in the post-quantum world cannot be done without a corresponding cryptanalysis toolbox, which neither exists nor has ever been studied. This is the big gap I have identified and that I plan to fill with this project.
Next, doubling the key length is not a trivial task and needs to be carefully studied. My ultimate aim is to propose efficient solutions secure in the post-quantum world with the help of our previously obtained quantum symmetric cryptanalysis toolbox. This will help prevent the chaos that big quantum computers would generate: being ready in advance will definitely save a great amount of time and money, while protecting our current and future communications.
The main challenge of QUASYModo is to redesign symmetric cryptography for the post-quantum world.","1330463","2017-09-01","2022-08-31"
"QUCO","The power of quantum computers","Julia Kempe","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Quantum computation has been put forward as a way to overcome current computational limits, by exploiting the quantum properties of nature. It is an interdisciplinary area at the interface of physics and computation. Since Shor&apos;s groundbreaking discovery in 1994 that a quantum computer can factor numbers efficiently, strong, steady advances have been made in understanding the advantages of quantum resources and numerous experimental efforts are underway to implement this model of computation. Our research goal is to expand our understanding of quantum resources and computation and its interplay with classical computation, as well as to open new directions. More specifically, we want to advance progress in algorithm design, using new paradigms we have recently helped to develop, and to deepen our understanding of the computational power of physical resources like shared quantum states (entanglement), quantum communication, quantum memory and restricted quantum systems. An important aspect of this proposal is to parallel this investigation into the power of quantum systems with a study of its limits. Specifically we want to identify potential hard problems for quantum computers and to explore their hardness properties as a base for new classical cryptosystems that are secure against quantum adversaries. We also want to deepen our understanding of the effects of an adversary with quantum resources in cryptography. In a society where information is a crucial economic resource it is important to protect today&apos;s information against tomorrow&apos;s possible progress in the implementation of quantum computers. In this line of research we also propose to explore the systematic use of the emerging toolbox of quantum techniques to find solutions for classical problems with quantum arguments.","744000","2008-07-01","2011-11-30"
"QUENTRHEL","Quantum-coherent drive of energy transfer along helical structures by polarized light","Elisabetta Collini","UNIVERSITA DEGLI STUDI DI PADOVA","Electronic energy transfer (EET) is a ubiquitous photophysical process that plays a crucial role in the light-harvesting capabilities of natural antenna complexes. Emerging experimental breakthroughs indicate that the dynamics of light harvesting is not fully described by a classical random-walk picture, but also quantum coherent transfer takes place. Interestingly, coherent EET processes were recently detected also in a conjugated polymer at room temperature, suggesting that coherent EET may play a key role in artiﬁcial systems, as well. This suggests a new way to think about the design of future artificial photosynthetic systems and can potentially open a revolutionary avenue for the effective use of biological systems and conjugated polymers as quantum devices or resources for quantum information processing. The main goal of the project is to give an important contribution in this breakthrough field, looking for a piece of information still missing: the possible presence of a relation between structure and coherent mechanisms. The main challenge is to develop new spectroscopic tools able to unveil the presence and the nature of vibrational modes acting during the energy migration and possibly driving coherent mechanisms. To this aim, a new 2D technique is proposed, which merges together the sensitivity of circular dichroism to structural deformations and the power of 2D photon echo in detecting coherent effects. Instead of natural light-harvesting antennae, the objects of this project will be model systems, more stable and easier to manipulate and modify ad hoc. The attention will be mainly focused on multichromophoric systems with helical arrangements because they mimic a ubiquitous motif that nature exploited to develop highly efficient EET. The helical core can act indeed as a wire, directionally driving the energy migration and, perhaps, preserving long-lived coherences.","1479480","2012-03-01","2018-02-28"
"QUERG","Quantum entanglement and the renormalization group","Frank Verstraete","UNIVERSITAT WIEN","Among the most defining events in physics during the last decade were the spectacular advances in the field of strongly correlated quantum many body systems: the observation of quantum phase transitions in optical lattices and the realization that many body entanglement can be exploited to build quantum computers are only two of the notable breakthroughs. The description of strongly correlated quantum systems and the associated entanglement structure is still largely unexplored territory. This field represents one of the big challenges and opportunities in theoretical physics. In a recent evolution, we showed that the tools developed in the context of quantum computing and entanglement theory lead to a novel understanding of the structure of the wavefunctions that arise as ground states of strongly correlated quantum Hamiltonians. This approach opens up a wealth of new research opportunities that will be investigated, such as a description of quantum phases of matter with nonlocal order parameters and an explicit characterization of quantum states exhibiting critical behaviour and/or topological quantum order. Such theories cannot be described within the conventional Landau theory of phase transitions. The theory of entanglement also provides a new language in which one can describe real-space renormalization group methods, and this is resulting in a long anticipated extension of their range of applicability. A crucial part of the project will consist of developing stable numerical methods that generalize the very successful DMRG method to two dimensions and to non-equilibrium situations. One of the main objectives is to simulate the phase diagram of the Hubbard model in two dimensions. Preliminary results are promising, and we are confident that this work will impact the way we understand, observe and manipulate the quantum world. This is especially relevant since quantum effects will play an increasingly dominant role in future technologies.","1274254","2009-11-01","2014-10-31"
"QUESPACE","Quantifying Energy Circulation in Space Plasma","Minna Maria Emilia Palmroth","ILMATIETEEN LAITOS","The project aims to quantify energy circulation in space plasmas. Scientifically, energy transfer is a fundamental plasma physical problem having many applications in a variety of plasma environments ranging from coronal heating on the Sun to electric heating in the ionosphere. Technologically, understanding the plasma and energy transport properties is a step toward predictions of the space environment needed for spacecraft design and operations. The space physics community lacks an accurate and self-consistent numerical model capable of describing the global plasma system in particular in the inner magnetosphere, where major magnetic storms can cause serious damage to space-borne technology. The project has two goals: 1. Novel integration of observations from ESA’s four-spacecraft Cluster mission with simulation results to gain quantitative understanding of global energy transport properties in the near-Earth space; 2. Development of a new self-consistent global plasma simulation that describes multi-component and multi-temperature plasmas to resolve non-MHD processes that currently cannot be self-consistently described by the existing global plasma simulations. The new simulation methods are now feasible due to the increased computational capabilities. Our existing simulation environment and unique analysis methods have brought exciting new results on magnetospheric energy circulation. Seven years after launch, the Cluster database is now large enough to quantitatively assess these effects. The proposing team has a long record in observational research of global energetics and a world-leading role in developing global magnetospheric computer simulations.","699985","2008-09-01","2013-08-31"
"QUEST","Quantitative electron and spin transport theory for organic crystals based devices","Stefano Sanvito","THE PROVOST, FELLOWS, FOUNDATION SCHOLARS & THE OTHER MEMBERS OF BOARD OF THE COLLEGE OF THE HOLY & UNDIVIDED TRINITY OF QUEEN ELIZABETH NEAR DUBLIN","Predicting the electron and spin transport properties of organic crystals is a formidable theoretical challenge as these are determined both by the electronic structure of the individual molecules and by the morphology of the crystal. Quest's research program seeks at developing a fully quantitative theory for electron and spin transport in organic crystals, which does not rely on external parameters and can be applied to materials underpinning a multitude of applications, ranging from organic electronics, to spintronics, to energy. In particular we aim at combining state of the art density functional theory with advanced quantum transport methods and Monte Carlo simulations. We will then construct a hierarchical computational protocol enabling us to evaluate electron and spin transport across different length scales at finite temperature, including effects originating from external fields (electric and magnetic). Our developed tools will form a software package to be distributed freely to academia.","1492728","2012-12-01","2018-11-30"
"QUEST","QUantum Hall Edge State Tunnelling spectroscopy","Benjamin Pierre Alexis Sacépé","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The quantum nature of an electronic fluid is ubiquitous in many solid-state systems subjected to correlations or confinement. This is particularly true for two-dimensional electron gases (2DEGs) in which fascinating quantum states of matter, such as the integer and fractional quantum Hall (QH) states, arise under strong magnetic fields. The understanding of QH systems relies on the existence of one-dimensional (1D) conducting channels that propagate unidirectionally along the edges of the system, following the confining potential. Due to the buried nature of 2DEG commonly built in semiconducting heterostructures, the considerable real space structure of this 1D electronic fluid and its energy spectrum remain largely unexplored.
This project consists in exploring at the local scale the intimate link between the spatial structure of QH edge states, coherent transport and the coupling with superconductivity at interfaces. We will use graphene as a surface-accessible 2DEG to perform a pioneering local investigation of normal and superconducting transport through QH edge states. A new and unique hybrid Atomic Force Microscope and Scanning Tunneling Microscope (STM) operating in the extreme conditions required for this physics, i.e. below 0.1 kelvin and up to 14 teslas, will be developed and will allow unprecedented access to the edge of a graphene flake where QH edge states propagate.
Overall, the original combination of magnetotransport measurements with scanning tunnelling spectroscopy will solve fundamental questions on the considerable real-space structure of integer and fractional QH edge states impinged by either normal or superconducting electrodes. Our world-unique approach, which will provide the first STM imaging and spectroscopy of QH edge channels, promises to open a new field of investigation of the local scale physics of the QH effect.","1761412","2015-10-01","2020-09-30"
"QUESTDO","Quantum electronic states in delafossite oxides","Philip David KING","THE UNIVERSITY COURT OF THE UNIVERSITY OF ST ANDREWS","One of the most active challenges of modern solid state physics and chemistry is harnessing the unique and varied physical properties of transition-metal oxides.  From improved electrodes for solar cells to loss-less transmission of power, these compounds hold the potential to transform our daily lives.  Subtle collective quantum states underpin their diverse properties.  These complicate their physical understanding but render them extremely sensitive to their local crystalline environment, offering enormous potential to tune their functional behaviour.  To date, the vast majority of work has focussed on transition-metal oxides based around cubic “perovskite” building blocks.  In contrast, exploiting the layered traingular network of the delafossite structure, the QUESTDO project aims to establish delafossite oxides as a completely novel class of interacting electron system with properties and potential not known in more established systems.  

Its scope bridges three of the most important current themes in condensed matter, investigating and controlling the delicate interplay of (i) frustrated triangular and honeycomb lattice geometries, (ii) interacting electrons, and (iii) effects of strong spin-orbit interactions.  It brings together advanced spectroscopic measurement with precise materials fabrication.  Through these studies, QUESTDO promises critical new insight on the quantum many-body problem in solids, and will advance our understanding and demonstrate atomic-scale control of the physical properties of delafossites.  Ultimately, it seeks to establish new design methodologies for the targeted creation of emergent and topological phases in this little-studied family of transition-metal oxides, paving the route for their further study and ultimate application.","1999825","2017-01-01","2021-12-31"
"QuFerm2D","Quantum simulation of two-dimensional fermionic systems","Giacomo Roati","CONSIGLIO NAZIONALE DELLE RICERCHE","Two-dimensional fermionic systems show remarkable physical properties, not only of interest for fundamental science but also directed towards technological application. Two paradigmatic examples are layered high-Tc superconductors and graphene. However, despite of decades of investigations, their theoretical comprehension is far from being complete.
In QuFerm2D, I propose to use atomic Fermi gases to study the physics of two-dimensional strongly correlated fermions. Indeed, ultracold atoms are “ideal” quantum simulators of many-body phenomena thanks to the unprecedented possibility of controlling most of the relevant parameters. I want to set up a new machine that will benefit of the recent advances in ultracold atomic system, such as single-site addressability and the full control of the interparticle interactions. Tailoring arbitrary optical potentials will create the perfect environment for implementing quantum models.
I want to characterize both the normal and the superfluid phases of layered fermions. At high temperatures I will measure the equation of state to check the validity of the Fermi liquid description, pointing out also the role of fluctuations. In the superfluid regime, I will study the interlayer tunneling, discriminating the coherent Josephson dynamics from the single-particle hopping, and determining the superfluid energy gap. By adding disorder I want to simulate the physics of granular superconductors, testing the robustness of the order parameter and the onset of metallic phases at higher temperatures. The comprehension of these topics will be the natural background to implement the many-body Fermi-Hubbard Hamiltonians in square and honeycomb lattices that are expected to unveil the microscopic mechanisms of high-Tc superconductors and of graphene in presence of strong interactions.
I believe that the successful realization of this project will shed new light on the exciting and interdisciplinary field of strongly correlated fermions.","1243200","2012-11-01","2017-10-31"
"QULIMA","Ensemble based advanced quantum light matter interfaces","Hugues De Riedmatten","FUNDACIO INSTITUT DE CIENCIES FOTONIQUES","The ability to transfer information between light and material memories and processors has led to a technological revolution in the way information is processed and communicated. Now scientists are going one step further by harnessing the coherent and reversible transfer of quantum information between matter and light, enabling the realization of a quantum memory for light. This will allow the realization of quantum information networks, which hold promise for revolutionary advances in information processing. Demonstrations of photonic quantum memories have been reported. Yet, with their limited properties, they can only be seen as proof of principle. The goal of this project is to turn quantum memories into a practical useful quantum device. The main scientific objective is to demonstrate ensemble based novel quantum light matter interfaces with enhanced capabilities and unprecedented properties. Two quantum physical systems will be investigated: solid state quantum memories implemented with rare-earth doped solids and cold atomic gases. These quantum memories will be used to explore new avenues in the quantum control of matter-matter entanglement. Progress beyond the state of the art will go along three ways: (1) the demonstration of long lived and robust entanglement between two remote solid state quantum memories. These systems will facilitate the scalability and integration in large scale quantum networks. (2)  The demonstration of a quantum gate between two collective matter qubits stored in cold atomic ensembles. This requires the quantum control of single collective atomic Rydberg excitations. (3) The demonstration of entanglement between a solid state quantum memory and a cold atomic ensemble. This would provide the first example of entanglement between different quantum material objects and would pave the way towards hybrid quantum networks. These results will open new avenues towards the practical realization of scalable quantum networks and repeaters.","1483618","2011-10-01","2017-05-31"
"quMercury","Ultracold mercury for a measurement of the EDM","Simon STELLMER","RHEINISCHE FRIEDRICH-WILHELMS-UNIVERSITAT BONN","The Standard Model of particle physics (SM), while largely successful, fails to accurately describe the state of the Universe, e.g. with respect to the evident matter/antimatter asymmetry. Various theories seek to conciliate the SM with observations by extending it, and most of these extensions introduce a massive violation of the combined charge invariance and parity (CP) symmetry. The CP violation would reflect in a sizeable permanent electric dipole moment (EDM) of fundamental particles, large enough to be detected by realistic future experiments.

A few pioneering experiments already set out to measure the EDM of neutrons, electrons, or atoms. The most stringent upper limit to any EDM is currently obtained by an experiment based on room-temperature gases of mercury. I propose to take this approach to the quantum world by employing ultracold or even quantum-degenerate mercury samples.

To this end, we will construct a dedicated quantum gas experiment. We will develop advanced cooling methods, obtain the world’s first Bose-Einstein condensate and degenerate Fermi gas of mercury, and introduce vacuum ultraviolet (VUV) lasers to the field. These ground-breaking innovations will increase the coherence time of the sample, enable a higher detection efficiency, and exploit coherent effects, thereby increasing the sensitivity tremendously. Our measurements of the Hg-199 atomic EDM will complement cold-molecule measurements of the electron's EDM.

Technologies developed here can readily be utilized to improve the performance of Hg lattice clocks and will inspire quantum simulations of unique many-body systems.

The principal investigator of this project is highly respected for his pioneering work on degenerate quantum gases of strontium. His current work on a nuclear optical clock introduced him to VUV optics and strengthened his footing in the community. Bringing together his expertise in these two fields – quantum gases and VUV optics – will lead the project to success.","1939263","2018-04-01","2023-03-31"
"QUNNECT","A Fiber Optic Transceiver for Superconducting Qubits","Johannes FINK","INSTITUTE OF SCIENCE AND TECHNOLOGYAUSTRIA","Many researchers in basic science and large IT companies are convinced that superconducting quantum processors will soon help solve complex problems faster, improve optimization and simulation, and boost the progress in artificial intelligence. A worldwide quantum web is the next logical step. It would not only improve communication security, it represents the key to unlock the full potential of the new quantum-computing paradigm. 

Unfortunately, research in optical quantum networks and superconducting devices has progressed largely independently so far. While superconducting qubits are ideally suited for on-chip integration and fast processing, they are problematic for quantum communication. In fact, no solution exists to connect remote qubits via a room temperature link. The small energy scales in the electrical circuit make the fragile information carriers (single microwave photons) susceptible to interference, thermal noise and losses, which has hindered any significant progress in this direction. 

Only just now we have gained sufficient insight into low loss materials, the required fabrication technology, and the precision measurement techniques necessary to bridge the two worlds, by controlling individual photons and phonons quantum coherently. We propose to integrate silicon photonics for low-loss fiber optic communication with superconducting circuits for quantum processing on a single microchip. As intermediary transducer we will focus on two approaches: (1) quantum ground state cooled nanoscale mechanical and (2) low-loss electro-optic nonlinear circuit elements. The novelty of our approach is the tight on-chip integration facilitated by the PIs interdisciplinary background in both, superconducting circuits and silicon nanophotonics. Integration will be the key for realizing a low-loss and high-bandwidth transceiver, for preparing remote entanglement of superconducting qubits, and for extending the range of current fiber optic quantum networks.","1500000","2018-02-01","2023-01-31"
"QUORUMPROBES","An Integrated Chemical Platform to Elucidate Eukaryotic Sensing of Bacterial Crosstalk","Michael Meijler","BEN-GURION UNIVERSITY OF THE NEGEV","The term quorum sensing (QS) describes the ability of a population of unicellular bacteria to act as a single multicellular organism in a cell-density-dependent manner. Bacteria achieve this feat by the use of small diffusible molecules to exchange information among themselves. Examples of QS-controlled behaviors are bioluminescence, virulence factor expression and biofilm formation. These processes are advantageous to a bacterial population only when they are carried out simultaneously by its members. In recent years, a surprising new role has been found for several QS molecules diverse eukaryotes have been found to react strongly to the presence of these compounds. My aim is to examine the hypothesis that diverse eukaryotic species have developed mechanisms to react to the presence of specific bacterial QS molecules in a receptor-mediated fashion. Specifically, we aim to identify receptors that are highly specific for the Pseudomonas aeruginosa QSM 3-oxo-C12-AHL, as no receptor has been identified yet. This is a significant challenge, that we will address developing an innovative platform of chemical, biochemical and microbiological investigations. Identification of specific QSM receptors in eukaryotes will allow us to further understand the complex mechanisms of coexistence and evolution of coexistence between prokaryotes and eukaryotes. The insight obtained from these experiments could lead to: a) an increased understanding of important principles that guide the evolution of symbiotic relationships between competing species; b) new approaches in the treatment of P. aeruginosa infections, as well as to potential new drugs for the treatment of autoimmune diseases; c) the development of an integrated platform that will enable the discovery of unknown receptors for small hydrophobic bioactive compounds.","1392000","2009-12-01","2014-11-30"
"QUPOL","Quantum gases of ultracold polar molecules","Giovanni Modugno","LABORATORIO EUROPEO DI SPETTROSCOPIE NON LINEARI","I propose to realize quantum gases of ultracold molecules with a permanent electric dipole moment. This project aims at a significant extension of the field of ultracold quantum gases towards more complex particles interacting via long-range, anisotropic interactions. Quantum gases of polar molecules would allow to study novel kinds of matter waves, to solve open questions in modern condensed matter physics, to explore novel quantum phases and to implement novel quantum computing schemes. Weakly bound heteronuclear dimers formed in ultracold atomic quantum mixtures via magnetic Feshbach resonances will be coherently transferred to deeply-bound ro-vibrational states using laser fields. The formation of both bosonic and fermionic molecules will be explored in different alkali mixtures. The high degree of coherence of the molecular quantum gases will allow to prepare them in selected rotational states of the absolute ground electronic and vibrational state. The molecular electric dipoles will be manipulated via electric and microwave fields. The precise dynamical control of the shape and strength of the dipole-dipole potential will allow to engineer a variety of quantum states and to study many interdisciplinary phenomena. The following themes will be explored: phenomenology of dipolar quantum gases, lattice spin models, polar molecules as qubits.","1230000","2008-08-01","2013-07-31"
"QUSCO","Quantum superiority with coherent states","Eleni DIAMANTI","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The unique features of quantum mechanics enable communication and computation tasks impossible to achieve by classical means. This opens a tremendous potential for enhancing the power, efficiency and security of everyday interactions in our information-based society. The quest of the demonstration in physical systems of quantum superiority has led to milestone implementations, most notably of the distribution of secret keys with unconditional security or of elementary computations such as the efficient sampling of bosonic distributions. 

In this project, we propose to develop and experimentally demonstrate a framework where quantum resources can be used to outperform their classical counterparts for a much larger range of problems than key distribution or boson sampling, with applications in algorithms, cryptography, communications, scheduling, routing, data mining, process monitoring and control or DNA sequencing. The theoretical basis of our framework is decomposed in three well defined elements-circuits, of increasing complexity, which can individually be used for specific quantum enhanced applications and together lead to the demonstration of the holy grail of quantum information science – quantum superiority for hard computation algorithms. The implementation of all the elements will be based on a photonic experimental platform exploiting a mapping of quantum information protocols involving multiple quantum bits of information to protocols based on coherent states of light in a superposition of optical modes. This is extremely appealing from a practical point of view and will be fully explored, in particular using silicon photonic technologies that allow for scalable devices involving fast switching operations, compact delay lines, and reconfigurable couplers, the main components of our circuits.

Our project sets a highly ambitious target, providing on the way powerful and readily accessible applications of quantum technologies.","1494738","2018-01-01","2022-12-31"
"QUSIMGAS","Quantum Simulation of Many-Body Physics in Ultracold Gases","Lode Corneel Pollet","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","Ultracold atoms in an optical lattice constitute a clean, controlled and tunable implementation of many intractable models in condensed matter physics. In the long run, cold gas experiments may act as quantum simulators and shed new light on those models, and tell us to what extent the real material is described by the model. The prerequisite is, however,  that those experiments are benchmarked and validated against models with known results.  A lot of recent attention was therefore devoted to the quantitative analysis of these experiments: we have shown for instance that interference patterns for bosonic gases in an optical lattice  can be in one-to-one agreement with quantum Monte Carlo simulations. In recent years, as the many-body physics has become more apparent, the numerical component in explaining cold gas experiments has grown stronger. Unfortunately, no numerical methods exist today that can control the answer in the new parameter regimes that gradually become accessible in cold gas experiments.

In this proposal we want to develop novel diagrammatic quantum Monte Carlo methods aiming at controlled answers for intractable models in parameter regimes relevant for cold atoms. By a combination of cluster dynamical mean-field theory and diagrammatic Monte Carlo we will investigate the Hubbard model in the pseudo-gap regime. For fermions with long-range interactions such as dipolar and Coulombic systems, diagrammatic Monte Carlo will also be developed. This can have far reaching consequences for high Tc superconductors, exchange functionals and material science. Sideprojects in this proposal include the study of spinor bosonic systems in higher dimensions, bosonic polar molecules with anisotropic interactions in different geometries, polaronic effects in cold gases, and circuit quantum electrodynamics, where the strong light-matter coupling opens a different avenue for quantum simulation, but with its own challenges.","1489414","2013-03-01","2018-02-28"
"R-EVOLUTION-M-R","Magnetic Resonance of Heterogeneous Matter in Rotating Magnetic Fields: Applications to Living Systems","Dimitrios Sakellariou","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","The goal of this project is to establish the foundations for the next generation diagnostic magnetic resonance for in-vivo studies of biochemical processes. Magnetic resonance, unlike other techniques, probes in a non-invasive manner the local structure and dynamics of chemical and biological function. Many systems (e.g. organisms) are however heterogeneous and, currently, studies by high-resolution NMR spectroscopy necessitate fast sample rotation (thousands of revolutions per second) at the so-called “magic angle” with the magnetic field. Practical, scientific and ethical issues are raised when we extrapolate this technique to small animals or even to living cell cultures.We aim to push the technology of magnetic resonance to the next level by introducing rotating magnetic fields around static samples. Rotating magnets capable of offering ultra high-resolution NMR and MRI will be built and used to acquire metabolic signatures from living organisms at cellular and macroscopic level of observation. This interdisciplinary project aims at developing frontier concepts in magnetic resonance and gathering state-of-the-art approaches in the areas of bioengineering, magnetism, magnetic resonance microscopy and spectroscopy to place them at the service of medicine and biochemistry.Our strategy consists of forming a new, independent and perennial research team, of 3 researchers, 1 technician, 2 post-docs and 2 PhDs. The concept, fabrication and evaluation of a prototype magnet will occur within the first three years, and applications to cell cultures and small animals are envisaged until the end of the 5 years. Spectroscopic MRI in rotating fields has never been investigated and will produce new methodological and instrumentation advances. The outcome will be groundbreaking since for the first time localized metabolism will be studied in living organisms. This research will therefore open new avenues to non-invasive monitoring biomedical tools for diagnosis and prevention.","1827564","2008-11-01","2014-10-31"
"RAC","Randomness and Computation","Ronen Shaltiel","UNIVERSITY OF HAIFA","""This project is concerned with the necessity and availability of randomness in computation. This research area (often referred to as the """"Theory of Derandomization'') is one of the most active and exciting areas in theoretical computer science. We intend to study some of the main questions of this area: In what setups can randomized algorithms be efficiently simulated by deterministic ones? In cases that randomness is essential, how can computers obtain random bits? More specifically, how can computers generate secret random keys for running secure cryptographic protocols? Furthermore, once random keys are generated, how can computers maintain the secrecy of their keys in the presence of side-channel attacks? These are important real-world problems and our research is intended to lay the theoretical foundation for achieving actual solutions.

Our main approach for these problems is to design ``pseudorandom generators'' and ``randomness extractors'' that are efficient algorithms that manipulate randomness in various ways. The PI is one of the leading figures in this research area and many of the concrete research directions suggested in this proposal are related to and build on past work of the PI.

The goals outlined in this proposal are important open problems in this area. Some of them (such as derandomizing bounded memory randomized algorithms and constructing 2-source extractors and dispersers for low min-entropy) are famous longstanding open problems and solving either of them will be a dramatic breakthrough in theoretical computer science. We suggest concrete (and we believe novel) approaches to attack these problems. Along the way we identify important and accessible intermediate goals.

In addition to the development of the theory of derandomization, past work in this area (including work of the PI) had big impact on other areas of Computer Science and Mathematics such as Combinatorics, Cryptography, Coding Theory and Ramsey Theory.""","1178839","2011-10-01","2016-09-30"
"RACCOON","A Rigorous Approach to Consistency in Cloud Databases","Alexey Gotsman","FUNDACION IMDEA SOFTWARE","Modern Internet services store data in novel cloud databases, which partition and replicate the data across a large number of machines and a wide geographical span. To achieve high availability and scalability, cloud databases need to maximise the parallelism of data processing. Unfortunately, this leads them to weaken the guarantees they provide about data consistency to applications. The resulting programming models are very challenging to use correctly, and we currently do not have advanced methods and tools that would help programmers in this task.

The goal of the project is to develop a synergy of novel reasoning methods, static analysis tools and database implementation techniques that maximally exploit parallelism inside cloud databases, while enabling application programmers to ensure correctness. We intend to achieve this by first developing methods for reasoning formally about how weakening the consistency guarantees provided by cloud databases affects application correctness and the parallelism allowed inside the databases. This will build on techniques from the areas of programming languages and software verification. The resulting theory will then serve as a basis for practical implementation techniques and tools that harness database parallelism, but only to the extent such that its side effects do not compromise application correctness.

The proposed project is high-risk, because it aims not only to develop a rigorous theory of consistency in cloud databases, but also to apply it to practical systems design. The project is also high-gain, since it will push the envelope in availability, scalability and cost-effectiveness of cloud databases.","1498312","2017-01-01","2021-12-31"
"RadFeedback","The radiative interstellar medium","Stefanie Walch-Gassner","UNIVERSITAET ZU KOELN","The pressure, radiation, and ionization from the warm (UV emitting) and hot (X-ray emitting) gas has a significant impact on the cold, star-forming interstellar medium. We propose to carry out a comprehensive 3D study of the turbulent, multi-phase ISM in different environments that includes, for the first time, a proper treatment of UV and X-ray emission from stellar (primary) sources and extended (secondary) sources like cooling shock fronts and evaporating clouds. We do this by means of massively parallel, high-resolution 3D simulations that capture the complex interplay of gravity, magnetic fields, feedback from massive stars (ionizing radiation, radiation pressure, stellar winds, supernovae), heating and cooling including X-rays and cosmic rays, and chemistry. We are developing a novel, original and highly efficient method to accurately treat the transfer of radiation from multiple point and extended sources in the 3D simulations. Radiation and chemistry will be coupled to achieve self-consistent heating, cooling, and ionization rates. Moreover, accurate synthetic observations covering the large dynamic range from X-rays down to radio emission will be generated to set the results in the proper observational context. This will enable us to address the key science questions: How efficient is stellar feedback in different environments and which feedback process is dominant? What is the precise role of UV radiation and X-rays, also from secondary sources? Are the observations following the key dynamical players? How do we best interpret ISM observations from ALMA, SKA, or ATHENA? How do we assist in designing future observations? With the resources requested here we will perform the most self-consistent theoretical study of the multi-phase ISM so far, thus building up a leading group for ISM research in Europe. To stimulate worldwide scientific activities and interactions we will make all data available to the community through an open-access web interface.","1488048","2016-09-01","2021-08-31"
"radioforegrounds","Enabling cosmology with radio astronomy surveys: dealing with foreground contamination","Clive Dickinson","THE UNIVERSITY OF MANCHESTER","The most important tool for studying the Universe on the largest scales has been the Cosmic Microwave Background (CMB) radiation. CMB polarization provides a new window on the early Universe and will provide constraints on inflation via gravitational waves. However, the success of future CMB experiments lies in the improved understanding and removal of foreground emission from our own Galaxy. The first part of the project lies in developing our understanding of diffuse Galactic radiation through observation and modelling. This information will be used in simulations and for improving component separation algorithms and for optimising the design of future CMB space missions. A joint analysis of the forthcoming Planck data, combined with state-of-the-art low frequency foreground surveys will be made, to give the most precise CMB power spectrum from Planck.

The future of cosmology at radio wavelengths lies in the mapping of the hyperfine 21cm line of neutral atomic hydrogen (HI). This is one of the key science drivers for several future radio telescopes including the Square Kilometre Array (SKA). Measuring the distribution of HI as a function of redshift will revolutionize cosmology, including dark matter and dark energy. However, like for the CMB, a major obstacle will be Galactic foregrounds, which are at least 3 orders of magnitude brighter than HI. Many of the techniques used for the CMB can be used in HI mapping, including the study of Baryon Acoustic Oscillations (BAO). We will make simulations of HI and foregrounds to study how well the HI signal can be extracted by employing techniques used for CMB data. Guided by these simulations, we will analyze data from telescopes that are soon to come online (e.g. MeerKAT, ASKAP). We will also look to develop new instrumentation to detect BAO such as the Baryon acoustic oscillation with Integrated Neutral Gas Observations (BINGO) experiment. The ultimate goal will be to detect BAO using one of these approaches.","1493227","2013-01-01","2017-12-31"
"RadNu","Radio detection of the PeV - EeV cosmic-neutrino flux","Krijn DE VRIES","VRIJE UNIVERSITEIT BRUSSEL","With the detection of the high-energy cosmic-neutrino flux by the IceCube neutrino observatory at the South-Pole, IceCube opened the field of neutrino astronomy. Nevertheless, due to the steeply falling energy spectrum, IceCube runs low in statistics at energies above a few PeV. To probe this flux at the highest energies (>PeV), therefore asks for an even larger detection volume than the cubic-kilometer currently instrumented by IceCube. 

Due to its long attenuation length the radio signal is an ideal probe to cover such a large volume. When a high-energy cosmic neutrino interacts in a dense medium like ice, a relativistic particle cascade is induced. In 1962 Askaryan already predicted that due to the net charge build-up inside the cascade, coherent radio emission is expected. However, this signal is only detectable for initial neutrino energies in access of a few EeV. Therefore, currently there is a sensitivity gap to probe the high-energy cosmic neutrino flux in the PeV – EeV energy range.

This project aims to fill this sensitivity gap by the development of a novel radio detection technique to measure high-energy particle cascades in dense media, the radar detection technique. By directly probing the ionization plasma which is left behind after the neutrino induced particle cascade propagates through the medium, the radio detection energy threshold is lowered to a few PeV. The feasibility of the radar detection technique, was shown in a recent experiment. To determine the radar scattering efficiency more accurately, a new beam-test at the SLAC facility is planned as part of this proposal.

Once the scattering parameters have been determined accurately, a detailed modeling and sensitivity study will be performed to achieve the main goal of this research proposal: The construction of an in-nature experiment at the South-Pole with the sensitivity to observe 1-10 cosmic neutrino events per year in the PeV – EeV energy range.","1410000","2019-02-01","2024-01-31"
"RAM","Regularity theory for area minimizing currents","Camillo De Lellis","UNIVERSITAT ZURICH","""The Plateau's problem consists in finding the surface of least area spanning a given contour. This question has attracted the attention of many mathematicians in the last two centuries, providing a prototypical problem for several fields of research in mathematics. For hypersurfaces a lot is known about the existence and regularity thanks to the classical works of De Giorgi, Almgren, Fleming, Federer, Simons, Allard, Simon, Schoen and several other authors.

In higher codimension a quite powerful existence theory, the ``theory of currents'', was developed by Federer and Fleming in 1960. The success of this theory relies on its homological flavor and indeed it has found several applications to problems in differential geometry. Many geometric objects which are widely studied in the modern literature are naturally area-minimizing currents: two examples among many are special lagrangians and holomorphic subvarieties. However the understanding of the regularity issues is, compared to the case of hypersurfaces, much poorer. Aside from its intrinsic interest, a good regularity theory is likely to provide more insightful geometric applications. A quite striking example is Taubes' proof of the equivalence between the Gromov and Seiberg-Witten invariants.

A very complicated and far reaching regularity theory has been developed by Almgren thirty years ago in a monumental work of almost 1000 pages. The first part of this project aims at reaching the same conclusions of Almgren with a more flexible and accessible theory. In the second part I wish to go beyond Almgren's work and attack some of the many open questions which still remain in the field.""","919500","2012-09-01","2017-08-31"
"RANDGEOM","Random Geometry","Asaf Nachmias","TEL AVIV UNIVERSITY","The objective of this proposal is an investigation of the geometric structure of random spaces that arise in critical models of statistical physics. The proposal is motivated by inspiring yet non-rigorous predictions from the physics community and the models studied are some of the most popular models in contemporary probability theory such as percolation, random planar maps and random walks.

One set of problems are on the topic of random planar maps and quantum gravity, a thriving field on the intersection of probability, statistical physics, combinatorics and complex analysis. Our goal is to develop a rigorous theory of these maps viewed as surfaces (rather than metric spaces) via their circle packing. The circle packing structure was recently used by the PI and Gurel-Gurevich to show that these maps are a.s. recurrent, resolving a major conjecture in this area. Among other consequences, this research will hopefully lead to progress on the most important open problem in this field: a rigorous proof of the mysterious KPZ correspondence, a conjectural formula from the physics literature allowing to compute dimensions of certain random sets in the usual square lattice from the corresponding dimension in the random geometry. Such a program will hopefully lead to the solution of the most central problems in two-dimensional statistical physics, such as finding the typical displacement of the self-avoiding walk, proving conformal invariance for percolation on the square lattice and many others.

Another set of problems is investigating aspects of universality in critical percolation in various high-dimensional graphs. These graphs include lattices in dimension above 6, Cayley graphs of finitely generated non-amenable groups and also finite graphs such as the complete graph, the Hamming hypercube and expanders. It is believed that critical percolation on these graphs is universal in the sense that the resulting percolated clusters exhibit the same mean-field geometry.","1286150","2016-01-01","2020-12-31"
"RanDM","Randomness and pseudorandomness in discrete mathematics","David-GERARD Conlon","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Discrete mathematics has seen enormous advances in the last few years, with solutions being found to a number of famous and long-standing questions, such as the Erdos distinct distance problem and the existence conjecture for combinatorial designs. Much of this progress owes to an increased understanding of random and pseudorandom objects. An entire framework, known as the probabilistic method, has grown around the application of randomness to combinatorial problems, while pseudorandomness is playing an increasingly important role.

In this proposal, we will consider a range of problems, some stemming from the direct study of random and pseudorandom objects and others arising in areas where randomness and pseudorandomness have proved to be of particular importance. We will be particularly concerned with extensions of the regularity method to sparse graphs and improving bounds for a number of classical problems in graph Ramsey theory. These problems are of a fundamental nature and any progress is likely to lead to new techniques with broader scope for application.","1106719","2016-07-01","2021-06-30"
"RandMat","Spectral Statistics of Structured Random Matrices","Antti Kenneth Viktor KNOWLES","UNIVERSITE DE GENEVE","The purpose of this proposal is a better mathematical understanding of certain classes of large random matrices. Up to very recently, random matrix theory has been mainly focused on mean-field models with independent entries. In this proposal I instead consider random matrices that incorporate some nontrivial structure. I focus on two types of structured random matrices that arise naturally in important applications and lead to a rich mathematical behaviour: (1) random graphs with fixed degrees, such as random regular graphs, and (2) random band matrices, which constitute a good model of disordered quantum Hamiltonians.

The goals are strongly motivated by the applications to spectral graph theory and quantum chaos for (1) and to the physics of conductance in disordered media for (2). Specifically, I will work in the following directions. First, derive precise bounds on the locations of the extremal eigenvalues and the spectral gap, ultimately obtaining their limiting distributions. Second, characterize the spectral statistics in the bulk of the spectrum, using both eigenvalue correlation functions on small scales and linear eigenvalue statistics on intermediate mesoscopic scales. Third, prove the delocalization of eigenvectors and derive the distribution of their components. These results will address several of the most important questions about the structured random matrices (1) and (2), such as expansion properties of random graphs, hallmarks of quantum chaos in random regular graphs, crossovers in the eigenvalue statistics of disordered conductors, and quantum diffusion.

To achieve these goals I will combine tools introduced in my previous work, such as local resampling of graphs and subdiagram resummation techniques, and in addition develop novel, robust techniques to address the more challenging goals. I expect the output of this proposal to contribute significantly to the understanding of structured random matrices.","1257442","2017-01-01","2021-12-31"
"RANDOM-KAHLER","Kähler-Einstein metrics, random point processes and variational principles","Robert Berman","CHALMERS TEKNISKA HOEGSKOLA AB","In broad terms the aim of this proposal is to introduce a new probabilistic approach to the study of Kähler-Einstein (K-E) metrics on complex manifolds. A precise procedure, based on a blend of Statistical Mechanics, Pluripotential theory and Kähler Geometry.  will be used to show that

• when a K-E metric exists on a complex manifold X it can be obtained from the “large
N limit” of certain canonical random point processes on X with N particles.

The canonical point processes are directly defined in terms of algebro-geometric data and the thrust of this approach is thus that it gives a new link between algebraic geometry on one and hand and complex differential (Kähler) geometry on the other. A major motivation for this project comes from the fundamental Yau-Tian-Donaldson conjecture in Kähler geometry, which aims at characterizing the obstructions to the existence of a K-E metric on a Fano manifold in terms of a suitable notion of algebro-geometric “stability”, notably K-Stability. In this project a new “probabilistic/statistical mechanical” version of stability will be introduced referred to as Gibbs stability, which also has an interesting purely algebro-geometric definition in the spirit of the Minimal Model Program in current algebraic geometry and another specific aim of this project is to prove or at least make substantial progress towards proving,

• There is a (unique) K-E metric on a Fano manifold X precisely when X is asymptotically Gibbs stable

The canonical random point processes will be defined as certain “beta-deformations” of determinantal point processes and share certain properties with the ones appearing in Random Matrix Theory and in the study of quantum chaos and zeroes of random polynomials (and random holomorphic sections)  But a crucial new feature here is that the processes are independent of any back-ground data, such as a potential or a metric.","1200000","2013-01-01","2018-06-30"
"RAPID","Rapid parsimonious modelling","Alexander Bronstein","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","Parsimony, manifested as variously structured sparse and low rank representations of data, has been shown as a tremendously successful model in numerous domains of science, including signal and image processing, computer vision, and machine learning problems. Despite this success, parsimonious representation pursuit approaches practiced today face serious limitations stemming from their reliance on iterative optimization. In this project, we propose to develop a novel approach to parsimonious modeling that puts the pursuit process itself at the center, surfacing crucial aspects that are currently lost deep inside the optimization machinery. First, we will study the theoretical performance limitations of pursuit processes constrained by a fixed computational complexity budget, devising bounds on the tradeoff between performance and complexity (in the spirit of the rate-distortion tradeoff). Second, we will develop a principled way to construct families of pursuit processes that approach optimal performance at fixed complexity given a specific input data distribution, and devise tools for learning such processes on real data. Abandoning iterative representation pursuit in favour of a learned fixed-complexity function can lead to a dramatic improvement in performance, enabling previously impossible applications. It will also allow including parsimonious models into higher-level optimization problems, leading to novel modeling capabilities. In lieu of the existing generative parsimonious models, we will develop novel discriminative counterparts for uni- and multi-modal data, and show their utility in large-scale similarity learning. We will also construct efficient parsimonious modeling tools for problems involving unknown data transformation or correspondence. We will apply these methods to several challenging real-world problems in signal processing, computer vision, medical imaging, and multimedia retrieval, which will be developed to the level of prototype systems.","1469200","2013-11-01","2019-08-31"
"RARENOISE","Low-probability, large fluctuations of the noise in detectors of gravitational waves","Livia Conti","ISTITUTO NAZIONALE DI FISICA NUCLEARE","The search of gravitational waves requires astonishingly large and sensitive detectors, with displacement sensitivity approaching the limit set by the Uncertainty Principle. The displacement fluctuations that are continuously monitored by the detectors should follow a Gaussian distribution; on the contrary up to now any gravitational wave detector shows large fluctuations that cause low probability tails superimposing to the Gaussian distribution which is followed for most of the time, thus lowering the chances for detection. This research proposal aims at understanding phenomena that might explain at least part of the low-probability, large fluctuations of the noise of gravitational wave detectors on the basis of fundamental Physics. The research will have large impact on the detection capabilities of present gravitational wave detectors, reducing the false alarm rate, and on the design of their next generations. Table-top experiments will be performed on mechanical systems that will be subjected to conditions similar to that possibly originating the low-probability, large fluctuations of the noise of gravitational wave detectors. A theoretical investigation of the phenomenon will guide the experiments and will attempt to get insight on more general results from the experimental outcome.","1000000","2008-07-01","2013-06-30"
"RASPA","Towards more efficient materials for technological processes","Sofia Calero Diaz","UNIVERSIDAD PABLO DE OLAVIDE","With the increasing need for efficient, energy-saving, and environmentally friendly procedures, adsorbents with tailored structures and tunable surface properties have to be found. To make an informed choice of material for a given application, one must first have knowledge of its adsorption behaviour as a function of molecular composition and morphology. My aim is to provide new insights for material design with a computational investigation of adsorption and diffusion processes in porous materials. As adsorbents I will focus on zeolites because of their high stability and on MOFs because of their structural diversity and versatility. As adsorbates I am interested on chiral molecules such as ibuprofen or limonene (separation of chiral enantiomers), volatile organic compounds –VOCs- (control of VOCs emissions from industrial processes), water, alcohols (solvent dehydration), carbon dioxide and methane (production of cheap and clean fuel from natural gas).

The central focus of this research is that computer simulations can be used not only as a screening tool for known structures, but they can also provide structural design guides even before experimental synthesis. My approach is to perform a classical simulation study to identify the effect of the geometry and the chemical composition of the material on storage/release of molecules and on separation of mixtures. The fundamental information that I am planning to obtain from this study will provide the underlying knowledge from a molecular point of view that may guide to the development of more efficient processes, to fine-tune materials for a particular application and also to steer the experimental effort in successful directions.","1369080","2012-01-01","2016-12-31"
"RAWG","Random walks and Growth of Groups","Anna Ershler","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The goal of this project is to study random walks on groups, with the focus on boundary theory. We plan to establish new criteria for estimates of the entropy and Poisson-Furstenberg boundary triviality and apply this method to study the following question: which groups admit simple random walks with trivial boundary? In particular, we want to produce a classification for classes of
solvable groups, more generally elementary amenable groups, and groups acting on rooted trees. We plan to make a contibution in the solution of the conjecture of Vershik and Kaimanovich, posed in the early eighties, that states that any group of exponential growth admits a symmetric measure with non-trivial boundary. We plan to study applications of random walks to growth of groups. In my previous work I have produced a method to use boundaries in order to obtain new low estimates for groups of Grigorchuk of intermediate growth. We plan to construct new classes of groups of intermediate growth, and to refine the existing method to obtain sharp bounds of the growth function. We also want to address Grigorchuk's conjecture about the gap in the range of possible growth functions of groups. Further applications include large scale geometrical properties of amenable groups, including amenable groups acting on rooted trees, as well as groups of orientation preserving diffeomorphisms of the interval, in particular, Richard Thompson group F","856320","2010-09-01","2015-08-31"
"RDC@CATALYSIS","Structure and dynamics of catalytically active species from Residual Dipolar Couplings","Christina Marie Thiele","TECHNISCHE UNIVERSITAT DARMSTADT","The main goal of chemistry and material science is to create structures, which in turn exhibit specific
functions. One class of functional molecules are catalysts. Although homogeneous catalysts are widely used
in synthetic chemistry, there is very limited knowledge about the relationship between their activity and their
structure and especially dynamics in solution.
Based on a first investigation, in which we showed that only after investigating the structural and dynamic
properties of a photoswitchable organocatalyst using residual dipolar couplings (RDCs), its reactivity could
be understood, it shall be demonstrated within this project, that additional and more accurate structural
information on catalytically active species is accessible from RDCs leading to an improved understanding of
catalysis. This will finally allow rational catalyst design.
The structure in solution and information about solution dynamics is usually obtained from solution state
NMR spectroscopy. In contrast to the short-range structural information obtained from the conventional
NMR parameters the information obtained from the recently introduced RDCs is global, which allows the
relation of non-interacting parts of compounds. RDCs furthermore contain information on dynamics of the
system investigated. Especially the time scale on motional information that is accessible is intriguing.
With this unique quality of structural information from RDCs in hand, precise structural models for
catalytically active species can be constructed. This will certainly lead to a better understanding of catalytic
processes and in the end revolutionize catalyst design.","1498200","2010-11-01","2016-04-30"
"Re-SENSE","RESOURCE-EFFICIENT SENSING THROUGH DYNAMIC ATTENTION-SCALABILITY","Marian Kristien VERHELST","KATHOLIEKE UNIVERSITEIT LEUVEN","It is hard to stand on one leg if we close our eyes. We have trouble tasting food without smelling. And when we talk with other people, we observe their lips to understand them better. We, humans, are masters in sensor fusion as we can seamlessly combine information coming from different senses to improve our judgements. Intriguingly, in order to fuse information efficiently, we do not always devote the same level of attention or mental effort to each of the many sensory streams available to us. This dynamic attention-scalability allows us to always extract the maximum amount of relevant information under our limited human computational bandwidth.

Would it not be great if electronics had the same capabilities? While many devices are nowadays equipped with a massive amount of sensors, they typically cannot effectively fuse more than a few of them. The rigid way in which sensory data is combined results in large computational workloads, preventing effective multi-sensor fusion in resource-constrained applications such as robotics, wearables, biomedical monitoring or user interfacing.

The Re-SENSE project will bring attention-scalable sensing to resource-scarce devices, which are constrained in terms of energy, throughput, latency or memory resources. This is achieved by jointly:
1) Developing resource-aware inference and fusion algorithms, which maximize information capture in function of hardware resource usage, dynamically tuning sensory attention levels
2) Implementing dynamic, wide-range resource-scalable inference processors, allowing to exploit this attention-scalability for drastically improved efficiency
The attention-scalable sensing concept will be demonstrated in 2 highly resource-constrained applications: a) latency-critical cell sorting and b) energy-critical epilepsy monitoring.
This combination of processor design, reconfigurable hardware and embedded machine learning fits perfectly to the PI’s expertise gained at Intel Labs, UC Berkeley and KULeuven.","1484562","2017-03-01","2022-02-28"
"REACT","Realizable Advanced Cryptography","Zvika BRAKERSKI","WEIZMANN INSTITUTE OF SCIENCE LTD","In a free society, there is persistent tension between utility and privacy. Citizens have the basic right to keep their personal information private. However, sometimes keeping our data private could significantly reduce our ability to use this data to benefit ourselves or society. This tension is multiplied many times over in our modern data driven society, where data is utilized using remote algorithms. 

State of the art research suggests that new advanced cryptographic primitives can mitigate this tension. These include computing on encrypted data via fully homomorphic encryption, fine grained access control to encrypted data via attribute based encryption, and most recently general purpose program obfuscation, which on paper can solve many of cryptography's long standing problems. However, these primitives are largely either too complicated or not sufficiently founded to be considered for real world applications. 

Project REACT will apply foundational theoretical study towards removing the barriers between advanced cryptographic primitives and reality. My viewpoint, supported by my prior research success, is that orders-of-magnitude improvement in efficiency and security requires foundational theoretical study, rather than focusing on optimizations or heuristics. My projection is that progress in this direction will both allow for future realistic implementation of these primitives, reducing said tension, as well as contribute to basic cryptographic study by opening new avenues for future research.

To achieve this goal, I will pursue the following objectives: (i) Studying the computational complexity of underlying hardness assumptions, specifically lattice based, to better understand the level of security we can expect of proposed primitives. (ii) Simplifying and extending the LWE/trapdoor paradigm that underlies many of the new primitives, and that I find incomplete. (iii) Constructing cryptographic graded encoding schemes and obfuscators.","1493803","2017-10-01","2022-09-30"
"REACT","REsponsive theranostic nanosystems for Advanced Cancer Treatment","Eduardo RUIZ-HERNANDEZ","THE PROVOST, FELLOWS, FOUNDATION SCHOLARS & THE OTHER MEMBERS OF BOARD OF THE COLLEGE OF THE HOLY & UNDIVIDED TRINITY OF QUEEN ELIZABETH NEAR DUBLIN","REACT aims to dramatically impact the targeted release of diagnostic agents and drugs with nanomedicines that respond to biological cues or changing pathophysiological conditions, thus enabling ultrasensitive diagnosis and exquisite therapy selectivity. Nanomedicine research against cancer focuses on the local targeted delivery of chemotherapeutics to enhance drug efficacy and reduce side effects. Despite all the efforts in the design of chemotherapeutic agents as nanomedicines, hardly any improvement has been translated into benefits for patients’ survival. There is an urgent need for improved carrier systems able to deliver high doses of diagnostic agents and anti-cancer drugs to the tumor. Stimuli responsive carriers are promising candidates since the release of the cargo can be triggered locally in the tumor environment. Currently, there exists an unparalleled effort to identify genes, proteins and metabolites implicated in human disease and utilize systems biology and mathematical approaches in order to develop new prognostic tools for the treatment of cancer and develop more targeted therapies for patients. As an expert in drug delivery systems, the PI intends to bring all these efforts and advances into the design of stimuli responsive organic-inorganic hybrid nanoparticles that can adapt their response to the biological milieu. The novel engineered delivery systems will consist of an inorganic porous matrix surface-modified with tumor-specific molecules with the ability to sense changes in the environmental conditions and react by providing a proportional release. These nanosystems can potentially be employed for early in vitro diagnosis through effective screening of deadly tumors, such as neuroblastoma and glioblastoma. Moreover, through the sustained delivery of the nanosystems from injectable gels that can be locally implanted in patients at risk of developing a tumor, a clinically relevant tool for in vivo diagnosis and targeted therapy can be achieved.","1498346","2019-01-01","2023-12-31"
"Real-PIM-System","Memristive In-Memory Processing System","shahar KVATINSKY","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","Our project aims to develop a new computer architecture that enables true in-memory processing based on a unit that can both store and process data using the same cells. This unit, called a memristive memory processing unit (mMPU), will substantially reduce the necessity to move data in computing systems, solving the two main bottlenecks exist in current computing systems, i.e., speed ('memory wall') and energy efficiency ('power wall'). Emerging memory technologies, namely memristive devices, are the enablers of the mMPU. While memristors are naturally used as memory, these novel devices can also perform logical operations using a technique we have invented called Memristor Aided Logic (MAGIC). This combination is the basis of mMPU.
The goal of this research is to design a fully functional mMPU, and by that, to demonstrate a real computing system with significantly improved performance and energy efficiency. We have identified four main research tasks which must be completed to demonstrate a full system utilizing mMPU: mMPU design, system architecture and software, modeling and evaluation, and fabrication.  Both memristive memory array and mMPU control will be designed and optimized for different technologies in the first objective. The second objective will deal with the different aspects of the system, including programming model, different mMPU modes of operation and their corresponding system implications, compiler and operating systems. For system evaluation, we will develop models and tools in the third objective in order to measure the performance, area and energy and to compare them to other state-of-the-art computing systems. Lastly, we will fabricate the different parts of the system to demonstrate the full system.
Encouraged from our preliminary experimental results, we expect to achieve 10X improvement in performance, and 100X improvement in energy efficiency as compared to state-of-the-art von Neumann systems when working with appropriate workloads.","1500000","2018-01-01","2022-12-31"
"realFlow","Virtualization of Real Flows for Animation and Simulation","Nils Thuerey","TECHNISCHE UNIVERSITAET MUENCHEN","Besides their application in a very broad range of engineering disciplines, physically-based simulations have become a prevalent tool for all forms of computer-generated environments. Simulations of fluids are a particularly important area that has applications ranging from the design of aerodynamic bodies to the realization of impressive visual effects in movies.  However, despite their extremely wide-spread use there are fundamental problems: a lack of practical flow capturing technologies and the inherent difficulties of resolving crucial features such as turbulence. These issues are show-stoppers preventing the area from realizing its full potential for animations and interactive environments.

The goal of this research is to address these central problems: 1) by designing novel data-centric approaches for fluid simulations to enable re-use, and 2) by establishing an innovative pipeline for capturing flow motions yielding details that were previously unachievable.  To this end, I plan to design algorithms that tightly integrate accurate  physics solvers with methods for the reconstruction of motion from volumetric images. At the same time, I will work on novel structured representations that decompose complex flows into hierarchical space-time structures. These structured virtual counter-parts will not only enable powerful avenues for intuitive editing, but will also serve as a basis for robust and efficient data-driven fluid simulations. This virtualization of flows opens the door for interactive design tools and medical applications, allowing to incorporate complex physical constraints.  In addition, this research could radically change the way we work with synthetic and captured flow data, and has significant potential to break new ground in terms of data-centric work-flows for fluid simulations.","1465604","2015-05-01","2020-04-30"
"REALITY CG","Computer Graphics of the Real World - Realistic Rendering, Modelling, and Editing of Dynamic, Complex Natural Scenes","Marcus Andreas Magnor","TECHNISCHE UNIVERSITAET BRAUNSCHWEIG","Scope of this project is to pioneer a novel concept for rendering, modelling, and editing in computer graphics. Instead of relying on manually created models of virtual worlds, I propose to investigate an alternative approach based on real-world models acquired using synchronized, calibrated multi-view video recordings. Challenges of this real world-based approach concern rendering quality, model quality, and editing capabilities.
As its first goal, the project will complement existing model reconstruction approaches with advanced error-concealed rendering algorithms to obtain visually realistic rendering results from only approximate models. The second goal consists of improving visual quality of reconstructed models by perception-aware modelling methods. The third goal is to endow real world-based modelling with the degree of flexibility required for digital content creation by pioneering intuitive 2D image editing as a new, intuitive 3D digital content creation concept.
The project s comprehensive scope, unprecedented approach, and innovative methodology make it frontier research. Key innovations of the project include EEG-based methods to assess rendering quality in computer graphics, error-concealed rendering algorithms to mask visual artefacts, introduction of perceptual constraints into model reconstruction, and the notion of using 2D image retouching and editing methods for 3D digital content creation.
The proposed research will provide enabling technology to open up the real world to computer graphics methodology and applications. By extending the scope of computer graphics beyond virtual scenes, the project aims at making a profound impact on the field of visual computing, pioneering new research directions as well as break ground for novel applications areas.","1494731","2011-02-01","2016-01-31"
"RealTCut","Towards real time multiscale simulation of cutting in non-linear materials 
with applications to surgical simulation and computer guided surgery","Stéphane Pierre Alain Bordas","UNIVERSITE DU LUXEMBOURG","""Surgeons are trained as apprentices. Some conditions are rarely encountered and surgeons will only be trained in the specific skills associated with a given situation if they come across it. At the end of their residency, it is hoped that they will have faced sufficiently many cases to be competent. This can be dangerous to the patients.
If we were able to reproduce faithfully, in a virtual environment, the audio, visual and haptic experience of a surgeon as they prod, pull and incise tissue, then, surgeons would not have to train on cadavers, phantoms, or on the patients themselves.
Only a few researchers in the Computational Mechanics community have attacked the mechanical problems related to surgical simulation, so that mechanical faithfulness is not on par with audiovisual. This lack of fidelity in the reproduction of surgical acts such as cutting may explain why most surgeons who tested existing simulators report that the """"sensation"""" fed back to them remains unrealistic. To date, the proposers are not aware of Computational Mechanics solutions addressing, at the same time, geometrical faithfulness, material realism, evolving cuts and quality control of the solution.
The measurable objectives for this research are as follows:
O1:Significantly alleviate the mesh generation and regeneration burden to represent organs’ geometries, underlying tissue microstructure and cuts with sufficient accuracy but minimal user intervention
O2:Move away from simplistic coarse-scale material models by deducing tissue rupture at the organ level from constitutive (e.g. damage) and contact models designed at the meso and micro scales
O3:Ensure real-time results through model order reduction coupled with the multi-scale fracture tools of O2
O4:Control solution accuracy and validate against a range of biomechanics problems including real-life brain surgery interventions with the available at our collaborators’""","1343955","2012-01-01","2016-12-31"
"REALUMAN","Real uniruled manifolds","Jean-Yves Welschinger","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The following list of questions describe the four main directions which I want to develop.
1) Topology of real uniruled manifolds.
May the connected sum of two closed hyperbolic manifolds of dimension at least three be Lagrangian embedded in a uniruled symplectic manifold? Being able to answer to this question through the negative using the symplectic field theory introduced by Eliashberg-Givental and Hofer requires to understand pseudo-holomorphic curves in the cotangent bundle of such a connected sum. For this purpose, one needs some understanding of closed geodesics on such manifolds. Conversely, what are the simplest real three-dimensional projective manifolds which have hyperbolic or SOL manifolds in their real loci?
2) Enumerative problems in real uniruled manifolds.
Is it possible to extract integer valued invariants from the count of real rational curves of given degree in the projective three-space (for instance) which interpolate an adequate number of real lines? Same question in dimensions greater than three for curves passing through points.
3) Lagrangian strings in symplectic manifolds.
I would like to investigate the interactions between closed Lagrangian strings and open Lagrangian strings in symplectic manifolds. These strings -which I recently introduced- interact through holomorphic disks both punctured on their boundaries and interiors. What can be the analogous TQFT associated to coherent sheaves on complex projective manifolds? How are these strings related to Gromov-Witten invariants?
4) Volume of linear systems of real divisors.
The theory of closed positive currents provides probabilistic informations on the topology of real hypersurfaces in Kähler manifolds. I want to push a work in progress as far as possible in this subject.","932626","2010-12-01","2015-11-30"
"RECOGNIZE","Physical principles of recognition in the immune system","Aleksandra Maria Walczak","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""Receptor proteins on the surfaces of B- and T-cells interact with pathogens, recognize them and initiate an immune response. The diversity and complexity of immune receptors poses a challenge to nonequilibrium many-body physics and our understanding of the physical principles that control the emergent functional properties of biological systems, such as recognition. The diversity of the composition of the immune repertoire emerges as a self-organized process, stimulated by interactions with the environment. The goal of the proposed research is to study the self-organization of the immune repertoire in the face of its pathogenic environment at the molecular and evolutionary level, by using a combination of data analysis and statistical mechanics modeling.

Recent experiments have determined the set of B-cell receptors found in a zebrafish and T-cells in humans – data that allows for theoretical analysis and hypotheses rejection that were never possible before. I will theoretically study the problem of recognition from four unique and complementary directions:
- guided by statistical signatures in the data I will propose evolutionary models of how selection and mutation in the sequences lead from the genomic precursors to a functional repertoire of receptors,
- I will quantify, under simplifying assumptions, the question of the optimal repertoire for recognition in a varying but partially predictable pathogenic environment using maximum likelihood,
- analyzing sequence data I will build probabilistic models to characterize the molecular scenarios that generate the repertoire,
- I will use information theory and statistical methods to build data-driven models of the molecular nature of recognition based on yeast display experiments.

Describing interactions between elements of receptor sequences will be an important step towards a physical understanding of recognition in the immune system, a crucial concept in grasping the onset of allergies and auto-immune diseases.""","1267914","2012-11-01","2017-10-31"
"RECONFMATTER","From colloidal joints to reconfigurable matter","Daniela Jutta KRAFT","UNIVERSITEIT LEIDEN","Self-assembly of colloidal particles has emerged as the most promising strategy to obtain fundamental insights into otherwise prohibitively complex systems as well as to create new functional materials from the bottom up. However, most self-assembled colloidal structures are static and thus limited in their functionality. 

Building on our recent discovery of colloidal joints, which enable a hinging-like motion between linked particles, I propose to unravel how such flexible bonds can be leveraged to obtain reconfigurable materials with unprecedented properties. I will investigate the impact of bond flexibility on the self-assembly, (multi-) stable configurations and phase behaviour of reconfigurable colloidal structures, and use these insights to create next generation materials that adapt their shape and thus functionality to external cues. 

To reach these goals, the project will consist of three work packages:
1) I will elucidate how bond flexibility can be exploited to create and understand reconfigurable structures. 
2) I will unravel the phase behaviour and hierarchical assembly of flexible colloidal molecules. 
3) I will introduce active and actuatable elements to control switching between different configurations and create shape-changing and self-propelled structures. 

Taking the concept of reconfigurability to the colloidal length scale will not only allow us to investigate the principles and consequences of structural flexibility on thermally excited objects, but also to develop the next generation of smart materials: materials with an adaptable shape and thus properties. These reconfigurable and actuatable structures have great potential for materials science and in biomedicine as they may feature switchable optical and acoustic properties, and ultimately could be employed in sensors, actuators, advanced coatings, and more complex functional devices such as micro-robots.","1499956","2017-10-01","2022-09-30"
"RECONMET","Reconstruction of methane flux from lakes: development and application of a new approach","Oliver Heiri","UNIVERSITAET BERN","Reconstruction of methane flux from lakes: development and application of a new approach","1554000","2009-12-01","2014-11-30"
"RECOSAMP","Sampling and Reconstruction driven by Sparsity Models with Applications in Sensor Networks and Neuroscience","Pier Luigi Dragotti","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","The problem of reconstructing or estimating partially observed or
sampled signals is animportant one that finds application
in many areas of signal processing and communications. Traditional
acquisition and reconstruction approaches are heavily influences by
classical Shannon sampling theory which gives an exact sampling
and interpolation formula for bandlimited signals. Recently, the
emerging theory of sparse sampling has challenged the way
we think about signal acquisition and has demonstrated that, by
using more sophisticated signal models, it is possible to break away
from the need to sample signals at the Nyquist rate.
The insight that
sub-Nyquist sampling can, under some circumstances, allow perfect
reconstruction is revolutionizing signal processing, communications
and inverse problems.
Given the
ubiquity of the sampling process, the implications of these new
research developments are far reaching.

This project is based on the applicant's recent work on the sampling
of sparse continuous-time signals and aims to extend the existing theory to include more
general signal models that are closer to the physical
characteristics of real data, to explore new domains where sparsity
and sampling can be effectively used and  to provide a set
of new fast algorithms with clear and predictable performance.
As
part of this work, he will also consider timely important problems
such as the localization of diffusive sources in sensor networks and
the analysis of neuronal signals of the brain. He will, for the
first time, pose these as sparse sampling problems and in this way
he expects to develop technologies with a step change in
performance.","1451162","2011-11-01","2016-10-31"
"REDOX SHIELDS","Protection of Redox Catalysts for Cathodic Processes in Redox Matrices.","Nicolas PLUMERE","RUHR-UNIVERSITAET BOCHUM","Biological or molecular catalysts built from Earth-abundant elements are envisioned as economically viable alternatives to the scarce noble metals that are currently used in renewable energy conversion. However, their fragility and O2 sensitivity have been obstacles to their adoption in industry. We have recently proposed O2 quenching matrices for protecting intrinsically O2-sensitive catalysts for use in anodic (oxidative) processes. We have demonstrated that even hydrogenases, the highly sensitive metalloenzymes that oxidize H2, can be used under the harsh conditions encountered in operating fuel cells. However, attempts to reverse the concept for the protection of cathodic (reductive) processes, such as H2 evolution, have been unsuccessful so far. In this case, the electrode generates the reducing agents in the form of electrons, which are needed for both H2 generation and reductive O2 quenching. The competition between the two reactions results in insufficient protection from O2 and deactivation of the catalyst.
The objective is to design an alternative electron pathway that relies on H2 as a charge carrier to efficiently shuttle the reductive force to the matrix boundaries and quench the incoming O2. We will develop novel electron mediators with dual functionalities to enable the reversible H2/H+ interconversion and to achieve the complete reduction of O2 to water. We will focus on organic systems, as well as metal complexes based on Earth-abundant elements with tunable ligand spheres, to adjust their redox potentials for the desired direction of the electron flow and toward fast O2 reduction kinetics. The synthetic efforts will be supported by electrochemical modelling to predict the required properties of the redox matrix for efficient protection. After establishing the protection principle, we will demonstrate its practical use for implementing sensitive bio-catalysts for electrochemical H2 evolution under conditions relevant to energy conversion processes.","1483093","2017-03-01","2022-02-28"
"ReduceSearch","Rigorous Search Space Reduction","Bart Maarten Paul JANSEN","TECHNISCHE UNIVERSITEIT EINDHOVEN","In our world of big data and theoretically intractable problems, automated preprocessing to simplify problem formulations before solving them algorithmically is growing ever more important. Suitable preprocessing has the potential to reduce computation times from days to seconds. In the last 15 years, a framework for rigorously studying the power and limitations of efficient preprocessing has been developed. The resulting theory of kernelization is full of deep theorems, but it has overshot its goal: it does not explain the empirical success of preprocessing algorithms, and most questions it poses do not lead to the identification of preprocessing techniques that are useful in practice. This crucial flaw stems from the fact that the theoretical kernelization framework does not address the main experimentally observed cause of algorithmic speed-ups: a reduction in the search space of the subsequently applied problem-solving algorithm. 

REDUCESEARCH will re-shape the theory of effective preprocessing with a focus on search-space reduction. The goal is to develop a toolkit of algorithmic preprocessing techniques that reduce the search space, along with rigorous mathematical guarantees on the amount of search-space reduction that is achieved in terms of quantifiable properties of the input. The three main algorithmic strategies are: (1) reducing the size of the solution that the solver has to find, by already identifying parts of the solution during the preprocessing phase; (2) splitting the search space into parts with limited interaction, which can be solved independently; and (3) identifying redundant constraints and variables in a problem formulation, which can be eliminated without changing the answer. 

This will raise the scientific study of preprocessing to the next level. Since physical limits form a barrier to further speeding up computer hardware, future advances in computing power rely on algorithmic breakthroughs as envisioned here.","1473020","2019-01-01","2023-12-31"
"ReEngineeringCancer","Re-engineering the tumor microenvironment to alleviate mechanical stresses and improve chemotherapy","Triantafyllos Stylianopoulos","UNIVERSITY OF CYPRUS","Current chemotherapeutic agents are potent enough to kill cancer cells. Nonetheless, failure of chemotherapies for many cancers (e.g. breast and pancreatic cancers and various sarcomas) is primarily because these agents cannot reach cancer cells in amounts sufficient to cause complete cure. The abnormal microenvironment of these tumors drastically reduces perfusion and results in insufficient delivery of therapeutic agents. Tumor structural abnormalities is in large part an effect of mechanical stresses developed within the tumor due to unchecked cancer cell proliferation that strains the tumor microenvironment. Alleviation of these stresses has the potential to normalize the tumor, enhance delivery of drugs and improve treatment efficacy. Here, I propose to test the hypothesis that re-engineering the tumor microenvironment with stress-alleviating drugs has the potential to enhance chemotherapy. To explore this hypothesis, I will make use of a mixture of cutting-edge computational and experimental techniques. I will develop sophisticated models for the biomechanical response of tumors to analyze how stresses are generated and transmitted during tumor progression. Subsequently, I will perform animal studies to validate model predictions and indentify the drug that more effectively alleviates stress levels, normalizes the tumor microenvironment and improves chemotherapy. Successful completion of this research will reveal the mechanisms for stress generation and storage in tumors and will lead to new strategies for the use of chemotherapy.","1440360","2014-01-01","2018-12-31"
"Regenerate","Self-regenerating Functional Surfaces – Towards a Technology Platform for New Materials and Devices","Karen Lienkamp","ALBERT-LUDWIGS-UNIVERSITAET FREIBURG","Self-regenerating materials are intriguing. If critical properties of a failing material or device could be easily restored, this would significantly extend its life time. In this proposal, we describe our vision of a technology platform for the self-regeneration of surface properties. Functional surfaces fail due to damage or contamination. Instead of trying to make “better” functional materials, we here propose a fundamental change in the design of such materials. We want to make a material that can shed its entire contaminated or defective layer, like a reptile shedding its skin, and present a clean, functional one. By shedding defined layers, instead of just eroding a polymer surface, the emerging surface stays homo¬geneous, and cavities or crater-like erosion are avoided. Further¬more, our vision is a material where we can selectively disintegrate the topmost degradable layer of a stack, while the other degradable layers remain in place. That way, the process can be repeated until all layers of the stack have been sequentially removed. The selective and sequential shedding of polymer layers from a multi-stack, with the aim of obtaining functionally intact successive functional layers, is so far an unresolved and therefore extremely attractive, fundamentally important concept, both from a basic science and an application point of view. We want to demonstrate the feasibility of our concept with two research objectives: (1) the sequential regeneration of a functional surface property, exemplified by antimicrobial activity; (2) the regeneration of the activity of a functional device, exemplified by a glucose sensor. We will assemble our target materials, and study their properties and disassembly using surface plasmon resonance/optical waveguide spectroscopy (SPR/OWG), atomic force microscopy, ellipsometry, and other relevant techniques. We will also develop a method to measure water diffusion through thin films using SPR/OWG.","1498988","2015-05-01","2020-04-30"
"REINVENT","REsummation-Improved moNtecarlo eVEnt geNeraTor","Simone ALIOLI","UNIVERSITA' DEGLI STUDI DI MILANO-BICOCCA","With the start of the second run of the Large Hadron Collider and the discovery of a particle compatible with the Standard Model Higgs boson, the high-energy particle physics community faces the task to carry out precise measurements of the properties of this new particle, in order to establish its nature. At the same time, it will be equally important to keep looking for the yet elusive signs of New Physics. Both tasks rely on the ability to accurately predict the expected signals and to disentangle them from the known backgrounds. At  hadronic colliders like the LHC, accurate modeling of the strong interactions is crucial to interpret the experimental outcomes.

The goal of this project is to push forward the frontier of precision QCD for event simulations.  The key idea is to combine the three possible theoretical description (fixed-order perturbative expansion, resummed calculations and parton showers) into the same theoretical framework, in order to benefit from the advantages of each. The innovative approach proposed here improves over past efforts thanks to the inclusion of higher-logarithmic resummation, which bridges the gap between the perturbative description of hard radiation and the shower domain.  This brings together three important advantages: the ability to use the best theoretical  description in each region, the sizable reduction of the theoretical uncertainties gained by replacing the shower evolution with the higher-logarithmic resummation, and the ability to  produce hadron-level events that can be directly interfaced to detector simulations.

By going beyond the state-of-the-art,  REINVENT will obtain the most precise theoretical predictions for  the LHC in an event generator form that allows for direct comparison to data, producing tools that will be used by both experimentalists and theorists. The technology developed for this  project will also have important applications for precision studies at future lepton colliders.","1500000","2017-11-01","2022-10-31"
"RELOS","Reducing empiricism in luminescence geochronology: Understanding the origins of luminescence from individual sand grains","Jan-Pieter Oswald Carolus Buylaert","DANMARKS TEKNISKE UNIVERSITET","Sediments preserve a history of the evolution of the Earth’s surface and its response to a changing climate – a history that can only be read reliably if we know the age of the sediments. Luminescence dating is widely used in quaternary geology and archaeology, and is applicable to almost all sediments from the last 0.5 Ma – it dates the last time the sediment grains were exposed to daylight. RELOS will improve the reliability of luminescence dating by determining the sources of unexpected spread (over-dispersion) in measured doses derived from sand-sized grains. New hypotheses concerning charge imbalance, charge transport and dose calibration of luminescence signals will be tested by: (i) quantifying the effect of grain size and irradiation geometry/quality on grain-to-grain dose dispersion, and particularly the importance of charge particle equilibrium at these scales; (ii) quantifying dispersion arising from grain-to-grain variations in environmental dose rate; (iii) developing measurement procedures giving the same luminescence response per unit dose as in nature; (iv) developing  a dispersion budget and new conceptual/numerical models for luminescence production based on (i) to (iii); and (v) testing the results of these investigations using well-defined natural samples. This project investigates fundamental issues of charge (de)trapping and recombination at small scales that have been completely ignored in previous studies, and problems of luminescence response that are sidestepped in the literature, in part by the unsatisfactory approach of arbitrary data rejection. These studies will result in major improvements in our understanding of the small-scale dosimetry of mixed radiation fields and a step change in the reliability of single-grain luminescence ages. The project links these fundamental studies to clear outcomes of considerable potential value to a variety of fields including earth sciences, archaeology and palaeoanthropology.","1314963","2015-09-01","2020-08-31"
"RelRepDist","Relative representation theory and distributions  on reductive groups over local fields","Dmitry Gourevitch","WEIZMANN INSTITUTE OF SCIENCE","One can view the representation theory of a topological group as non-commutative harmonic analysis on the group. For compact groups this view is justified by the Peter-Weyl theorem. The relative representation theory of a group is harmonic analyses on spaces with transitive group action.

I work in relative representation theory of reductive (algebraic) groups over local fields, e.g. the general linear group over the  field of real numbers or the  field of p-adic numbers. This theory has applications to the theory of automorphic forms, in particular to the relative trace formula. 

There are many similarities between the real and p-adic cases, and some results can be formulated uniformly for all local fields, but their proofs are usually specific to each type of local fields.  An important tool in this theory, that is applicable for all local fields, is the analysis of equivariant  distributions on the group. However, this analysis is quite different for the two kinds of fields. 

In the first part of this proposal I describe my ongoing work on some tools that will help to  approach invariant distributions uniformly for all fields. I also propose to advance, using those tools, towards the proofs of some long-standing conjectures on density of orbital integrals, comparison of Lie algebra homologies, and classification of (non-compact) Gelfand pairs. 

The second part of this proposal concerns generalized Whittaker models, or equivalently harmonic analyses on the quotient of a reductive group by a unipotent subgroup. In 1987 Moeglen and Waldspurger comprehensively described the role of a representation in this harmonic analyses in terms of a certain collection of nilpotent orbits attached to this representation. This result, as well as previous results on Whittaker models have many applications in representation theory and in the theory of automorphic forms. I propose to obtain  an archimedean analog of this result.","1196215","2015-03-01","2020-02-29"
"REM","Resonant Electromagnetic Microscopy: Imaging Cells Electronically","Mehmet Selim HANAY","BILKENT UNIVERSITESI VAKIF","Microfluidics technology has been quite successful in fabricating small, low-cost devices with excellent analyte handling capabilities. However, the main detection paradigm in microfluidics has still been optical microscopy — which is a bulky and expensive technique. A chip-scale detection scheme that can provide multidimensional information is much needed for the widespread adoption of lab-on-a-chip technology. So far, successful capacitive and resonant electrical sensors have been deployed in the field; yet the focus of these sensors has been to obtain the electrical volume or location of a particle — which constitutes only a limited piece of information about the analytes. Here we propose to redesign and utilize resonant electrical sensors in a radically different way to obtain images of cells in a microfluidic channel. The technique proposed can also multiplex on-chip cytometry greatly, accomplish low-cost and high-throughput single-cell transit-time characterization, obtain not only the electrical but also the geometrical size of analytes, determine the dielectric permittivity of analytes, in addition to capturing 1D profile or 2D images of cells. At the basic science level, the project will enhance our understanding of the interaction of electromagnetic fields and living matter at the single cell level and may provide new insights on cell motility, growth and mechanics.","1500000","2018-02-01","2023-01-31"
"REMOTE","Real-time monitoring of load induced remodeling in tissue-engineered bone","Sandra Hofmann Boss","TECHNISCHE UNIVERSITEIT EINDHOVEN","The maintenance of the skeleton is tightly coupled with balanced bone formation and resorption processes that are mediated by osteoblasts and osteoclasts, respectively. Loss of this balance results in skeletal pathologies representing some of the most significant public health threats faced by the growing and ageing population. Tissue engineering investigates various health aspects such as drug development, fundamental research and regenerative medicine. State-of-the-art approaches are lacking to mimic one essential functional property of bone: to adapt its 3D morphology according to imposed mechanical loads. As most drugs for skeletal diseases act on this anabolic-catabolic balance, an engineered system serving as a human in vitro model for drug discovery/testing needs to be able to mimic this process. This proposal aims at combining real-time monitoring of mineralized extracellular matrix with bone tissue engineering culture standards in advanced bioreactors and will design a reliable 3D in vitro model system to mimic load induced remodeling of tissue-engineered human bone. The following particulars will be systematically addressed: i) Establishment of a co-culture of human bone-forming cells and human bone resorbing cells capable of mimicking bone remodeling; ii) Real-time monitoring platform in 3D in order to take the temporo-spatial development of the tissue into account and to allow specific adapted and controlled interventions depending on the actual environmental situation; iii) Quantitative simulation of morphological bone adaptation induced by mechanical load. The proposed research activity will have important implications in fields ranging from pharmacology and biotechnology to biomechanics and medicine. It will result in a ground-breaking platform that could be applied to screen initial bone drug effects and will improve our fundamental understanding of structure-function relationships in normal and diseased bone conditions.","1496859","2013-11-01","2019-04-30"
"REPLI","Self replication in dynamic molecular networks","Sijbren Otto","RIJKSUNIVERSITEIT GRONINGEN","""Self-replicating molecules have most likely played an important role in the origin of life. This proposal explores the use of dynamic combinatorial chemistry for identifying new self-replicating molecules. We will make dynamic combinatorial libraries of macrocyclic molecules of different ring sizes that can exchange building blocks through reversible covalent chemistry. The building blocks are equipped with peptides that are predisposed to form beta-sheets. The building blocks, which carry only one peptide, do not self-associate, while sufficiently large macrocycles, which display multiple peptides, will self-assemble into extended tubular structures.
The self-assembly process will drive the synthesis of the very macrocycle that assembles by shifting the equilibrium in favour of its formation. There is also a kinetic effect: the ends of the resulting fibres promote the formation of more of the constituent macrocycles. Breaking fibres through agitation is an efficient way of generating more fibre ends, which allows for exponential growth of the replicator. The dependence of growth on mechanical energy translates into a selection criterion when multiple replicators compete for the same building block. The replicator that forms fibres that fragment most readily under a given set of agitation conditions will win.
We have already identified the first building block that shows the behaviour described above. We now propose to investigate in detail:
(1) The mechanism of replication. We aim to find out what exactly happens at the molecular level at the fibre end during fibre growth.
(2) The influence of the peptide sequence and aromatic core of the building blocks. We expect to be able to tune the size of the replicating macrocycle by altering the strength of the interactions between the peptides.
(3) The competition between replicators in an """"open system""""; i.e. with a continuous influx of building block and """"death"""" of some of the replicators through an outflux of a fraction""","1499702","2011-09-01","2017-02-28"
"RESHAPE","REstoring the Self with embodiable HAnd ProsthesEs","Giovanni Di Pino","UNIVERSITA CAMPUS BIO MEDICO DI ROMA","Amputation distorts the body representation, a fundamental aspect of self-consciousness. Hand prostheses counteract sensorimotor impairment, but poor attention has been posed to target the alteration of body-image.
RESHAPE aims to study prosthesis embodiment, identify what makes a hand prosthesis easily embodiable, and test non-invasive brain stimulation to facilitate the embodiment.
Amputees claim to perceive prostheses as tools; RESHAPE enables amputees to project their self into the prosthesis, improving in parallel their dexterity.
The first of three phases develops the enabling technology and defines the embodiment protocol.
The following phase evaluates thirty myoelectric-prosthesis users and the first of two amputees implanted with peripheral neural electrodes, for functional ability, prosthesis embodiment and acceptability and for phantom limb pain (PLP), before and after neuromodulation.
In the last phase, a neuro-controlled prosthesis is optimized in line with the specifications defined in the previous phase and tested in the second implanted amputee.
An embodiment and a sensory/manipulation platform, integrating a discrimination setup with sensorized wearable systems, induce and weigh the embodiment and its impact on prosthesis performance.
Embodiment neural correlates are investigated with EEG and fMRI-based techniques, thanks to a prosthesis virtual model controllable inside the scanner.
Patients are stimulated with a homeostatic plasticity-based rTMS either on premotor cortex or on intraparietal sulcus. A robot-aided TMS compensates head-coil relative displacement, allowing the subject to operate the prosthesis during the stimulation.
RESHAPE is a paradigm shift in Prosthetics. It offers the guidelines for highly-embodiable prostheses, four technological platforms beyond the state-of-the-art, novel insights on how tools shape the body-image, the proof of a TMS-induced embodiment and a new strategy to readdress amputees’ aberrant plasticity and PLP.","1490750","2016-09-01","2021-08-31"
"ResiBots","Robots with animal-like resilience","Jean-Baptiste Nicolas Mouret","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","Despite over 50 years of research in robotics, most existing robots are far from being as resilient as the simplest animals: they are fragile machines that easily stop functioning in difficult conditions. The goal of this proposal is to radically change this situation by providing the algorithmic foundations for low-cost robots that can autonomously recover from unforeseen damages in a few minutes. The current approach to fault tolerance is inherited from safety-critical systems (e.g. spaceships or nuclear plants). It is inappropriate for low-cost autonomous robots because it relies on diagnostic procedures, which require expensive proprioceptive sensors, and contingency plans, which cannot cover all the possible situations that an autonomous robot can encounter. It is here contended that trial-and-error learning algorithms provide an alternate approach that does not require diagnostic, nor pre-defined contingency plans. In this project, we will develop and study a novel family of such learning algorithms that make it possible for autonomous robots to quickly discover compensatory behaviors. We will thus shed a new light on one of the most fundamental questions of robotics: how can a robot be as adaptive as an animal? The techniques developed in this project will substantially increase the lifespan of robots without increasing their cost and open new research avenues for adaptive machines.","1499501","2015-05-01","2020-04-30"
"ResolutioNet","Resolving the Tussle in the Internet: Mapping, Architecture, and Policy Making","Georgios Smaragdakis","TECHNISCHE UNIVERSITAT BERLIN","The Internet has revolutionalized the way people and corporations communicate, publish, access, and search for information. As our globally-connected digital civilization increasingly relies on its smooth operation any disruption has a direct negative impact on both the economy and society. However, the Internet was not designed to serve its current role nor was foreseen to be a public good. On the contrary, it was designed to be fully decentralized and thus administrated by the owners of independent networks. Today, the various Internet players have diverse and often conflicting objectives. Indeed, the tussle between Internet players or between them and governments hit the news and the negative externalities affect the life of potentially billions of Internet users worldwide and harm innovation in the Internet.

We propose a research agenda to resolve the tussle in the Internet. First, we propose the use of sophisticated techniques to collect and analyze massive network data to unveil the complex interactions among the various Internet players that lead to disputes and to identify the conditions under which conditions a resolution is possible. Second, we utilize additional degrees of freedom to resolve the tussle in the Internet by enabling coordination of the various Internet players. To this end, we introduce expressiveness of all the involved parties in existing and emerging protocols and enable agile deployment of third-party services and applications inside operational networks. Third, we contribute to the Internet policy making debate by providing an unbiased view of the state and health of the Internet as well as providing recommendations on how to resolve the Internet tussle. This is an interdisciplinary effort to foster a dialogue for Internet's future and sustainability in light of its ever-increasing growth and competitiveness.","1499875","2017-02-01","2022-01-31"
"RESTRICTION","Restriction of the Fourier transform with applications to the Schrödinger and wave equations","Keith Mckenzie Rogers","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","In 1967, Stein proved that the Fourier transform of functions in L^p could be meaningfully restricted to the sphere for certain p>1. The restriction conjecture, which asserts the maximal range of such p, was solved by Fefferman in two dimensions, but the conjecture remains open in higher dimensions. Strichartz considered the same question but with the sphere replaced by the paraboloid or the cone, and a great deal of progress has been made in the last two decades by Bourgain, Wolff and Tao, among others. Due to the fact that the adjoint operators of the restriction operators to the paraboloid and cone correspond to the Schrödinger and wave evolution operators, respectively, this work has been hugely influential. The main goal of this proposal is to improve the state of the art for the mixed norm analogues of these conjectures.","950000","2011-09-01","2017-08-31"
"RETURN","RETURN – Rethinking Tunnelling in Urban Neighbourhoods","Debra Fern Laefer","UNIVERSITY COLLEGE DUBLIN, NATIONAL UNIVERSITY OF IRELAND, DUBLIN","This project addresses important challenges at the forefront of geotechnical engineering and building conservation by introducing an entirely new workflow and largely unexploited data source for the predic-tion of building damage from tunnel-induced subsidence. The project will also make fundamental and ground-breaking advances in the collection and processing of city-scale, aerial laser scanning by avoiding any reliance on existing data for building location identification, respective data affiliation, or building fea-ture recognition. This will create a set of techniques that are robust, scalable, and widely applicable to a broad range of communities with unreinforced masonry buildings. This will also lay the groundwork to rapidly generate and deploy city-scale, computational models for emergency management and disaster re-sponse, as well as for the growing field of environmental modelling.","1500000","2013-01-01","2016-12-31"
"ReverseAndCat","Reversible Creation of Non-Inherent Reactivity Patterns in Catalytic Organic Synthesis","Pawel Franciszek DYDIO","CENTRE INTERNATIONAL DE RECHERCHE AUX FRONTIERES DE LA CHIMIE FONDATION","Current methods in organic synthesis only enable reactions at the most reactive bonds or at bonds predisposed by specific directing groups. Consequently, many less reactive bonds, including numerous C-H and C-C bonds, cannot be functionalized, enormously limiting the scope of possible transformations. To overcome these limitations, I propose Reverse&Cat, a revolutionary strategy using a novel method to change the reactivity pattern of molecules. This strategy combines the dynamic equilibrium mediated by the first catalyst and a functionalization reaction catalyzed by the second catalyst. The originality of the transformation stems from exploiting three simultaneous processes: (i) the dynamic exchange of one functional group (FG) for another FG that modulates the reactivity of the substrate; (ii) the functionalization of the temporarily activated bond; and (iii) the restoration of the initial FG. In essence, the processes (i) and (iii) – the components of the dynamic equilibrium – realize the novel concept of the temporary creation of non-inherent reactivity of a substrate.
The program is divided in three phases, which will establish the full potential of the strategy. In phase A, I will develop a set of new reactions enabled by the bi-catalytic systems. I will exploit two types of reversible reactions: (1) reversible oxidation of alcohols, which delivers temporarily activated aldehydes/ketones, with the distinct reactivity of their C-H bonds; and (2) reversible retro-hydrofunctionalization of nitriles or their analogues, which delivers temporarily activated alkenes, containing allylic C-H and C=C bonds. In phase B, I will conduct detailed mechanistic studies to gain the mechanistic understanding and enable further rational development. In phase C, I will establish the utility of this new strategy in practical organic synthesis. Overall, the strategy will open a new dimension of reactivity, with prospective applications in production of fine-chemicals and materials.","1731250","2018-11-01","2023-10-31"
"ReVolusions","Quantifying Recycling Fluxes of Earth Surface Materials and Volatiles in Subduction Zones using Melt Inclusions","Janne KOORNNEEF","STICHTING VU","Plate tectonics are fundamental to the geochemical cycles that link Earth’s mantle and exosphere and in turn control the atmosphere’s composition and our climate. Currently a major unknown is the exact fate of surface materials at destructive plate-tectonic boundaries (subduction zones). Specifically, what proportions of volatiles, sediments and oceanic crust are transported into the deep mantle, or are returned, i.e. ‘recycled’ to the crust and atmosphere. Global recycling flux estimates, including carbon, are contradictory highlighting the need for a new, more precise approach to their quantification.

I propose to better quantify global recycling fluxes by a geochemical study of two types of subduction zones: continental and oceanic zones of which the former has higher volatile fluxes and plays a key role in past and present-day climate change. The proposed work utilises isotope analyses of deeply formed melt inclusions, tiny pockets of melt trapped in minerals, to directly determine what comes back up in subduction zones. Undertaking a multiple isotope study of these inclusions is now possible, owing to my recent success in significantly improving mass spectrometer detector amplifier technology that led to an order of magnitude improvement in precision for isotope analyses of small samples. 

I will integrate two ground-breaking techniques to identify recycled components and determine volcanic and deep mantle fluxes: 1) coupled Sr-Nd-Pb isotope ratio analysis of individual melt inclusions using the high-gain amplifier method I pioneered; 2) carbon and oxygen isotope analysis of CO2 from melt inclusions using a newly developed crushing technique. The outcomes of ReVolusions will provide crucial understanding of how subduction geodynamics control the distribution of elements between Earth’s major reservoirs (atmosphere, crust and mantle) that affects short and long-term climate changes.","1709240","2018-01-01","2022-12-31"
"REWOCRYPT","Theoretically-Sound Real-World Cryptography","Tibor JAGER","UNIVERSITAET PADERBORN","Novel technologies like Cloud Computing, Ubiquitous Computing, Big Data, Industry 4.0, and the Internet
of Things do not only come with a huge demand for practical and efficient cryptosystems, but also with many novel attack surfaces. The security properties required from cryptographic building blocks for these innovative applications go beyond classical security goals.

Modern theoretical cryptography has very successfully developed powerful techniques that enable the design and rigorous formal analysis of cryptosystems in theoretical security models. Now that these techniques are readily available, we have to take the next important step: the evolution of these techniques from idealized theoretical settings to the demands of real-world applications.

The REWOCRYPT project will tackle this main research challenge at the intersection of theoretical and real-world cryptography. It will provide a solid foundation for the design and mathematically rigorous security analysis of the next generation of cryptosystems that provably meet real-world security requirements and can safely be used to realize secure communication in trustworthy services and products for a modern interconnected society.","1498638","2019-04-01","2024-03-31"
"RFMIFICS","RF-enhanced Microprocessing for Fine Chemicals Synthesis using Catalysts Supported on Magnetic Nanoparticles","Evgeny Rebrov","THE UNIVERSITY OF WARWICK","This proposal aims at further strengthening the current line of the applicant’s research in the area of nontraditional energy sources and structured reactors. Novel and challenging reactor concepts and technologies are proposed for newly emerging liquid phase catalytic processes for, amongst others, fine chemicals and pharmaceuticals synthesis. Catalytic processes in the liquid phase are crucial in the manufacturing of fine and specialty chemicals. It is widely accepted that the activity of a solid catalyst suspended in a liquid phase can benefit greatly from the use of smaller catalyst particles to avoid mass-transfer limitations. However, the difficulties in recovering small particles from the reaction mixture severely circumvent their industrial applications. To overcome the above drawbacks, the separation of suspended magnetic catalyst bodies from the liquid system using an external magnetic field is proposed. Functionalized magnetic bimetallic nanoparticles are leading candidates for catalytic applications as a vector for magnetic guidance. Their appclication will provide reactors and processes for synthetic routes and high-value added products with optimal space-time yields, minimum waste production, minimum energy consumption, and minimum operating costs. Two novel reactor concepts are proposed in this ERC starting grant program with the aim to develop and demonstrate continuous flow reactors, viz. (1) the RF-heated reactor where catalytically active magnetic nanoparticles are hold in the reactor by an external magnetic field while being heated, and (2) the micro-flow through reactor for magnetic NP manipulation, where mixing in laminar flow is improved by quadrupolar actuation created by a quadrupolar micro magnet arrangement along the channel.","1242000","2011-09-01","2015-12-31"
"RGGC","Random Graph Geometry and Convergence","Agelos Georgakopoulos","THE UNIVERSITY OF WARWICK","We propose an intradisciplinary research programme in pure mathematics, with graph theory at the epicenter and rich connections to other fields.

Although the bonds between graph theory and other branches of mathematics have been growing in recent years, large parts of graph theory are still almost isolated from the rest of mathematics, and conversely, there are fields based on graphs that still make hardly any use of graph-theoretic machinery: the study of Cayley graphs in combinatorial group theory, the recent notion of Benjamini– Schramm convergence, and the many instances of approximating continuous spaces by graphs in various contexts are such examples.

The RGGC project offers concrete graph theoretic approaches to important challenges in the afore- mentioned fields. At the same time, advances in graph theory using group theoretic and analytic machinery will be achieved. The project comprises 4 research Themes overarching a wide mathematical scenery.

Theme 1 unites the worlds of Benjamini–Schramm convergence and graph minor theory using tech- niques from enumerative and analytic combinatorics that were applied for the first time in this context by the PI.

Theme 2 builds on the deepest part of the PI’s past work to offer a new perspective to geometric random graphs profiting from a sophisticated theory triggered by Kesten’s random walks on groups.

Theme 3 aims at deepening the understanding of cover time of graphs by exploring its extremal and typical behaviour using the concept of cover cost, an approach pioneered by the PI.

Theme 4 introduces diffusions on continuous, graph-like spaces in the sense of Thomassen & Vella motivated by both theoretic and applied considerations.

The proposed research not only attacks challenging questions in each of thease areas, it also creates bridges for transferring knowledge and tools among them, through concrete novel approaches of the PI that have already achieved initial success.","1177905","2015-05-01","2020-04-30"
"RheoActive","Geometry, instability and activity in complex and biological fluids","Suzanne Fielding","UNIVERSITY OF DURHAM","Within the research area of soft condensed matter physics, this proposal concerns the remarkable flow (rheological) behaviour of complex fluids such as polymers, colloids and emulsions; and of
biologically active suspensions such as swarms of bacteria or sperm, and the cytoskeletal matrix of the biological cell. A unifying concept in two closely related research themes is the way
non-equilibrium dynamics underlies the rheological behaviour of these fluids. Theme I concerns non-equilibrium phase transitions induced by an externally applied flow. It addresses the challenge of predicting the onset, characteristics and implications of these transitions in the complicated flow geometries that arise experimentally and industrially: focusing on the two key issues that will form the basis of practical rheological prediction, by addressing the key concepts of underlying physics. The first concerns the way in which geometrical confinement can lead to a rich interplay between three dimensional (3D) phase transitions in the fluid bulk, and 2D surface transitions at the hard walls of the flow device. The second concerns instabilities in extensional (stretching) flows and how they interact with transitions in shearing flows, aiming to develop a unified understanding of both, and how they interact. Theme II turns to biological suspensions that exist in strongly non-equilibrium regimes due internal activity such as bacterial swimming. While much progress has been made predicting rules for single-swimmer propulsion, and emergent phenomena of many swimmers collectively, most work to date has been in a simple (Newtonian) suspending fluid. This is a major shortcoming: most biological swimming occurs in complex polymeric fluids. My aim is to forge a physical understanding of biological activity in these complex fluid environments. Emergent phenomena include banded and turbulent flows, with an obvious link to Theme I, and an overall aim is to cross fertilise concepts between the Themes.","1460342","2012-01-01","2016-12-31"
"RIMACO","Rigorous Mathematical Connections between the Theory of Computations and Statistical Physics","Dimitris Achlioptas","INSTITOUTO TECHNOLOGIAS YPOLOGISTONKAI EKDOSEON DIOFANTOS","The proposed research aims to enhance the study of randomness in computation by using ideas of statistical physics. In particular, it aims to place the connection between computation and statistical physics --- the subject of wide heuristic discussion for more than three decades --- on rigorous mathematical ground. Its main methodological vehicle is the study of random Constraint Satisfaction Problems (CSPs). CSPs are the common abstraction of numerous real-life problems and occur in areas ranging from aerospace design to biochemistry. Their ubiquity makes the design of efficient algorithms for CSPs extremely important. At the foundation of this endeavor lies the question of why certain CSP instances are exceptionally hard while other, seemingly similar, instances are easy. Probability distributions over instances allow us to study this phenomenon in a principled way, with each CSP distribution controlled by its ratio of constrains to variables (known as constraint density). By now, it has been established that random CSPs have solutions at densities much beyond the reach of any known efficient algorithm. Understanding the origin of this gap and designing algorithms that overcome it is the main focus of the proposed research. Ideas from statistical physics will play an important role here. Specifically, in recent years, physicists have proposed a heuristic but deep theory for the evolution of the solution-space geometry of random CSPs according to which algorithmic barriers correspond to phase transitions in this evolution. Examining the validity of the physics theory is a major research undertaking that must develop and reconcile notions shared by computation and statistical physics, e.g., the role of long-range correlations. A rigorous mathematical theory of such notions will enable a much more energetic exchange of ideas between the two fields, and has the potential to bring substantial fresh ideas to the study of efficient computation.","749996","2008-07-01","2014-06-30"
"RISeR","Rates of Interglacial Sea-level Change, and Responses","Natasha Louise Mary BARLOW","UNIVERSITY OF LEEDS","Global sea-level rise is one of our greatest environmental challenges and is predicted to continue for hundreds of years, even if global greenhouse-gas emissions are stopped immediately. However, the range, rates and responses to sea-level rise beyond 2100 are poorly understood. Current models that project sea-level rise centuries into the future have large uncertainties because the recent observations upon which they are based, encompass too limited a range of climate variability. Therefore, it is crucial to turn to the geological record where there are large-scale changes in climate. Global temperatures during the Last Interglacial were ~1oC warmer than pre-industrial values and 3-5oC warmer at the poles (a pattern similar to that predicted in the coming centuries), and global sea level was 6-9 m higher, far above that experienced in human memory.

Through the RISeR project, I will lead a step-change advance in our understanding of the magnitude, rates and drivers of sea-level change during the Last Interglacial, to inform both global and regional sea-level projections beyond 2100. Specifically I will:

1. Develop new palaeoenvironmental reconstructions of Last Interglacial sea-level change from northwest Europe;
2. Provide the first ever chronological constraints on the timing, and therefore rates, of relative sea-level change that occurred in northwest Europe during the Last Interglacial;
3. Use state-of-the-art numerical modelling to distinguish the relative contributions of the Greenland and Antarctica ice sheets to global sea-level rise during the Last Interglacial;
4. Provide estimates of the land areas and exposed populations in northwest Europe at risk of inundation by long-term (2100+) sea-level rise, providing high-end scenarios critical for coastal-risk management practice.

These ambitious objectives will result in a state-of-the-art integrated study of the most appropriate analogue for a critical global environmental challenge; future sea-level rise.","1997681","2019-02-01","2024-01-31"
"ROBOCHIP","MicroRobotic toolkit to deliver spatiotemporally resolved physicochemical signals and control cell sociology","Mahmut Selman SAKAR","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Biological organisms have spectacular three-dimensional morphologies in which correct multiscale organization and physicochemical regulation are essential for proper function. Understanding the dynamic interactions among cells connected through a structurally complex three-dimensional (3D) fibre network requires a rigorous system identification effort and engineering analysis. To develop accurate and comprehensive models of cell sociology, we need to identify how several different components act together as a connected system and define high-order emergent programs that constitute layers of signals in time and space. The objective of this proposal is to engineer new microrobotic technologies that can be seamlessly integrated with high-throughput bioengineering platforms and produce physiologically relevant signals to establish an active dialogue with the members of the tissue community and drive the cell-network evolution. We propose to build cutting-edge robotic micromanipulation systems to perform automated operations on 3D biological samples with exceptional dexterity and high spatiotemporal resolution. Wirelessly powered micromachines will modulate local mechanochemical signaling within the tissues and move through fibrillar scaffolds. Multi-functional integrated platforms will simultaneously apply external and internal signals, and advanced imaging systems will visualise whole tissue activity at the cellular resolution. Our data will be used to establish computational models of tissue-level mechanical response to dynamical perturbations with particular focus on transmission of signals and feedback mechanisms. The ability to hack cellular communication links and interfere with the biological processes in 3D systems will lead to a better understanding of morphogenesis and regeneration, and can aid in developing treatments that ensures a microenvironment with a distribution of signals that minimizes disease progression.","1748773","2017-03-01","2022-02-28"
"RoboExNovo","Robots learning about objects from externalized knowledge sources","Barbara Caputo","FONDAZIONE ISTITUTO ITALIANO DI TECNOLOGIA","While today’s robots are able to perform sophisticated tasks, they can only act on objects they have been trained to recognize. This is a severe limitation: any robot will inevitably face novel situations in unconstrained settings, and thus will always have knowledge gaps. This calls for robots able to learn continuously about objects by themselves. The learning paradigm of state-of-the-art robots is the sensorimotor toil, i.e. the process of acquiring knowledge by generalization over observed stimuli. This is in line with cognitive theories that claim that cognition is embodied and situated, so that all knowledge acquired by a robot is specific to its sensorimotor capabilities and to the situation in which it has been acquired. Still, humans are also capable of learning from externalized sources – like books, illustrations, etc – containing knowledge that is necessarily unembodied and unsituated. To overcome this gap, RoboExNovo proposes a paradigm shift. I will develop a new generation of robots able to acquire perceptual and semantic knowledge about object from externalized, unembodied resources, to be used in situated settings. As the largest existing body of externalized knowledge, I will consider the Web as the source from which to learn from. To achieve this, I propose to build a translation framework between the representations used by robots in their situated experience and those used on the Web, based on relational structures establishing links between related percepts and between percepts and the semantics they support. My leading expertise in machine learning applied to multi modal data and robot vision puts me in a strong position to realize this project. By enabling robots to use knowledge resources on the Web that were not explicitly designed to be accessed for this purpose, RoboExNovo will pave the way for ground-breaking technological advances in home and service robotics, driver assistant systems, and in general any Web-connected situated device.","1496277","2015-06-01","2020-05-31"
"ROBOTAR","Robot-Assisted Flexible Needle Steering for Targeted Delivery of Magnetic Agents","Sarthak Misra","UNIVERSITEIT TWENTE","Diagnostic agents are currently injected into the body in an uncontrolled way and visualized using non-real-time imaging modalities. Delivering agents close to the organ and magnetically guiding them to the target would permit a myriad of novel diagnostic and therapeutic options, including on-site pathology and targeted drug delivery. Such an advance would truly revolutionize minimally invasive surgery (MIS). Presently MIS often involves manual percutaneous insertion of rigid needles. These needles deviate from their intended paths due to tissue deformation and physiological processes. Inaccurate needle placement may result in misdiagnosis or ineffective treatment. Thus, the goal of ROBOTAR is to design a robotic system to accurately steer flexible needles through tissue, and enable precise delivery of agents by magnetically guiding them to a designated target.

There are several challenges: 3D models describing the evolving needle shape are not available, real-time control of flexible needles using 3D ultrasound (US) images has not been demonstrated, and US-guided tracking of magnetic agents has not been attempted. These challenges will be overcome by using non-invasively (via US) acquired tissue properties to develop patient-specific biomechanical models that predict needle paths for pre-operative plans. Intra-operative control of flexible needles with actuated tips will be accomplished by integrating plans with data from US images and optical sensors. Ultrafast US tracking methods will be coupled to an electromagnetic system to robustly control the agents. A prototype will be evaluated using microrobots and clusters of nanoparticles in scenarios with realistic physiological functionalities. The knowledge gained will be applicable to a range of flexible instruments, and to an assortment of personalized treatment scenarios. This research is motivated by the existing need to further reduce invasiveness of MIS, minimize patient trauma, and improve clinical outcomes.","1500000","2015-06-01","2020-05-31"
"RobSpear","Robust Speech Encoding in Impaired Hearing","Sarah Verhulst","UNIVERSITEIT GENT","The prevalence of hearing impairment amongst the elderly is a stunning 33%, while the younger generation is sensitive to noise-induced hearing loss through increasingly loud urban life and lifestyle. Yet, hearing impairment is inadequately diagnosed and treated because we fail to understand how the components that constitute a hearing loss impact robust speech encoding. 
A recent and ground-breaking discovery in animal physiology demonstrated the existence of a noise-induced hearing deficit -cochlear neuropathy- that coexists with the well-studied cochlear gain loss deficit known to degrade the audibility of sound. Cochlear neuropathy is thought to impact robust encoding of the audible portions of speech and occurs before standard hearing screening methods indicate problems, implying that a large group of noise-exposed people with self-reported hearing problems is currently not screened, nor treated. To design effective hearing restoration strategies, it is crucial to understand how cochlear neuropathy interacts with other hearing deficits to affect robust speech encoding in every-day listening conditions. 
Through an interdisciplinary approach, RobSpear targets hearing deficits along the ascending stages of the auditory pathway to revolutionize how hearing impairment is diagnosed and treated. RobSpear can yield immense reductions of health care costs through effective treatment of currently misdiagnosed patients and studies the impact of noise-induced hearing deficits on our society. To achieve this, RobSpear: 
(i) Builds a hearing profile that, based on a computational model of the auditory periphery, develops physiological measures that differentially diagnose hearing deficits in listeners with mixtures of deficits.
(ii) Designs individually tailored speech enhancement algorithms that work in adverse conditions and target perceptually relevant speech features, using an unprecedented validation approach that combines novel psychoacoustic and physiological metrics.","1499780","2016-10-01","2021-09-30"
"ROBUSTFINMATH","Robust Financial Mathematics: model-ambiguous framework for valuation and risk management","Jan Krzysztof Obloj","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","""The last forty years have seen a remarkable interplay between Mathematics and contemporary Finance. At the heart of the successful growth of Mathematical Finance was a perfect fit between its dominant model--specific framework and the tools of stochastic analysis. However, this approach has always had important limitations, and the dangers of overreach have been illustrated by the dramatic events of the recent financial crisis.
I set out to create a coherent mathematical framework for valuation, hedging and risk management, which starts with the market information and not an a priori probabilistic setup. The main objectives are: (i) to incorporate both historical data and current option prices as inputs of the proposed robust framework, and (ii) to establish pricing-hedging duality, define the concept of no-arbitrage and prove a Fundamental Theorem of Asset Pricing, all in a constrained setting where the market information, and not a probability space, is fixed from the outset. Further, I will test the performance of robust valuation and hedging methods.
The project proposes a genuine change of paradigm. It requires building novel mathematical tools combining  pathwise stochastic calculus, embedding problems, martingale optimal transport, variation inequalities as well as numerical methods.
Significant research efforts have focused on introducing and investigating a form of model uncertainty in Financial Mathematics. This project makes an important next step. Motivated by recent contributions, it builds a framework which consistently combines model ambiguity with a comprehensive use of market information. Further, it has built-in flexibility to interpolate between the model-specific and model-independent settings. It offers a new theoretical foundation opening horizons for future research. Moreover, it provides novel tools which could be applied by the financial industry.""","1218639","2014-01-01","2018-12-31"
"ROC-CO2","Carbon dioxide (CO2) emissions by rock-derived organic carbon oxidation","Robert George Hilton","UNIVERSITY OF DURHAM","The global carbon cycle controls Earth’s climate by the emission and drawdown of carbon dioxide (CO2) from the atmosphere. One of the major sources of CO2 is thought to be the oxidation of organic carbon contained in rocks during chemical weathering. Since the industrial revolution, this flux has been accelerated by burning fossil fuels. However, the natural rates of CO2 emission by rock-derived organic carbon oxidation are very poorly constrained – the only major CO2 source that has not been properly quantified – and the dominant controls on this flux remain unclear. The CO2 release is likely to be ~100 TgC/yr, similar to degassing from volcanoes. Without knowing the rate of CO2 emission and the controls on this flux, it is not possible to fully understand the evolution of atmospheric CO2 and global climate over geological timescales, nor to project future changes over hundreds to thousands of years. To address this deficit and quantify a major geological CO2 source, the proposal will:
1) Assess which factors govern rock-derived organic carbon oxidation.
2) Determine how environmental changes impact oxidation rates and CO2 release.
3) Quantify the global CO2 emissions by rock-derived organic carbon oxidation during chemical weathering, and assess how they may have varied both over Earth history and via anthropogenic change. 
By quantifying a major CO2 emission for the first time, this project will provide a step change in our understanding of the geological, as opposed to the anthropogenically-modified, carbon cycle. Measurement of rock-derived organic carbon oxidation will require a new approach, harnessing state-of-the-art geochemical proxies carried by rivers (rhenium). Data from river catchments spanning large gradients in the likely environmental controls (erosion, temperature), will elucidate the main factors governing this process, and enable construction of a data-driven numerical model to provide the first quantification of CO2 emissions by this process.","1499696","2016-03-01","2021-02-28"
"RockDEaF","Dynamics of rock deformation at the brittle-plastic transition and the depth of earthquake faulting","Nicolas BRANTUT","UNIVERSITY COLLEGE LONDON","The lithosphere is the thin outer shell of the Earth that supports the weight of mountains, plate tectonic forces, and stores the elastic energy that is released during earthquakes. The strength of the lithosphere directly controls the formation of tectonic plates and the generation and propagation of devastating earthquakes.

The strongest part of the lithosphere is where the deformation processes in rocks transition from brittle fracture to plastic flow. This transition controls the strength of tectonic plate interfaces, the coupling between mantle flow and surface tectonics, as well as the complex fault slip patterns recently highlighted by geophysical records (e.g., tremors and slow slip).

Despite its fundamental importance, the transitional behaviour remains very poorly understood. In this regime, we still do not know how rock deformation processes and properties evolve with depth and, critically, time. We also do not know exactly where the transition occurs in nature, if and how it may move over time, and what are the prevailing conditions there.

The aim of this project is to provide unprecedented quantitative constrains on the key material properties and processes associated with deformation and fluid flow at the brittle-plastic transition, and arrive at a clear understanding of the prevailing conditions and the dynamics of fault slip at the transition.

I propose to conduct laboratory rock deformation experiments at the high pressure and temperature conditions relevant to the transitional regime, and achieve unprecedented quantitative physical measurements by developing state-of-the-art in-situ instrumentation, taking advantage of the latest sensor technologies. I will focus on quantifying the effects of time and fluids, which are currently unexplored.

The ultimate outcome of the project is to detect the transition in nature by understanding its geophysical signature, and constrain the strength of faults and plate boundaries throughout the seismic cycle.","1499990","2019-01-01","2023-12-31"
"ROSETTA","Rosetta s Way Back to the Source: 
Towards Reverse Engineering of Complex Software","Hendrik Jaap Bos","STICHTING VU","We propose a research program (Rossetta) towards reverse engineering of complex software that is available only in binary form. Most of the commercial software industry assumes that compilation (the translation of source code to binary code), is irreversible in practice for real applications. The research question for Rosetta is whether this irreversibility assumption is reasonable. If successful, the project will have a major impact on the software industry.

The challenge is daunting, because binary code after compilation lacks most of the visible structure and semantic information that is available at the source code level. There is no definition of data structures, no helpful names of variables and functions, no semantic information, and no indication of what chunks of instructions are supposed to do.

However, the Rosetta project has a clear methodology for source recovery. Reverse engineering is approached as an iterative process with an initial focus on recovering data structures, followed by recovery of code. We combine static and dynamic techniques with usage monitoring and machine learning. A key insight is that even if all visible structure has been removed from the data in memory, the structures will still be *used* in a way that corresponds to the source code. By observing the use of data and application of machine learning techniques, we will recover both the data and the source.

We store all information that we uncover in the Rosetta database. The database provides a handle on both the data structures and large sections of the code (and at various levels of abstraction). We believe that our methods will allow reverse engineering of very complex commercial software. Doing so will be our main criterion for success. In addition, however, we propose to demonstrate the usefulness of our analysis by automatically  hardening  software (to make it resilient against many types of attack) without requiring any access to the source co","1339000","2011-05-01","2016-04-30"
"RPT","Rough path theory, differential equations and stochastic analysis","Peter Karl Friz","TECHNISCHE UNIVERSITAT BERLIN","We propose to study stochastic (classical and partial) differential equations and various topics of stochastic analysis, with particular focus on the interplay with T. Lyons' rough path theory:
1) There is deep link, due to P. Malliavin, between the theory of hypoelliptic second order partial differential operators and certain smoothness properties of diffusion processes, constructed via stochastic differential equations. There is increasing evidence (F. Baudoin, M. Hairer &) that a Markovian (=PDE) structure is dispensable and that Hoermander type results are a robust feature of stochastic differential equations driven by non-degenerate Gaussian processes; many pressing questions have thus appeared.
2) We return to the works of P.L. Lions and P. Souganidis (1998-2003) on a path-wise theory of fully non-linear stochastic partial differential equations in viscosity sense. More specifically, we propose a  rough  path-wise theory for such equations. This would in fact combine the best of two worlds (the stability properties of viscosity solutions vs. the smoothness of the Ito-map in rough path metrics) to the common goal of the analysis of stochastic partial differential equations. On a related topic, we have well-founded hope that rough paths are the key to make the duality formulation for control problems a la L.C.G. Rogers (2008) work in a continuous setting.
3) Rough path methods should be studied in the context of (not necessarily continuous) semi-martingales, bridging the current gap between classical stochastic integration and its rough path counterpart. Related applications are far-reaching, and include, as conjectured by J. Teichmann, Donsker type results for the cubature tree (Lyons-Victoir s powerful alternative to Monte Carlo).","850820","2010-09-01","2016-08-31"
"RuProLight","Light-activatable ruthenium-based anticancer prodrugs","Sylvestre Bonnet","UNIVERSITEIT LEIDEN","Chemotherapy is, after surgery, the second most efficient therapy against cancer. However, it has many side effects for cancer patients because anticancer drugs kill cancer cells but also healthy ones. My project aims at synthesizing new metal-containing compounds that 1) are poorly toxic in the dark; 2) can be attached via a light-sensitive bond to liposomes that will carry them into cancer cells; and 3) detach from their carriers and become toxic upon light irradiation, thus killing cancer cells.

These new compounds contain ruthenium, a metal combining photochemical and anticancer properties. I will replace the weakly bound chloride ligands of known cytotoxic ruthenium compounds by strongly bound sulfur ligands. By doing so, the DNA- and protein-binding ability of the ruthenium compounds will be lowered, which will lower their toxicity in the dark. Thioether-lipid conjugates will be used to attach the ruthenium prodrugs to liposomes carriers that are well taken up by cancer cells.

Techniques to irradiate tumors in vivo are nowadays available in the clinics. By shining light onto the ruthenium-enriched cancer cells photochemical cleavage of the Ru-S bond will take place, thus detaching the metal complex from its carrier and allowing it for binding to biological molecules. Thus, the ruthenium prodrug will be transformed inside cancer cells into a highly toxic molecule that will kill the cells. I will study mononuclear compounds and molecules containing several ruthenium centres; visible light activation and near infrared light activation. The final aim is to obtain ruthenium-functionalized liposomes that are poorly toxic in the dark, preferentially go into cancer cells, and become toxic at the place of irradiation, using light that penetrates well into biological tissues.

Because of this unique combination of properties my new light-activatable ruthenium prodrugs will ultimately lead to selective anticancer treatment showing low side effects for cancer patients.","1418400","2013-06-01","2018-05-31"
"RyD-QMB","Rydberg dressed quantum many-body systems","Christian Gross","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","The project “Rydberg dressed quantum many-body systems” (RyD-QMB) will experimentally study long-range interacting atomic quantum many-body systems with tailored microscopic interactions. It will explore supersolidity expected for ultracold bosonic systems with soft-core interactions and realize quantum magnets, that are designable almost at will and feature unprecedented coupling strengths. As such, it opens new directions for atomic quantum simulation and paves the way towards the experimental study and the design of frustrated magnets, interacting topological systems and artificial quantum gauge fields, topics, that not only push the limits of quantum simulators for solid state physics, but also reach out into the field of high energy physics.
The unique interaction features RyD-QMB plans to exploit emerge from the combination of ultracold atoms with Rydberg atoms. Using Rydberg dressing long-range interactions will be induced, whose strength, distance dependence and isotropy is controlled. Simultaneously, the gap between the timescales of atomic motion and lifetime of the Rydberg states will be bridged.
Strong optical coupling to the Rydberg state is key for the implementation of useful Rydberg dressing. Therefore, RyD-QMB will use high power single photon coupling in the ultraviolet to induce the interactions. RyD-QMB will explore Rydberg dressing of continuous as well as lattice systems and aims to:

* Demonstrate microscopic interaction design of quantum many-body systems using strong Rydberg dressing of potassium.
* Explore exotic superfluidity in two dimensional long-range interacting systems and, especially, realize soft-core interacting many-body systems that are predicted to show supersolidity.
* Study ground states and non-equilibrium dynamics of strongly interacting quantum magnets with designed long-range spin couplings that are induced by the light field.","1497375","2016-06-01","2021-05-31"
"S-RNA-S","Small ribonucleic acids in silico","Giovanni Bussi","SCUOLA INTERNAZIONALE SUPERIORE DI STUDI AVANZATI DI TRIESTE","Ribonucleic acid (RNA) is acquiring a larger importance in cell biology, as more functions that it accomplishes are discovered. In particular, as it has emerged in the last decade, it has a crucial role in the post-transcriptional regulation of gene expression. The comprehension of these mechanisms and, in general, of RNA properties at a molecular level is a key issue in the study of many diseases and paves the way to possible applications in molecular medicine. Particularly interesting in this context are riboswitches, small portions of messenger RNAs which change their conformation in presence of specific metabolites and consequently inhibit or enhance gene transcription or translation. Another issue of paramount interest is RNA metabolism, crucial for viral replication, and in particular of the interaction of RNAs and the molecular motors devoted to its manipulation and unwinding. One of the goals of the present project is to develop new in silico approaches based on molecular dynamics for the study of small RNA molecules. In particular, the research will focus on (a) the development of ad hoc techniques to accelerate molecular dynamics simulations of RNA and of RNA-protein complexes and (b) to predict the three-dimensional structure of small RNAs. Another major goal of the present project is to apply the aforementioned techniques, in combination with standard molecular dynamics and state-of-the-art rare-event methods, to the study of three extremely relevant topics: (a) the interaction between RNA and the helicases, which are the enzymes devoted to its unwinding in the cell, (b) the conformational transitions in riboswitches, and (c) the structure of non-coding inverted short interspersed elements (SINE).","1295700","2012-10-01","2017-09-30"
"SACCRED","Structured ACCREtion Disks: initial conditions for planet formation in the time domain","Ágnes KÓSPÁL","MAGYAR TUDOMANYOS AKADEMIA CSILLAGASZATI ES FOLDTUDOMANYI KUTATOKOZPONT","In this ERC Starting Grant, I propose an ambitious research program to target important challenges in predicting realistic initial conditions for the planet formation process. I will perform a large systematic study of the accretion-driven eruptions of newborn stars, and evaluate their influence on the structure, composition, and chemistry of the terrestrial planet forming zone in the circumstellar disk. The research will focus on three main questions:

- How does the mass accretion proceed in realistic, structured, non-axisymmetric disks?
- What physical mechanisms explain the accretion-driven eruptions?
- What is the effect of the eruptions on the disk?

My new research group will study young eruptive stars, pre-main sequence objects prone to episodes of extremely powerful accretion-driven outbursts, and combine new observations, state-of-the-art numerical modelling, and information from the literature. With a novel concept, we will first model the time-dependence of mass accretion in circumstellar disks, taking into account the latest observational results on inhomogeneous disk structure, and determine what fraction of young stellar objects is susceptible to high mass accretion peaks. Next, we will revise the paradigm of the eruptive phenomenon, compelled by recently discovered young eruptive stars whose outbursts are inconsistent with current outburst theories. Finally, we will determine the impact of accretion-driven eruptions on the disk, by considering the increased external irradiation, internal accretion heating, and stellar winds. With my experience and track record, I am in a position to comprehensively synthesize existing and newly acquired information to reach the proposed goals. The expected outcome of the ERC project is a conclusive demonstration of the ubiquity and profound impact of episodic accretion on disk structure, providing the initial physical conditions for disk evolution and planet formation models.","1370200","2017-07-01","2022-06-30"
"SagnacSpeedmeter","Interferometry beyond the Standard Quantum Limit using a Velocity Sensitive Sagnac Interferometer","Stefan Hild","UNIVERSITY OF GLASGOW","From the very first Michelson Interferometer invented over 100 years ago to today’s kilometre-scale gravitational wave detectors the sensitivity of interferometric length measurements has been improved by about 10 orders of magnitude and is now limited by the so-called Standard Quantum Limit (SQL), a manifestation of Heisenberg’s Uncertainty Principle. The SQL is comprised of the inevitable combination of sensing noise (photon shot noise) and back action noise (photon radiation pressure noise) when repeatedly measuring the position of a test mass. However, by measuring a different variable, i.e. the test mass velocity (speedmeter) instead of its position (position-meter), it is possible to evade back action noise. The momentum of a free test mass can be measured continuously to arbitrary accuracy without being limited by the SQL. Since a Sagnac interferometer is sensitive only to the time-dependent part of the arm-length difference it is automatically a speed meter and therefore brings measurements beyond the SQL into our reach.

Theoretical analyses have shown that the speedmeter approach is the most promising track towards wide-band sub-SQL measurements. An experimental test of this technique is urgently required! Therefore, my three main objectives of this proposal are: 1) Realisation of an ultra-low noise, quantum radiation pressure dominated speedmeter test bed. 2) Experimental demonstration of back action noise supression in a Sagnac speedmeter. 3) Development of speedmeter based sub-SQL interferometery for future gravitational wave detectors such as the Einstein Telescope.

By the end of this project I will have demonstrated the sub-SQL potential of the Sagnac speedmeter configuration. A positive outcome of this project is expected to lead to the Sagnac speedmeter superseding the Michelson interferometer as state-of-the-art instrument for ultra-high sensitivity lengths measurements.","1399260","2012-09-01","2017-08-31"
"SAMBA","Sustainable and Advanced Membranes By Aqueous Phase Separation","WIEBE MATTHIJS DE VOS","UNIVERSITEIT TWENTE","Membranes play a critical role in the production of safe drinking water and in the treatment of human waste streams. However, membranes themselves are nearly always produced using costly, harmful and environmentally unfriendly aprotic solvents such as N-methyl-pyrrolidone (NMP), dimethylformamide (DMF), or dimethylacetamide (DMAC). This proposal describes a highly novel approach allowing the production of the next generation of advanced membranes without the need to use any organic solvents. Here we make use of so-called responsive polymers that can switch under aqueous conditions from a hydrophilic to a hydrophobic state by a simple change of, for example, pH. In the hydrophilic state, water dissolves the polymers and the so obtained solution can be cast as a thin film. Sudden immersion in a bath at a pH where the polymer becomes hydrophobic, leads to very sudden phase separation whereby the polymer coagulates into a porous film, a membrane. Control over the kinetics of this aqueous phase separation process allows for the fabrication of a large variety of porous structures. Furthermore, this process also works for two oppositely charged polymers, where polyelectrolyte complexation is used to induce phase separation. Crosslinking will be a natural way to guarantee membrane stability but can also be used to further modify/improve membranes. The very nature of this aqueous phase separation process is such that membrane additives that are typically associated with advanced membranes (responsive polymers, enzymes, polyzwitterions, metallic nanoparticles) can readily be incorporated. As such, aqueous phase separation not only allows solvent free membrane production, it also provides a very simple and versatile route for the production of membranes with advanced properties. Finally, the porous structures and novel materials developed within this project could be directly useful for other applications, ranging from adsorption processes and coatings to biomedical materials.","1500000","2017-01-01","2021-12-31"
"SAW","Symplectic Aspects of Weak KAM theory","Patrick Bernard","UNIVERSITE PARIS DAUPHINE","""The least action principle is one of the most classical tools in the study of convex Hamiltonian systems. It consists in finding specific orbits  by minimizing the Lagrangian action functional. Another powerful classical tool in Hamiltonian dynamics is the theory of canonical transformations, which provides a large class of admissible changes of coordinates, allowing to put many systems into simplified normal forms.
These two tools are difficult to use simultaneously because the Lagrangian action does not behave well under canonical transformations. A large part of the development of symplectic geometry in the second half of the last century consisted in bridging this gap, by developing a framework encompassing a large part of  both theories. For example, the direct study of the Hamiltonian action functional (which, as opposed to the Lagrangian action functional, behaves well under canonical transformations) allowed to recover, refine,  and generalize beyond the convexity hypothesis, most of the results concerning the existence of periodic orbits which had been proved with  the least action principle.
Twenty years ago, under the impulsion of John Mather,  a renewed use of the least action principle led to the proof of  the existence of complicated invariant sets and unstable orbits. This collection of new methods has been called weak KAM theory in view of some similarities with the classical KAM theory.
Weak KAM theory, however, uses the least action principle in such a fundamental way that it   does not not enter yet  into the symplectic framework. My project is to address this problem. This  overarching goal federates   a number of questions in weak  KAM theory, in Hamiltonian dynamics, in symplectic geometry and even in partial differential equations which  will be the starting directions of my investigations.""","840000","2012-09-01","2017-08-31"
"SBS3-5","Stimulated Brillouin Scattering based RF to Optical Signal Transduction and Amplification","Krishna COIMBATORE BALRAM","UNIVERSITY OF BRISTOL","While the detection of weak signals (down to the single photon level) in the optical frequency range is routine on account of the high photon energy (compared to thermal excitation energy kBT) and the availability of efficient detectors, this is not the case in the radio frequency (RF) and microwave frequency regimes wherein thermal (Johnson) noise in detectors swamps out the faint RF signals (in applications from radio astronomy, MRI to radar) and requires the use of cryogenic amplifiers. The ability to map signals efficiently from the microwave to optical regime becomes paramount for distant systems to communicate with each other using low loss telecom fibers. Both classical (radio over fiber systems) and quantum (linking two superconducting qubit processors in two dilution fridges) information processing systems will benefit greatly from the development of an efficient RF to optical signal transducer.
 I have been developing efficient RF to optical transduction schemes in GaAs cavity optomechanical systems (KC Balram et al., Nature Photonics (2016)) by exploiting its favorable piezoelectric (for coupling RF signals to propagating acoustic waves) and elasto-optic (for engineering strong acousto-optic interactions) properties. In this project, I would like to extend this work and address the issue of weak RF signal detection by up-converting RF signals to the optical domain using integrated Stimulated Brillouin Scattering (SBS) and shot-noise limited optical detection. Piezoelectric SBS systems can also be used to build high frequency, high gain RF amplifiers with noise figures that can be lower than conventional RF amplifiers. Working in a novel GaAs on insulator platform helps provide some unique advantages (tightly confined acoustic and optical modes with large modal overlap and a large elasto-optic coefficient leading to significant Brillouin gain) while holding the potential for interfacing complex circuitry in a well-established III-V materials platform.","1712581","2018-01-01","2022-12-31"
"SCADAPT","""Large-scale Adaptive Sensing, Learning and Decision Making: Theory and Applications""","Rainer Andreas Krause","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","""We address one of the fundamental challenges of our time: Acting effectively while facing a deluge of data. Massive volumes of data are generated from corporate and public sources every second, in social, scientific and commercial applications. In addition, more and more low level sensor devices are becoming available and accessible, potentially to the benefit of myriads of applications. However, access to the data is limited, due to computational, bandwidth, power and other limitations. Crucially, simply gathering data is not enough: we need to make decisions based on the information we obtain. Thus, one of the key problems is: How can we obtain most decision-relevant information at minimum cost?

Most existing techniques are either heuristics with no guarantees, or do not scale to large problems. We recently showed that many information gathering problems satisfy submodularity, an intuitive diminishing returns condition. Its exploitation allowed us to develop algorithms with strong guarantees and empirical performance. However, existing algorithms are limited: they cannot cope with dynamic phenomena that change over time, are inherently centralized and thus do not scale with modern, distributed computing paradigms. Perhaps most crucially, they have been designed with the focus of gathering data, but not for making decisions based on this data.

We seek to substantially advance large-scale adaptive decision making under partial observability, by grounding it in the novel computational framework of adaptive submodular optimization. We will develop fundamentally new scalable techniques bridging statistical learning, combinatorial optimization, probabilistic inference and decision theory to overcome the limitations of existing methods. In addition to developing novel theory and algorithms, we will demonstrate the performance of our methods on challenging real world interdisciplinary problems in community sensing, information retrieval and computational sustainability.""","1499900","2012-11-01","2017-10-31"
"SCALABIM","Scalable Bayesian Methods for Machine Learning and Imaging","Matthias Seeger","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Machine learning seeks to automatize the processing of
large complex datasets by adaptive computing, a core strategy to meet growing
demands of science and applications.
Typically, real-world problems are mapped to penalized estimation tasks (e.g.,
binary classification), which are solved by simple efficient algorithms. While
successful so far, I believe this approach is too limited to
realise the potential of adaptive computing. Most of the work, such as data
selection, feature construction, model calibration and comparison, still has to
be done by hand. Demands for automated decision-making (e.g., tuning
data acquisition during an experiment) are not met.

Such problems are naturally addressed by Bayesian reasoning about uncertain
knowledge, which however remains infeasible in most large scale settings.
The main goal of this proposal is to unite the strengths of penalized
estimation and Bayesian decision-making, exploiting the former's advanced state
of the art in order to implement substantial improvements coming with
the latter in large scale applications. A major focus is on improving magnetic
resonance imaging (MRI) by way of new Bayesian technology, driving robust
nonlinear
reconstruction from less data, and optimizing the acquisition through
Bayesian experimental design, applications not previously attempted by machine
learning. Far beyond the reach of present methodology, these goals demand
a novel computational foundation for approximate Bayesian inference through
numerical algorithmic reductions.

This project will have high impact on probabilistic machine learning, raising
the bar for scalable Bayesian computations. It will help to open up a whole new
range of medical imaging applications for machine learning. Moreover,
substantial impact on MRI reconstruction research is anticipated. There is
strong recent interest in savings through compressive sensing, whose full
potential is realised only by way of adaptive technology such as projected
here.","1401697","2012-01-01","2016-12-31"
"ScaleML","Elastic Coordination for Scalable Machine Learning","Dan ALISTARH","INSTITUTE OF SCIENCE AND TECHNOLOGYAUSTRIA","Machine learning and data science are areas of tremendous progress over the last decade, leading to exciting research developments, and significant practical impact. Broadly, progress in this area has been enabled by the rapidly increasing availability of data, by better algorithms, and by large-scale platforms enabling efficient computation on immense datasets. While it is reasonable to expect that the first two trends will continue for the foreseeable future, the same cannot be said of the third trend, of continually increasing computational performance. Increasing computational demands place immense pressure on algorithms and systems to scale, while the performance limits of traditional computing paradigms are becoming increasingly apparent. Thus, the question of building algorithms and systems for scalable machine learning is extremely pressing. The project will take a decisive step to answer this challenge, developing new abstractions, algorithms and system support for scalable machine learning. In a nutshell, the line of approach is elastic coordination: allowing machine learning algorithms to approximate and/or randomize their synchronization and communication semantics, in a structured, controlled fashion, to achieve scalability. The project exploits the insight that many such algorithms are inherently stochastic, and hence robust to inconsistencies. My thesis is that elastic coordination can lead to significant, consistent performance improvements across a wide range of applications, while guaranteeing provably correct answers. ScaleML will apply elastic coordination to two specific relevant scenarios: scalability inside a single multi-threaded machine, and scalability across networks of machines. 
Conceptually, the project’s impact is in providing a set of new design principles and algorithms for scalable computation. It will develop these insights into a set of tools and working examples for scalable distributed machine learning.","1494121","2019-03-01","2024-02-29"
"ScaleOpt","Scaling Methods for Discrete and Continuous Optimization","László VÉGH","LONDON SCHOOL OF ECONOMICS AND POLITICAL SCIENCE","One of the most important open questions in optimization is to find a strongly polynomial algorithm for linear programming. The proposed project aims to tackle this problem by combining novel techniques from two different domains: discrete optimization and continuous optimization. We expect to contribute to exciting recent developments on the interface of these two fields. 
We use and develop new variants of the classical scaling technique. From the discrete optimization side, recent work of the PI on generalized flows extends classical network flow theory and opens up new domains for strongly polynomial computability beyond integer constraint matrices. We will apply this novel scaling technique to obtain strongly polynomial algorithms for broad classes of linear programs.
From the continuous optimization side, we aim to build the theory of geometric rescaling algorithms for linear and convex optimization. This approach combines first-order methods with geometric rescaling techniques to obtain a new family of polynomial-time algorithms. We expect to devise variants efficient in theory and in practice, which we will use in a wide range of applications.
Our discrete and continuous techniques will have important applications in submodular function minimization. We will develop new, efficient algorithms for the general problem as well as for specific applications in areas such as machine learning and computer vision.
In summary, the project will develop novel approaches for some of the most fundamental optimization problems. It will change the landscape of strongly polynomial computability, and make substantial progress towards finding a strongly polynomial algorithm for linear programming.","1488674","2018-01-01","2022-12-31"
"SCALPL","ScalPL : A Scalable Programming Language","Klaus Ostermann","PHILIPPS UNIVERSITAET MARBURG","""The goal of this project is the development of a growable programming language: a language whose vocabulary can easily be extended for purposes such as the development of domain-specific languages. The proposal addresses the long-standing """"holy grail"""" of programming: Removing the """"representational gap"""", making a program look like a description of a domain expert. This project is certainly not the first one that adresses this goal. The main novelty of this approach is its emphasis on scalability. We call an approach to grow a language scalable, if it is easy to compose multiple extensions of a language (composability), and if an extended language can be extended with the same concepts and techniques as the original language (regularity). Without composability, components can only be refined in a linear fashion. Without regularity, a different technology is required on each level of size and abstraction, thereby inhibiting scalability. If this project is successfull, it can substantially contribute to a radically new approach to programming, where many different kinds of techniques to provide domain-specific abstractions, such as frameworks, containers, libraries, domain-specific languages, code generators, and interpreters are subsumed by a single technology to grow or define a language. To this end, this project will combine and extend research results from the domains of (embedded) domain-specific languages, generative and model-driven development, aspect-oriented programming, and advanced type- and module systems.""","1382680","2008-10-01","2014-09-30"
"Scan2CAD","Scan2CAD: Learning to Digitize the Real World","Matthias NIESSNER","TECHNISCHE UNIVERSITAET MUENCHEN","""One of the most fundamental challenges in the digital age is to automatically create accurate, high-quality 3D representations of the world around us. This would have far-reaching impact, from enabling advances entertainment and immersive technologies (e.g., mixed reality) to medical applications and industrial manufacturing pipelines. Despite remarkable progress in scanning devices and 3D reconstruction algorithms, the resulting models remain highly impractical for display or use in virtual environments. This is due to the limited quality of these reconstructed 3D models, which is still far from the quality of assets designed by professional artists in countless of working hours. We believe that the key to addressing these shortcomings is understanding the design process of artist-created assets. We can then learn the correlation to real-world observations and replicate the process conditioned on these real-world input scans. In this proposal, we will answer the question """"How can we turn 3D scans into CAD-quality 3D assets?"""".

""","1500000","2019-01-01","2023-12-31"
"SCARCE","Sustainable Chemical Alternatives for Re-use in the Circular Economy","Ewan James MCADAM","CRANFIELD UNIVERSITY","This proposal seeks to develop a novel non-invasive, real-time direct observation methodology to provide new knowledge on the mechanisms underpinning crystal growth and harvesting within membrane crystallisation reactor technology. Crystallisation represents one of the most important separation processes in the chemical industry and will play a critical role in the circular economy through enabling the recovery of resources from wastewater to yield an array of sustainable low cost chemicals for use in European industries. Existing crystallisation reactor designs suffer from imperfect mixing and inhomogeneous solvent removal which makes control of crystal quality and consistency problematic and can limit application of the final product. 

Membrane crystallisation reactor technology is a disruptive innovation that combines process intensification with the capability to achieve significant control over the crystallisation process at a fraction of the scale thus ameliorating many of the problems associated with existing crystallisers. However, before this disruptive membrane based technology can be realised at full scale, there is a critical need to understand the role of shear forces in mediating the growth and harvesting of crystals at the solvent-membrane boundary which has to date received little attention. With no reliable and accurate description of the shear force behaviour within the boundary layer, there is considerable risk incurred in the scaling up of membrane crystallisation reactor design which could lead to inconsistent and inefficient performance. Development of the novel non-invasive, real-time direct observation methodology will enable direct measurement of these discrete forces. The arising new knowledge will be challenged at various process sizes to evolve the science underlying process scale-up of membrane crystallisers and in doing so will deliver internationally competitive research, placing the applicant at the forefront of his academic field.","1499656","2017-04-01","2022-03-31"
"SCARE","Side-Channel Aware Engineering","Billy Bob BRUMLEY","TAMPEREEN KORKEAKOULUSAATIO SR","""As the recent """"HeartBleed"""" bug in OpenSSL demonstrates, the security of cryptographic software and devices cannot be understated. They build the foundation for basic security guarantees such as confidentiality and authentication, enabling technologies such as secure communication. For example, Transport Layer Security enables e-commerce, a 1.9 trillion USD global industry in 2016.

The more modern trend, especially in the embedded space, is towards hardware-assisted security. Here the aim is to leverage hardware to accomplish security goals that are simply unrealistic in software-only solutions. One example is Trusted Execution Environments (TEE) that provide a secure sandbox to execute security-critical software. TEEs, often driven by ARM TrustZone Technology, are present in the majority of smartphones on the market today.

Side-channel analysis (SCA) is a cryptanalytic technique that targets not the formal description of a cryptographic primitive but the implementation of it. Examples of side-channels include power consumption, electro-magnetic radiation, acoustic emanations, and various timings. Attackers then use this auxiliary signal to recover critical algorithm state and, in combination with cryptanalytic techniques, secret key material. This is a young but very active field within security and cryptography stemming from covert channels.

SCA is the focus of SCARE. Objectives include the discovery of next generation covert channels, paving the way for novel SCA classes, and extending these to full-fledged end-to-end SCA attacks by identifying specific vulnerabilities in widely-deployed cryptography software libraries such as OpenSSL and hardware-assisted security technologies such as TEEs. In turn, SCARE will deliver a methodology for SCA security assurance: not just development, evaluation, and deployment of acute countermeasures, but bringing SCA into the product life cycle as part of continuous integration.""","1499950","2018-12-01","2023-11-30"
"sCENT","Cryptophane-Enhanced Trace Gas Spectroscopy for On-Chip Methane Detection","Jana JAGERSKA","UNIVERSITETET I TROMSOE - NORGES ARKTISKE UNIVERSITET","Sensitivity of on-chip gas sensors is still at least 2-3 orders of magnitude lower than what is needed for applications in atmospheric monitoring and climate research. For optical sensors, this comes as a natural consequence of miniaturization: sensitivity scales with interaction length, which is directly related to instrument size. The aim of this project is to explore a new concept of combined chemical and spectroscopic detection for on-chip sensing of methane, the principal component of natural gas and a potent climate forcer.

The sought-after sensitivity will be achieved by pre-concentrating gas molecules directly on a chip surface using cryptophanes, and subsequently detecting them using slow-light waveguides and mid-infrared laser absorption spectroscopy. Cryptophanes are macromolecular structures that can bind and thus pre-concentrate different small molecules, including methane. Spectroscopic detection of methane in a cryptophane host is an absolute novelty, and, if successful, it will not only contribute to unprecedented sensitivity enhancement, but will also address fundamental questions about the dynamics of small molecules upon encapsulation. The actual gas sensing will be realized using evanescent field interaction in photonic crystal waveguides, which exhibit both large evanescent field confinement and long effective interaction pathlengths due to the slow-light effect. The waveguide design alone is expected to improve the per-length sensitivity up to 10 times, while another 10 to 100-fold sensitivity enhancement is expected from the pre-concentration. 

The targeted detection limit of 10 ppb will revolutionize current methods of atmospheric monitoring, enabling large-scale networks of integrated sensors for better quantification of global methane emissions. Beyond that, this method can be extended to the detection of other gases, e.g. CO2 and different volatile organic compounds with equally relevant applications in the medical domain.","1499749","2018-01-01","2022-12-31"
"SCOPE","""Scandium-based multifunctional nitrides for optoelectronic, polaritonic and ferro/magnetoelectric devices""","Michelle Anna Moram","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","""Device performance is ultimately limited by material properties. To break existing performance limits, I plan to develop a new multifunctional Sc-based nitride materials platform in which tuneable semiconducting, ferroelectric and magnetic functionalities may be achieved for the first time. I can then address three major challenges: (1) solving a 20-year-old problem in solid-state-lighting using highly efficient green light-emitters, (2) creating the first electrically pumped full-spectrum polaritonic lasers, and (3) developing an optimised artificial synapse for adaptive computing (plus similar related non-volatile memory devices). Combined, such advances could lead towards the “spintronics dream” of devices with integrated optoelectronic, photonic and magnetic functionality. Importantly, the mature growth and device processing technologies in place for conventional nitride semiconductors will allow rapid exploitation of this versatile new group of materials.

A theme is planned for each objective. At Stage 1 (months 1 – 30), I plan to grow and characterise crystalline thin films of the Sc-based nitrides relevant to each theme, aiming to control the structure and properties of the individual layers and simple heterostructures from which devices will be constructed. Themes 1 and 2 will use specific Sc/III-nitride alloys created by molecular beam epitaxy, a flexible industry-proven growth technique. Theme 3 will explore and use a broad range of transition-metal/ScGaN alloys created by electron-beam epitaxy, a relatively low-cost technique optimised for transition metals. Full experimental data on layer properties will be input into models, which will be used to design full device structures. Optimised device designs will be chosen at Month 30. At Stage 2 (months 31 – 60), test structures will be created, processed and characterised, mostly using techniques already established for III-nitride semiconductor devices, leading to working prototype devices for each theme.""","1486342","2012-11-01","2017-10-31"
"SCoTMOF","Combined Chemo- and Radiotherapies by Controlling the Surface Chemistry of Truncated Metal Organic Framework Nanoparticles","Ross Stewart Forgan","UNIVERSITY OF GLASGOW","Cancers are the leading cause of death in the developed world, with populations facing a 30% chance of developing the disease by the age of 75. As part of a concerted effort to open up new treatments and improve patients’ experiences with existing ones, the concept of drug delivery – using non-toxic carriers to transport medicines directly to the location of disease – has emerged. Metal-organic frameworks (MOFs), porous materials comprised of organic linkers and metal joints, show considerable promise as drug delivery vectors due to their high storage capacities, amenability to functionalization and the ability to prepare entirely non-toxic nanoparticulate derivatives. This proposal will use the PI’s expertise in advanced MOF synthetic methods to facilitate dramatic technological breakthroughs through unprecedented control of MOF self-assembly and surface chemistry. Management of MOF surface chemistry will allow installation of stimuli responsive release mechanisms and offer control over the trapping and release of cargo within MOFs, ensuring drugs are released only at the site of disease in the body. Surface incorporation of sophisticated biotargeting units such as peptides and aptamers will facilitate selective uptake of the MOFs by diseased tissues only. Rapid clean microwave syntheses will allow metal radionuclides to be incorporated for PET imaging, offering a novel alternative to traditional chelates. Comprehensive in vitro and in vivo testing will ensure that this multidisciplinary streamlining of materials, supramolecular and medicinal chemistries with the biosciences will generate highly efficient theranostic devices, offering more efficient, targeted drug delivery to improve treatment efficiency, mitigate side effects and open up new therapeutic avenues such as siRNA delivery. The fundamental advances required to generate these novel materials will also impact across the many applications of MOFs, from molecular storage and separations to catalysis.","1493777","2016-09-01","2021-08-31"
"SCS","Strongly Coupled Systems","David Mark Tong","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Strongly coupled systems comprise some of the most difficult and important problems in physics, from high temperature superconductivity, to quantum chromodynamics, to gravity at the Planck scale. Understanding even the most basic questions about these systems --- such as the phase structure or the relevant degrees of freedom --- remains a challenge. It is necessary to introduce new, imaginative techniques to tackle these problems.

In the past decade, several new ideas have emerged from research in string theory. While string theory is often paraded as a “theory of everything”, a less trumpeted facet is the way in which is gives new perspectives to study more down to earth systems, in particular the strongly coupled phases of quantum field theories.

The goal of this project is to develop these techniques to extract information about strongly coupled quantum field theories, both relativistic and non-relativistic, including theories at conformal points and those with a mass gap. The focus is on the application of gauge/gravity duality to new arenas. However, we will also explore the constraints imposed by supersymmetry on non-relativistic systems.  These results will be important for communities interested in quantum field theory, string theory, and condensed matter physics.","1271623","2011-10-01","2017-09-30"
"SEAQUEL","Structured Ensembles of Atoms for Quantum Engineering of Light","Alexei Ourjoumtsev","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","This project aims at building a new versatile platform for quantum engineering of light, with the unique ability to create deterministic coherent photon-photon interactions tunable in range, strength and dimensionality. It will explore a new avenue towards this goal, combining cutting-edge advances of atomic physics with ideas inspired by nanophotonics: a cold micro-structured gas of interacting atoms will act as a Bragg mirror saturable by a single photon, strongly coupling a controlled number of spatial modes in an optical resonator. This flexible, efficient, dynamically-controlled system will be used to test the limits of fundamental no-go theorems in quantum logic, measure physical quantities inaccessible to standard detectors, and deterministically engineer massively entangled light beams for Heisenberg-limited sensing. Ultimately, it will give access to a yet unexplored regime where intracavity photons form a strongly correlated quantum fluid, with spatial and temporal dynamics ideally suited to perform real-time, single-particle-resolved simulations of non-trivial topological effects appearing in condensed-matter systems.","1500000","2016-07-01","2021-06-30"
"SEC","Stereoretentive-Enantioconvergent Catalysis: A New Concept in Asymmetric Synthesis","Andrew LAWRENCE","THE UNIVERSITY OF EDINBURGH","This project will experimentally establish a new concept in asymmetric synthesis: stereoretentive-enantioconvergent catalysis. This will represent a completely new method for accessing enantiopure materials starting from racemic substrates and will therefore impact all areas of synthetic chemistry. The ability to synthesise chiral molecules in enantiopure form is vitally important, most recognisably for the pharmaceutical industry. This is because the molecules of life are chiral (e.g., D-sugars and L-amino acids) and enantiomers often interact very differently with living organisms. Classically, asymmetric synthesis utilising racemic substrates is limited to achieving a maximum yield of 50% (e.g., kinetic resolutions). Enantioconvergent catalysis avoids this limitation with both enantiomers of the starting material being converted into a single enantioenriched product, thanks to complex stereoablative or stereomutative de-racemisation processes. This project will establish a conceptually new stereoretentive-dimerisation approach that results in both enantiomers of the starting material being incorporated into the product with no de-racemisation required. This new concept will prove highly valuable for the synthesis of small enantiopure building blocks, which will be of high value in many areas of synthesis, and also for more complex late-stage transformations in complex molecule synthesis. Several approaches will be pursued to demonstrate proof-of-principle, and applications in the synthesis of complex natural and unnatural products will then be used to demonstrate the potential of stereoretentive-enantioconvergent catalysis in target-orientated synthesis.","1498915","2017-11-01","2022-10-31"
"SECOMP","Efficient Formally Secure Compilers to a Tagged Architecture","Catalin HRITCU","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","Severe low-level vulnerabilities abound in today’s computer systems, allowing cyber-attackers to remotely gain full control. This happens in big part because our programming languages, compilers, and architectures were designed in an era of scarce hardware resources and too often trade off security for efficiency. The semantics of mainstream low-level languages like C is inherently insecure, and even for safer languages, establishing security with respect to a high-level semantics does not guarantee the absence of low-level attacks. Secure compilation using the coarse-grained protection mechanisms provided by mainstream hardware architectures would be too inefficient for most practical scenarios. This project is aimed at leveraging emerging hardware capabilities for fine-grained protection to build the first, efficient secure compilers for realistic programming languages, both low-level (the C language) and high-level (ML and a dependently-typed variant). These compilers will provide a secure semantics for all programs and will ensure that high-level abstractions cannot be violated even when interacting with untrusted low-level code. To achieve this level of security without sacrificing efficiency, our secure compilers will target a tagged architecture, which associates a metadata tag to each word and efficiently propagates and checks tags according to software-defined rules. We will experimentally evaluate and carefully optimize the efficiency of our secure compilers on realistic workloads and standard benchmark suites. We will use property-based testing and formal verification to provide high confidence that our compilers are indeed secure. Formally, we will construct machine-checked proofs of full abstraction with respect to a secure high-level semantics. This strong property complements compiler correctness and ensures that no machine-code attacker can do more harm to securely compiled components than a component in the secure source language already could.","1498444","2017-01-01","2021-12-31"
"SEDBIOGEOCHEM2.0","Hardwiring the ocean floor: the impact of microbial electrical circuitry on biogeochemical cycling in marine sediments","Filip Meysman","STICHTING NIOZ, KONINKLIJK NEDERLANDS INSTITUUT VOOR ONDERZOEK DER ZEE","Although it is well known that microbial cells can exhibit sophisticated cooperative behaviour, none of the recent advancements in geomicrobiology has been so perplexing as the proposal that microbial populations are capable of fast, electrical communication over centimetre scale distances. This metabolic tour-de-force was recently documented from laboratory incubations with marine sediments. Clearly, the phenomenon is so thought provoking, and its consequences are so far reaching, that independent verification is absolutely needed. Recently, my research group has collected strong evidence that long-distance electron transport is not merely a laboratory phenomenon, but that it effectively happens under in situ conditions in marine sediments. These observations open a broad avenue for new research, since at present, we no understanding of the prevalence of long-distance electron transport in natural environments, let alone, its impact on biogeochemical cycling. In response, this ERC project proposes an in depth investigation into long-distance electron transport in aquatic sediments: when and where does it occur, which redox pathways and microbial players are involved, what is the effective mechanism of electron transfer, and what are its biogeochemical implications. Clearly, this idea of long-distance electron transport would add a whole new dimension to microbial ecology, radically changing our views on microbial cooperation. Yet, the consequences for carbon sequestration and mineral cycling in sediments and soils could even be more astounding, allowing an unprecedented flexibility in redox pathways. Since the same type of extracellular electron transport is at work in engineered systems like microbial fuel cells, it could also improve our understanding of such biotechnological applications.","1497996","2012-09-01","2017-08-31"
"SEDMORPH","The origins of galaxy bimodality: Linking mergers, starbursts and feedback in observations and simulations","Vivienne Wild","THE UNIVERSITY COURT OF THE UNIVERSITY OF ST ANDREWS","Understanding how and why galaxies form and evolve is one of the most challenging problems in modern astrophysics. Our own galaxy, the Milky Way, shows order and structure, as do most massive galaxies in our local neighbourhood. Yet when we look to very distant galaxies they are disordered and chaotic. The leading theory for the origin of this transformation invokes gas-rich mergers, which trigger massive starbursts leading to bulge and supermassive black hole growth. The aim of this project is to provide conclusive observational evidence to confirm or refute this fundamental theory of galaxy evolution.

Considerable quantities of high quality data are now available for both local and distant galaxies; new methodology is urgently required to enable the translation of this data into an improved understanding of galaxy formation. In this project I will lead a team to develop a suite of new techniques to: (1) statistically link galaxy populations traditionally studied in isolation (starbursts, post-starbursts, mergers, remnants); (2) combine information from both the multi-wavelength spectral energy distributions and morphologies of galaxy samples; (3) visualise the information contained in multiple large datasets. My team will compare directly with merger models to interpret the data in terms of the physical processes driving galaxy evolution. The new techniques will provide stringent observational constraints on models, improve robustness of model-data comparison and highlight areas for improvement.

As the only researcher with access to all four of the newest world-leading surveys for galaxy evolution, I am uniquely placed to build an integrated picture of the dominant physical processes that drive galaxy evolution over 3/4 of cosmic time. An ERC grant will allow me to build a team to fully exploit the information provided by all four surveys, through novel analysis techniques and concurrent comparison with models.","1430622","2012-09-01","2018-01-31"
"SeeSuper","Probing nanoscale and femtosecond fluctuations in high temperature superconductors","Simon Wall","FUNDACIO INSTITUT DE CIENCIES FOTONIQUES","One of the major outstanding challenges in condensed matter physics is the origin of high temperature superconductivity. Low temperature BCS superconductivity is mediated by the electron-phonon interaction, but this interaction is believed to be too weak to explain high temperature superconductivity. Instead electron interactions are considered responsible, but experimental proof has been difficult to obtain. Despite over thirty years of research, the mechanism responsible for generating the superconducting state still remains unknown. 

SeeSuper aims to break this deadlock by applying new experimental techniques to study the superconducting state. Our strategy is to probe high temperature superconductors through their nanoscale and femtosecond fluctuations. We will focus on three key parameters in superconductors: phonons, spins and nanoscale phase separation, with the aim of revealing the coupling mechanism. 

Our approach combines transient optical spectroscopy and time-resolved diffuse X-ray scattering to measure the lattice response to large amplitude coherent vibrations, time-resolved non-linear optical spectroscopy to directly probe spin dynamics, and resonant soft X-ray holography to image dynamics on the nanoscale. 

We will use these cutting edge techniques to prove our hypothesis, that lattice anharmonicity is the key missing ingredient to explain the origins of high temperature superconductivity. If demonstrated, the impact of such a result will lead to a step-change in our understanding of how superconductivity at high temperature occurs, help guide the search for materials with higher transition temperatures, and influence how we view and understand a much broader class of materials. Furthermore, the experimental techniques that we will develop can be applied to understand a range of materials and will, therefore, have an impact also on the broader field of condensed matter physics.","1789165","2017-11-01","2022-10-31"
"SEEVS","Self-Enforcing E-Voting System: Trustworthy Election in Presence of Corrupt Authorities","Feng Hao","UNIVERSITY OF NEWCASTLE UPON TYNE","""This project aims to develop a new generation of e-voting called the “self-enforcing e-voting system”. The new system does not depend on any trusted authorities, which is different from all currently existing or proposed e-voting schemes. This has several compelling advantages. First, voting security will be significantly improved. Second, the democratic process will be enforced as a whole. Third, the election management will be dramatically simplified. Fourth, the tallying process will become much faster.

The idea of a “self-enforcing” e-voting system has so far received little attention. Although several researchers have attempted to build such a system in the past decade, none were successful due to inefficiencies in computation, bandwidth and the number of rounds. My preliminary investigation indicates that a """"self-enforcing e-voting system"""" is not only practically feasible, but has the potential to be the future of e-voting technology. I have identify several major research problems in the field, which need to be addressed urgently before a self-enforcing e-voting system can finally become viable for practical use. The problems span three disciplines: security, dependability and usability.

My main hypothesis is: “a secure, dependable and usable self-enforcing e-voting system will trigger a paradigm shift in voting technology”.  I believe e-voting has great potential that has yet to be exploited, and this project is to develop that potential to the full. The work program involves six work packages: 1) to develop supportive security primitives to lay foundation for future e-voting; 2) to research the impact of “self-enforcing” requirement on dependability; 3) to develop assistive technologies to improve the usability in voting; 4) to design system architectures suitable for different election scenarios; 5) to build open source prototypes; 6) to conduct real-world trial elections and evaluate the full technical, social, economic and political impacts.""","1484713","2013-01-01","2018-12-31"
"SEIC","Setting Earth's Initial Conditions: A fluid dynamics study of core-mantle differentiation","Renaud DEGUEN","UNIVERSITE LYON 1 CLAUDE BERNARD","The initial conditions of the Earth and other terrestrial planets were set 4.5 Gy ago during their accretion from the solar nebula and their concomitant differentiation into an iron-rich core and a silicate mantle. Accretion in the solar system went through several different dynamical phases involving increasingly energetic and catastrophic impacts and collisions. The last phase of accretion, in which most of the Earth mass was accreted, involved extremely energetic collisions between already differentiated planetary embryos (1000 km size), which resulted in widespread melting and the formation of magma oceans in which metal and silicates segregated to form the core and mantle. Geochemical data provide critical information on the timing of accretion and the prevailing physical conditions, but it is far from a trivial task to interpret the geochemical data in terms of physical conditions and processes. 
I propose here a fluid dynamics oriented study of metal-silicate interactions and differentiation following planetary impacts, based in part on fluid dynamics laboratory experiments. The aim is to answer critical questions pertaining to the dynamics of metal-silicate segregation and interactions during each core-formation events, before developing parameterized models of metal-silicate mass and heat exchange, which will then be incorporated in geochemical models of the terrestrial planets formation and differentiation. The expected outcomes are a better understanding of the physics of metal-silicate segregation and core-mantle differentiation, as well as improved geochemical constraints on the timing and physical conditions of the terrestrial planets formation.","1258750","2017-04-01","2022-03-31"
"SEISMIC","Slip and Earthquake Nucleation in Experimental and Numerical Simulations: a Multi-scale, Integrated and Coupled Approach","Andre Niemeijer","UNIVERSITEIT UTRECHT","Earthquakes represent one of the deadliest and costliest natural disasters affecting our planet – and one of the hardest to predict. To improve seismic hazard evaluation in earthquake-prone regions, an understanding of earthquake nucleation and of the underlying microphysical and chemical processes is crucial. A better understanding of the processes that control earthquake nucleation is also of rapidly growing importance for mitigation of induced seismicity, caused by activities such as gas and oil production, and geological storage of CO2 or gas. The SEISMIC project is a multi-scale study aimed at understanding the parameters that control slip (in)stability in experiments and models addressing earthquake nucleation. A central question to be tackled is what controls the velocity-dependence of fault friction and hence the potential for accelerating, seismogenic slip, and on what length scales the processes operate. A novel acoustic imaging technique will be developed and applied in experiments to obtain direct information on the internal microstructural evolution of fault slip zones during deformation, and on how this evolution leads to unstable slip. The SEISMIC project will link experiments with sophisticated numerical models of grain-scale frictional processes. Using both experiments and grain scale modelling, the SEISMIC project will in turn directly test boundary element models for large scale fault slip. The coupling of experiments with grain-scale numerical models, based on in-situ imaging, will provide the first, integrated, multiscale physical basis for extrapolation and upscaling of lab friction parameters to natural conditions. Ultimately, the SEISMIC project will test and validate the resulting models for fault slip by simulating and comparing patterns of seismicity for two natural-laboratory cases: a) for the l’Aquila region of Central Italy, and b) for a reservoir-scale case study involving induced seismicity in the Netherlands.","1499600","2013-09-01","2019-02-28"
"SELFCHEM","Information Transfer through Self-organization Processes in Systems Chemistry","Nicolas Giuseppone","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Today, one of the greatest challenges facing physics, chemistry, and (bio)materials science, is to precisely design molecules so as to program their spontaneous  bottom-up  assembly into functional nano-objects and materials, based on recognition and self-organization processes. Beyond that, in order to reach higher-performing new materials and to bridge the gap between materials science and life science, it appears essential to bring together both multiple responsive levels of hierarchical organization and time-dependent processes.
The objectives of the SelfChem research project are part of this bundle of explorations and thus lie within an area inquiry which encompasses a better understanding of complex systems, self-organization, and emergence of order from chaos. The main specificity and novelty of the SelfChem project is to focus on an issue that has not been approached to date, namely the possibility to transfer chemical or physical information, in space and time, through the self-induced organization of their own supramolecular carriers. In other words, we wish to show that the circulation of information can be the driving force for the self-assembly of systems that will in turn serve to transfer this very information. The main axes of the proposal are three-fold and deal with: a) the duplication of chemical information towards several generations of bounded systems that couple small molecular self-replicators within self-replicating vesicles (reproduction); b) the transfer and conversion of chemical information between two compartments separated by a non permeable membrane (transduction); and c) the transport of physical information, i.e. electric charges, by the enforced self-organization of molecular wires between two electrodes (conduction). In addition to these fundamental investigations, we plan to use the knowledge produced for the design of smart, responsive, and adaptive (bio)materials.","1494075","2010-11-01","2015-10-31"
"SELFOR","How our adaptive immune system separates “self” from “foreign” – a physicochemical study of binding in cell contacts","Peter JÖNSSON","LUNDS UNIVERSITET","How can our immune system efficiently fight foreign pathogens at the same time as it is selective enough to leave our own cells alone? This, so called antigen discrimination, is still not understood on a molecular level. I will here illuminate this question by developing a physicochemical understanding of how T-cell receptors (TCRs) bind peptide-loaded major histocompatibility complexes (pMHCs), and importantly, how this differs for pMHC displaying self and foreign peptides. The interaction between TCRs and pMHCs on contacting immune cells is the first step in initiating an adaptive immune response. However, binding between membrane-anchored TCR and pMHC is a challenging physicochemical problem that has not been solved, and the binding kinetics is strongly affected by the membrane. Well-controlled measurements between TCR and pMHC under relevant conditions are needed to better understand the TCR/pMHC kinetics. A key discovery to achieve this is a method recently developed by me, which allows for extremely weak protein-protein interactions in contacting cells to be measured. I will in this project, backed up by immunologist in Oxford and theoretical chemists in Lund, build on this breakthrough and for the first time measure the binding kinetics between TCR in model cell membranes and pMHC on single cells, and how binding varies between self and foreign pMHC. Parameters such as adhesion molecules, applied force on the bond, the co-receptor CD4 and TCR clustering are all crucial for proper T-cell signalling, but their influence on the TCR/pMHC binding kinetics is unknown, and will here be studied. The obtained data will finally be used to evaluate, and improve on, kinetic models of antigen discrimination. Complemented with molecular dynamics simulations this altogether opens up for a fundamental understanding of binding between membrane-anchored molecules in general, and how this affects the TCR/pMHC interaction and antigen discrimination in particular.","1500000","2018-01-01","2022-12-31"
"SEMANTICS","Semiconducting and Metallic nanosheets: Two dimensional electronic and mechanical materials","Jonathan Nesbitt Coleman","THE PROVOST, FELLOWS, FOUNDATION SCHOLARS & THE OTHER MEMBERS OF BOARD OF THE COLLEGE OF THE HOLY & UNDIVIDED TRINITY OF QUEEN ELIZABETH NEAR DUBLIN","We will develop simple, scalable methods to exfoliate layered compounds into monolayer nanosheets. These materials have exciting properties. Recently, graphene has taken the nanomaterials community by storm. However graphene is only one branch of a family of two dimensional layered compounds. Other examples include hexagonal BN, metal dichalcoginides such as MoS2 and metal oxides such as MnO2. We propose that all layered compounds can be exfoliated in certain solvents by the addition of ultrasonic energy. Such a method has not been demonstrated because the vast majority of solvents are unsuitable for this. We propose that suitable solvents can be identified by matching their surface energy to that of the nano crystal, rendering the exfoliation process energy neutral. This will open the gate to a wide range of nano-materials science and makes possible experiments that have been impossible using standard techniques. We will pick a set of layered compounds such as the semiconductors; hexagonal BN, MoS2 and TaO3 and the metals TaS2 and MnO2. We will learn to exfoliate these materials, studying the physics and chemistry of the solvent-nanosheet interaction. Once we can generate large volumes of highly exfoliated few-layer nanosheets at high concentration, we will study the physics of these materials. We will prepare free standing films of restacked sheets and polymer-sheet composites for mechanical applications. Thin films can also be studied as transparent conductors and capacitor dielectrics. Hybrid films can be used to devices such as photodetectors. Much more challenging will be the production of large quantities of monolayer nanosheets. We will learn to deposit nanosheets on substrates and measure their thickness and size with AFM. Semiconducting monolayers will be characterised by photoluminescence spectroscopy leading to a spectroscopic metric for monolayer population. By optimising the link between exfoliation procedure and monolayer population, we will develop methods to produce monolayer enriched samples. This will pave the way to nanostructured devices such as light emitting diodes.","1405633","2010-10-01","2016-09-30"
"SenseX","Sensory Experiences for Interactive Technologies","Marianna Obrist","THE UNIVERSITY OF SUSSEX","The senses we call upon to interact with technology are still very limited relying mostly on visual and auditory senses. The grand challenge and vision of this project is to gain a rich and integrated understanding on touch, taste, and smell experiences for interactive technologies.

We aim to achieve this ambitious grand vision by 1) creating a ‘sensory interaction framework’ on the bases of a systematic empirical investigation of touch, taste, and smell experiences, 2) integrating the generated understanding on the three senses into meaningful and efficient experiential cross-sensory gamuts and interaction principles, and 3) demonstrating the added value of the created experiential understanding on touch, taste, and smell – aka the experiential gamuts – through their integration into the development of multi-sensory systems verifying the short-, mid- and long-term societal and scientific impact (short-term: multi-sensory media experiences; mid-term: interaction concepts for partially sensory impaired people; long-term: multi-sensory interaction approach for life beyond Earth).

This research will pioneer novel interaction concepts for interactive technologies in relation to essential components of multi-sensory experiences. This project will transform existing interaction paradigm in Human-Computer Interaction (HCI) and likewise impact other disciplines such as sensory and cognitive sciences by delivering ground-breaking new insights on the experiential dimensions underlying neurological processes and human perception.

The PI’s research excellence and her track record (three recent seminal papers on touch, taste, and smell at the premier HCI – CHI conference) provide a rock solid foundation for the feasibility of the proposed scientific venture. This ERC project would enable her to establish an independent and interdisciplinary team to revolutionize multi-sensory research within the HCI community with impact across disciplines.","1494865","2015-04-01","2020-03-31"
"SENSiSOFT","New sensor devices based on soft chemistry assisted nanostructured functional oxides on Si integrated systems","Adrien CARRETERO","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Piezoelectrics are the active elements of many everyday applications, from ink-jet printers to ultrasound generators, representing a billion euro industry. They are the key elements of motion sensors and resonators present in any wireless network sensor (WNS) node. However, an increased production of piezoelectrics in a sustainable way is to-date a milestone. SENSiSOFT proposes to come up with materials that can provide a solution to this problem: piezoelectric materials that are abundant, cheap and harmless. The aim of this project is to produce new piezoelectric devices of nanometer size with an unusual limit for wireless mechanical sensors, using direct and combined chemical integration of quartz, perovskite and hollandites materials as nanostructured epitaxial thin films on silicon. This is a major challenge that demands bridging the gap between soft-chemistry and microfabrication techniques. Three strategies are proposed for this goal: 
i) Implement a soft chemistry unified, monolithic process that will allow integrating epitaxial quartz, hollandite and perovskite oxide thin layers on silicon substrate with high piezoelectric response. 
ii) Nanostructuration of piezoelectric epitaxial oxide thin films into controllable morphologies or nanostructures, in particular porous structure and 1D nanowires or nanorods, allowing excellent properties of oxides to be exploited to the fullest, mainly by avoiding clamping and improving its sensitivity.
iii) Fabrication of nanostructured SAW resonator-based and a LAMB-WAVE multisensor for monitoring mechanical parameters (mass, forces, pressure…). We will use MEMs technology in order to be able to define resonating structures (plates, membranes, bridges…) by silicon micromachining. 
So, SENSiSOFT presents three innovative strategies to develop sensor devices capable to answer the metrology demand, with a detection threshold 10 to 100 times more sensitive resulting from a 1D and 2D configuration of novel piezoelectric oxides.","1499360","2019-01-01","2023-12-31"
"SENSQUID","Scanning SQUID view of emergent states at interfaces","Beena Kalisky","BAR ILAN UNIVERSITY","The emergence of novel states of matter in low-dimensional systems is one of the most intriguing current topics in condensed matter physics. For instance, interfaces between certain non-magnetic insulating oxides were shown to give rise to surprising metallic, superconducting, and magnetic states, which are still far from being understood. I have recently demonstrated in LaAlO3/SrTiO3 that there is a strong influence of the constituent’s structure on the interface conductivity (quasi-1D rather than 2D) and sub-micron ferromagnetic patches that coexist with inhomogeneous superconductivity. However, the origin of the interface magnetism, its relation to transport properties, and the mechanisms that control the different interface states are yet to be understood. I believe that the only way to fully understand the electronic and magnetic behavior in reduced dimensions is by combining extremely sensitive, non-invasive, local techniques, but such characterization tools are lacking. The aim of this project is to investigate the rich phenomena that appear at transition metal oxides interfaces, starting with LaAlO3/SrTiO3 as a model system, and expanding to other ground states (e.g. multiferroics, quantum materials, metal-insulator), as well as to other low-dimensional systems, including 2D-superconductors, topological insulators and carbon nanotube coils. To this end, I will develop an advanced scanning SQUID technology for higher temperatures, improved resolution, simultaneous mapping of orthogonal properties, and high throughput. By detecting nano-magnetism, traces of superconductivity, and non-invasively mapping the path of current flow, our tool will detect new states of matter, follow their interactions, correlations, and response to modulation in the local potential with extreme sensitivity. Our results will open up access to fundamental physics in atomically engineered materials, and to the control of their properties for use in next generation nanoelectronics.","1499778","2015-04-01","2020-03-31"
"SENTIENT","SCHEDULING OF EVENT-TRIGGERED CONTROL TASKS","Manuel MAZO ESPINOSA","TECHNISCHE UNIVERSITEIT DELFT","The advances in electronic communication and computation have enabled the ubiquity of Cyber-Physical Systems (CPS): digital systems that regulate and control all sorts of physical processes, such as chemical reactors, water distribution and power networks. These systems require the timely communication of sensor measurements and control actions to provide their prescribed functionalities. Event-triggered control (ETC) techniques, which communicate only when needed to enforce performance, have attracted attention as a mean to reduce the communication traffic and save energy on (wireless) networked control systems (NCS). However, despite ETC’s great communication reductions, the scheduling of the aperiodic and largely unpredictable traffic that ETC generates remains widely unaddressed – hindering its true potential for energy and bandwidth savings.

To address this problem, I will take up the following scientific challenges: (1) the construction of models for ETC’s communication traffic; (2) the design of schedulers based on such models guaranteeing prescribed performance levels. To reach these goals, I will employ scientific methods at the cross-roads between theoretical computer science, control systems and communications engineering. I propose to follow a two step approach that I have recently demonstrated: 
(i) modeling as timed-priced-game-automata (TPGA) the timing of communications of event-triggered control systems; and (ii) solving games over TPGAs to prevent data communication collisions and ensure prescribed performances for the control tasks.

I will produce algorithms facilitating the efficient implementation of control loops over shared communication resources and increasing the energy efficiency of wireless NCS by orders of magnitude. The advances will be demonstrated on automotive and wireless water-distribution control applications, showcasing the potential economic impact from the reduction of implementation and maintenance costs on CPSs.","1499941","2018-02-01","2023-01-31"
"SEQUENCES","New Strategies for Controlling Polymer Sequences","Jean-François André Victor Lutz","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Sequence-controlled polymerizations play a key role in Nature. Although formed from a rather modest library of monomers, sequence-defined macromolecules such as proteins or nucleic acids are largely responsible for the complexity and diversity of the biological world. By analogy, one may predict that synthetic sequence-defined polymers could play an important role in modern applied materials science. Paradoxically, very little effort has been spent within the last decades for developing sequence-specific polymerization methods.

In this scientific context, the target of the present proposal is to develop new approaches for controlling macromolecular sequences. In particular, new possibilities for controlling comonomer sequences in standard synthetic processes such as chain-growth polymerizations (e.g. controlled radical polymerization) and step-growth polymerizations will be investigated. The strategies for controlling sequences will be principally chemical (e.g. controlled monomer insertion, organocatalysis, sequential monomer additions) but physical (e.g. confinement, transient monomer complexation) and eventually biochemical (e.g. biocatalysis) routes will be also considered.

The essence of this project is indeed highly fundamental. Indeed, the control over polymer sequences remains one of the last  holy grails  in polymer science. Nevertheless, on a longer term, this research may be also extremely relevant for applications. Indeed, sequence-controlled polymers are most likely the key towards new generations of functional sub-nanometric materials.","1200000","2010-11-01","2014-10-31"
"SEQUNET","Semiconductor-based quantum network","Hendrik Bluhm","RHEINISCH-WESTFAELISCHE TECHNISCHE HOCHSCHULE AACHEN","Quantum networking would enable the connection of quantum processing nodes to increase computing power, long distance intrinsically secure communication, and the sharing of quantum resources over wide networks. Fully realizing these prospects requires local nodes with many coupled qubits connected by photonic links. Currently, qubits with good prospects for scaling to large numbers provide no optical interface, while optically addressable systems appear difficult to scale. This project aims to establish the fundamentals for quantum networks consisting of potentially scalable semiconductor spin qubits in gated GaAs quantum dots. These electrically controlled qubits have been proven viable for quantum computing, but so far have not been interfaced coherently with photons.
To achieve the latter, we plan to use local electric fields generated by gate electrodes on both sides of a quantum well to create bound exciton states in a semiconductor structure that also hosts quantum dot qubits. These hybrid devices will make results from semiconductor quantum optics and self-assembled quantum dots applicable to gate-defined quantum dots. Besides laying the foundations for our technological goal, such a connection of two very active subfields will open a broad range of new possibilities. 
Building on the capability to optically address our qubits, we plan to implement a protocol to transfer their quantum state to a photon. In addition, we plan to implement exchange-based two-qubit gates for two-electron spin qubits, which promise a much higher fidelity than the demonstrated Coulomb-coupled gates. Such high fidelity entangling gates are essential for quantum information processing. We then aim to integrate a photon interface into a two-qubit device in order to entangle a photonic flying qubit and a scalable semiconductor qubit. Finally, two such devices will be used to entangle separate semiconductor qubits via a photonic link, thus demonstrating a minimal network.","1500000","2016-05-01","2021-04-30"
"SEQUOIA","A scalable quantum architecture","Brian David Gerardot","HERIOT-WATT UNIVERSITY","Scientists world-wide are in pursuit of radical proposals to exploit coherent quantum states for a diverse range of applications including communication, information processing, and metrology. Similar to conventional technologies, the quantum machinery will most likely consist of photons and semiconductor devices to create, transmit, receive, and process the quantum information. Indeed, a range of coherent solid-state quantum states which interact with photons have been developed over the last decade. However, the inherent semiconductor promise of scalability has yet to be realized for these quantum systems.

SEQUioA takes a highly inventive approach to quantum scalability. It will allow new fundamental investigations into quantum coherence in the solid state and the further development of quantum technologies. SEQUioA will build and exploit a novel architecture to deterministically create a non-local coherent interaction, or entanglement, between multiple remotely located semiconductor quantum dots. The architecture achieves independent optical and electrical control of each quantum dot on the same semiconductor chip. Two distinct types of non-local interactions will be investigated. The first, based on projective measurements of indistinguishable photons, probes and exploits the ideal atom-like behaviour of quantum dots. The second coupling mechanism, mesoscopic in nature, is based on a long-range magnetic interaction between confined spins in spatially remote dots.","1498457","2013-01-01","2017-12-31"
"SeSaMe","Sustainable routes for Smart photonic Materials","Silvia Vignolini","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Structural colour arises from constructive interference of light that is reflected at interfaces within periodic arrays of transparent materials. Their optical response is well understood and widely described in various biological organisms. Despite the maturity of the research field, many important questions remain completely unsolved.
In order to elucidate the design principles that underlie the development of such structures in nature, I aim to study the assembly and optical response of both natural and bio-mimetic materials made by using the same materials as nature: cellulose and chitin. Bio-mimetic using natural building blocks will also reveal if disorder, always present in natural structures, is a direct consequence of intrinsic material limitations or if it has a biological significance.
Furthermore, understanding the assembly of natural materials will also allow the production of low cost, biodegradable photonic materials.","1500000","2015-10-01","2020-09-30"
"SFEROT","Secure Function Evaluation – from Theory to Tools","Benny Pinkas","BAR ILAN UNIVERSITY","Modern cryptography is known for the introduction of public key cryptography, which has been widely applied in practice. However, the theory of cryptography provided additional powerful (and less intuitive) tools. One of its most attractive contributions is secure computation, also known as secure function evaluation - SFE, which allows multiple participants to implement a joint computation that, in real life, may only be implemented using a trusted party. The participants, each with its own private input, communicate without the help of any trusted party, and can compute any function without revealing any information about the inputs except for the value of the function. A classic example of such a computation is the “millionaires’ problem”, in which two millionaires want to find out who is richer, without revealing their actual worth. Thus far, secure computation techniques have rarely been applied in practice, and are typically considered to have mostly theoretical significance. In this research proposal we intend to build tools that translate these theoretical results into practical applications. Our goal is that secure computation solutions, which today are usually stated as mathematical theorems, will be available as tools usable by non-experts, similar to state-of-the-art tools for technologies such as public key encryption, linear programming, or data compression. The research will proceed in two directions: First, we will develop generic tools (essentially compilers) which translate functions defined using a high-level language to distributed programs that implement secure evaluation of the defined functions. We also expect that this effort will unearth many questions of theoretical interest, which we will investigate. Our other direction of research is the design of specialized, and highly efficient, solutions to key tasks which have conflicting goals of respecting privacy and enabling legitimate usage of data.","606000","2008-10-01","2013-09-30"
"SHAPE","Structure-dependent microkinetic modelling of heterogeneous catalytic processes","Matteo Maestri","POLITECNICO DI MILANO","Despite the fact that the catalyst structure has been an important factor in catalysis science since the discovery of structure sensitive reactions in single crystal studies, its effect on reactivity is neglected in state-of-the-art microkinetic modelling. In reality, the catalyst is dynamic by changing its structure, shape and size in response to the different conditions in the reactor. Thus, the inclusion of such effects within the framework of microkinetic modelling, albeit extremely complex, is of outmost importance in the quest of engineering the chemical transformation at the molecular level. This proposal aims to approach this grand challenge by developing a hierarchical multiscale methodology for the structure-dependent microkinetic modelling of catalytic processes in applied catalysis. In particular this challenging objective will be achieved by acting on two main fronts: 
i. development of a hierarchical multiscale methodology for the prediction of the structural changes of the catalyst material as a function of the operating conditions in the reactor and the analysis of the structure-activity relations through the development of structure-dependent microkinetic models;
ii. show the applicability of the methodology by the assessment of the structure-activity relation in the context of relevant processes in energy applications such as the short-contact-time CH4 reforming with H2O and CO2 on supported-metal catalysts.
The inherent complexity of the problem will be tackled by hierarchically combining novel methods at different levels of accuracy in a dual feed-back loop between theory and experiments. This will require interdisciplinary efforts in bridging among surface science, physical-chemistry and chemical engineering. The fundamental nature and impact of the methodology will be unprecedented and will pave the way toward the detailed analysis and design of the structure-activity relation by tuning shape and size to tailoring activity and selectivity.","1496250","2016-05-01","2021-04-30"
"ShapeForge","ShapeForge: By-Example Synthesis for Fabrication","Sylvain Lefebvre","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","Despite the advances in fabrication technologies such as 3D printing, we still lack the software allowing for anyone to easily manipulate and create useful objects. Not many people possess the required skills and time to create elegant designs that conform  to precise technical specifications.
'By--example' shape synthesis methods are promising to address this problem: New shapes are automatically synthesized by assembling parts cutout of examples. The underlying assumption is that if parts are stitched along similar areas, the result will be similar in terms of its low--level representation: Any small spatial neighborhood in the output matches a neighborhood in the input. However, these approaches offer little control over the global organization of the synthesized shapes, which is randomized.
The ShapeForge challenge is to automatically produce new objects visually similar to a set of examples, while ensuring that the generated objects can enforce a specific purpose, such as supporting weight distributed in space, affording for seating space or allowing for light to go through. This properties are crucial for someone designing furniture, lamps, containers, stairs and many of the common objects surrounding us. The originality of my approach is to cast a new view on the problem of 'by--example' shape synthesis, formulating it as the joint optimization of 'by--example' objectives, semantic descriptions of the content, as well as structural and fabrication objectives. Throughout the project, we will consider the full creation pipeline, from modeling to the actual fabrication of objects on a 3D printer. We will test our results on printed parts, verifying that they can be fabricated and exhibit the requested structural properties in terms of stability and resistance.","1301832","2012-12-01","2017-11-30"
"ShapingRoughness","Emergence of Surface Roughness in Shaping, Finishing and Wear Processes","Pastewka Lars","ALBERT-LUDWIGS-UNIVERSITAET FREIBURG","Roughness on most natural and man-made surfaces shares a common fractal character from the atomic to the kilometer scale, but there is no agreed-upon understanding of its physical origin. Yet, roughness controls many aspects of engineered devices, such as friction, adhesion, wear and fatigue. Engineering roughness in surface finishing processes is costly and resource intensive. Eliminating finishing steps by controlling roughness in primary shaping or in subsequent wear processes could therefore revolutionize the way we manufacture, but this requires a deep understanding of the relevant processes that is presently lacking. Roughness emerges during mechanical deformation in processes such as folding, scratching or chipping that shape surfaces. Deformation occurs in the form of avalanches, individual bursts of irreversible motion of atoms. The central hypothesis of this project is that roughness is intrinsically linked to these deformation avalanches, which themselves are well-documented to be fractal objects. This hypothesis will be tested in large-scale atomic- and mesoscale simulations of plastic forming and fracture on state of the art high performance computing platforms. Results of these calculations will be used to develop process models for evolving the topography of large surface areas under the action of an external mechanical force, such as experienced in shaping, finishing or wear. In addition to these simulations, a public repository for sharing topography data will be build. This repository is the connection to experiments: It is a database of experimental topographies whose contents will be mined for features identified in simulations. Beyond the present project, this web-repository will advance sharing, visualization and analysis of topography data, and aid researchers to correlate surface topography with surface functionality and processing. Simulations and database lay the foundation for a rational design of surface functionality in manufacturing.","1499101","2018-02-01","2023-01-31"
"SHESTRUCT","Understanding the structure and stability of heavy and superheavy elements","Paul Thomas Greenlees","JYVASKYLAN YLIOPISTO","""The aim of the project is to further our understanding of the structure and stability of atomic nuclei at the extreme upper end of the chart of the nuclides. One of the major goals of contemporary Nuclear Physics experiments is to locate and chart the fabled superheavy element """"Island of Stability"""". Experiments which aim to directly produce the heaviest elements may provide only a limited number of observables, such as decay modes or half-lives. Detailed Nuclear Structure investigations provide extensive data which can be used as a stringent test of modern self-consistent theories. Such theories require input from the study of nuclei with extreme proton-to-neutron ratios. The upper part of the chart of the nuclides is one region in which this data is much sought after. The project will employ state-of-the-art spectrometers at the Accelerator Laboratory of the University of Jyväskylä, Finland (JYFL) to acquire such data. The spectrometers are part of a multi-national collaboration of European institutes. Results obtained in the course of the project will have a direct impact on current nuclear structure theories. The unique nature of the facilities at JYFL means that it will be impossible to obtain data of comparable quality elsewhere in the world. The project should yield a large number of publications and result in the training of several Ph.D students. The students will benefit from the fact that the Accelerator Laboratory is part of a large and well-respected University.""","1249608","2008-09-01","2014-02-28"
"SHIFTIDES","Shifting the oligomerization equilibrium of proteins: a novel therapeutic strategy","Assaf Friedler","THE HEBREW UNIVERSITY OF JERUSALEM","The aim of my project is to establish a multidisciplinary platform for quantitative biophysical analysis of protein-protein interactions in health and disease as a basis for drug design: (1) Analyzing protein-protein interactions at the molecular level in healthy systems; (2) Understanding what goes wrong in disease at the molecular level; (3) Development of drugs that will restore the biological system to its healthy conditions. My team will apply this approach to establish the concept of shifting the oligomerization equilibrium of proteins as a therapeutic strategy. I will expand the concepts of allosteric inhibitors and chemical chaperones, and develop the “shiftides”: peptides that shift the oligomerization equilibrium of a protein to modulate its activity, as a new and widely applicable methodology for drug design. I will apply this concept for: (1) inhibiting a protein by binding preferentially to the inactive oligomeric state and shifting the oligomerization equilibrium of the protein towards it; I have demonstrated the feasibility of this approach and developed promising anti-HIV peptides that inhibit the HIV-1 integrase and consequently HIV-1 replication in cells by shifting the integrase oligomerization equilibrium from the active dimer to the inactive tetramer. My team will further develop these peptides, and apply the same approach to inhibit the HIV proteins reverse transcriptase and protease; (2) Activating a protein by binding preferentially to the active oligomeric state and shifting the oligomerization equilibrium towards it: This will be applied for activation of the tumor suppressor p53, by shifting its oligomerization equilibrium from the inactive dimer to the active tetramer. Such shiftides will serve as anti-cancer lead compounds. My project will open new doors in the field of drug design, and at the end of the five-year period will result in a general new methodology to affect protein function for medical purposes.","1250000","2008-07-01","2013-06-30"
"SHINING","Stable and High-Efficiency Perovskite Light-Emitting Diodes","Feng GAO","LINKOPINGS UNIVERSITET","Light-emitting diodes (LEDs), which emit light by a solid-state process called electroluminescence, are considered as the most promising energy-efficient technology for future lighting and display. It has been demonstrated that optimal use of LEDs could significantly reduce the world’s electricity use for lighting from 20% to 4%. However, current LED technologies typically rely on expensive high-vacuum manufacturing processes, hampering their widespread applications. Therefore, it is highly desirable to develop low-cost LEDs based on solution-processed semiconductors. 

A superstar in the family of solution-processed semiconductors is metal halide perovskites, which have shown great success in photovoltaic applications during the past few years. The same perovskites can also been applied in LEDs. Despite being at an early stage of development with associated challenges, metal halide perovskites provide great promise as a new generation of materials for low-cost LEDs.

This project aims to develop high-efficiency and stable perovskite LEDs based on solution-processed perovskites. Two different classes of low-dimensional perovskites will be investigated independently. These new perovskites materials will then be coupled with novel interface engineering to fabricate perovskite LEDs with the performance beyond the state of the art. At the core of the research is the synthesis of new perovskite nanostructures, combined with advanced spectroscopic characterization and device development. This project combines recent advances in perovskite optoelectronics and low-dimensional materials to create a new paradigm for perovskite LEDs. This research will also lead to the development of new perovskites materials which will serve future advances in photovoltaics, transistors, lasers, etc.","1499759","2017-03-01","2022-02-28"
"SHPEF","Stability and hyperbolicity of polynomials and entire functions","Olga Holtz","TECHNISCHE UNIVERSITAT BERLIN","The project is devoted to the theory, algorithms and applications of hyperbolic and stable multivariate polynomials. This line of research is meant to lead to new fundamental results in analysis, matrix and operator theory, combinatorics, and theoretical
computer science.
The central goal of the project is to develop a comprehensive, seamless, theory of hyperbolic and stable multivariate polynomials. The four areas and four objectives of the project are as follows:
Classical analysis: revisit and expand the theory of hyperbolic and stable polynomials and entire functions in both the univariate and the multivariate setting. Applications: apply the theory of hyperbolic and stable polynomials to problems of matrix theory,
combinatorics and theoretical computer science.
Operator theory: develop the theory of hypo- and hyperoscillating operators and apply it to problems of fluid dynamics. Algorithms: develop fast and accurate algorithms for testing hyperbolicity/stability and for related problems.","880000","2010-08-01","2015-07-31"
"ShuttleCat","Shuttle Catalysis for Reversible Molecular Construction","Bill MORANDI","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Homogeneous catalysis is one of the pillars of modern chemical synthesis because it enables the sustainable preparation of molecules that find applications in medicinal chemistry, agrochemistry, and materials science. However, many catalytic reactions use hazardous reagents, are unpractical on laboratory-scale or limited in scope. Moreover, while a relatively broad set of catalytic reactions are available to construct chemical bonds, methods to cleave those, which could find applications in biomass and waste valorization, are rare.  
Inspired by the synthetic power of other metal-catalyzed reversible reactions, such as transfer hydrogenation and alkene metathesis, I herein describe a groundbreaking approach to homogeneous catalysis that makes use of a novel paradigm called “shuttle catalysis”, defined as the catalytic reversible transfer of chemical moieties between two molecules, to construct and deconstruct organic compounds. The first part of the proposal describes the invention of reversible catalytic functionalization reactions of alkenes following this principle. The second part addresses the challenge of developing safer catalytic carbonylation and decarbonylation reactions that do not use nor release toxic carbon monoxide gas. Finally, the last part proposes to apply the shuttle catalysis concept to the invention of unprecedented C–X (X = P, O, S, N) bond metathesis reactions. This new approach to catalysis has the potential to revolutionize the preparation, and streamline the discovery, of numerous important molecules with applications across the molecular sciences. Importantly, in order to mitigate the high risks inherent to such a groundbreaking approach, we have collected preliminary results to demonstrate the feasibility of each of the proposed subprojects.","1485000","2018-01-01","2022-12-31"
"SHYNE","Stellar HYdrodynamics Nucleosynthesis and Evolution","Raphael Hirschi","UNIVERSITY OF KEELE","Stars, massive stars in particular, play a key role through the light they shine, all the chemical elements they produce and the supernova explosions that mark their death. Stars are complex 3D laboratories that relate to many scientific disciplines: nuclear & astro-physics, applied mathematics and computer science. Unfortunately stars cannot be modelled fully in 3D. The best way to model them is to develop synergy between 3D and 1D models to improve 1D stellar evolution models.

This programme will tackle this multi-disciplinary challenge, which is out of reach of individual research groups. For this purpose, my team, with the help of a network of world leading experts in their respective disciplines, will use an innovating approach using and extending techniques from other disciplines such as Monte Carlo simulations (regularly used in other disciplines of physics) to develop a unique suite of software tools able to: (1) simulate the most complex processes in stars,
(2) apply these models to stars of all masses and (3) use stars as virtual nuclear physics laboratories.

This unique software suite will provide a pipeline for the fast production of comprehensive datasets (superseding currently limited ones) that will guide and provide a framework of analysis for large European nuclear physics experiments (FAIR at GSI, D) and astronomical observing facilities (ESO VLT, E-ELT & ESA GAIA) thus enhancing the return on these huge investments. The SHYNE software suite will also boost the innovation cycle of future developments in stellar modelling far beyond the term of this grant and will enable the resolution of many challenging questions and unsolved problems such as: What is the fate of the most massive stars known to date? How frequent are pair-creation and electron-capture supernovae? How much of each chemical element do stars produce? The multi-disciplinary SHYNE programme will thus have a wide ranging impact in science and benefit Europe at many levels.","1407265","2012-11-01","2017-10-31"
"SIERRA","Sparse Structured Methods for Machine Learning","Francis Bach","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","Machine learning is now a core part of many research domains, where the abundance of data has forced researchers to rely on automated information processing. In practice, today, machine learning techniques are applied in two stages: practitioners first build a large set of features; then, off-the-shelf algorithms are used to solve the appropriate prediction tasks, such as classification or regression. While this has led to significant advances in many domains, I believe that the potential of machine learning is far from being fulfilled. The tenet of this proposal is that to achieve the expected breakthroughs, this two-stage paradigm should be replaced by an integrated process where the specific structure of a problem is taken into account explicitly in the learning process. This will allow the consideration of massive numbers of features, in both numerically efficient and theoretically well-understood ways. I plan to attack this problem through the tools of regularization by sparsity-inducing norms. The scientific objective is thus to marry structure with sparsity: this is particularly challenging because structure may occur in various ways (discrete, continuous or mixed) and my targeted applications in computer vision and audio processing lead to large-scale convex optimization problems. My research program is expected to have a high impact on statistical machine learning research, notably by providing new solutions to the open problem of non-linear variable selection. Moreover, my general methodology will be directly applied to domains where the natural structure of data has been recognized as crucial but is still underused by learning techniques, namely computer vision (object recognition, image denoising) and audio processing (speech separation, music recognition).","1468248","2009-12-01","2014-11-30"
"SIGMA-VISION","Sparsity, Image and Geometry to Model Adaptively Visual Processings","Gabriel Louis-Jean Peyré","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","SIGMA-Vision will develop the next generation algorithms and methodologies for image process- ing. These algorithms will rely on several mathematical breakthroughs in image modeling: structured sparsity, geometric representations and adaptivity. They will be implemented using fast optimization codes that can handle massive datasets with gigapixels images and videos. These algorithms will have far reaching applications in computer vision, graphics and neu- roscience. These cutting edge mathematical approaches will go beyond traditional image processing scenarios and impact significantly object recognition, dynamical special effects and exploration of the visual cortex.","1414960","2011-10-01","2016-09-30"
"SILION","Design, Synthesis, Characterization and Catalytic Application of Silyliumylidene Ions","Shigeyoshi Inoue","TECHNISCHE UNIVERSITAET MUENCHEN","This ERC-StG 2014 proposal, SILION, outlines a strategy for the one-pot synthesis, characterization, and reactivity investigation of a positively charged, electron-deficient, highly Lewis acidic, cationic silicon(II) species, denoted “silyliumylidene ions”. Silyliumylidene ion has only four valence electrons, consisting of a lone pair of electrons and two vacant orbitals on the central silicon atom. It will be expected to bear the best combined character of both silylenes as silicon analogue of carbene and silylium ions as silicon analogue of carbenium ions. The program described herein is also aimed at the development of silyliumylidene ion as novel catalysts based on main group elements. 
The proposed silyliumylidene ions should fulfil the following criteria: 
a) compounds can be synthesized by a facile one-pot reaction of the corresponding dichlorosilane with two equivalents of N-heterocyclic carbenes, 
b) silyliumylidene ions will potentially possess three reactive sites including (i) a lone pair at the silicon center, (ii) p-orbital at the silicon center, and (iii) N-heterocyclic carbenes, 
c) thanks to their strong sigma-donor as well as pi-acceptor ability, the highly reactive silyliumylidene ions are expected to serve as innovative reagents for activation of organic small molecules, excellent catalysts, and strikingly versatile coordination ligands toward transition metals.  
The target of this proposal is the introduction of facile accessible silyliumylidene ions, which are combined the best properties of silylene and silylium ions, and development of its reactivity and catalytic activity. The synthesis of silyliumylidene ions is straightforward and should allow the investigation of electronic and steric properties of the substituents and N-heterocyclic carbenes. It is anticipated that novel silyliumylidene ions can be used for a promising new building block for low-valent organosilicon compounds and high-performance catalysts.","1499942","2015-12-01","2020-11-30"
"SIMCOMICS","Simulation of droplets in complex microchannels","Francois Gallaire","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","In droplet-based microfluidics, the elementary units transporting reagents from one functional site to another (mixer, sensor or analyzer) are droplets, which are carried by an inert wetting fluid. This research project aims at the development of numerical models of flowing droplets in thin spatially extended microchannels, designed at avoiding the exponential complexity of parallelized 1-D networks. We aim at simulating the trajectory of droplets transported by a pressure-driven carrier fluid, as they evolve in a surface energy gradient, generated by channel depth variations or surface tension inhomogeneities.

To this end, we exploit the remarkable aspect ratio of these microfluidic devices to propose a depth-averaged description of the pancake shaped droplets. The resulting equations, called Brinkman's equations, combine the 2D Stokes equations with 2D Darcy potential-flow-like equations. Their diphasic simulation relies on the adaptation of existing algorithms to this particular free interface problem. Pressure corrections due to the thickness variations of the lubricating thin films will also be included.

Surfactant and heat dynamics will then be added to model thermo- and soluto-capillary forcing. The depth-averaged model will be finally generalized to account for arbitrary depth variations, so as to add dynamics to the quasi-static description of droplets moving along successive minimal surface energy locations.

A specific part of the project is also devoted to the development of an experimental expertise: it is indeed essential to the success of the project to conduct fundamental microfluidic experiments in order to validate our new models. While SIMCOMICS aims at shrinking the gap between present computations of droplets flowing in microchannels and the increasing number of application-oriented experimental studies, it both raises fundamental questions and opens promising perspectives for the engineering design of new microcarved microchannels.","1405796","2012-01-01","2016-12-31"
"SIMONE","Single Molecule Nano Electronics (SIMONE)","Kasper Moth-Poulsen","CHALMERS TEKNISKA HOEGSKOLA AB","""The development of micro fabrication and field effect transistors are key enabling technologies for todays information society. It is hard to imagine superfast and omnipresent electronic devices, information technology, the Internet and mobile communication technologies without access to continuously cheaper and miniaturized microprocessors. The giant leaps in performance of microprocessors from the first personal computing machines to todays mobile devices are to a large extent realized via miniaturization of the active components. The ultimate limit of miniaturization of electronic components is the realization of single molecule electronics. Due to fundamental physical limitations, single molecule resolution cannot be achieved using classical top-down lithographic techniques. At the same time, existing surface functionalization schemes do not provide any means of placing a single molecule with high precision at a specific location on a nanostructure. This project has the ambitious goal of establishing the first method ever allowing for self-assembly of multiple single molecule devices in a parallel way and thereby provide the first method ever allowing for multiple individual single molecule components to operate together in the same device.
The impact of the technology platforms described herein goes vastly beyond the field of single molecule electronics and utilization in ultra-sensitive plasmonic biosensors with a digital single molecule response will be explored in parallel with the main roadmaps of the project.""","1500000","2014-02-01","2019-01-31"
"SIMP","Ultra-high-Q Physics: Towards single molecules and phonons","Tobias Jan August Kippenberg","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The proposed research program builds on the previously developed ultra-high-Q monolithic micro-resonators by the applicant during his dissertation at the “California Institute of Technology”. These micro-resonators offer unprecedented confinement of light in micro-scale volumes for extended amounts of time and have opened many lab-on-chip applications ranging from nonlinear optics, quantum optics to biochemical sensing. This present proposal is concerned to use ultra-high-Q optical micro-cavities as vehicles to study two novel and emerging research opportunities. The first endeavor investigates the possibility to use radiation pressure to cool a mechanical oscillator to the quantum ground state. The significance of the research program lies in its attempt to exploit the opto-mechanical system as a paradigm for the investigation of quantum processes of mechanical objects – a field which has sparked widespread interest in contemporary physics for quiet some time, but which to date remains experimentally unexplored and which is intimately related to concepts used in fields such as gravitational wave detection or scanning probe techniques. From a conceptual point of view, this research could show how a mechanical, macroscopic object reveals quantum mechanical behavior. Ultra-sensitive measurements are also part of a second, interdisciplinary line of research. To date, only a few widely applied techniques in Biophysics are available for label free detection of ligand-receptor binding, which lack single to resolve single molecule binding events. Building on recent advances of the applicant, the proposed methodology will use membrane functionalized micro-resonators in aqueous solution as novel technique to resolve single binding events. By developing a methodology with which label free single molecule sensitivity in biomolecular recognition can be attained, this research could enable to open new frontiers to Biophysicists.","1332000","2008-09-01","2012-12-31"
"SIMPLELCGPS","Simple locally compact groups: exploring the boundaries of the linear world","Pierre-Emmanuel Caprace","UNIVERSITE CATHOLIQUE DE LOUVAIN","The theory of locally compact groups stretches out between two antipodes: on one hand, connected groups whose structure, according to the solution to Hilbert fifth problem, is governed by Lie theory and is thus relatively rigid, and on the other hand, discrete groups, which are subject to a spectacular variety of behaviours, going from the most stringent rigidity properties to the most intriguing pathological ones. The goal of this research program is to explore the wide space lying between these two extremes.

The entire program is built around two major open problems: performing an exhaustive study of compactly generated simple locally compact groups, and finding an algebraic characterization of those locally compact groups which are linear. Although these problems do not seem to be directly approachable given the current state of knowledge, they are nevertheless considered as guidelines suggesting a number of specific questions and conjectures which are envisaged in detail under various perspectives of algebraic, geometric, arithmetic and analytic nature. Each of these specific questions presents independent interest; answers to  any of them will moreover provide insight into the guiding problems.","848640","2011-12-01","2016-11-30"
"SiMS","Simulated Majorana states","Attila GERESDI","TECHNISCHE UNIVERSITEIT DELFT","Quantum computation using topologically protected Majorana bound states is a promising direction towards scalable quantum architectures due to their inherent noise immunity provided by the nonlocal storage of quantum information. Thus far, Majorana states have mostly been investigated in superconductor-semiconductor heterostructures which rely on induced superconductivity in a quasi-one-dimensional conductor. However, despite tremendous efforts in material development, these devices are still limited by uncontrolled local fluctuations due to disorder and it is unclear if future developments will solve these problems. Furthermore, disorder may even mimic the transport signatures of topological ordering, hindering an unambiguous identification of the Majorana states.
Here I propose a way to overcome these limitations: I will work towards the direct quantum simulation of the one dimensional topological superconductor with Majorana bound states. I will use chains of semiconductor quantum dots, which is an emerging platform to simulate exotic many-body electron states. Building on this platform, I will be able to demonstrate for the first time the emergence of coherent, non-local superconducting states bound to the entire device similarly to the Kitaev chain model of topological superconductivity.
To demonstrate quantum coherence of the chain, we will build the first Andreev molecule quantum bit, which, while not topologically protected, will already combine advantages of superconducting and semiconductor qubits. Going one step further, we will investigate the simulated Kitaev chain. Upon establishing the presence of the simulated Majorana states, we will work towards a simple braiding protocol to demonstrate the non-Abelian nature of the edge modes.
This research direction, combining the scalability of semiconductor structures and the topological protection of Majorana states, will open new avenues towards universal quantum computation.","1997513","2019-02-01","2024-01-31"
"SINCAT","Single Nanoparticle Catalysis","Christoph Langhammer","CHALMERS TEKNISKA HOEGSKOLA AB","Imagine a sustainable society where clean energy is produced from sunlight, and water is converted into hydrogen to fuel a fuel cell, which produces electric energy to power the electric motor in a car. At the same time, CO2 emissions are captured and converted to hydrocarbons that are again used as fuel or as resource for fine chemical synthesis. At the heart of this vision is heterogeneous catalysis. Hence, for it to become reality, tailored highly efficient catalyst materials are of paramount importance. The goal of this research program is therefore to establish a new experimental paradigm, which allows the detailed scrutiny of individual catalyst nanoparticles and their reaction products under application conditions.
The catalytic performance of nanoparticles is directly controlled by their size, shape and chemical composition. Current studies are, however, conducted on ensembles of nanoparticles. Therefore, such studies are plagued by averaging effects, which deny access to the key details related to how size, shape and composition control catalyst performance. To eliminate this problem, we will nanofabricate a unique nanofluidic reactor device that will enable us to scrutinize catalytic processes and products at the individual catalyst nanoparticle level. In a second step, we will integrate plasmonic optical probes with the nanoreactor to be able to simultaneously monitor the dynamics of the catalyst particle state during reaction.
Finally, we will apply the nanoreactor to investigate the role of the catalyst oxidation state in Fischer-Tropsch catalysis. In parallel, we will explore novel plasmon-induced hot electron-mediated reaction pathways for catalytic CO2 reduction, as part of a carbon-neutral energy cycle. We anticipate unprecedented insight into the role of catalyst particle state, size and shape in these processes. This will facilitate the development of more efficient catalyst materials in the quest for an energy-efficient and sustainable future.","1500000","2016-01-01","2020-12-31"
"SINDAM","Sunlight-Induced Nonadiabatic Dynamics of Atmospheric Molecules","Basile CURCHOD","UNIVERSITY OF DURHAM","Our atmosphere appears to be a reservoir of inert gaseous molecules, while in reality, it is also composed of a plethora of highly reactive molecules and radicals. Among these are the volatile organic compounds (VOCs) that contribute substantially to both global warming and air pollution. Detailed atmospheric models attempt to describe the incredibly complex network of chemical reactions resulting from VOC oxidation in our troposphere and to predict its composition, paramount for informing societal and political decisions and regulations. A surprising observation, though, is that the role of sunlight in the reactions of VOC intermediates is scarcely understood, even though excited-state dynamics, triggered by sunlight absorption, initiate most of the atmosphere’s chemistry. This lack of information is partly due to the challenge in conducting photochemical experiments on transient VOC species. As a result, VOC-related chemical mechanisms are mostly based on a ground electronic state chemistry, leading to some critical deviations between the predicted and observed composition of the atmosphere. Also, vastly neglecting the rich photochemistry of these VOC species leads to a poorer understanding of their resulting environmental impact.
This project, SINDAM, launches the field of in silico atmospheric photochemistry by using recent breakthroughs in theoretical and computational chemistry to establish the importance of photochemical processes of VOCs in the atmosphere, either in isolation or in contact with water molecules. SINDAM will answer the simple yet critical question: how ubiquitous are photophysical and photochemical effects in the atmospheric chemistry of VOCs? Hence, this project will stimulate an exciting new synergistic area at the edge between theoretical and atmospheric chemistry, leading to a real, societal impact by improving the predictive power of atmospheric models in the context of global warming and air quality.","1499457","2019-01-01","2023-12-31"
"SINGFISS","Singlet exciton fission as a route to more efficient dye-sensitized solar cells","Ferdinand Cornelius Grozema","TECHNISCHE UNIVERSITEIT DELFT","One of the greatest scientific challenges of the coming decades will be to produce sufficient energy to meet consumption demands, particularly as fossil fuel reserves decline. A leading alternative method of producing energy is the conversion of solar energy to electricity. At present, energy produced by photovoltaic cells is significantly more expensive than that obtained by burning fossil fuels. Therefore, we need to find a method of producing solar cells more cheaply. The prime example of such a cheap solar cell is the dye-sensitized solar cell. However, the efficiency of these cells is currently too low to be commercially interesting. In this project, a process called singlet exciton fission is proposed as a new route to more efficient dye-sensitized solar cells. In this process, a singlet excited state formed by photo-excitation converts into a pair of triplet states by a spin-allowed transition. When both triplet excited states lead to a charge separation event, the theoretical maximum efficiency of dye sensitized solar cells can be increased from 32% to ~46% for a cell combining a singlet fission absorber with a normal dye. This project will have a two-fold benefit: it will be the first major systematic study of the fundamentals of the singlet fission process, and it will explore the use of singlet fission dyes in photovoltaics. Using a variety of disciplines, ranging from organic synthesis to ultrafast spectroscopy and quantum chemical calculations, this project will deliver the clearest picture yet of the exciton fission process. In addition, this research will enable the design of specific chromophores possessing optimal triplet fission yield and, by doing so, will open exciting new possibilities for the production of more efficient dye-sensitized solar cells.","1200000","2009-12-01","2014-11-30"
"single-C","Automatized Catalysis and Single-Carbon Insertion","Abraham MENDOZA VALDERREY","STOCKHOLMS UNIVERSITET","This project is aimed at accelerating the synthesis of important organic molecules through key enabling technologies towards automatized catalysis and single-carbon insertion reactions. Transferring the simplest carbon units to organic molecules has the potential to change the way we approach synthesis planning through new asymmetric skeletal homologations and rearrangements of simple raw materials, for which only long workarounds exist now. These methods can reduce to half the manipulations required to access relevant medicines, organocatalysts, ligands, bio-molecular tools and photovoltaic devices. They target unreactive functions to introduce fundamental one-carbon units (CO or C) that are present in virtually any organic compound. New powerful reagents that resemble these basic single-carbon units in excited electronic configurations are to be developed for this purpose. The new catalytic methods needed are based on the solid grounds of carbene-transfer reactions and the recent advances of my group in the development of new homogeneous catalysts. Moreover, a new catalyst platform will be developed to complement our existing portfolio for success in the challenging processes targeted in this proposal. We aim to pioneer a fully automatized workflow for research in catalysis that devoid the synthesis of organic ligands replacing them by combinatorial assemblies built in situ from un-structured simple molecules. The new reactions arising from these new catalysts and reagents will expedite the valorization of raw materials (such as carbonyls, olefins and hydrocarbons) into important chiral molecules in a single transformation. This bold aim is a priority of the European Commission for the coming years as it will save time, protect the environment and reduce cost at once. Thus, these innovative technologies have the potential of transforming the research workflow in homogeneous catalysis and the logics of retrosynthesis of organic molecules at a fundamental level.","1487245","2017-04-01","2022-03-31"
"SINGLECELLDYNAMICS","Optofluidic toolkit for characterizing single-cell dynamics in systems immunology","Savas Tay","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Immune cells constantly receive signalling inputs such as pathogen-emitted molecules, use gene regulatory pathways to process these signals, and generate outputs by secreting signalling molecules like cytokines. Characterizing the input-output relationship of a biological system helps understanding its regulatory mechanisms, and allows building models to predict how the system will operate in complex physiological scenarios, such as population tissue response to infection.  A major obstacle in this endeavor has been the so-called “biological noise”, or significant variability in measured molecular parameters between cells. Such variability makes time-dependent single-cell analysis crucial to understand how biological systems operate. Development of new analytical tools with improved functionality, accuracy, and throughput is needed to realize the full potential of single-cell analysis. We propose to develop automated, high-throughput, Optofluidic single-cell analysis systems with unprecedented capabilities, and to use them in understanding how immune cells organize in tissue during response to infection. Microfluidic membrane-valves, nanodroplets, optics, and automation will be integrated to achieve an unparalleled degree of control over single immune cells. Multi-functional lab-on-chip devices will simultaneously measure: a) The activity of immune regulatory proteins such as NF-κB, and b) Inflammatory cytokines secreted from single immune cells in a time-dependent manner, under precisely defined biochemical inputs. Characterizing macrophage cytokine secretion dynamics under combinatorial regiments of bacterial and apoptotic-cell signals will allow dissecting the signalling mechanism responsible from the resolution of inflammation. We will identify the role of the NF-κB pathway in regulation of cytokine dynamics. We will use our data to develop a computer model of tissue-level immune response to pathogens through the NF-κB pathway and cytokine signaling.","1499165","2013-10-01","2018-09-30"
"SINGLEOUT","Single-Photon Microwave Devices: era of quantum optics outside cavities","Mikko Pentti Matias Möttönen","AALTO KORKEAKOULUSAATIO SR","The past couple of years have witnessed the rise of on-chip quantum optics. This has been enabled by the fabrication of high-finesse superconducting resonators made out of coplanar waveguides, and by the coupling of these resonators to superconducting quantum bits, qubits. This so-called circuit quantum electrodynamics (cQED) has proven superior compared with the standard cavity QED with photons coupled to atoms in three-dimensional space. Namely, the coupling of the cavity photons with the qubit has reached strengths completely out of reach with traditional techniques. The energy levels and their populations in the qubits can be controlled in-situ, which has also offered the possibility prepare the quantum mechanical state of the photons in the cavity to arbitrary superpositions of the low-lying photon number states.

Although great focus is put worldwide into cQED in superconducting cavities, the field of manipulation and measurement of single microwave photons outside cavities is essentially missing. This ERC starting grant project, aims to expand the power of microwave photons witnessed in cavities to free photons in waveguides. The cornerstone is the design and implementation of a single-photon click detector for microwaves. The detector will allow for the single-shot measurement of the photon state in the waveguide in a similar fashion as the photon detectors are routinely used in optical quantum computing (OQC). Thus together with the already demonstrated single-photon source, the detector will be a critical step towards quantum information processing with microwave photons. In addition to opening this novel field in physics, the detector can be utilized in the characterization of microwave components and devices at ulra-high sensitivities. In this project, we will implement a platform for such characterization and build several circuit elements to manipulate single microwave photons in the same way as beam splitters are used in OQC.","1499445","2012-01-01","2016-12-31"
"SINGLESENS","Single metal nanoparticles as molecular sensors","Carsten Sönnichsen","JOHANNES GUTENBERG-UNIVERSITAT MAINZ","Optical spectroscopy of single plasmonic nanoparticles (NPs) has evolved into a recognized tool for nanoscopic sensing applications, using the sensitivity to the NP's environment,charge, size, shape, and proximity to other NPs. Here, I propose taking advantage of the nanoparticle s minuscule size approaching molecular dimensions in novel ways. Single particle plasmon sensors are in many ways the smallest possible giving unprecedented access to molecular events. The small size amplifies fluctuations by molecular events, allows massive parallel detection of analytes within tiny devices, and to monitor single nanoparticle formation and electrochemical surface reactions in real time.
The objective of this project is therefore to develop and explore single-particle plasmon spectroscopy as a novel tool to study such molecular processes. The objective will be reached by (1) building three new setups progressing far beyond current technology and increasing time resolution, spectral sensitivity, and parallelization capability many orders of magnitude, (2) synthesizing nanoparticles with optimal plasmon sensing properties, and (3) simulating plasmon properties to guide the experiments and understand the physics behind the observed phenomena. The single-particle plasmon spectroscopy technique will be applied in four scientific directions to demonstrate its potential: (4) analyzing distance fluctuations of particle pairs linked by (bio-)polymers, (5) recording coverage fluctuations of biomolecules bound to nanoparticles, (6) demonstrating parallel detection of many analytes in multiplexed microfluidic devices, and (7) following particle formation and chemical reactions in a  single particle reactor .
Single-particle plasmon spectroscopy has the potential to provide a revolutionary new tool to study molecular processes and to become a major commercial analytical tool, especially for pharmaceutical research and development.","1510000","2011-01-01","2015-12-31"
"SINGULARITY","Singularities and Compactness in Nonlinear PDEs","Filip RINDLER","THE UNIVERSITY OF WARWICK","The emergence of singularities, such as oscillations and concentrations, is at the heart of some of the most intriguing problems in the theory of nonlinear PDEs. Rich sources of these phenomena can be found for instance in the equations of mathematical material science and hyperbolic conservation laws.

Building on recent pioneering work of the PI, The SINGULARITY project will investigate singularities through innovative strategies and tools that combine geometric measure theory with harmonic analysis. The potential of this approach is far-reaching and has already led to the resolution of several long-standing conjectures as well as opened up new avenues to understand the fine structure of singularities.

The project comprises three inter-connected themes:

Theme I investigates condensated singularities, i.e. singular parts of (vector) measures solving a PDE. A powerful structure theorem was recently established by the PI and De Philippis, which will be developed into a fine structure theory for PDE-constrained measures.

Theme II is concerned with the development of a compensated compactness theory for sequences of solutions to a PDE, which is capable of dealing with concentrations. The central aim is to study in detail the (non-)compactness properties of such sequences in the presence of asymptotic singularities, for instance in relation to the Bouchitt ́e Conjecture in shape optimization.

Theme III investigates higher-order microstructure, i.e. nested periodic oscillations in sequences, such as laminates. The main objective is to understand the effective properties of such microstructures and to make progress on pressing open problems in homogenization theory and on the fundamental Morrey Conjecture. We will employ the promising tool of microlocal compactness forms, recently invented by the PI.

All three themes tackle challenging and important open questions, which will serve as guiding lights towards a robust framework for the effective study of singularities.","1483943","2018-04-01","2023-03-31"
"SINK","Subduction Initiation reconstructed from Neotethyan Kinematics (SINK): An iterative geological and numerical study of the driving forces behind plate tectonics","Douwe Jacob Jan Van Hinsbergen","UNIVERSITEIT UTRECHT","""The concept of Plate Tectonics, as fundamentally unifying to Earth Sciences as Darwin’s Evolution Theory is to Life Sciences, mathematically describes the complex evolution of Earth’s outer shell in terms of lithosphere plates and their interactions. There is no widely accepted dynamic mechanism, however, that explains why plate tectonics developed and continues. Subduction of oceanic lithosphere into the mantle, compensated by spreading of new oceanic lithosphere elsewhere is a key element of plate tectonics. Half of the subduction zones active today formed in the Cenozoic, and subduction initiation must be a common and fundamental element of plate tectonics. Geophysical models demonstrate that forcing is required to initiate subduction at weakness zones. Mechanisms producing this forcing remain unexplored, but may include clogging of existing subduction zones with continental lithosphere, formation of high plateaus as a result of absolute plate motions and arrival of mantle plumes below plates.
I aim to identify the mechanisms that force subduction initiation, using a novel and multidisciplinary approach. (1) I will design a Natural Laboratory, in which subduction initiation events, absolute and relative plate motions, continental subduction and mantle plumes are reconstructed. The Alpine-Himalayan mountain range that formed during closure of the Neotethyan Ocean is an ideal natural laboratory in which subduction initiation events and geological expressions of all potential driving mechanisms have been and will be reconstructed. (2) To test whether the reconstructed geological ‘incidents’ are causally related, a Numerical Laboratory will be designed, to conduct numerical modeling experiments based on fundamental geophysics. SINK will iteratively integrate the natural and numerical laboratories to advance our understanding of the processes that drive subduction initiation, as an essential step towards a dynamic and quantitative model to explain plate tectonics.""","1499880","2012-09-01","2017-08-31"
"SINOXYGEN","Advancing the Green Chemistry of Singlet Oxygen and Applying it to Synthetic Challenges","Georgios Vasilikogiannakis","PANEPISTIMIO KRITIS","Novel synthetic methods are vital to the work of a host of key chemical disciplines; from new materials and nanotechnology to pharmaceuticals, practitioners constantly need cleaner, greener, milder and more efficient ways to synthesize their chosen targets. In this proposal, we seek to develop, and then apply to some very challenging scenarios, a set of particularly powerful and beyond the state-of-the-art  new methods, using singlet oxygen, that will meet all these tough criteria.
Singlet oxygen is a remarkable reagent; it is a natural, cheap, green and atom-efficient oxidant. It also makes an ideal initiator for cascade reaction sequences through which molecular complexity is enhanced very rapidly and effectively. With this chemistry protecting groups and toxic heavy metal oxidants, both normally associated with the construction of molecules rich in oxygen functionality, are not needed.
In the projects described within this proposal, singlet oxygen will be manipulated to orchestrate a diverse range of cascade reaction sequences, and “super cascade” reaction sequences, by which complex polyoxygenated and polycyclic molecular architectures will be synthesized, from very simple and readily accessible furan precursors, in one-pot. Polyoxygenated-polycyclic motifs are common synthetic targets across a range of disciplines. In our case, we will focus research efforts towards bioactive natural products because these highly complex and intricate structures provide the best, and most challenging, testing grounds for any new set of chemical methods. The natural products chosen belong to the azaspiracid, pinnatoxin/pteriatoxin, spirolide and pectentoxin families, respectively.
We also hope to further promote the widespread application of these singlet oxygen-based chemical solutions to a host of problems by developing a prototype Continuous Flow Reactor that will facilitate large scale photooxygenations.","1338000","2011-10-01","2017-09-30"
"SINSLIM","Smart Inorganic Nanocrystals for Sub-diffraction Limited IMaging","Dan Oron","WEIZMANN INSTITUTE OF SCIENCE LTD","""The goal of this proposal is to design and fabricate """"smart"""" inorganic fluorophores, which could replace to replace currently used organic dyes for far-field sub-diffraction limited microscopy applications. Delicate band-gap engineering of the structure and composition of colloidal semiconductor nanocrystals is suggested as a path to achieving the required nonlinear all-optical control over their luminescent properties. In conjunction with the inherent photostability, tunability and ease of excitation of these nanocrystals, this can pave the way towards greatly simplified instrumentation and techniques, implying dramatically reduced costs and significantly broader accessibility to sub-diffraction limited imaging.
The proposed research is a concerted effort both on colloidal synthesis of complex multicomponent semiconductor nanocrystals and on time and frequency resolved photophysical studies down to the single nanocrystal level. Several schemes for photoactivation and reversible photobleaching of designed nanocrystals, where the localization regime of excited carriers differs between the electrons and the holes, will be explored. These include effective ionization of the emitting nanocrystal core and optical pumping of two-color emitting QDs to a single emitting state. Fulfilling the optical and material requirements from this type of system, including photostability, control of intra-nanocrystal charge- and energy-transfer processes, and a large quantum yield, will inevitably reveal some of the fundamental properties of the unique system of strongly coupled quantum dots in a single nanocrystal.""","1496600","2010-11-01","2015-10-31"
"SIP","SPECIFICALLY INTERACTING POLYMERS
– From Selective Adhesion toward Specific Recognition","Hans, Gerhard Boerner","HUMBOLDT-UNIVERSITAET ZU BERLIN","""The ability to control interactions in synthetic polymers as precisely as in proteins will have a significant impact on polymer and material sciences. The present proposal explores broadly applicable routes toward synthetic polymers having the capability of specifically interacting with distinct targets such as small molecules, surfaces, or material interfaces.

Novel design strategies for peptide-polymer conjugates will be established, which broadly exploit combinatorial methods for the selection of suitable peptide segments. Understanding the requirements for specific interactions will ultimately enable the synthesis of non-peptidic block copolymers that can mimic specific peptide interactions.

The striking opportunities that arise from the newly established platform of “specifically interacting polymers” will be demonstrated in three case-based studies on: (1) Specific solubilizers & transporters, which specifically host, transport and actively liberate problematic high-potential drugs. (2) Specific surface recognizers, which enable material-specific adhesion to generate, with office laser printers, penta-functional paper surfaces or 3D-channels. (3) Specific compatibilizers, which allow tailoring of internal material interfaces to fine-tune sustainable composites via specific interface design.

This project will encourage the synthesis of next-generation functional polymers that offer exciting new possibilities for specific materials sciences.""","1499800","2013-01-01","2017-12-31"
"SIPA","Semidefinite Programming with Applications in Statistical Learning","Alexandre Werner Geoffroy Gobert D'aspremont Lynden","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Interior point algorithms and a dramatic growth in computing power have revolutionized optimization in
the last two decades. Highly nonlinear problems which were previously thought intractable are now
routinely solved at reasonable scales. Semidefinite programs (i.e. linear programs on the cone of positive
semidefinite matrices) are a perfect example of this trend: reasonably large, highly nonlinear but convex
eigenvalue optimization problems are now solved efficiently by reliable numerical packages. This in turn
means that a wide array of new applications for semidefinite programming have been discovered,
mimicking the early development of linear programming. To cite only a few examples, semidefinite
programs have been used to solve collaborative filtering problems (e.g. make personalized movie
recommendations), approximate the solution of combinatorial programs, optimize the mixing rate of
Markov chains over networks, infer dependence patterns from multivariate time series or produce optimal
kernels in classification problems.
These new applications also come with radically different algorithmic requirements. While interior point
methods solve relatively small problems with a high precision, most recent applications of semidefinite
programming in statistical learning for example form very large-scale problems with comparatively low
precision targets, programs for which current algorithms cannot form even a single iteration. This
proposal seeks to break this limit on problem size by deriving reliable first-order algorithms for solving
large-scale semidefinite programs with a significantly lower cost per iteration, using for example
subsampling techniques to considerably reduce the cost of forming gradients.
Beyond these algorithmic challenges, the proposed research will focus heavily on applications of convex
programming to statistical learning and signal processing theory where optimization and duality results
quantify the statistical performance of coding or variable selection algorithms for example. Finally,
another central goal of this work will be to produce efficient, customized algorithms for some key
problems arising in machine learning and statistics.","1148460","2011-05-01","2016-04-30"
"SIREAL","Seismology in the ionosphere? This is REAL! 
Ionosphere as a natural indicator of numerous geophysical events","Elvira Astafyeva","INSTITUT DE PHYSIQUE DU GLOBE DE PARIS","We propose to perform a wide spectrum of ionosphere-related research, from ionosphere seismology to ionospheric storms and GNSS/GPS performance during extreme ionospheric and space weather events. The main focus will be made on such rare subject as ionosphere seismology that aims to study ionosphere response to large earthquakes, to investigate the main properties of different kinds of ionospheric disturbances occurred due to seismic and seismic-like events, including tsunamis, volcano eruptions and explosions. The 11/03/2011 Tohoku megaquake has opened new challenges for modeling of co-seismic effects and has indicated new directions for further research. In particular, we found that the ionosphere is able of showing images of a seismic fault slip rupturing about ~8 minutes after an earthquake, which opens new opportunities for short-time tsunami warnings.
In addition to the ionosphere seismology, the project includes fundamental multi-instrumental studies of the global dynamics of the ionosphere under geomagnetically disturbed and quiet conditions. Great attention will be paid on investigation of features of traveling ionospheric disturbances and on the ionosphere behavior during variations of interplanetary parameters such as magnetic field (IMF) and electric field (IEF). The latter is directly connected to the last subject of our proposal - solar- and ionosphere-induced GPS -failures.
The main advantages for Europe coming with this project are: 1) a highly interdisciplinary project on a sufficiently new branch of the science and with rare applications; 2) extension of the fundamental ionosphere studies in Europe that will increase the competitiveness of Europe among other world-famous research schools on the Earth’s ionosphere; 3) the results of our work on GNSS operation quality will be useful for the future Galileo mission as well, and would help to improve the system.","858000","2012-10-01","2018-06-30"
"SIREN","Securing Internet Routing from the Ground Up","Michael Schapira","THE HEBREW UNIVERSITY OF JERUSALEM","The Internet is made up of dozens of thousands of smaller networks, called Autonomous Systems (ASes), ranging from multinational corporations to small businesses and schools, e.g., Google, Deutsche Telekom, AT&T, and Hebrew U. Routing between ASes is handled by the Border Gateway Protocol (BGP), which is the glue that holds the Internet together. Alarmingly, despite the Internet's critical societal and economic role, BGP routing is dangerously vulnerable to configuration errors and attacks, and, consequently, every year or so a major Internet outage makes the news.

To remedy BGP’s many security vulnerabilities, researchers and practitioners have invested much effort into designing security solutions for BGP routing. Yet, despite over a decade of Herculean efforts, many technological, political, and economic hurdles hinder, and possibly even prevent, deployment. I argue that the reasons for this are deeply rooted in today’s centralized, top-down, hierarchical paradigm for securing Internet routing. The aim of the planned research project is to put forth and explore a radically new paradigm for securing routing on the Internet. The proposed alternative roadmap for securing the Internet consists of two steps:

1) Jumpstarting BGP security: A novel approach to routing security that bypasses the obstacles facing today’s agenda. Specifically, the proposed design will be flat, decentralized, fully automated, avoid dependency on a single root-of-trust, and not require modifying/replacing legacy BGP routers. 

2) A long-term vision for Internet routing: Leveraging the vast computational resources in modern datacenters, and research on Secure Multi-Party Computation, to outsource routing to a small number of entities while retaining flexibility, autonomy and privacy.

I believe that, put together, these can lead to a more secure Internet in the short-run, and outline a promising, yet uncharted, new direction for the future of Internet routing.","1468200","2016-02-01","2021-01-31"
"SIRIUS","Simulations for Inertial Particle Microfluidics","Timm KRUEGER","THE UNIVERSITY OF EDINBURGH","Cancer and bacterial infections are projected to kill 18 million people worldwide annually by 2050. Fast and reliable diagnostics are essential for early and targeted treatments. Microfluidics is at the heart of the miniaturisation of diagnostics, enabling novel portable and low-cost point-of-care devices. Inertial particle microfluidics (IPMF) is a novel and competitive method with applications in cancer cell and bacteria separation. Yet, the physics behind IPMF is not well understood, making progress slow and costly. Novel design rules are in urgent need to avoid trial-and-error experiments. I will numerically investigate the underlying physical mechanisms and develop the first predictive toolkit for engineering applications of IPMF.

In particular, I will address five ambitious challenges in SIRIUS:
1. Develop an accurate numerical model for IPMF.
2. Understand the impact of particle softness.
3. Investigate the effect of finite particle concentration.
4. Improve the currently low separation efficiency of small particles.
5. Develop a toolkit to enable simulation-driven design.

These objectives are feasible through novel numerical approaches based on the lattice-Boltzmann method and state-of-the-art high-performance computing. SIRIUS will pursue an innovative simulation campaign, validated with existing experimental data, to generate both physical insight and scaling laws for simulation-driven design.

For the first time, SIRIUS will produce robust numerical methods for IPMF. My pioneering research will uncover the physics behind particle separation and culminate in a design toolkit for IPMF engineers. SIRIUS will fill a critical gap and open up an entirely new research field: “Simulations for inertial particle microfluidics”. Results of SIRIUS will be published as open-source codes, open-access articles, and open data. This will ultimately enable faster, less costly and more innovative research in the field of microfluidics for diagnostics.","1499290","2019-01-01","2023-12-31"
"SISI","Seismic Imaging of the Solar Interior using space-based data","Laurent Gizon","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","The broad science objective of this proposal is to search for the root causes of solar magnetic activity by establishing connections and physical relationships between internal solar properties and the various components of magnetic activity in the solar interior and atmosphere. The physical processes inside the Sun are best studied with helioseismology, i.e. the observation and interpretation of solar seismic waves. Helioseismology is on the verge of a revolution with (1) the development of new techniques of data analysis and interpretation and (2) forthcoming observations by NASA’s Solar Dynamics Observatory (SDO, scheduled for launch in 2008), which represents a major technological step beyond the ESA/NASA SOHO mission: higher spatial resolution, higher cadence, and a complete view of the solar disk and the corona. I propose to develop an active program of research in local helioseismology, a relatively young science. New methods of analysis will be developed to make 3D images of the solar interior and to infer the subsurface structure of sunspots using full-waveform modeling techniques. In order to take full advantage of space-based observations, I propose to establish a computing center for helioseismology at the Max Planck Institute for Solar System Research, which will provide the infrastructure and the manpower required to process the relevant SOHO/MDI and SDO/HMI data and to deliver original science data products.","500000","2008-08-01","2013-01-31"
"SKILLS4ROBOTS","Policy Learning of Motor Skills for Humanoid Robots","Jan Peters","TECHNISCHE UNIVERSITAT DARMSTADT","The goal of SKILLS4ROBOTS is to develop a autonomous skill learning system that enables humanoid robots to acquire and improve a rich set of motor skills. This robot skill learning system will allow scaling of motor abilities up to fully anthropomorphic robots while overcoming the current limitations of skill learning systems to only few degrees of freedom. To achieve this goal, it will decompose complex motor skills into simpler elemental movements – called movement primitives – that serve as building blocks for the higher-level movement strategy and the resulting architecture will be able to address arbitrary, highly complex tasks – up to robot table tennis for a humanoid robot. Learned primitives will be superimposed, sequenced and blended. 

Four recent breakthroughs in the PI’s research will make this project possible due to successes on the representation of the parametric probabilistic representations of the elementary movements, on probabilistic imitation learning, on relative entropy policy search-based reinforcement learning and on the modular organization of the representation. These breakthroughs will allow create a general, autonomous skill learning system that can learn many different skills in the exact same framework without changing a single line of programmed code. To accomplish this goal, our skill learning system will autonomously extract the necessary movement primitives out of observed trajectories, learn to generalize these primitives to different situations and select, sequence or combine them such that complex behavior can be synthesized out of the primitive building blocks. We will evaluate our autonomous learning framework on a real humanoid robot platform with 60 degrees of freedom and show that it can learn a large variety of new skills.","1405573","2015-07-01","2020-06-30"
"SKIPPERAD","Simulation of the Kinetics and Inverse Problem
for the Protein PolymERization
in Amyloid Diseases (Prion, Alzheimer’s)","Marie Doumic","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","Amyloid diseases are of increasing concern in our aging society. These diseases all involve the aggregation of misfolded proteins, called amyloid, which are specific for each disease (PrP for Prion, Abeta for Alzheimer's). When misfolded these proteins propagate the abnormal configuration and aggregate to others, forming very long polymers also called fibrils. Elucidating the intrinsic mechanisms of these chain reactions is a major challenge of molecular biology: do polymers break or coalesce? Do  specific sizes polymerize faster? What is the size of the so-called nucleus, i.e., the minimum stable size for polymers?  On which part of the reactions should a treatment focus to arrest the disease ? Up to now, only very partial and partially justified answers have been provided. This is mainly due to the extremely high complexity of the considered processes, which may possibly involve an infinite number of species and reactions (and thus, an infinite system of equations).

The great challenge of this project is to design new mathematical methods in order to model fibril reactions, analyse experimental data, help the biologists to discover the key mechanisms of polymerization in these diseases, predict the effects of new therapies.
Our approach is based on a new mathematical model which consists in the nonlinear coupling of a size-structured Partial Differential Equation (PDE) of fragmentation-coalescence type, with a small number of Ordinary Differential Equations.
On the one hand, we shall solve new and broad mathematical issues, in the fields of PDE analysis, numerical analysis and statistics. These problems are mathematically challenging and have a wide field of applications. On the other hand we want to test their efficacy on real data, thanks to an already well-established collaboration with a team of biophysicists. With such a continuing comparison with experiments, we aim at constantly aligning our mathematical problems to biological concerns.","1203569","2012-12-01","2018-07-31"
"SLAB","Signal processing and Learning Applied to Brain data","Alexandre Marc Gramfort","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","Understanding how the brain works in healthy and pathological conditions is considered as one of the challenges for the 21st century. After the first electroencephalography (EEG) measurements in 1929, the 90’s was the birth of modern functional brain imaging with the first functional MRI and full head magnetoencephalography (MEG) system. In the last twenty years, imaging has revolutionized clinical and cognitive neuroscience.
After pioneering works in physics and engineering, the field of neuroscience has to face two major challenges.
The size of the datasets keeps growing. The answers to neuroscience questions are limited by the complexity of the signals observed: non-stationarity, high noise levels, heterogeneity of sensors, lack of accurate models.
SLAB will provide the next generation of models and algorithms for mining electrophysiology signals which offer unique ways to image the brain at a millisecond time scale.
SLAB will develop dedicated machine learning and signal processing methods and favor the emergence of new challenges for these fields. SLAB focuses on five objectives: 1) source localization with M/EEG for brain imaging at high temporal resolution 2) representation learning to boost statistical power and reduce acquisition costs 3) fusion of heterogeneous sensors 4) modeling of non-stationary spectral interactions to identify functional coupling between neural ensembles 5) development of fast algorithms easy to use by non-experts.
SLAB aims to strengthen mathematical and computational foundations of brain data analysis. The methods developed will have applications across fields (computational biology, astronomy, econometrics). Yet, the primary impact of SLAB will be on neuroscience. The tools and high quality open software produced in SLAB will facilitate the analysis of electrophysiology data, offering new perspectives to understand how the brain works at a mesoscale, and for clinical applications (epilepsy, autism, tremor, sleep disorders).","1492253","2016-09-01","2021-08-31"
"SLaMM","Magnetic Solid Lipid Nanoparticles as a Multifunctional Platform against Glioblastoma Multiforme","Gianni CIOFANI","FONDAZIONE ISTITUTO ITALIANO DI TECNOLOGIA","Central nervous system (CNS) tumors are an important cause of morbidity and mortality worldwide. Among them, glioblastoma multiforme (GBM) is the most aggressive and lethal, characterized by extensive infiltration into the brain parenchyma. Under the standard treatment protocols, GBM patients can expect a median survival of 14.6 months, while less than 5% of patients live longer than 5 years. This poor prognosis is due to several factors, including the highly aggressive and infiltrative nature of GBM, resulting in incomplete resection, and the limited delivery of therapeutics across the blood-brain-barrier (BBB).
The present project aims at addressing these therapeutic challenges by proposing a nanotechnology-based approach for the treatment of GBM, focused on the selective uptake of drug-loaded multifunctional magnetic solid lipid nanoparticles (SLNs). An external magnetic guidance will help the SLN accumulation on the cerebral endothelium, where, owing to their lipid nature, they will be allowed to enter the CNS. Here, appropriate surface ligands will drive their internalization inside cancer cells. The chemotherapeutic payload will undergo release, allowing a targeted pharmaceutical treatment that will be combined to hyperthermia upon appropriate radiofrequency application. A synergic attack against GBM will thus be performed, consisting of a chemical attack thanks to the drug, and a physical attack thanks to hyperthermia, that will dramatically enhance the possibilities of therapeutic success.
By demonstrating the effectiveness of the platform to cross the BBB and to support tumor regression, a huge impact on human healthcare is envisioned. Moreover, further outcomes of this project are expected by considering the development of nanotechnology-based, multi-functional solutions that can easily be adapted to many other high-impact diseases, in particular at the brain level, where BBB crossing poses a crucial obstacle to many therapeutic approaches.","1499997","2017-03-01","2022-02-28"
"SLIM","Strain Localisation in Magma","Yan Lavallée","THE UNIVERSITY OF LIVERPOOL","The tendency of geomaterials to localise deformation is a measure of “the fragility of the Earth” – a threshold to the occurrence of geological hazards. At volcanoes, the remarkable, unpredictable and alarming occurrence of eruptions, switching from low-risk effusive to high-risk explosive eruptive behaviour is a direct consequence of strain localisation in magma (SLiM).

A deformation mechanism map of magma subjected to strain localisation will allow numerical models with higher accuracy, which, coupled to an understanding of the mechanics driving the monitored geophysical signals precursor to failure, will enhance eruption forecasts.

We propose a truly innovative and interdisciplinary approach to a description of SLiM. This collaborative study will pioneer in the integration of efforts from field geologists, experimentalist, mineralogists, petrologists, seismologists, and numerical modellers to underline the effects of microscopic processes responsible for the large-scale impacts of strain localisation in magma during transport and eruption.","1908000","2012-10-01","2018-06-30"
"SLRA","Structured low-rank approximation: Theory, algorithms, and applications","Ivan Valentinov Markovsky","VRIJE UNIVERSITEIT BRUSSEL","Today's state-of-the-art methods for data processing are model based. We propose a fundamentally new approach that does not depend on an explicit model representation and can be used for model-free data processing. From a theoretical point of view, the prime advantage of the newly proposed paradigm is conceptual unification of existing methods. From a practical point of view, the proposed paradigm opens new possibilities for development of computational methods for data processing.

The underlying computational tool in the proposed setting is low-rank approximation. Recent work by the applicant, co-workers, and others has demonstrated advantages of computational methods based on low-rank approximation over classical methods, based on solution of linear systems of equations. In this proposal, we will further advance the theory and algorithms for low-rank approximation by developing robust and efficient local optimisation methods and methods based on convex relaxations.

Low-rank approximation has applications in systems and control, signal processing, computer algebra, and machine learning, to name a few. Generic examples in system theory and signal processing are model reduction and system identification. Dimensionality reduction, classification, and information retrieval problems in machine learning can be formulated and solved as low-rank approximation problems, thus benefiting from the theory, algorithms, and numerical software tools developed in this research proposal. Beyond the scope of the proposal, we envisage that the newly proposed paradigm will catalyse cross-disciplinary research, leading to selection of the best theoretical tools and computational methods available as well as development of new ones by a synergy of ideas from different application domains.","782960","2011-01-01","2015-12-31"
"SM-DNA-REPAIR","New single-molecule techniques and their application in the study of DNA break repair","Fernando Moreno Herrero","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","Unrepaired DNA breaks can lead to genomic instability or cell death. They occur frequently during normal cellular metabolism and are caused, for example, by the collapse or stalling of the replication fork in response to DNA damage. Proper DNA-end processing and handling are essential for the survival of the cell and prevention of carcinogenesis. Cells possess robust mechanisms to repair DNA breaks. One such DNA repair mechanism is homologous recombination where the sister chromatid is used as a template for the faithful repair of the DNA break. In Bacteria, this pathway is initiated when a DNA end is processed to a 3-ssDNA overhang terminated at a recombination hotspot (Chi) sequence. This is a substrate for formation of a RecA nucleoprotein filament that catalyses strand exchange to promote repair. Recent data implicate the AddAB helicase-nuclease and the SMC (Structural Maintenance of Chromosomes) complex in the DNA break processing mechanism of the model organism Bacillus subtilis. Interaction between these machines provides a molecular link between DNA dynamics and the initiation of DNA break processing that may co-ordinate replication fork collapse and DNA repair. Single-molecule manipulation and imaging techniques offer huge potential to investigate DNA break repair reactions in completely new ways, providing information that is inaccessible to conventional ensemble experiments. The aim of this project is two-fold: firstly, to develop novel biophysical instruments for fast Atomic Force Microscopy imaging in liquid and a combined Optical and Magnetic Tweezers setup; and secondly, to monitor and characterize the real-time dynamics of these DNA-repair processes using these new and complementary biophysical approaches. Single-molecule investigation will be supported by statistical analysis of the data and conventional bulk biochemical techniques.","1624230","2008-08-01","2013-07-31"
"SMAC","Statistical machine learning for complex biological data","Jean-Philippe Vert","ASSOCIATION POUR LA RECHERCHE ET LE DEVELOPPEMENT DES METHODES ET PROCESSUS INDUSTRIELS","This interdisciplinary project aims to develop new statistical and machine learning approaches to analyze high-dimensional, structured and heterogeneous biological data. We focus on the cases where a relatively small number of samples are characterized by huge quantities of quantitative features, a common situation in large-scale genomic projects, but particularly challenging for statistical inference. In order to overcome the curse of dimension we propose to exploit the particular structures of the data, and encode prior biological knowledge in a unified, mathematically sound, and computationally efficient framework. These methodological development, both theoretical and practical, will be guided by and applied to the inference of predictive models and the detection of predictive factors for prognosis and drug response prediction in cancer.","1496004","2012-02-01","2018-01-31"
"SMART","Scanning Microscopy using Active Resonating nanoTips","Bernard Legrand","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","This SMART application proposes a technological breakthrough in the field of Atomic Force Microscopy (AFM). It aims at introducing Micro/Nano Electromechanical Systems (MNEMS) as a new generation of AFM probes having outstanding performances in terms of sensitivity and acquisition rate. More precisely, we aim at using bulk mode microresonators to drive an oscillating nanotip in the GHz range. Many applications are expected in the emerging fields of nanobiosciences. AFM systems have been widely used for 20 years in academic and industrial work. They give access to microscopy images at the nanoscale and derived techniques allow many physical characterizations. Many labs are currently trying to use the oscillating mode of AFM to probe biological nanosystems and their dynamics in a liquid environment. However, AFM performances are limited by the AFM oscillator itself. It is typically made of a tip supported by a cantilever beam whose oscillating properties are drastically degraded once placed in a liquid. This phenomenon is due to the hydrodynamic drag and the added mass of the liquid. Consequently, the resonant frequencies and quality factors are too low in liquids to support the force sensitivity and acquisition rate required to probe biological nanosystems dynamics. The SMART project proposes to change the overall AFM oscillator and to choose an oscillation mode in the GHz range that reduces the liquid velocity gradient around the resonator. This new generation of high sensitivity AFM force sensor will be an unprecedented tool for imaging biological and chemical systems at the nanoscale and the possibility of kinetic spectroscopy in liquids. AFM performances are expected to be increased by 3 orders of magnitude. The SMART investigator has a 10 year background in AFM and a 7 year experience in MNEMS resonators. Today, there is no project strictly similar to this one at international level. If successful, Europe could become a leader in this high level competition.","1500000","2008-09-01","2013-08-31"
"SMART","Statistical Mechanics of Active Matter","Roberto Di Leonardo","UNIVERSITA DEGLI STUDI DI ROMA LA SAPIENZA","The study of living matter has to be considered as an exciting and substantive
part of the modern definition of physics. Whether a general statistical
mechanics exists for broad classes of of active systems and what is the
quantitative predictive power that we could expect from those theories remain
still open and debated questions. Reductionism and approximations, the most
powerful weapons of theoretical physicists have to face a degree of complexity
that has no analogue in non living matter. That results in a number of
phenomenological parameters whose connection to microscopic quantities is
rarely supported by experimental data. Moreover some of the most peculiar and
potentially groundbreaking properties of active matter can only be evidenced in
the presence of external force fields and are still largely unexplored.

We propose to combine frontier research tools for 3D holographic
micromanipulation and 3D two-photon microfabrication to gain an unprecedented
active role in probing active matter dynamics, from few bodies interactions up
to collective behavior.  For example, we will study bacteria interactions by
grabbing, orienting and releasing individual bacteria in a sort of
bacteria-bacteria scattering experiments. On the other hand optical energy
landscapes, which can be structured in space and time, will allow to study
collective response to the tunable and smooth fields that are particularly
suited for theoretical treatment.  Furthermore two-photon lithography will
allow the microfabrication of arbitrarily shaped 3D structures that will be
used as probes for the highly non-trivial correlations and response functions
in non equilibrium active baths.

Besides providing a playground for theoretical developments in non-equilibrium
physics,  this project will explore novel opportunities to exploit active
matter as a SMART material, capable of performing useful tasks in micro and
nano engineered devices.","1448400","2012-11-01","2017-10-31"
"SMART","Shapeable Magnetoelectronics in Research and Technology","Denys Makarov","HELMHOLTZ-ZENTRUM DRESDEN-ROSSENDORF EV","In our everyday life we are surrounded by sensing devices designed in a way to meet requirements of a certain application, which is determined primarily by their shape and size. What I propose here is to break this old vision in sensor engineering and to develop shapeable magnetoelectronics allowing to reshape sensors upon demand after fabrication. These devices are unique as the same initial sensor can be used for multiple purposes: an elastic magnetic sensor integrated in a fluidic tubing can be applied for therapeutic needs; namely, diagnostics of cancer diseases by examining magnetically tagged living cells. Alternatively the same sensor can be mounted on a curved surface of a stator in a tiny gap between rotor and stator in electrical machines to provide a regulation for the rotor position; these sensors can help to reduce energy consumption of electrical machines.
European added value / Sustainability: My group is the first which produced shapeable magnetic sensors and pointed out industrial applications of this technology. Supporting this initiative gives advantage to the EU in development of a unique class of devices with important functionality being not only flexible and fast, but also with the ability to react and respond to a magnetic field. There are no alternatives for this technology; therefore as a long term perspective this project will provide a leading position to the EU in the field of shapeable (magnetic) sensorics. The project is in line with main initiatives of the EU: (i) reduction of energy consumption and (ii) development of novel concepts for disease diagnostics. The European principles of gender diversity management will be obeyed.
To accomplish the goals of the project, I need to expand my team by 1 Postdoc and 3 PhDs working on this interdisciplinary project combining physics, engineering, fluidics, and biology. The grant requested is 1499.76 kEUR. The department of research funding (IFW Dresden) will support me to manage the project.","1499760","2013-01-01","2017-12-31"
"SMART","Strong Modular proof Assistance: Reasoning across Theories","Cezary Seweryn KALISZYK","UNIVERSITAET INNSBRUCK","Formal proof technology delivers an unparalleled level of certainty and security. Nevertheless, applying proof assistants to the verification of complex theories and designs is still extremely laborious. High profile certification projects, such as seL4, CompCert, and Flyspeck require tens of person-years. We recently demonstrated that this effort can be significantly reduced by combining reasoning and learning in so called hammer systems: 40% of the Flyspeck, HOL4, Isabelle/HOL, and Mizar top-level lemmas can be proved automatically.

Today's early generation of hammers consists of individual systems limited to very few proof assistants. The accessible knowledge repositories are isolated, and there is no reuse of hammer components.

It is possible to achieve a breakthrough in proof automation by developing new AI methods that combine reasoning knowledge and techniques into a smart hammer, that works over a very large part of today's formalized knowledge. The main goal of the project is to develop a strong and uniform learning-reasoning system available for multiple logical foundations. To achieve this, we will develop: (a) uniform learning methods, (b) reusable ATP encoding components for different foundational aspects, (c) integration of proof reconstruction, and (d) methods for knowledge extraction, reuse and content merging. The single proof advice system will be made available for multiple proof assistants and their vast heterogeneous libraries.

The ultimate outcome is an advice system able to automatically prove half of Coq, ACL2, and Isabelle/ZF top-level theorems. Additionally we will significantly improve success rates for HOL provers and Mizar. The combined smart advice method together with the vast accumulated knowledge will result in a novel kind of tool, which allows working mathematicians to automatically find proofs of many simple conjectures, paving the way for the widespread use of formal proof in mathematics and computer science.","1449000","2017-03-01","2022-02-28"
"SMART","Structured nonlinear Metamaterials for efficient generation and Active functional control of Radiation of THz light","Tal ELLENBOGEN","TEL AVIV UNIVERSITY","The terahertz optical regime, covering the long wavelength end of the optical spectrum, has been for many years the least explored spectral regime. Recent interest in this regime has led to important emerging applications spanning many disciplines including medical, biological, materials sciences, communications, security, and basic sciences. However, advances in these emerging applications are held back by the lack of good and controllable terahertz light sources.
I propose to lead a potential breakthrough in this field by developing a new family of THz sources with unmatched functionality. The developed sources will be based on nano-engineered nonlinear heterostructured metamaterials, man-made materials with artificial optical properties.  The proposal is based on very recent studies that show that metamaterials can be used to emit THz light with excellent efficiency, comparable to the best available nonlinear materials in nature. In addition it relies on our recent experimental demonstrations of functional nonlinear metamaterials that allow unprecedented control of nonlinear optical interactions. We will apply this recent knowledge to design novel active metamaterials that efficiently emit THz light at any desired frequency, shape and polarization, focus it directly from the emitter to a desired sample location and even actively steer and modify its radiation properties all-optically. In addition, we will enhance the THz generation efficiency from metamaterials by more than three orders of magnitude compared to the state of the art. We will also use our expertise to fabricate large scale and multi-layered THz light emitting metamaterials by leveraging novel nanolithography methods. Overall I expect that the outcome of this research will be in development of one of a kind family of THz light emitters that will lead to the, long sought for, leap in THz technology and will open the door to new applications and to new tools for advancing fundamental science.","1937500","2017-01-01","2021-12-31"
"SMART DESIGN","Spin-orbit mechanism in adaptive magnetization-reversal techniques, for magnetic memory design","Ioan Mihai Miron","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Compared to existing Random Access Memories, the Magnetic RAM (MRAM) has the advantage of being non-volatile. Though the basic requirements for reading and writing a single memory element are fulfilled, the present approach based on Spin Transfer Torque (STT) suffers from an innate lack of flexibility. 
The solution that I propose is based on the discovery of a novel phenomenon, where instead of transferring spin angular momentum from a neighbouring layer, magnetization reversal is achieved by angular momentum transfer directly from the crystal lattice. There is a long list of advantages that this novel approach has compared to STT, but the goal of this project is to focus only on their most generic difference: flexibility.
The singularity of spin-orbit torque is that the in-plane current injection geometry decouples the “read” and “write” mechanisms. The disconnection is essential, as unlike STT where the pillar shape of the magnetic trilayer sets the current path, in the case of SOT the composing elements may be shaped separately. The liberty of shaping the current distribution allows to spatially modulate the torque exerted on the local magnetization. 
The central goal of my project is to explore the new magnetization dynamics, specific to the Spin-Orbit Torque (SOT) geometry, and design novel magnetization switching schemes.
I will begin by tackling the fundamental questions about the origin of SOT and try to control it by mastering its dependence on the layer structure. Materials with on-demand SOT will serve as playground for the testing of a broad range of magnetization reversal techniques. The most successful among them will become the building-blocks of complex magnetic objects whose switching behaviour is tightly related to their shape. To study their magnetization dynamics I plan to build a time-resolved near-field magneto-optical microscope, a unique tool for the ultimate spatial and temporal resolution.","1476000","2015-10-01","2020-09-30"
"SMARTBAYES","Intelligent Stochastic Computation Methods for Complex Statistical Model Learning","Jukka Ilmari Corander","HELSINGIN YLIOPISTO","Very recently, it has been claimed that the Bayesian paradigm has revolutionized statistical thinking in numerous fields of research, as a considerable amount of novel Bayesian statistical models and estimation algorithms have gained popularity among scientists. Despite of the evident success of the Bayesian approach, there are also many research problems where the computational challenges have so far proven to be too exhaustive to promote wide-spread use of the state-of-the-art Bayesian methodology. In particular, due to significant advances in measurement technologies, e.g. in molecular biology, a constant need for analyzing and modeling very large and complex data sets has emerged on a wide scale during the past decade. Such needs are even anticipated to rapidly increase in near future with the current technological advances. The prevailing situation is therefore somewhat paradoxical, as the theoretical superiority of the Bayesian paradigm as an uncertainty handling framework is widely acknowledged, yet it can be unable to provide practically applicable solutions to complex scientific problems. To resolve this issue, the research project will have a focus on stochastic computational and modeling strategies to develop methods that overcome problems associated with the analysis of highly complex data sets. With these methods we aim to be able to solve a multitude of statistical learning problems for data sets which cannot yet be reliably handled in practice by any of the existing Bayesian tools. Our approaches will build upon recent advances in Bayesian predictive modeling and adaptive stochastic Monte Carlo computation, to create a novel family of parallel interacting learning algorithms. Several significant statistical modeling problems will be considered to demonstrate the potential of the developed methods. Our goal is also to provide implementations of some of the algorithms as freely available software packages to benefit concretely the scientific community.","550000","2009-11-01","2014-10-31"
"SmartCore","Smart Core/shell nanorod arrays for artificial skin","Anna Maria COCLITE","TECHNISCHE UNIVERSITAET GRAZ","The replication of the circle of information coming from the environment, to the skin, to an action mediated by the brain, requires a lot of advances in smart technology and materials development. Embedding sensors in smart architectures that record the stimulus from the environment and transform it into action is the objective of artificial skins. At the moment, different sensors have to be implemented in the artificial skin matrix for each stimulus. 
The goal of this project is to develop a single multi-stimuli responsive material, which would allow a simplification of the artificial skin and enable unprecedented spatial resolution. The material will be comprised of a smart core, responsive to temperature and humidity, and a piezoelectric shell for pressure sensing. The swelling of the smart core upon stimuli will be sensed by the piezoelectric shell and produce a measurable potential. This architecture will be achieved thanks to the use of novel vapor-based technologies for material processing that allow fabrication at the nanoscale. The advantage of using a dry, vapor-based, polymerization for the smart core is that it will be possible to cumulate different functionalities and engineered composition gradients, which are difficult to obtain by conventional synthesis. Nano-structuration of such materials in core-shell site-specific arrays will allow to create a sensing network with spatial resolution down to 1mm and lower. The network will respond to the stimuli coming from the environment and recognize them in terms of location and type of stimuli. 
The successful execution of the SmartCore project will have a strong impact in the design and production of future structures, with consequences in sensoring, biotechnology and tissue engineering.","1499109","2016-12-01","2021-11-30"
"SMARTDRUGENTITIES","Sophisticated Well-Targeted Therapeutic Entities based on Biologically Compatible Ti(IV) Active Cores and Building Blocks","","THE HEBREW UNIVERSITY OF JERUSALEM","I propose to develop sophisticated anti-tumor agents targeted particularly to the location of activity. My team has recently introduced a new family of Ti(IV) complexes that demonstrates higher activity than known compounds with substantially higher stability and defined hydrolytic behavior, properties that were found to be essential. I propose to study various derivatives and identify the parameters affecting activity, including steric and electronic effects, enantiomeric purity, ligand lability etc., and elucidation various mechanistic aspects of reactivity. More importantly, I propose to construct pH-sensitive transport units that will allow protection of the sensitive active species throughout their delivery and release only near the target location based on the variable pH conditions of different human tissues. In particular, unique spherical molecules held together by metal-ligand interactions will be prepared. The building blocks will consist of the planar ligands of C3-axis bound to three biocompatible Ti(IV) ions each with defined angles and geometry. The resulting spherical compounds will be utilized to encapsulate the active complexes and release them upon hydrolysis at the desired pH based on the pH-dependent hydrolysis pattern already established for related compounds. Preliminary calculations have confirmed the possibility of forming these compounds, which are particularly matching in their expected size to encapsulate our complexes. Larger spheres will also be prepared as cavities for larger molecules, which may be linked together for the delivery of multiple drugs. These compounds may find applications in various areas where a protected environment or delivery of sensitive compounds is required, such as in gene therapy, nano-technology, and catalysis.","1400000","2009-10-01","2014-09-30"
"SmartGeometry","Structure-Aware Geometry Processing","Niloy Jyoti Mitra","UNIVERSITY COLLEGE LONDON","Geometric data is now ubiquitous. Such 3D content is either acquired using LiDAR scans, MRI scans, etc., or created using 3D modelers, or obtained as output of simulation processes. The data, however, come in low-level representations (e.g., points, polygons, voxel grids) and begets little understanding of the underlying objects or processes. This is unfortunate since such data have significant redundancies as they often measure related entities (e.g., humans in different poses, same room across multiple configurations, or chairs having different styles). While the data can potentially reveal significant self-similarities among objects and correlations across related objects, we are missing critically-needed tools for such analysis.

Hence, I propose to develop mathematical frameworks and computational tools to extract, represent, manipulate, and utilize relations among 3D model collections. Essentially, I propose to factorize model collections into consistent structures (i.e., relations among parts in and across multiple objects) and low-dimensional variations, with local geometric details playing a subordinate role. Jointly analyzing model collections can further reveal relationships
between form and their functions (e.g., chairs typically will have consistent back-seat relations, even if at the level of vertices the underlying models can be very different). This requires solving two coupled problems:
(i) jointly analyzing large model collections to decouple consistent structure from dominant modes of variations, and (ii) using the information towards next generation form-finding possibilities.

The grand goal is to lay the foundations of structure-aware geometry processing where computationally extracted geometric relations and constraints are automatically conformed to with potentially far-reaching implications in a range of disciplines like science, engineering, medicine, and product design.","1499130","2013-11-01","2018-10-31"
"SMILE","Statistical Mechanics of Learning","Lenka ZDEBOROVA","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""Computers are now able to recognize people, to tell a dog from a cat, or to process speech so efficiently that they can answer complicated questions. This was still impossible only a decade ago. This progress is largely due to the development of the artificial “deep-learned neural networks”. Nowadays, “deep learning” is revolutionizing our life, prompting an economic battle between internet giants and the creation of a myriad of start-ups. As attractive and performant as it is, however, many agree that deep learning is largely an empirical field that lacks a theoretical understanding of its capacity and limitations. The algorithms used to ""train"" these networks explore a very complex and non-convex energy landscape that eludes most of the present theoretical methodology in machine learning. The behavior of the dynamics in such complicated ""glassy"" landscape is, however, similar to those that have been studied for decades in the physics of disordered systems such as molecular and spin glasses.

In this project we pursue this analogy and use advanced methods of disordered systems to develop a statistical mechanics approach to deep neural networks. Our first main objective is to create a model for learning features from data via a multi-level neural network. We then regard this model as a kind of a spin glass system amenable to an exact asymptotic analysis via the replica and cavity method. Analyzing its phase diagram and phase transitions shall bring theoretical understanding of the principles behind the empirical success of deep neural networks. This approach will also lead to our second objective: the creation of a new class of fast, efficient, and asymptotically optimal message passing algorithms for deep learning. It is the synergy between the theoretical statistical physics approach and scientific questions from computer science that makes the project’s objectives feasible and enables a leap forward in our understanding of learning from data.""","1346795","2017-09-01","2022-08-31"
"SNOWISO","Signals from the Surface Snow: Post-Depositional Processes Controlling the Ice Core IsotopicFingerprint","Hans Christian Steen-Larsen","UNIVERSITETET I BERGEN","For the past 50 years, our use of ice core records as climate archives has relied on the fundamental assumption that the isotopic composition of precipitation deposited on the ice sheet surface determines the ice core water isotopic composition. Since the isotopic composition in precipitation is assumed to be governed by the state of the climate this has made ice core isotope records one of the most important proxies for reconstructing the past climate.

New simultaneous measurements of snow and water vapor isotopes have shown that the surface snow exchanges with the atmospheric water vapor isotope signal, altering the deposited precipitation isotope signal. This severely questions the standard paradigm for interpreting the ice core proxy record and gives rise to the hypothesis that the isotope record from an ice core is determined by a combination of the atmospheric water vapor isotope signal and the precipitation isotope signal. 

The SNOWISO project will verify this new hypothesis by combining laboratory and field experiments with in-situ observations of snow and water vapor isotopes in Greenland and Antarctica. This will enable me to quantify and parameterize the snow-air isotope exchange and post-depositional processes. I will implement these results into an isotope-enabled Regional Climate Model with a snowpack module and benchmarked against in-situ observations. Using the coupled snow-atmosphere isotope model I will establish the isotopic shift due to post-depositional processes under different climate conditions. This will facilitate the use of the full suite of water isotopes to infer past changes in the climate system, specifically changes in ocean sea surface temperature and relative humidity.

By establishing how the water isotope signal is recorded in the snow, the SNOWISO project will build the foundation for future integration of isotope-enabled General Circulation Models with ice core records; this opens a new frontier in climate reconstruction.","1497260","2018-01-01","2022-12-31"
"So2Sat","Big Data for 4D Global Urban Mapping – 10^16 Bytes from Social Media to EO Satellites","Xiaoxiang ZHU","TECHNISCHE UNIVERSITAET MUENCHEN","By 2050, around three quarters of the world’s population will live in cities. The new dimension of ongoing global migration into the cities poses fundamental challenges to our societies across the globe. Despite of increasing efforts, global urban mapping still drags behind the geometric, thematic and temporal resolutions of geo-information needed to address these challenges.

Nowadays diverse sets of incomplete data exist. For example, Earth observation (EO) satellites reliably provide geodetically accurate large scale geo-information of the cities on a routine basis from space. But the data availability is limited by resolutions and acquisition geometries of the sensors. Complementarily, massive imagery, text messages and GIS data from open sources and social media are temporally quasi-seamless, spatially multi-perspective, but with diversely unknown qualities. 

With So2Sat I will jointly exploit big data from social media and satellite observations for global urban mapping, and aim at breakthroughs in 3D/4D urban modelling, infrastructure occupancy classification, and very high resolution population density mapping on a global scale for revolutionizing urban geographic research. The following methodological and application objectives will be addressed: improving urban-related information retrieval from EO satellite data (MO1), mining urban imagery and text messages from social media data (MO2), information fusion from heterogeneous data sources (MO3), big data processing (MO4), as well as pilot applications in informal settlements classification (AO1) and global population density estimation (AO2).

The outcome of So2Sat will be the first and unique global and consistent spatial data set on urban morphology (3D/4D) of settlements, and a multidisciplinary application derivate assessing population density. This is seen as a giant leap for urban geography research as well as for formation of opinions for stakeholders based on resilient data.","1496500","2017-05-01","2022-04-30"
"SOCRATES","Serial Optical Communications for Advanced Terabit Ethernet Systems","Leif Katsuo Oxenløwe","DANMARKS TEKNISKE UNIVERSITET","The last two decades has seen an explosion in telecommunication bandwidth, a trend which has never ceased. Another current trend is the growing concern for the environmental footprint humankind is leaving due to various industries. The Internet traffic grows roughly by 60% per year, and internet servers today consume about 2% of the total global electric power consumption corresponding to a CO2 emission approaching 1% of the total emission caused by human beings. These trends have made it very clear that it is imperative to develop new technologies that can accommodate for the ever growing bandwidth demand and reduce power consumption. The key issue for modern telecommunication engineers and designers is no longer cost per bit, but power per bit. Using optical methods for carrying data and processing the data, without opto-to-electrical conversion, so-called all-optical methods, may help in this respect. This project will aim at developing an all-optical power-efficient communication scenario based on serial optical communications. In serial communications, fewer components will in general be used, and with ultra-short pulses, very high bit rates will become available. Historically, increases in the serial data rate have lead to cost savings, due to reduced complexity in management, reduced power consumption and a reduced number of components. We believe this will hold true, and will explore the fundamental physical limits of serial communications to reach the ultimate serial bit rate, and develop network scenarios to fully take advantage of the serial nature of the data, whilst maintaining a focus on limiting the power consumption. In particular we want to design network scenarios for optical serial multi-Tbit/s data and additionally build a 1 Tbit/s optical Ethernet scenario. We will develop stable ultra-fast switches , and mature them for a variety of functionalities, eventually leading to a validation of ultra-high-speed serial optical communication systems.","1518387","2009-09-01","2014-08-31"
"SOFI","SOFt Interfaces: control of interfacial layers for biotechnological applications","Jean-Christophe Baret","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Microfluidic systems have a tremendous potential for the miniaturization and automation of bio-chemical assays. Cells or genes can be encapsulated in droplets that are manipulated, fused, analysed and sorted at high-throughput. Such systems are extremely powerful for the selection of microorganisms. To go further, I propose now to develop new types of microreactors for biotechnological assays, combining microfluidics and encapsulation. I propose to generalize droplet production to other encapsulation procedures based on SOFt-Interfaces (surfactant-laden interfaces, particle-laden interfaces, soft polymer shells, biofilms...). First, I will characterize the mechanical properties of soft interfaces using interfacial rheology methods and I will develop novel microfluidic methods for the quantitative measurements of the mechanics of microcapsules. Next, I will use microfluidics as a tool to characterize the chemical stability of the microreactors and to induce the release of the contents of the capsules by external forcing. Finally, I propose to use these new types of microreactors in order to control the adhesion of cells inside the microcontainer to develop new high-throughput screening systems for adherent cells. I also envision a novel type of self-sorting microreactor. Based on the recent developments of self-propelled droplets, I will design a system where the droplets containing the object of interest (for example a cell) would sort themselves, by controlling droplet propulsion with an enzymatic reaction. Sorting of microorganisms based on a specific activity is of considerable interest for applications in diagnostics, or selection of organisms for specific tasks. Such self-sorting microreactors would have a huge potential as a new approach to select efficient microorganisms in an automated manner. Such systems can also be seen in the frame of synthetic biology as new microorganisms capable of motion which can be assembled in a bottom-up approach.","1498000","2013-01-01","2017-12-31"
"SOFT-PHOTOCONVERSION","Solar Energy Conversion without Solid State Architectures: Pushing the Boundaries of Photoconversion Efficiencies at Self-healing Photosensitiser Functionalised Soft Interfaces","Micheal Diarmaid SCANLON","UNIVERSITY OF LIMERICK","Innovations in solar energy conversion are required to meet humanity’s growing energy demand, while reducing reliance on fossil fuels. All solar energy conversion devices harvest light and then separate photoproducts, minimising recombination. Normally charge separation takes place at the surface of nanostructured electrodes, often covered with photosensitiser molecules such as in dye-sensitised solar cells; DSSCs. However, the use solid state architectures made from inorganic materials leads to high processing costs, occasionally the use of toxic materials and an inability to generate a large and significant source of energy due to manufacturing limitations. An alternative is to effect charge separation at electrically polarised soft (immiscible water-oil) interfaces capable of driving charge transfer reactions and easily “dye-sensitised”. Photoproducts can be separated on either side of the soft interface based on their hydrophobicity or hydrophilicity, minimising recombination. SOFT-PHOTOCONVERSION will explore if photoconversion efficiencies at soft interfaces can be improved to become competitive with current photoelectrochemical systems, such as DSSCs. To achieve this goal innovative soft interface functionalisation strategies will be designed. To implement these strategies an integrated platform technology consisting of (photo)electrochemical, spectroscopic, microscopic and surface tension measurement techniques will be developed. This multi-disciplinary approach will allow precise monitoring of morphological changes in photoactive films that enhance activity in terms of optimal kinetics of photoinduced charge transfer. An unprecedented level of electrochemical control over photosensitiser assembly at soft interfaces will be attained, generating photoactive films with unique photophysical properties. Fundamental insights gained may potentially facilitate the emergence of new class of solar conversion devices non-reliant on solid state architectures.","1499044","2017-04-01","2022-03-31"
"SOFTGROWTH","Growth and Shaping of Soft Tissue","Eran Sharon","THE HEBREW UNIVERSITY OF JERUSALEM","Many natural structures are made of soft tissue that undergoes complicated continuous shape transformations that accurately and reliably serve specific elaborate tasks. Such processes can be slow, as in growth of a tissue, leading from an initial, featureless, shape to the desired elaborate structure of the adult organ. In other cases continuous shape transformations of soft tissue are rapid and are used for the production of mechanical work, as in the case of the action of the hart. Our understanding of natural growth is limited and our ability to produce controlled motions of soft tissue is poor. A central problem in both cases is how to incorporate all local changes in the tissue in order to determine the mechanical state of the entire body. In addition, there are problems regarding how to measure a deforming body and how to characterize the deformation. Finally, there is a problem of how to control motion and growth in artificial and natural soft tissues. I propose a multi disciplinary study, based on an approach I have started developing. According to it there is an underlying common mathematical way to describe continuous large shape transformations of stretchable tissues. This approach clearly defines the way to determine the mechanical state of a deformed tissue and to measure its local growth/deformation. The project will involve a theoretical study within mechanics and differential geometry, an experimental-physics work, which will be focused on the construction of responsive deformable tissue elements and measurements of their shape evolution, and a biophysical work, in which the natural growth and motion of leaves will be measured and will be correlated with biological activities. Such an integrative study has the potential of advancing our understanding of the fascinating process of growth and to improve our ability to construct bio-inspired &quot;soft machinery&quot;.","1000000","2009-12-01","2014-11-30"
"SOFTWATER","Soft Water: understanding what makes a fluid behave like water","John RUSSO","UNIVERSITY OF BRISTOL","""Water is the most common and yet least understood material on Earth. Despite its simplicity, the structure of the water molecule is responsible for a vast array of properties that are unlike those of other fluids. Water anomalies, like the density maximum at 4 degrees Celsius, play a fundamental role in determining the Earth's climate, and ultimately the very existence of life. From an anthropic viewpoint, the properties of water are as if they were fine-tuned. It is this uniqueness that hinders our understanding of how water behaves in many natural systems and technological applications; for example, our inability to predict the stability limit of water in supercooled clouds, and the rates of ice crystallization, is credited as being one of the biggest unknowns in models of Climate Change, where the scattering of energy from suspended ice droplets in the clouds plays a central role in determining Earth's radiation budget.

In this proposal, we attempt an entirely novel route to understand what makes a fluid behave like water. Starting from the observation that the properties of water seem to appear fine-tuned, we are going to """"untune"""" water's interactions. This means that we are going to consider the interactions of water as one point in a higher dimensional space of possible interactions, and we are going to study how the properties of water change going from “real” water to models which behave like other simple liquids. This continuous change will allow us not only to understand the unique properties that are found in water, but will also provide a route to the potential discovery of new behaviour that cannot be captured with conventional approaches.

The process of gradually changing the interactions in water produces a family of models that we call """"Soft Water"""", and in this research proposal we are going to show how this new approach has the potential to solve the mysteries that real water is still hiding from us.""","952561","2018-02-01","2023-01-31"
"SOLARIS","Large-Scale Learning with Deep Kernel Machines","Julien MAIRAL","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","Machine learning has become a key part of scientific fields that produce a massive amount of data and that are in dire need of scalable tools to automatically make sense of it. Unfortunately, classical statistical modeling has often become impractical due to recent shifts in the amount of data to process, and in the high complexity and large size of models that are able to take advantage of massive data. The promise of SOLARIS is to invent a new generation of machine learning models that fulfill the current needs of large-scale data analysis: high scalability, ability to deal with huge-dimensional models, fast learning, easiness of use, and adaptivity to various data structures. To achieve the expected breakthroughs, our angle of attack consists of novel optimization techniques for solving large-scale problems and a new learning paradigm called deep kernel machine. This paradigm marries two schools of thought that have been considered so far to have little overlap: kernel methods and deep learning. The former is associated with a well-understood theory and methodology but lacks scalability, whereas the latter has obtained significant success on large-scale prediction problems, notably in computer vision. Deep kernel machines will lead to theoretical and practical breakthroughs in machine learning and related fields. For instance, convolutional neural networks were invented more than two decades ago and are today’s state of the art for image classification. Yet, theoretical foundations and principled methodology for these deep networks are nowhere to be found. The project will address such fundamental issues, and its results are expected to make deep networks simpler to design, easier to use, and faster to train. It will also leverage the ability of kernels to model invariance and work with a large class of structured data such as graphs and sequences, leading to a broad scope of applications with potentially groundbreaking advances in diverse scientific fields.","1498465","2017-03-01","2022-02-28"
"SOLARX","Riddle of light induced degradation in silicon photovoltaics","Hele Irene Savin","AALTO KORKEAKOULUSAATIO SR","The sun provides enough energy in one minute to supply the world's energy needs for one year. The grand challenge is to turn this enormous energy potential into electricity in a cost-efficient way. So far, silicon has been most successful at this – but we are still very far away from what is achievable. One of the major problems, which is currently limiting the state-of-the-art photovoltaic solar cells, is related to the material degradation under sun light. I address this issue from a novel perspective: I study the possibility that the root cause for the degradation is related to the interaction of light with copper ions.

The cornerstone of the proposal is to transfer my special knowhow from microelectronics to photovoltaics related to controlling copper behaviour in silicon. My proposal is against the commonly accepted theory, however, it could unveil many mysteries related to the degradation phenomenon. Moreover, if successful, the approach could lead to a rather simple solution in avoiding power loss: implementing charge on the surface to attract the copper ions. In this project I aim at verifying my hypotheses, formulating a new theory regarding the chemical reactions behind the degradation and finally demonstrating a method that allows fabrication of stable yet cheap silicon solar cells having potential for more than 30% power increase.","845770","2013-01-01","2017-12-31"
"SOLARX","Photon Management for Solar Energy Harvesting with Hybrid Excitonics","Akshay RAO","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","In nature, biological light harvesting complexes use antennas molecules to harvest photons, generate excitons and then funnel them to the reaction centre where their energy is used to drive photosynthesis. Inspired by this paradigm, SOLARX will explore new strategies for photon management in solar energy harvesting, based on the transfer and manipulation of excitons at hybrid interfaces. At the core of SOLARX is our development of a new femtosecond transient absorption imaging technique with sub-10fs time resolution and sub-diffraction limit spatial resolution. This opens completely new possibilities to explore excitonic physics at the nanoscale, directly visualising not just the motion of excitons but understanding how vibronic coupling and local structure affects their dynamics. Building on this platform we will deliver ground-breaking new insights into excitonic process in and at the interfaces between organic semiconductors, quantum dots, 2D monolayer semiconductors and lanthanide doped nanoparticles. We will elucidate the fundamental nanoscale dynamics of: (1) endothermic singlet fission, (2) the injection of triplet excitons into lanthanide doped nanoparticles and (3) the motion of excitons in 2D monolayer semiconductors and how these excitons can be funnelled over µm length scales to be transferred to quantum dots. We will then use these insights to develop proof of concept demonstrations of structures which harvest photons across the visible and NIR, efficiently converting high energy visible photons to two NIR photons and then concentrating these photons within structures with the potential to achieve concentration factors well above 100, thus concentrating light to drastically reduce the number of PV panels and hence dramatically reducing the cost of solar energy. SOLARX will thus explore and elucidate fundamental new excitonic physics and use these insights to bring a paradigm shift to solar energy harvesting technologies.","1499585","2018-04-01","2023-03-31"
"SOLCA","""Carbonic anhydrase: where the CO2, COS and H2O cycles meet""","Lisa Wingate","INSTITUT NATIONAL DE LA RECHERCHE AGRONOMIQUE","""Quantifying the carbon storage potential of terrestrial ecosystems and its sensitivity to climate change relies on our ability to obtain observational constraints on photosynthesis and respiration at large scales. Photosynthesis (GPP), the largest CO2 flux from the land surface, is currently estimated with considerable uncertainty. A recent estimate of global GPP was based on an atmospheric budget of the oxygen isotope composition (d18O) of atmospheric CO2 that strongly relies on the oxygen isotope exchange rates with leaf and soil water pools. This isotopic exchange is rapidly catalysed by carbonic anhydrase (CA) in leaves and to a lesser extent in soils. Soil CA activity was neglected in global CO18O studies until the project PI showed recently that CA activity in soils played an important role for determining the magnitude of global GPP using CO18O. The overall goal of SOLCA is to understand better the environmental and ecological causes behind the variability in CA activity observed in soils. A first hypothesis is that soil CA activity responds to thermal and osmotic stresses. This will be tested by probing CA activity of soil monoliths from around the world using a non-invasive gas exchange technique developed by the PI. Because probing soil CA activity from CO18O gas exchange data requires a knowledge of the d18O of soil water and CO2 diffusion processes, we will utilise additional tracers of CA activity: CO17O and carbonyl sulphide (COS) that will also be measured as they follow the same diffusional pathway as CO2 and are also taken up by CA. A second hypothesis is that soil CA activity can be predicted knowing only global indices of the soil microbial community. This will be tested using state-of-the-art molecular techniques to explain changes in CA activity levels. This project will construct novel algorithms for using additional tracers of the global CO2 budget and will lead to a revised estimate of terrestrial GPP.""","1701882","2014-02-01","2019-01-31"
"SolCat","Shedding light on catalyst systems for a brighter future with green oxidation chemistry","Wesley Richard Browne","RIJKSUNIVERSITEIT GRONINGEN","Sustainability in chemical synthesis requires that new methods and replacements for unsustainable existing methodologies should, where possible, focus on environmentally benign, atom- and energy-efficient processes that do not rely on toxic/precious metals. In this research program the focus is on the development of new sustainable and environmentally benign methodologies for selective oxidative transformations based on primarily first row transition metals using H2O2, O2 or electrochemical methods and on the tools required to obtain kinetic and mechanistic information that can further method development. The overall aim is to achieve selectivity and reactivity under mild, atom efficient conditions using non-toxic catalysts. In the research program mechanistic investigations run parallel with the development of synthetic methodologies, providing a perfect opportunity to combine very different skill sets towards a common goal.
Five distinct approaches will be taken aimed at increasing the repertoire of selective methods available for catalytic organic oxidative functional group transformations with the key goal of sustainability in chemical synthesis through atom efficiency and selectivity. Furthermore, a systems approach will be taken in mechanistic studies where understanding the role(s) played by each reaction component involved, including metals, ligands, oxidant, solvent, additives, substrates and product(s), will be central. Importantly the development of new techniques and approaches to mechanistic studies and reaction monitoring, including Raman detection in flow reactors and UV Raman spectroscopy, are an integral part of the program.
The research program is divided into four interrelated projects, which will run concurrently and be undertaken by one each of the PhD students involved and a smaller fifth project that is exploratory and high risk. As the project will fund the applicant for 50% of his time, this will enable him to take an active part.","1500000","2011-11-01","2016-10-31"
"SolidSpinQopt","Quantum Optics with Spins in Solid State: The Power of Ensembles","Caspar Heimen Van Der Wal","RIJKSUNIVERSITEIT GRONINGEN","Quantum states of optical pulses can be controlled with great accuracy, and in solid state precise control over quantum states of spins has been achieved. Conversion of such optical quantum states into spin quantum states, and vice versa, is highly relevant for quantum information science, but appears to be more challenging. This proposal aims at a study of such quantum-state conversion, and at developing it into a fast and robust technique with high fidelity. The project builds on the established idea that spins in semiconductors provide a promising system for such research, but pioneers two innovations. A key ingredient is to use ensembles of electron spins, instead of the more widely studied case of an individual spin in a quantum dot. Using ensembles gives access to strong interaction between spins and highly-directional optical fields in a robust manner, without a need for high-finesse cavities. This is vital for the second innovation, which is to use projective measurement with quantum optical techniques as a robust tool for preparing very pure correlations between quantum states of spins and optical pulses. The correlations originate from spin-flip Raman transitions at the single photon level, and this approach also gives access to studying entanglement between spins that are separated by a large distance.

During this project, the same quantum optical techniques will be used for time-resolved probing of the loss of quantum coherence, and initialization experiments aimed at preparing the solid-state environment in a state that yields longer spin coherence times. The experiments use donor-bound electron spins in GaAs where spin decoherence is mainly due to interaction with fluctuating nuclear spins in the host lattice. The project includes a new approach for pumping these fluctuations away. Work with donor-bound electrons in ZnSe, where the lattice has nuclear spin zero, explores how the single nuclear spin of the donor can act as a long-lived quantum memory.","1500000","2012-02-01","2017-01-31"
"SOLVE","Solvated Electrons in Water: Structure, Dynamics and Reactivity at Interfaces","Jan Raf Rogier Verlet","UNIVERSITY OF DURHAM","Understanding the solvation of an ion at the molecular level is a cornerstone of physical chemistry. For anions in water, the interactions are particularly complex, with some simple anions exhibiting surface activity. The most fundamental aqueous anion is the hydrated electron and it represents a benchmark for understanding solvation and electron-water interactions in solution. However, despite the hydrated electron’s apparent simplicity and its technological and scientific importance, it remains at the centre of much debate. Questions concerning its structure and solvation at interfaces are currently highly contentious. In this proposal, a number of complementary approaches are applied to the study of aqueous interfacial electrons with the aim of gaining a new understanding of the solvation structure, dynamics and reactivity of this species. Specifically, gas-phase clusters will be used as molecular-level laboratories to gain a fundamental understanding of the electron-water interactions as well as solvation motifs and dynamics. These will also be used to explore the reactivity of electrons bound at the interface of water with atmospheric and biologically pertinent molecules. In parallel to the cluster studies, dynamics and reactivity of the electron at the ambient water interface will also be studied using non-linear spectroscopy. Together, these studies will provide an in depth understanding of electron solvation and its importance as a reactive species at aqueous interfaces.","1839322","2013-01-01","2017-12-31"
"SOLVe","Connecting SOLar and stellar Variabilities","Alexander SHAPIRO","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","The proposed project aims at answering four general questions of high scientific importance:
- How typical is the Sun as active star?
- What is the solar role in climate change?
- What are the limitations for detectability of Earth-type planets in habitable zones?
- Can the solar paradigm help us to explain variability of other stars and to better detect and characterize
exoplanets?
These exciting questions can now be addressed thanks to recent progress in solar and stellar physics. On the
observational side, the unprecedented precision of broadband stellar photometry achieved with the CNES
CoRoT and the NASA Kepler satellites initiated a new era in studying stellar photometric variabilities.
Recent surveys with ground-based automated telescopes have significantly increased the number of stars
observed over their activity cycles. On the theoretical side, solar irradiance models have matured to
reproduce solar brightness variability in great detail. The enormous progress in magnetohydrodynamic
simulations of solar and stellar atmospheres as well as the development of new computationally efficient
radiative transfer schemes makes it finally possible to extend physics-based solar irradiance models to other
stars, thus opening an entirely new horizon for solar-stellar and solar-terrestrial connection studies.","1443000","2017-01-01","2021-12-31"
"SONGBIRD","SOphisticated 3D cell culture scaffolds for Next Generation Barrier-on-chip In vitro moDels","Maria TENJE","UPPSALA UNIVERSITET","The blood-brain barrier is a sophisticated biological barrier comprising several different cell types, structured in a well-defined order with the task to strictly control the passage of molecules - such as drugs against neurodegenerative diseases - from the blood into the brain. To reduce the ethical and economic costs of drug development, which in EU today uses 
~10 million experimental animals every year, we must develop in vitro models of the blood-brain barrier with high in vivo correlation, as these are completely missing today.

SONGBIRD aims to achieve this with the scientific approach to
- Develop advanced microfabrication methods to handle biologically derived materials
- Structure the materials into heterogeneous 3D multi-layer suspended cell culture scaffolds
- Incorporate blood-brain barrier cells with precise control on location and order
- Integrated the 3D scaffolds into a microfluidic network as a miniaturised screening platform

The vision is to develop and validate versatile microfabrication methods to mechanically structure and physically handle soft biological materials to unlock the use of next generation animal-free barrier-on-chip models that can be used to speed up drug development, serve as screening platforms for nanotoxicology and help medical researchers to gain mechanistic insight in drug delivery. During SONGBIRD, I will focus on the blood-brain barrier due to its urgent relevance for drug development for the ageing population but the final processing tool-box will be suitable for realising in vitro models of any biological barrier in the future.

SONGBIRD is proposed to run for 60 months and will include researchers with expertise in microsystem engineering (PI), hydrogel synthesis and drug delivery. The expected output is a validated 3D barrier-on-chip model as well as a microfabrication toolbox for biological materials enabling transformation from 2D to 3D cell cultures in several other life science research areas.","1500000","2018-01-01","2022-12-31"
"SOSNA","Expressive Power of Tree Logics","Mikolaj Bojanczyk","UNIWERSYTET WARSZAWSKI","Logics for expressing properties of labeled trees and forests figure importantly in several different areas of Computer Science, including verification (branching temporal logics) and database theory (many XML query languages). The goal of this project is to investigate the expressive power of tree logics, mainly those logics that can be captured by tree automata. A similar study, but for word languages, is one of the main lines of research in formal language theory. The study of the expressive power of word logics has lead to many beautiful and fundamental results, including Schutzenberger&apos;s characterization of star-free languages, and the Krohn-Rhodes decomposition theorem. We intend to extend this research for trees. The type of questions we want to answer is: what is the expressive power of first-order logic in trees? is there a Krohn-Rhodes decomposition theory for trees? what is a tree group? We expect that our study of tree logics will use algebraic techniques, possibly the setting of forest algebra (as introduced by the principal investigator and Igor Walukiewicz). We would also like to extend the algebraic setting beyond regular languages of finite trees, to e.g. infinite trees, or nonregular languages.","799920","2009-11-01","2014-10-31"
"SOTUF","SOot in TUrbulent Flames: a new look at soot production processes in turbulent flames leading to novel models for predictive large eddy simulations","Benedetta Franzelli","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Many practical systems emit soot into the atmosphere as a result of incomplete combustion of hydrocarbons. This pollutant emission is characterized by a distribution of solid carbon particles with different sizes and shapes, which have negative effects on human health and environment. Controlling such emission represents a societal issue and an industrial challenge that require a deep understanding of the intricate processes underlying soot production in the turbulent flames that generally characterize practical systems. In this context, progress in numerical simulations is essential to the successful design of low-emission combustion systems. Unfortunately, the Large-Eddy Simultations (LES) approach, which has successfully demonstrated its capacity to represent gaseous turbulent combustion processes, is far from being predictive for soot emission. Indeed, soot production in turbulent flames is a complex process which is not easy to be represented with the classical LES strategy: the long time scales and the broad range of length scales place soot processes outside the usual scale ranges of LES subgrid models. In this context, the goal of the present project is to provide new insights on the processes governing soot production in turbulent flames to develop novel LES models, encompassing the state-of-art and allowing reliable predictions of soot in turbulent flames. These objectives will be achieved by: (1) characterizing the turbulence-flame-soot coupling from novel well-controlled experiments employing advanced space and time resolved optical diagnostics; (2) developing new subgrid models based on information extracted from experiments and high-fidelity simulations; (3) validating and applying the developed LES modeling strategy on complex systems. The research results are expected to drastically improve the prediction of soot production in industrial configurations, helping to design new low-emission systems with notably reduced soot levels.","1436330","2018-06-01","2023-05-31"
"SPA4AstroQIT","Broadband Quantum-Limited Parametric Amplifier for Astronomy and Quantum Information Technology","Boon Kok TAN","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The emerging technology of superconducting parametric amplifiers (SPAs) can achieve quantum-limited sensitivity over broad bandwidth, by utilising the wave-mixing mechanism in a passive nonlinear transmission medium. They are compact, easy to fabricate with planar circuit technology, have ultra-low heat dissipation, and can be integrated directly with other detector circuits. Their performances are far superior to the state-of-the-art high electron mobility transistor (HEMT) amplifier, and they can operate from radio to THz frequencies. Therefore, they could potentially revolutionise almost every kind of microwave, millimetre (mm) and sub-millimetre (sub-mm) instrumentation: from observational astronomy to quantum information experiments. Their deployment as readout amplifiers could improve the heterodyne receiver sensitivity significantly, and enable the construction of large bolometric arrays. Their large bandwidth, high power handling and quantum-noise performance could have profound effect on quantum computing architecture, improve the fidelity to process hundreds of quantum bits (qubit). They can be used as front-end high frequency amplifiers operating at THz frequencies, which is hard to achieve with HEMT technology. In this proposal, I aim to develop: 1) ultra-broadband readout amplifiers for mm/sub-mm astronomical receivers and qubit experiments, which would enable the construction of large pixel-count system; 2) front-end amplifiers at mm frequencies for heterodyne receivers and B-mode Cosmic Microwave Background experiments; and 3) parametric frequency down-converter with positive conversion gain to replace Superconductor-Insulator-Superconductor (SIS) as ultra-low noise heterodyne mixer for large array application. The successful development of these programmes not only could transform the mm/sub-mm instrumentation in the future, but could also have huge impact on many other fields such as telecommunications, medical and quantum computing technology.","1991678","2019-02-01","2024-01-31"
"SPACE","Space-time structure of climate change","Thomas Rudolf LAEPPLE","ALFRED-WEGENER-INSTITUT HELMHOLTZ-ZENTRUM FUR POLAR- UND MEERESFORSCHUNG","I will determine and use the space-time structure of climate change from years to millennia to test climate models, fundamentally improve the understanding of climate variability and provide a stronger basis for the quantitative use of paleoclimate records.
The instrumental record is only a snapshot of our climate record. Two recent advances allow a deeper use of the paleo-record: 1.increased availability and number of paleoclimate records, 2.major advances in the understanding of climate proxies. In a recent PNAS paper, we showed that consistent estimates of regional temperature variability across instruments and proxies can now be obtained by inverting the process by which nature is sampled by proxies. Empirical evidence and physics suggest an intrinsic link between time scale and the associated spatial scale of climate variations: While fast variations such as weather are regional, glacial-interglacial cycles appear to be globally coherent. I will quantify this presumed tendency of the climate system to reduce its degrees of freedom on longer time scales and use it to constrain the sparse, noisy and at times contradictory evidence of past climate changes. By systematically analyzing instrumental and paleo-records, I will
1. determine the space-time structure of climate changes on annual to millennial time scales. This provides the prerequisite for mapping past climate changes and will allow me to confront climate models with robust estimates of climate variability across spatial scales;
2. provide a clearer separation of internal and external forced climate variability, by leveraging their distinct space-time structures;
3. examine the past relationship between mean-state and climate variability to predict how variability will change in a warmer future.
This will provide a key step forward to transform paleoclimate science from describing data to using the data as a quantitative test for models and system understanding in order to see more clearly into the future","1499082","2017-09-01","2022-08-31"
"SPADE","Sophisticated Program Analysis, Declaratively","Ioannis Smaragdakis","ETHNIKO KAI KAPODISTRIAKO PANEPISTIMIO ATHINON","Static program analysis is a fundamental computing challenge. We have recently demonstrated significant advantages from expressing analyses for Java declaratively, in the Datalog language. This means that the algorithm is in a form that resembles a pure logical specification, rather than a step-by-step definition of the execution. The declarative specification does not merely cover the main logic of the algorithm, but its entire implementation, including the handling of complex semantic features (such as native methods, reflection, threads) of the Java language. Surprisingly, the declarative specification can be made to execute up to an order of magnitude faster than the dominant pre-existing implementations of the same algorithms. Armed with this past experience, the SPADE project aims to develop a next-generation approach to the design and declarative implementation of static program analyses. This will include a) a substantially more flexible notion of context-sensitive analysis, which allows context to vary according to introspective observations; b) a flow-sensitive analysis framework that can be used as the basis for dataflow analysis; c) an approach to producing parallel implementations of analyses by exploiting the parallelism inherent in the declarative specification; d) an exploration of adapting analysis logic to multiple languages and paradigms, including C (using the LLVM infrastructure), functional languages (e.g., Scheme), and dynamic languages (notably, Javascript); e) client analyses algorithms (e.g., may-happen-in-parallel, bug finding analyses such as race and atomicity-violation detectors, etc.) expressed modularly over the underlying substrate of points-to analysis.

The work will have applications to multiple languages and a variety of analyses. Concretely, our precise and scalable analysis algorithms will enhance optimizing compilers, program analyzers for error detection, and program understanding tools.","1042616","2013-01-01","2019-03-31"
"SPADE","from SPArsity to DEep learning","Raja Giryes","TEL AVIV UNIVERSITY","Lately, deep learning (DL) has become one of the most powerful machine learning tools with ground-breaking results in computer vision, signal & image processing, language processing, and many other domains. However, one of its main deficiencies is the lack of theoretical foundation. While some theory has been developed, it is widely agreed that DL is not well-understood yet.

A proper understanding of the learning mechanism and architecture is very likely to broaden the great success to new fields and applications. In particular, it has the promise of improving DL performance in the unsupervised regime and on regression tasks, where it is currently lagging behind its otherwise spectacular success demonstrated in massively-supervised classification problems.

A somewhat related and popular data model is based on sparse-representations. It led to cutting-edge methods in various fields such as medical imaging, computer vision and signal & image processing. Its success can be largely attributed to its well-established theoretical foundation, which boosted the development of its various ramifications. Recent work suggests a close relationship between this model and DL, although this bridge is not fully clear nor developed.

This project revolves around the use of sparsity with DL. It aims at bridging the fundamental gap in the theory of DL using tools applied in sparsity, highlighting the role of structure in data as the foundation for elucidating the success of DL. It also aims at using efficient DL methods to improve the solution of problems using sparse models. Moreover, this project pursues a unified theoretical framework merging sparsity with DL, in particular migrating powerful unsupervised learning concepts from the realm of sparsity to that of DL. A successful marriage between the two fields has a great potential impact of giving rise to a new generation of learning methods and architectures and bringing DL to unprecedented new summits in novel domains and tasks.","1499375","2017-10-01","2022-09-30"
"SPAJORANA","Towards spin qubits and Majorana fermions in Germanium self-assembled hut-wires","Georgios Katsaros","INSTITUTE OF SCIENCE AND TECHNOLOGYAUSTRIA","A renewed interest in Ge has been sparked by the prospects of exploiting its lower effective mass and higher hole mobility to improve the performance of transistors. Ge emerges also as a promising material in the field of spin qubits, as its coherence times are expected to be very long. Finally, it has been proposed that strained Ge nanowires show an unusually large spin orbit interaction, making them thus suitable for the realization of Majorana fermions. In view of these facts, one is able to envision a new era of Ge in information technology.
The growth of Ge nanocrystals on Si was reported for the first time in 1990. This created great expectations that such structures could provide a valid route towards innovative, scalable and CMOS-compatible nanodevices. Two decades later the PI was able to realize the first devices based on such structures. His results show that Ge self-assembled quantum dots display a unique combination of electronic properties, i.e. low hyperfine interaction, strong and tunable spin-orbit coupling and spin selective tunneling. In 2012, the PI’s group went a step further and realized for the first time Ge nanowires monolithically integrated on Si substrates, which will allow the PI to move towards double quantum dots and Majorana fermions. In view of their exceptionally small cross section, these Ge wires hold promise for the realization of hole systems with exotic properties.
Within this project, these new wires will be investigated, both as spin as well as topological qubits. The objective of the present proposal is mainly to: a) study spin-injection by means of normal and superconducting contacts, b) study the characteristic time scales for spin dynamics and move towards electrical spin manipulation of holes, c) observe Majorana fermions in a p-type system. The PI’s vision is to couple spin and topological qubits in one “technological platform” enabling thus the coherent transfer of quantum information between them.","1675020","2014-01-01","2018-12-31"
"SPALORA","Sparse and Low Rank Recovery","Holger Rauhut","RHEINISCH-WESTFAELISCHE TECHNISCHE HOCHSCHULE AACHEN","Compressive sensing is a novel field in signal processing at the interface of applied mathematics, electrical engineering and computer science, which caught significant interest over the past five years. It provides a fundamentally new approach to signal acquisition and processing that has large potential for many applications. Compressive sensing (sparse recovery) predicts the surprising phenomenon that many sparse signals (i.e. many real-world signals) can be recovered from what was previously believed to be highly incomplete measurements (information) using computationally efficient algorithms. In the past year exciting new developments emerged on the heels of compressive sensing: low rank matrix recovery (matrix completion); as well as a novel approach for the recovery of high-dimensional functions.
We plan to pursue the following research directions:
- Compressive Sensing (sparse recovery): We aim at a rigorous analysis of certain measurement matrices.
- Low rank matrix recovery: First results predict that low rank matrices can be recovered from incomplete linear information using convex optimization.
- Low rank tensor recovery: We plan to extend methods and mathematical results from low rank matrix recovery to tensors. This field is presently completely open.
- Recovery of high-dimensional functions: In order to reduce the huge computational burden usually observed in the computational treatment of high-dimensional functions, a recent novel approach assumes that the function of interest actually depends only on a small number of variables. Preliminary results suggest that compressive sensing
and low rank matrix recovery tools can be applied to the efficient recovery of such functions.
We plan to develop computational methods for all these topics and to derive rigorous mathematical results on their performance. With the experience I gained over the past
years, I strongly believe that I have the necessary competence to pursue this project.","1010220","2011-01-01","2015-12-31"
"SPARCCLE","STRUCTURE PRESERVING APPROXIMATIONS FOR ROBUST COMPUTATION OF CONSERVATION LAWS AND RELATED EQUATIONS","Siddhartha Mishra","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","""Many interesting systems in physics and engineering are mathematically modeled by first-order non-linear hyperbolic partial differential equations termed as systems of conservation laws. Examples include the Euler equations of aerodynamics, the shallow water equations of oceanography, multi-phase flows in a porous medium (used in the oil industry), equations of non-linear elasticity and the MHD equations of plasma physics. Numerical methods are the key tools to study these equations and to simulate interesting phenomena such as shock waves.

Despite the intense development of numerical methods for the past three decades and great success in applying these methods to large scale complex physical and engineering simulations, the massive increase in computational power in recent years has exposed the inability of state of the art schemes to simulate very large, multiscale, multiphysics three dimensional problems on complex geometries. In particular, problems with strong shocks that depend explicitly on underlying small scale effects, involve geometric constraints like vorticity and require uncertain inputs such as random initial data and source terms, are beyond the range of existing methods.

The main goal of this project will be to design space-time adaptive \emph{structure preserving} arbitrarily high-order finite volume and discontinuous Galerkin schemes that incorporate correct small scale information and provide for efficient uncertainty quantification. These schemes will tackle emerging grand challenges and dramatically increase the range and scope of numerical simulations for systems modeled by hyperbolic PDEs. Moreover, the schemes will be implemented to ensure optimal performance on emerging massively parallel hardware architecture. The resulting publicly available code can be used by scientists and engineers to study complex systems and design new technologies.""","1220433","2012-12-01","2017-11-30"
"SPARCS","Statistical Physics Approach to                                    Reconstruction in Compressed Sensing","Florent Krzakala","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Compressed sensing is triggering a major evolution in signal acquisition: it indicates that most data, signals and images, that are usually compressible and have redundancy, can be reconstructed from much fewer measurements than what was usually considered necessary, resulting in a drastic gain of time, cost, and measurement precision. In order to make this groundbreaking improvement possible, compressed sensing deals with how measurements should be performed, and how, in a second step, to use computational power in order to reconstruct the original signal. Compressed sensing can be used for many applications (speeding up magnetic resonance imaging without the loss of resolution,  performing X-ray scans with less radiation exposure, sensing and compressing data simultaneously, measurements in acoustic holography, in system biology, faster confocal microscopy, etc ...). Currently used measurement protocols and reconstruction techniques, however, are still limited to acquisition rates considerably higher than what is theoretically necessary.

The aim of this project is to develop a new interdisciplinary approach to compressed sensing, based on a statistical physics inspired methodology, whose preliminary application by the PI already yield spectacular results. I propose to use both a new algorithm for the reconstruction algorithm, with a mean-field inspired “Belief Propagation” method, and a new class of compressed sensing measurement schemes, motivated by a statistical physics study of the problem and by the theory of crystal nucleation in first order transitions. For reasons detailed below, this statistical physics approach is extremely promising theoretical framework to tackle compressed sensing and I believe it can eventually lead to optimal performance. I expect that the progress we will make in this direction will be instrumental also for other inference and inverse problems at the crossroad between physics and computer science.","1077960","2012-10-01","2018-09-30"
"SpdTuM","SPD nanostructured magnets with tuneable properties","Andrea BACHMAIER","OESTERREICHISCHE AKADEMIE DER WISSENSCHAFTEN","The decrease of weight and the increase of efficiency of magnetic components are essential for the reduction of CO2-emission and an improvement of their performance. Nanostructuring can dramatically improve the magnetic properties of soft and hard magnetic materials, hence opening up entirely new possibilities for the development of novel magnets. Nanocomposite magnets, for example, have been the focus of research since two decades. One of the remaining key challenges is to synthesize bulk nanostructured magnets of a reasonable size. In this project, this challenge is explicitly addressed and the potential to fabricate bulk nanostructured magnets by severe plastic deformation (SPD) as an innovative processing route is evaluated. The aim of the project is not only to synthesize different nanostructured magnets by SPD, but also to tailor their microstructure to attain the desired magnetic properties. It has been shown by the applicant that the magnetic properties of SPD processed nanocrystalline materials can be modified in wide range by decomposition of metastable solid solutions. By using different immiscible systems, decomposition mechanisms and annealing treatments, unique nanostructures can be obtained and the magnetic properties can be optimized. Through the choice of different magnetic starting materials, such as soft, hard and antiferromagnetic-ferromagnetic powders, different types of hard magnetic nanocomposites will also be obtained. Fine tuning of the microstructure and resulting magnetic properties through adjustments in the composition, SPD processing parameters and annealing treatments is planned. The project systematically addresses the entire process from the synthesis to the in-depth microstructural characterization by electron microscopy and atom probe tomography. In combination with simultaneous measurements of magnetic properties, the newly developed knowledge will be used to improve the performance of SPD processed nanostructured magnets.","1499475","2018-01-01","2022-12-31"
"SPEAR","Specialisable, Programmable, Efficient and Robust Microprocessors","Robert Mullins","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The development of faster, cheaper and smaller transistors has been the driving force behind the exponential growth in computing power over the past 50 years. While our ability to fabricate better transistors has not yet ceased, continuing to translate these advances into better system-level performance is now a major challenge. This proposal seeks to research a new approach to building programmable digital systems, one that can offer the efficiency, robustness and flexibility required as we approach the end of the CMOS era and start to introduce new post-CMOS technologies. The ideas are centred upon a novel network-centric multiprocessor architecture, with contributions planned at every level from the circuit to the language level.","1271216","2012-09-01","2018-05-31"
"SPEAR","Series-Parallel Elastic Actuators for Robotics","Bram Kris Vanderborght","VRIJE UNIVERSITEIT BRUSSEL","Actuators are key components for moving and controlling a mechanism or system. However, the torque and efficiency of the current state-of-the-art actuators are insufficient and much lower than in humans. There are several applications (including prostheses, exoskeletons and running robots) where the unavailability of suitable actuators hinders the development of well-performing machines with capabilities comparable to a human. Remarkable, the power density and efficiency of electric motors are higher than a human muscle, so the problems of insufficient torque and efficiency resides in the transmission of the power and that the motors are not used at their highest efficiency. The first innovation of SPEAR is to solve the torque and efficiency problems, by investigating in depth a novel actuation paradigm, which I call Series-Parallel Elastic Actuation (SPEA) and that goes beyond variable impedance actuators. This new actuation paradigm is inspired by the series-parallel organisation of the muscle fibres. Modularity in actuation is currently introduced by placing in all joints the same motor, leading to over- or underactuated joints. In our body however, all the skeletal muscles are built of the same basic actuation unit: a muscle fibre. Modularity in actuation in a biological system is not at muscle level, but on a sublevel: the muscle fibre. SPEAR will introduce a second major innovation: the SPEA will introduce a basic actuation unit, a “transistor for actuation”. Such a SPEA-element is a missing link in robotics and will innovate the way robots are designed and built. The project will study the theoretical framework, the design principles, the control algorithms and the validation of demonstrators. SPEAR will fully answer all the research challenges and explore the frontiers of this novel actuation paradigm, leading to a tremendous impact on all engineered, actuated systems, especially in robotics.","1498620","2014-02-01","2019-01-31"
"SPEC","Secure, Private, Efficient Multiparty Computation","Claudio ORLANDI","AARHUS UNIVERSITET","MPC is a cryptographic technique that allows a set of mutually distrusting parties to compute any joint function of their private inputs in a way that preserves the confidentiality of the inputs and the correctness of the result. Examples of MPC applications include secure auctions, benchmarking, privacy-preserving data mining, etc.

In the last decade, the efficiency of MPC has improved significantly, especially with respect to evaluating functions expressed as Boolean and arithmetic circuits. These advances have allowed several companies worldwide to implement and include MPC solutions in their products.

Unfortunately, it now appears (and it’s partially confirmed by theoretical lower bounds) that we have reached a wall with respect to possible optimizations of current building blocks of MPC, which prevents MPC to be used in critical large-scale applications. I therefore believe that a radical paradigm-shift in MPC research is needed in order to make MPC truly practical. 

With this project, I intend to take a step back, challenge current assumptions in MPC research and design novel MPC solutions. My hypothesis is that taking MPC to the next level requires more realistic modelling of the way that security, privacy and efficiency are defined and measured. By combining classic MPC techniques with research in neighbouring areas of computer science I will fulfill the aim of the project and in particular:
1) Understand the limitations of current abstract models for MPC and refine them to more precisely capture real world requirements in terms of security, privacy and efficiency.
2) Use the new models to guide the developments of the next generation of MPC protocols, going beyond current performances and therefore enabling large-scale applications.
3) Investigate the necessary privacy-utility trade-offs that parties undertake when participating in distributed computations and define MPC functionalities that encourage cooperation for rational parties.","1495902","2019-01-01","2023-12-31"
"SPECAP","Precision laser spectroscopy of antiprotonic and pionic atoms","Masaki Hori","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","""This proposal has two objectives. First, I propose to measure the antiproton mass relative to the electron mass with a fractional precision of 0.1 ppb by carrying out sub-Doppler two-photon laser spectroscopy of antiprotonic helium atoms. The antiproton mass would then be better known than the proton mass. The (anti)proton-to-electron mass ratio is regarded as a one of several fundamental physical constants which, like the fine structure constant α, is a dimensionless quantity of nature that can be measured to ppb-scale precision. The experiment would constitute the highest precision confirmation of CPT symmetry involving atoms containing antiprotons. A new buffer gas cooling technique will be used to cool down the atoms to temperature T<1.5 K, thereby reducing the residual Doppler width of the two-photon resonance signals caused by the thermal motion of the atom in the experimental target. A new quasi-cw laser with high peak power and narrow linewidth will be developed. In the later half of the project, high-quality antiproton beam from the new ELENA storage ring constructed at CERN will be used to reach higher precisions of 10^-11
Second, I propose to measure the pion mass to a precision of <10^-8 by carrying out the first laser spectroscopy of pionic helium atoms, where the pion occupies a Rydberg state and the electron the 1s state. This corresponds to a >300-fold improvement in precision compared to the currently known mass, and approaches the fundamental limit imposed by the 26-ns lifetime of the pion. Past measurements of mπ show a bifurcation, i.e. two groups of experimental results near 139.570 and 139.568 MeV/c^2. The laser spectroscopy of pionic helium will provide an unambiguous value for the pion mass. This will improve the limit on the muon neutrino mass obtained from laboratory experiments, and reduce the uncertainty on the Fermi coupling constant. This will be the first time an atom containing a meson has been studied by laser spectroscopy.""","1428660","2013-02-01","2018-01-31"
"SPECGEO","Spectral geometric methods in practice","Emanuele RODOLA'","UNIVERSITA DEGLI STUDI DI ROMA LA SAPIENZA","Spectral geometry concerns the study of the geometric properties of data domains, such as surfaces or graphs, via the spectral decomposition of linear operators defined upon them. Due to their valuable properties analogous to Fourier theory, such methods find widespread use in several branches of computer science, ranging from computer vision to machine learning and network analysis.

Despite their pervasive presence, very little efforts have been devoted to the design and application of spectral techniques that deal with corrupted, missing, high-dimensional or abstract data undergoing complex transformations. This lack of focus is mainly motivated by the widespread acceptance, supported in part by theoretical results, that an ε-perturbation to the geometry of the data (as small as the removal of a single point) can induce arbitrary changes in the operator’s eigendecomposition – leading to a limited adoption of spectral models in real-world applications. This project challenges this view, contending that such presumption of instability is primarily due to a suboptimal choice of the analytical tools that are currently being employed, and which only provide part of the picture. In fact, strong evidence largely contradicts the expected behavior on real geometric data. The reason behind this apparent inconsistency lies in the different focus of current methods, which provide crude bounds and are directed toward other kinds of perturbation than those observed in real settings.

The ambitious goal of this project is to develop a novel theoretical and computational framework that will fundamentally change the way spectral techniques are constructed, interpreted, and applied. These tools will enable a range of currently infeasible uses of spectral methods on real data. They will deal with strong incompleteness, corruption and cross-modality, and they will be applied to outstanding problems in geometry processing, computer vision, machine learning, and computational biology.","1434000","2018-09-01","2023-08-31"
"SPECs","Sustainable plasmon-enhanced catalysis","Emilie RINGE","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Industries creating inorganic, organic, and agricultural chemicals use a staggering 4.2% of the worldwide delivered energy, mainly from unsustainable fossil fuels. Meanwhile, the sun provides energy that could be utilized to power photochemical reactions sustainably and cleanly. Recent advances revealing how localized surface plasmon resonances (LSPRs), light-driven electron oscillations in metal nanoparticles, can concentrate light at the molecular scale made the dream of efficient photochemistry one step closer. However, plasmonic materials are almost exclusively constructed from the rare and unsustainable metals Ag and Au. In addition to being incompatible with current industrial practices relying on catalytic surfaces to lower energy barriers and guide reactions, Ag and Au cause prohibitive cost challenges for real-world applications. But there is hope: several of the few metals predicted to sustain LSPRs and become potential alternatives to Ag and Au are amongst the most abundant, i.e. sustainable, elements on Earth (Al, Mg, Na, K).
The way forward, and key objective of my proposal, is thus to design, synthesize, and understand multimetallic nanostructures where a cheap, Earth-abundant plasmonic material traps and concentrates (sun)light directly at a catalytic surface to efficiently and intelligently power and choreograph chemical reactions. To achieve this ambitious goal, I devised a project concurrently advancing important aspects of sustainable plasmon-enhanced catalysis, from the development of two synthetic approaches for Earth-abundant plasmonic-catalysts, to the fundamental studies of light-trapping in these new materials with state-of-the-art numerical and experimental approaches and the unravelling of the relative contribution of plasmon-generated hot electrons, enhanced field, and heat using key model chemical reactions. These results will help develop a more sustainable future by lowering our reliance on both fossil fuels and rare metals.","1596481","2019-01-01","2023-12-31"
"SPECTRUM","Spectral theory of random operators","Alexander Sodin","QUEEN MARY UNIVERSITY OF LONDON","The theme of this proposal is the study of random operators associated with some geometric structure, and the influence of the geometry on the spectral properties of the operator. Such operators appear in problems from theoretical physics, and lead to new and interesting mathematical structures.
One circle of questions is related to random operators, which describe the motion of a quantum particle in a disordered medium, such as random band matrices. The behaviour of the particle is influenced by the underlying geometry, as quantified by the (non-rigorous) Thouless criterion for localisation in terms of the mixing time of the classical random walk; in the context of random band matrices, the predictions of the Thouless criterion are supported by additional (non-rigorous) arguments. These predictions have so far not been rigorously justified; an exception is my own result, validating it at the spectral edges. One of our goals is to develop new methods, which would be applicable in the bulk of the spectrum, for random band matrices and other operators with geometric structure.
Another circle of questions is given by random processes taking values in large random matrices. The spectral properties of the random matrix at every point of the underlying space are described by the random matrix theory; but how does the spectrum evolve along the underlying space? The richness of this question is apparent from the one-dimensional case of Dyson Brownian motion. We intend to study the local eigenvalue statistics of general matrix-valued random processes with multi-dimensional underlying space; to give a complete description of the random processes which appear in the limit, first for the spectral edges and then for the bulk of the spectrum, and to explore the appearance of these processes in a variety of basic questions of mathematical physics.","991750","2015-06-01","2020-05-31"
"SPECULOOS","SPECULOOS: searching for habitable planets amenable for biosignatures detection around the nearest ultra-cool stars","Michaël Gillon","UNIVERSITE DE LIEGE","""One of the most significant goals of modern astronomy  is establishing whether life exists around other stars. The most direct path towards its achievement is the detection and spectroscopic characterization of terrestrial planets orbiting in the habitable zone of their host stars. The ~1000 nearest ultra-cool stars (UCS, spectral type M6 and latter) represent a unique opportunity to reach this goal within the next decade. Due to their small luminosity, their habitable zone is 30-100 times closer than for the Sun, the corresponding orbital periods ranging from one to a few days. Thanks to this proximity, the transits of habitable planets are much more probable and frequent than for Earth-Sun analogs, while the small size of UCS (~1 Jupiter radius) leads to transits deep enough for a ground-based detection, even for sub-Earth sized planets. Furthermore, habitable planets transiting nearby UCS would be amenable for a thorough atmospheric characterization with near-to-come world-class facilities, including the detection of possible biosignatures. Detecting such planets is the goal of SPECULOOS. Its instrumental concept is optimized for the detection of planets of Earth-size and below transiting the nearest Southern UCS. It consists in several robotic 1m-class telescopes equipped with new generation CCD cameras optimized for the near-IR and operating from one of the best astronomical sites of the Southern hemisphere. SPECULOOS will perform the first exploration of the Terra Incognita of planets around UCS, and detect the first terrestrial planets amenable for atmospheric characterization.""","1963990","2014-01-01","2018-12-31"
"SPEEDER","Supercapacitive Polymer Electrodes for Directing Epithelial Repair","Lisa Maria Margareta ASPLUND","ALBERT-LUDWIGS-UNIVERSITAET FREIBURG","In this project we develop a new approach, using the conducting polymer poly (3,4-ethylene dioxythiophene) (PEDOT) to apply electrical fields (EFs) for guidance of cells. EFs are recognised as important guidance cues in the development and life cycle of human tissues. However, better tools are urgently needed to support experiments and applications. By developing supercapacitive PEDOT electrodes, able to support an ionic flow over extended time frames, we here target the most widely studied clinical application for EF stimulation, accelerated wound healing. Our technology facilitates the transfer from petri dish to device by offering an alternative driving process to metals. In addition, we establish a strategy where electrodes can be recharged in situ, by using intermediate periods of current flow in the reverse direction and below the threshold for triggering a biological response. Ionic flow driven by PEDOT electrodes can, in contrast to metals, be reversed with any small ion present in the electrolyte.
The project will be driven in several steps: after proving the principle in scratch assays in vitro, we will proceed to three dimensional culture systems. The versatility of our concept will allow more complex wound healing models to be studied including human ex-vivo models. We will employ microfluidics to make high-throughput screening possible, thereby efficiently mapping EF parameters and especially the effects of sub-threshold stimulation. The ultimate goal at the end of the project is to transfer technology in the form of a polymer based wound-dressing for accelerating repair, the SPEEDER. 
In summary, we present a new concept which greatly facilitates EF stimulation in vitro and shows great promise for clinical use. Studies will better reproduce the biological situation, provide data essential for understanding this important effect, and point the way for how it can best be exploited for future applications.","1488750","2018-02-01","2023-01-31"
"SPGSV","Some Problems in Geometry of Shimura Varieties","Andrei Yafaev","UNIVERSITY COLLEGE LONDON","""The Andre-Oort conjecture is an important problem in the theory of Shimura varieties. It also has significant applications in other areas of Number Theory, such as transcendence theory. The conjecture was proved assuming the Generalised Riemann Hypothesis by Klingler, Ullmo and Yafaev. Very recently, Jonathan Pila came up with a very promising strategy for proving the Andre-Oort conjecture unconditionally. The first main aim of this proposal is to combine Pila's ideas with the ideas of Klingler-Ullmo-Yafaev in order to obtain a proof of the Andre-Oort conjecture without the assumption of the GRH. We then propose to use these methods to attack the Zilber-Pink conjecture, a very vast generalisation of Andre-Oort. We also propose to consider several problems closely related to geometry of Shimura Varieties and the Andr\'e-Oort conjecture. Namely Coleman's cponjecture on finiteness of the number of Jacobians with complex multiplication for curves of large genus, the Mumford-Tate conjecture on Galois representations attached to abelian varieties over number field and Lang's conjecture on rational points on hyperbolic varieties in the context of Shimura varieties.""","697037","2012-10-01","2017-09-30"
"SPIA","Magnetic connectivity through the Solar Partially Ionized Atmosphere","Olena Khomenko","INSTITUTO DE ASTROFISICA DE CANARIAS","The broad scientific objective of the SPIA proposal is to understand the magnetism of the Sun and stars and to establish connections between the magnetic activity in sub-surface layers and its manifestation in the outer atmosphere. The complex interactions in magnetized stellar plasmas are best studied via numerical simulations, a new powerful method of research that appeared in astrophysics with the development of large supercomputer facilities. With a coming era of large aperture solar telescopes, ATST and EST, spectropolarimetric observations of the Sun will become available at extraordinary high spatial and temporal resolutions. New modelling tools are required to understand the plasma behaviour at these scales. I propose to consolidate a research group of bright scientists around the PI to explore a novel promising approach for the description solar atmospheric plasma under multi-fluid approximation. The degree of plasma ionization in the photosphere and chromosphere of the Sun is extremely low and significant deviations from the classical magneto-hydrodynamic description are expected. A major development of the SPIA proposal will be the implementation of a multi-fluid plasma description, appropriate for a partially ionized medium, relaxing approximations of classical magneto-hydrodynamics. With the inclusion of standard radiative transfer into the three-dimensional multi-fluid code to be developed by the project team, it will be possible to perform simulations of solar sub-photospheric and photospheric regions, up to the low chromosphere, with a realism not achieved before. The importance of the non-ideal plasma effect for the energy balance of the solar chromosphere will be evaluated, and three-dimensional time-dependent models of multi-fluid magneto-convection will be created. This effort will produce a significant step toward the solution of the long-standing question of the origin of solar chromosphere, one of the most poorly understood regions of the Sun.","969600","2012-01-01","2016-12-31"
"Spiders","Fundamental Physics Using Black Widow, Redback and Transitional Pulsar Binaries","Rene BRETON","THE UNIVERSITY OF MANCHESTER","I will use an innovative coupled approach to facilitate a paradigm shift in the use of neutron stars as a probe of high energy physics. The compactness, rapid rotation and large magnetic fields of neutron stars make them, along with black holes, the most extreme objects found in our Universe. Neutron stars are therefore formidable laboratories to study fundamental physics, as they allow us to probe regimes simply unavailable to Earth-based experiments. However, in spite of their potential utility, there are still a number of important outstanding questions in this field. Key amongst these are: What is the neutron star equation of state?  How do neutron stars become the fastest rotating stellar objects in the Universe? How do millisecond pulsars affect their environment? Answering these questions has a broad multi-disciplinary impact, far beyond the field of neutron star research alone; however, at present our ability to answer these questions is limited by cherry picking individual objects for study. In this proposal I describe how I will move away from the biases inherent in the idiosyncratic nature of particular sources to a comprehensive population analysis, and specifically, how I will use a particular class of neutron star systems, known as “spider binaries”, to discover the most massive and the fastest spinning neutron stars that exist. Such extreme cases are of crucial importance for constraining the neutron star equation of state and thus determining the boundary for stellar mass black hole formation. This knowledge in turn can tell us about possible phase transitions of matter at high densities and provide observable quantities that can then be tested against Earth-based experiments, feeding back into high energy physics theories. I will achieve these goals through a transformational observational approach to finding new spider systems and drawing on innovative techniques from other fields to improve parameter estimation.","1995656","2017-04-01","2022-03-31"
"SPIKEHEAR","Spiking neural models of auditory perception","Romain Brette","ECOLE NORMALE SUPERIEURE","In classical connectionism, the information is conveyed by the firing rate of neurons. Spiking neuron models offer an additional dimension to the rate: synchrony. Synchronous spike trains are more effective than uncorrelated ones in driving the responses of target neurons. Because neurons can encode their inputs in a sequence of precisely timed spikes, input similarity translates into synchronous spiking, which can be easily detected by afferent neurons. The dual properties of synchronization and coincidence detection lead to a new computing paradigm, where neurons perform a similarity operation instead of a summation. Because synaptic plasticity favor correlated neuron groups, synchrony-based computation should play an important role in developed neural circuits. The presence of neural correlations has been demonstrated in early sensory systems, but their computational role is still unclear. In auditory perception, the fine temporal structure of sounds is thought to play an important role, in particular for pitch perception and spatial localization of sounds. It has long been proposed that the auditory system exploits the structure of neural correlations to infer information about those properties, but it is still unclear how this computation is physiologically implemented. In this project, I propose to investigate synchrony-based computation and learning in the auditory system, using computational neural modeling. The expected impact of the project is 1) the development of spike-based neural network theory, 2) a better understanding of the role of neural synchronization in auditory perception, 3) industrial applications (music transcription, auditory scene analysis) and medical applications (stimulation procedures for cochlear implants) with neural simulation technology.","1244640","2009-10-01","2014-09-30"
"SPINAM","Electrospinning: a method to elaborate membrane-electrode assemblies for fuel cells","Sara Cavaliere","UNIVERSITE DE MONTPELLIER","""This project leads to the development of novel MEAs comprising components elaborated by the electrospinning technique. Proton exchange membranes will be elaborated from electrospun ionomer fibres and characterised. In the first stages of the work, we will use commercial perfluorosulfonic acid polymers, but later we will extend the study to specific partially fluorinated ionomers developed within th project, as well as to sulfonated polyaromatic ionomers. Fuel cell electrodes will be prepared using conducting fibres prepared by electrospinning as supports. Initially we will focus on carbon nanofibres, and then on modified carbon support materials (heteroatom functionalisation, oriented carbons) and finally on metal oxides and carbides. The resultant nanofibres will serve as support for the deposition of metal catalyst particles (Pt, Pt/Co, Pt/Ru). Conventional impregnation routes and also a novel “one pot” method will be used.
Detailed (structural, morphological, electrical, electrochemical) characterisation of the electrodes will be carried out in collaboration between partners. The membranes and electrodes developed will be assembled into MEAs using CCM (catalyst coated membrane) and GDE (gas diffusion electrode) approaches and also an original """"membrane coated GDE"""" method based on electrospinning. Finally the obtained MEAs will be characterised in situ in an operating fuel cell fed with hydrogen or methanol and the results compared with those of conventional MEAs.""","1352774","2013-01-01","2018-06-30"
"SpinBound","Exploring the Spin Physics at the Boundaries of Materials with Strong Spin-Orbit Interaction","Sergio Osvaldo Valenzuela","FUNDACIO INSTITUT CATALA DE NANOCIENCIA I NANOTECNOLOGIA","I propose to investigate a new research frontier on spin physics at the boundaries (surfaces) of materials with strong spin-orbit interaction (SOI). Although the properties of these materials have been studied for more than half a century, researchers are just starting to grasp the richness of SOI phenomena that occur at them. SOI leads to surface and boundary states with unusually large spin splitting in simple heavy elements. It can also produce a nontrivial topology in band insulators that brings about metallic surface states with exotic spin textures that are protected by time reversal symmetry.
Despite their fundamental and technological interest, information on the nature of charge transport at such states is scarce, while spin transport remains unexplored. I plan to use our cutting-edge expertise on all-electrical lateral spin injection and detection methods to unravel the spin dynamics in them, providing a wealth of information that could not be otherwise obtained. A comprehensive set of objectives will include material integration with ferromagnets and insulators, and innovative devices and measurement protocols. My team will gather information on injection, accumulation, scattering and relaxation processes of spins in the surface and boundary states. Complementary experiments will focus on current-induced spin-torque on a contacting ferromagnet and will address the question of whether spin-split metallic surfaces can enable spin information processing when deposited onto semiconductors.
This is a highly challenging project, with a strong interdisciplinary nature across physical and material sciences that will open new avenues for the realization of exotic states of matter such as axions and Majorana fermions. While the research program is driven by the quest for fundamental understanding, new directions for applications could emerge from our basic science results.","1628744","2013-02-01","2018-01-31"
"SPINCAD","Spin correlations by atomic design","Alexander F. Otte","TECHNISCHE UNIVERSITEIT DELFT","Unexpected physical phenomena occurring in materials often result from the complex interplay between spins on the atomic scale. The field of quantum magnetism aims to capture the rich emergent physics that arises when multiple spins interact. While experimental techniques are available to probe the global quantum magnetic properties of materials, they fail to reveal the role of local features such as individual defects or phase boundaries.
 
I propose to use custom-designed spin lattices created by low-temperature scanning tunnelling microscopy (STM) to probe spin correlations locally at the atomic scale, providing the ultimate tool to trace particle-like excitations (spinons) as they propagate. Since these excitations exist on timescales that are much too short to be accessed directly in real time, I will introduce ‘spinon traps’: atom-built detector bits that record the presence of a spinon and store this information immediately on the atomic scale, to be read out at a later time. This approach opens the possibility to study emergent material properties as a function of system size and test theories for quantum magnetism that were previously experimentally inaccessible.

Based on preliminary experiments in my group that indicate the feasibility of SPINCAD, I will realize and demonstrate the spinon trapping technique by employing it to the study of attractive interactions between spinons, which may be forged independently through atom manipulation. In addition, I will observe the motion of spinons through two-dimensional frustrated spin lattices. Being the first technique to perform local injection and readout of excitations in custom-designed spin structures, SPINCAD will generate a long-lasting synergy between the fields of theoretical quantum magnetism and experimental surface science.","1449983","2016-08-01","2021-07-31"
"SpinMelt","Visualizing melting magnetic order and spin fluctuations in the cuprates","Milan Peter ALLAN","UNIVERSITEIT LEIDEN","The aim of this proposal is to bring new insight and understanding into the remarkable phase diagram of the cuprates, representing the larger family of unconventional superconductors, by visualizing the process of melting magnetic order and the resulting spin fluctuations (magnons) on the atomic scale.

The cuprate phase diagram not only represents many of the long-standing, core problems in physics, but also is remarkably similar to all other families of unconventional superconductors. It is my opinion that a main driving force behind the exotic phases are antiferromagnetic spin fluctuations that might stem from the melting process, but little is known about them and their relation to the exotic phases.

With this proposal, I will unveil these key processes on the atomic scale for the first time as existing techniques only probe sample-averaged parameters. Using my innovative, nanofabricated ‘smart tips’ concept, I will develop radically novel spin-sensitive, GHz-compatible scanning tunneling microscopy (STM) techniques to visualize the melting (and fluctuating) order. I will then develop electron spin resonance - STM, to measure and image the spin fluctuations, as well as their relation to pseudogap, charge order, and superconductivity.

I will capitalize on my track record in the field of quantum materials (such as having discovered nematic order in iron-based superconductors) and my unique background in circuit quantum electrodynamics and STM to build unprecedented instrumentation that will enable a new, holistic look at the mystery of unconventional superconductors.","1847500","2017-10-01","2022-09-30"
"SPINTROS","Spin Transport in Organic Semiconductors","Luis Hueso Arroyo","Asociacion - Centro de Investigacion Cooperativa en Nanociencias - CIC NANOGUNE","Spintronics is an area of electronics that aims to exploit the spin of the electron. Although it is one of the areas selected to play a role in the post-CMOS electronics, spintronics still has to prove its full potential in many fields. A particularly important is the long distance spin transport and manipulation. Organic semiconductors (OSC) can play an important role in the development of spintronics as they have very small spin-orbit and hyperfine interactions, which lead to very long spin coherence times and make them ideal for spin transport. However, the basic mechanisms of spin injection, transport and manipulation in OSC are still obscure, thus impeding further advances in the field.
The objective of this project is to understand and control spin transport in organic semiconductors.
To achieve this ambitious objective we will employ a multidisciplinary approach, merging materials science, electronics and physics. In the two initial workpackages, we will study the unique combination of ferromagnetic spin-polarized injectors and OSC spin transporters, especially their energetic and magnetic interactions at the interface. We will also create optimized organic field-effect transistors (OFET) with nanometre channel lengths, the only device that would allow us to understand spin transport in a controllable fashion. In the third workpackage we will create and investigate the Spin OFET. Thanks to this device we will quantify the spin coherence length of OSC and we shall be able to control spin transport either by external (magnetic or electric field) or internal (crystallographic) effects. Finally, we will produce and characterize spin single molecular FETs. With this radical downscaling we will explore effects inaccessible in other transport regimes. For example, we will look at the direct coupling between the spin and molecular vibrational modes, or to the effect of the spin on the Kondo effect.","1283400","2011-04-01","2016-03-31"
"SPQRel","Entanglement distribution via Semiconductor-Piezoelectric Quantum-Dot Relays","Rinaldo Trotta","UNIVERSITA DEGLI STUDI DI ROMA LA SAPIENZA","The development of scalable quantum devices that generate and distribute quantum entanglement over distant parties will bring about a revolution in communication science and technology. Epitaxial quantum dots (QDs) embedded in conventional diodes are arguably the most attractive quantum devices, since they combine the capability of QDs to deliver triggered and high-quality entangled photons with the tools of the mature semiconductor technology. However, it is at present impossible to use remote QDs for the distribution of entangled photons over large distances, mainly due to the lack of control over their electronic structure.
Recently, the PI has grasped that the solution to this problem resides in hybrid technologies. He has conceived and developed a novel class of semiconductor-piezoelectric quantum devices where different external fields are combined to reshape the electronic structure of any arbitrary QD so that single and polarization-entangled photons can be generated with unprecedented quality, efficiency, and speed, a major breakthrough for solid-state-based quantum communication.
In this project the PI will make the next pioneering step and develop the hybrid technology to the limit where advanced quantum communication protocols previously inaccessible to QDs can now be performed. The objective of the proposal is mainly to i) develop the first electrically-controlled wavelength-tunable source of indistinguishable and entangled photons, which can be exploited to ii) teleport entanglement over two distant QD-based qubits (the quantum relay) and to iii) attempt the construction of a quantum network where entangled photons from remote quantum relays are interconnected using warm atomic vapours.  
The new hybrid technology that will be developed in this project to achieve i) will open new grounds in research fields well beyond quantum optics and quantum communication, and in particular the whole research area of strain-engineering of semiconductor thin-films.","1499963","2016-03-01","2021-02-28"
"Spray-Imaging","Detailed Characterization of Spray Systems using Novel Laser Imaging Techniques","Edouard Jean Jacques Berrocal","LUNDS UNIVERSITET","The multiple scattering of light is a complex phenomenon, commonly encountered but rarely desired. In imaging it induces strong blurring on the recorded photographs, limiting the range of applicability and accuracy of modern optical instruments. A typical example concerns the laser diagnostics of spray systems. The PI has revealed in 2008 a technique based on structured illumination with the important capability to remove the contributions from multiple light scattering, allowing the unique possibility of visualising through dense sprays. Based on this acquired knowledge, the aim of this proposal is to develop and apply three novel imaging techniques for the complete characterizations of spray systems:

The first technique will focus on visualizing with both high contrast and high resolution various spray phenomena that have not been observed in the past; such as complex spray breakup mechanisms in the near-nozzle region. 

The second technique is related to the characterization of the formed droplets field. This concerns the accurate measurement of both droplets size and concentration using a three-dimensional imaging approach.

Finally, a third important task is the mapping of the spray temperature over the whole spray system. This information would lead to the determination of heat transfer and evaporation rate, which are key factors in the performance of combustion devices.

By extracting these important quantities - dynamics, droplets size/concentration and thermometry - fundamental insights which are still missing to fully understand the process of atomization will be provided. This will also serve at validating modern CFD models, leading to reliable predictions of spray behaviours. Even though this work can directly benefit to a large number of medical and industrial spray applications, it will mostly focus on fuel spray injections used in combustion devices.","1500000","2015-03-01","2020-02-29"
"SPRS","Stochastic Processes on Random Surfaces","Jason MILLER","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","In the last several decades, two canonical theories of random surfaces have emerged.  The first, so-called Liouville quantum gravity (LQG), has its roots in conformal field theory and string theory from the 1980s and 1990s.  The second, so-called random planar maps (RPM), has its roots in combinatorics from the 1960s.  There has been immense progress in recent years in making rigorous sense of LQG, in the study of the large-scale behavior of RPM, and developing connections between them as well as with other mathematical objects such as the Schramm-Loewner evolution (SLE).

The purpose of this project is to study stochastic processes on LQG and RPM.

The first part of the proposed research is focused on developing a theory of growth processes on LQG, the so-called quantum Loewner evolution (QLE).  QLE, introduced in joint work with Sheffield, is a family of processes which conjecturally describe the scaling limits of discrete growth processes such as diffusion limited aggregation (DLA), the Eden model, and the dielectric breakdown model (DBM) on LQG.  QLE has proved to be a powerful tool in the study of LQG and RPM and was used in joint work with Sheffield to unite LQG with gamma=sqrt(8/3) with the Brownian map, the metric space scaling limit of random planar maps.  Nevertheless, the development of QLE is still in its infancy and many important problems remain to be solved.

It has long been conjectured that large RPM equipped with a statistical physics model, such as percolation or a uniform spanning tree, embedded into the plane in a conformal manner should be described by a form of LQG decorated by an SLE.  The embedding problem is intimately connected to understanding random walk on a RPM, which has proved to be challenging.  The second part of the proposal is aimed at developing new methods to settle long-standing questions about random walk on RPM, and ultimately the embedding problem for RPM.","1489406","2019-01-01","2023-12-31"
"SRMS4HESUS","Super-resolution mass spectrometry for health and sustainability","Yury Tsybin","SPECTROSWISS SARL","Health and sustainability are the grand-challenges the world faces in the XXI century. The recent progress in science and technology allows now considering these prioritized directions of humanity development at the molecular level. Mass spectrometry is a key analytical technique in receiving the required molecular information from minute amounts of samples present in the gas, solid or liquid phase. The current state-of-the-art in MS applications confirms that investing into basic chemical and physical research is as essential as ever to come prepared for the growing applications challenges and overall for the Europe’s long-term competitiveness and well-being.

Within the current proposal, we suggest an interdisciplinary research program to address the challenges in health and sustainability through the scientifical and technological developments in the analytical science. The current project specifically aims at developing the super-resolution mass spectrometry to achieve high resolving power in a short acquisition time period or ultra high resolving power in a long acquisition time period. The former one is crucial for a number of applications in life science and environmental science-oriented proteomics, petroleomics, metabolomics, and imaging. Whereas the latter one would contribute to the long term vision originating from the project’s objectives, specifically molecular isomer differentiation or weighing the chemical bonds in a molecule directly by mass spectrometry.

The fundamental insights and knowledge gained during the proposed research can have a deep impact in physical, (bio)chemical and life sciences. The envisioned scientific achievements in the health and sustainability applications may translate into the improved drug discovery, early diagnosis and prognosis for preventive and personalized medicine. The planet and humanity should further benefit from an increase in the sustainability of the energy production and processing.","1425728","2011-11-01","2016-10-31"
"SSX","""State Space Exploration: Principles, Algorithms and Applications""","Malte Helmert","UNIVERSITAT BASEL","""State-space search, finding paths in huge, implicitly given graphs, is a fundamental problem in artificial intelligence and other areas of computer science. State-space search algorithms like A*, IDA* and greedy best-first search are major success stories in artificial intelligence, and hundreds of papers based on variations of these algorithms are published every year. Due to this success, the major assumptions of these algorithms are rarely questioned.

We argue that the current generation of state-space search algorithms has three significant deficiencies that impede further progress in the field:

1. They explore a monolithic model of the world rather than applying a factored perspective.
2. They do not learn from mistakes and hence tend do commit the same mistake thousands of times.
3. In the case of satisficing (i.e., suboptimal) search, the design of the major algorithms has been based on ad-hoc intuitions rather than sound theoretical principles.

This proposal targets these three issues. We propose to develop a rigorous theory of factored state-space search, a rigorous theory of learning from information gathered during search, and a
decision-theoretic foundation for satisficing search algorithms. Based on these insights we will design and implement new state-space search algorithms addressing the deficiencies of current methods. Finally, we will apply the new algorithms to application domains of state-space search to raise the state of the art in these areas.""","1499737","2014-02-01","2019-01-31"
"STABAGDG","Stability and wall-crossing in algebraic and differential geometry","Jacopo Stoppa","SCUOLA INTERNAZIONALE SUPERIORE DI STUDI AVANZATI DI TRIESTE","""This proposal centers on the special interaction of algebraic and differential geometry which arises from the notion of stability, going back to the celebrated correspondence between polystable vector bundles and Hermite-Einstein connections.

A conjecture of Yau, Tian and Donaldson seeks to extend this correspondence to projective manifolds, formulating an algebro-geometric notion of stability (K-polystability) which should be equivalent to the existence of a (unique) Kaehler metric of constant scalar curvature in the first Chern class of an ample line bundle. The necessity of stability is now settled, thanks to the work of Donaldson, myself (Adv. in Math. 2009) and Mabuchi. The existence implication however seems to be out of reach with current techniques. In this project I will motivate the need to go beyond the notion of K-stability, and select some crucial open problems which arise naturally in this context, especially in connection with Donaldson's program for Fano manifolds and his conjecture that a Fano manifold with a Kaehler-Einstein metric is ``birationally stable"""". Another surprising application of algebro-geometric stability to differential geometry has recently emerged in the physical work of Gaiotto, Moore and Neitzke. They showed (conjecturally) how to use the stability and wall-crossing of ``BPS states'' to reconstruct the Hitchin hyperkaehler metric on a class of moduli spaces of Higgs bundles. In this project I propose to study some exciting mathematical questions which arise from this theory.

This project aims at attacking some central problems which stem from the connection between stability and special metrics, and will be carried out by myself as P.I. and Gabor Szekelyhidi as a Team Member, over a period of four years. We will be supported by two Post-Docs (each position lasting two years) and a graduate student (three years).""","511936","2012-10-01","2017-09-30"
"StableChaoticPlanetM","Stable and Chaotic Motions in the Planetary Problem","Gabriella Pinzari","UNIVERSITA DEGLI STUDI DI PADOVA","The planetary problem consists in determining the motions of n  planets, interacting among themselves and with a sun, via gravity only. Its deep comprehension has relevant consequences in Mathematics, Physics, Astronomy and Astrophysics. 
The problem is by its nature perturbative, being well approximated by the much easier (and in fact exactly solved since the XVII century) problem where each planet interacts only with the sun. However, when the mutual interactions among planets are taken into account, the dynamics of the system is much richer and, up to nowadays, essentially unsolved. Stable and unstable motions coexist as well. 
In general, perturbation theory allows to describe qualitative aspects of the motion, but it does not apply directly to the problem, because of its deep degeneracies. 
During my PhD, I obtained important results on the stability of the problem, based on a new symplectic description, that allowed me to write, for the first time, in the framework of close to be integrable systems, the Hamilton equations governing the dynamics of the problem, made free of its integral of motions, and degeneracies related. By such results, I was an invited speaker to the ICM of 2014, in Seoul.
The goal of this research is to use such recent tools, develop techniques, ideas and wide collaborations, also by means of the creation of post-doc positions, assistant professorships (non-tenure track), workshops and advanced schools, in order to find results concerning the long-time stability of the problem, as well as unstable or diffusive motions.","900000","2016-03-01","2021-02-28"
"StabMAEinstein","Stability and Instability in the Mathematical Analysis of the Einstein equations","Gustav Horst Holzegel","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","The present proposal is concerned with the global analysis of solutions to the Einstein equations of general relativity. This subject lies at the intersection of the analysis of partial differential equations, differential geometry and theoretical physics and is a field of intense current activity, with several important advances having been achieved in the last decade only.

The main objective of the proposal is to establish a research group based at Imperial College to develop novel mathematical techniques that would allow one to move considerably beyond the current limits of the field. These techniques will be devised and mature in the context of two fundamental problems, which we intend to solve.
1) Instability of AdS: The stability of Minkowski space and the stability of de Sitter space are celebrated theorems in mathematical general relativity. In contrast, the dynamics near Anti de Sitter (AdS) space, the maximally symmetric solution with negative cosmological constant, is mathematically entirely unexplored. Heuristic andnumerical arguments suggest instability of this spacetime. Instability problems are typically much more intricate than stability problems and require very different techniques. A rigorous proof of instability would resolve a major conjecture in general relativity and have important implications for theoretical physics.
2) The Black Hole Stability Problem: A central problem of general relativity is to prove the full non-linear stability of the 2-parameter Kerr family of black holes. Very recently, the dynamics of linear waves on such stationary black holes has been satisfactorily understood. The proposal suggests to suitably enhance the techniques developed for linear scalar waves to be applicable in the non-linear, tensorial setting of of the Einstein equations. Key will be to establish important estimates on the curvature in a class of (non-stationary) spacetimes which are assumed to converge to a fixed member of the Kerr family.","1264652","2013-11-01","2018-10-31"
"STAQAMOF","Statistical modelling across price and time scales: a quantitative approach to modern financial regulation","Mathieu Felix Rosenbaum","ECOLE POLYTECHNIQUE","This project aims at providing a new quantitative approach to financial regulation, notably in the context of high frequency trading. The key idea of our method is to build relevant statistical models through price and time scales, connecting the microstructure of financial markets to the long term behavior of prices.  Doing so, we will be able to understand and quantify the macroscopic consequences of regulatory measures modifying the microscopic design of the market. Succeeding in this modelling task will require to address several intricate statistical problems.  In particular new results will be needed in the fields of limit theory for semi-martingales, multifractal processes, rough stochastic differential equations, Hawkes processes and high-dimensional statistics. Hence, through this project, we not only have the hope to provide groundbreaking tools for worldwide regulation of financial markets but, concurrently, to answer important and challenging mathematical problems. In term of analyzing concrete regulatory measures, particular attention will be devoted to the issue of the choice of a proper tick value, that is the minimal price increment allowed on a financial market. Indeed, the tick value is the tool which seems to be favored by most policy makers in order to regulate high frequency trading.","1165625","2016-10-01","2021-09-30"
"STARLIGHT","Steering attosecond electron dynamics in biomolecules with UV-XUV LIGHT pulses","Francesca Calegari","STIFTUNG DEUTSCHES ELEKTRONEN-SYNCHROTRON DESY","One of the challenges facing science is to understand the chemical origin of DNA damage-induced mutations. Upon exposure to UV light, DNA nucleobases become electronically excited. This process potentially favors mutagenic miscoding of the DNA sequence.
The main target of STARLIGHT is to study with unprecedented temporal resolution (few-femtoseconds/attoseconds) the electron dynamics occurring in UV photoexcited biomolecules. I will mainly consider aromatic complexes including DNA nucleobases (and ultimately DNA) with the aim of tracking in real time the electron dynamics preceding structural changes potentially leading to damage. The proposed research is based on a bottom-up approach: it allows one to understand the physical origin of a variety of light-driven processes occurring in more complex biological systems of crucial importance in photochemistry and photobiology, with tremendous prospects in phototherapy.
Electron motion in molecules occurs on a temporal scale ranging from few femtoseconds down to attoseconds. Attosecond science is nowadays a well-established field and electron dynamics has been successfully studied in atoms and small molecules. The work recently conducted by the PI has demonstrated that this technology is mature and ready to be applied to more complex systems such as biomolecules.
Electron dynamics will be resonantly activated in biomolecules by few-cycle UV pump pulses and subsequently probed by as-XUV pulses or few-fs-UV pulses. Through time-resolved measurements of the molecular photo-fragmentation, gas-phase spectroscopy will be used to elucidate the role of electrons in the photoreactivity of the molecule in a solvent-free environment. With the final goal of steering the electronic motion, circularly polarized UV pulses will be also used to induce electronic currents in cyclic biomolecules. These ring currents can be exploited to generate intense magnetic fields with promising applications in molecular electronics and quantum control.","1497000","2015-04-01","2020-03-31"
"STARS2","Simulations of Turbulent, Active and Rotating Suns and Stars","Allan Sacha Brun","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","The STARS2 project aims at modelling on massively parallel supercomputers in a self-consistent and three-dimensional way, the complex, time dependent and nonlinear dynamics operating in the Sun and stars. In particular we wish to understand how stars generate the wide variety of magnetic activity that is observed, with the Sun - given its proximity and its influence on our technical society - playing a central role in characterizing, studying, and constraining the dynamical processes acting in stellar convection and radiation zones. Studying the solar-stellar connection is crucial because it will allow us to understand why depending on the spectral type of the star considered, this activity can be cyclic, irregular, or simply modulated. The mechanism thought to be at the origin of the magnetism seen in late type stars is likely to be linked to dynamo action in the upper convective layers of such stars. The simultaneous existence in stars of convective turbulent motions, of rotation and its associated shear layers, favour the emergence of a small and/or large scale magnetic field through induction. For more massive stars, possessing a convective core, understanding the interaction between the dynamo generated magnetic field in the core and the magnetic field of their radiative envelope constitute major challenges in stellar fluid dynamics. To achieve these challenging scientific goals, the STARS2 project propose to federate a team of young bright scientists around the PI and to perform and to analyse sophisticated and more realistic high performance global MHD numerical simulations of the Sun and other stellar spectral types. These simulations are at the front-edge of current research in astrophysics, they require the use of the latest class of supercomputers available in Europe and will lead to real scientific breakthroughs. Understanding the interactions between convection, turbulence, shear, rotation and magnetic fields in stars IS the main goal of this project.","880000","2008-09-01","2013-08-31"
"STATOPINS","Theory of statistical topological insulators","Anton Roustiamovich Akhmerov","TECHNISCHE UNIVERSITEIT DELFT","Topological insulators (TI) are a novel class of materials with insulating bulk and conducting surface. The conduction of the surface is protected by the topological properties of the bulk, as long as a fundamental symmetry is present (for instance time-reversal symmetry). My goal is to investigate to what limits does the protection hold in cases where the protecting symmetry is broken, and only present in statistical sense, after averaging over the disordered ensemble. In a pilot study I showed that materials that are protected by such average symmetry, which I have called “statistical topological insulators” (STI) significantly extend the classification of topological phases of matter and promise new methods to robustly control the conducting surface properties. I plan to develop a general theory of STI for physically relevant symmetries, describe the observable properties of their protected surface states, invent ways to predict whether materials are expected to be STI, and explore the generalization of STIs to strongly interacting topological phases of matter. I expect that the outcome of my research will significantly extend our understanding of topological phases of matter, and provide new ways to design materials with robust properties.","1355103","2015-03-01","2020-02-29"
"STATOR","STATic analysis with ORiginal methods","David Pascal Monniaux","UNIVERSITE GRENOBLE ALPES","Since the beginning of computing, software has had bugs. If a word processor crashes, consequences are limited. If a networked application has security bugs (e.g. buffer overflows), important information (e.g. financial or medical) can leak. More importantly, today's planes are flown by computers, voting machines as well medical devices such as infusion pumps are computerized, and surgeries are performed by robots. Clearly, it is in the best interest of society that such software is bug-free.

BUGS ARE NOT A FATALITY!

Traditionally, software is tested, i.e. run on a limited number of test cases. Yet, testing cannot prove the absence of bugs in untested configurations. Formal methods, producing mathematical proofs of correctness, have long been proposed as a means to give strong assurance on software. They unfortunately had a (not entirely undeserved) reputation for not scaling up to real software.
Faster, automated static analysis methods were however produced in the 2000s, which could cope with some specific classes of applications: predicate abstraction, based on decision procedures (e.g. Microsoft's device driver verifier) and abstract interpretation (e.g. Polyspace and Astrée, for automotive, aerospace etc.). Yet such systems are still unusable on more common programs: they reject some program constructs, they give too many false alarms (about nonexistent problems) and/or they take too much time and memory.
In the recent years, I and others proposed techniques combining decision procedures and classical abstract interpretation, so as to decrease false alarms while keeping costs reasonable. These techniques are still in their infancy. The purpose how STATOR is to develop new combination techniques, so as to break the precision/efficiency barrier.
Since the only way to see if a technique really works is to implement and try it, STATOR will produce a practical static analysis tool and experiment it on real programs.","1472495","2013-01-01","2018-12-31"
"STEIN","TOPOLOGY OF STEIN MANIFOLDS","Alexandru Oancea","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The goal of this project is to study the topology of Stein manifolds from the viewpoint of symplectic and contact geometry. It addresses the fundamental questions of the subject: - How does the Lagrangian skeleton of a Stein manifold determine the Stein structure? - To what extent the study of Stein structures can be reduced to a combinatorial study of the skeleton? - How are the symplectic invariants of Stein manifolds, respectively the contact invariants of their boundary, determined by the skeleton? For the topological part, we will use as a source of inspiration the theory of spines and shadows of 3- and 4- manifolds. One of the goals of this research project is to adapt it to the setup of Stein manifolds and develop a calculus of Lagrangian shadows. Concerning invariants of contact manifolds, we aim to interpret symplectic homology of Stein manifolds and contact homology of their boundaries in topological terms, with the skeleton playing a central role. Further ramifications of this research project include the development of string topology on singular (stratified) spaces and the symplectic study of singularities.","1053101","2010-09-01","2016-08-31"
"StellarAges","Accurate ages of stars","Saskia Hekker","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Age is a fundamental property of stars. It is an essential tool to understand many diverse phenomena in astrophysics, including the evolution of stars, planetary systems, and the Galaxy. However, age is currently the most poorly known property of a star, often to no better than 30-40% accuracy, which is not good enough.
The ages of stars cannot be measured directly; they can only be determined by comparing age-sensitive observables with model predictions. Asteroseismology, the study of stellar oscillations, offers the unique opportunity to estimate the ages of stars to within 5-10% of their lifetime. Using state-of-the-art space observations (CoRoT and Kepler) of stellar oscillation frequencies combined with ground-based spectroscopy (e.g. APOGEE), I propose to uniformly determine accurate ages of thousands of stars with unprecedented precision. Building on my extensive experience in this field, I plan to develop and implement new asteroseismic diagnostics for a large number of main-sequence stars, subgiants and red giants. These new age determination methods are expected to be calibrated using stars in binary systems and clusters, and compared with classical methods.
Uniform age determinations for a large sample of stars in different directions in the sky will greatly advance the study of stellar populations in the Galaxy. This project is ambitious, and success requires a dedicated approach from a competent team with the right resources and the right leader.","1483200","2013-10-01","2018-09-30"
"STEM","Structural energy harvesting composite materials","Juan José Vilatela Garcia","FUNDACION IMDEA MATERIALES","The purpose of this project is the development of new multifunctional structural composite materials that combine high-performance mechanical properties and the possibility to harvest energy. The multifunctional composites are based on a continuous macroscopic fibre made up of highly aligned carbon nanotubes that has bulk mechanical, electrical and thermal properties already superior to carbon fibre and the mesoporosity and chemical resistance of an activated carbon; which will be combined with nanostructured semiconductors that can transfer charge/energy when subjected to external stimuli (piezoelectric, photovoltaic) and integrated in a polymer matrix to form composite ply structures. Such composites will be fabricated from bottom to top, resulting in a 3-component hierarchical structure. Load, charge and energy transfer processes at the nanocarbon/inorganic interface, for example, will be carefully controlled through tailoring the structure and optoelectronic properties of the two components during their synthesis, and by exploiting the role of the fibre surface to template the growth of inorganic semiconductors and form an electronic junction. The project comprises a detailed multiscale study of materials synthesis and properties, including in-situ spectroscopy, electron microscopy and synchrotron XRD during mechanical testing, junction characterisation (emission/absorption spectroscopy, impedance) and photocurrent measurements. The uniqueness of the proposal lies in exploiting advanced optoelectronic processes in macroscopic strong composites on a composite ply length-scale, in the quest for a new generation of light-weight multifunctional structural materials.","1448787","2016-06-01","2021-05-31"
"STEMOX","Under the light of electrons","Maria Varela Del Arco","UNIVERSIDAD COMPLUTENSE DE MADRID","Here, we propose to explore and characterize new emerging phenomena in low dimensional (LD) and artificially structured oxide based systems by means of advanced electron microscopy techniques. Complex oxides have a large range of applications, since their properties change drastically as their precise composition and structure changes. When obtained in LD configurations new functionalities arise which are of fundamental interest in electronics, spintronics, energy or nanophotonics. We will use atomic resolution imaging and spectroscopy in the aberration corrected electron microscope to map their electronic, optical and magnetic properties by means of electron chiral dichroism. We intend to combine spectroscopic magnetic imaging with plasmonic measurements in order to explore, for the first time ever, the sensitivity of electron spectroscopy to magneto-optical properties at high spatial resolution. Our scientific mission will be to a) synthetize and characterize high quality oxide based LD systems and develop new imaging techniques, in order to b) explore new phenomena in systems showing unexpected behaviors. Our approach relies on comprehensive studies with atomic resolution, in real space, and when possible, at work (under the relevant temperature or pressure conditions). We will combine experiments with theory in order to interpret results and design new avenues to follow. This proposal has also a dual strategic component: 1) to create a new group in Spain devoted to study materials physics in such a way, and 2) to establish a world-class collaboration connecting the group to established growers and theorists, assembling a multidisciplinary team. The potential payoffs we envision are large, and many new and unusual materials, devices and phenomena are anticipated.","1700000","2009-11-01","2015-10-31"
"STEMREPAIR","Novel mesenchymal stem cell based therapies for articular cartilage repair","Daniel John Kelly","THE PROVOST, FELLOWS, FOUNDATION SCHOLARS & THE OTHER MEMBERS OF BOARD OF THE COLLEGE OF THE HOLY & UNDIVIDED TRINITY OF QUEEN ELIZABETH NEAR DUBLIN","Once damaged articular cartilage has a limited reparative capacity and thus lesions often progress to arthritis. This has motivated the development of cell based therapies for the repair of cartilage defects such as autologous chondrocyte implantation (ACI). Such therapies are limited in two ways. Firstly, they do not result in the regeneration of hyaline cartilage and hence the repair is temporary. Secondly, widespread adaptation into the clinical setting is impeded by practical issues such as the high fiscal cost and time required for culture expansion of chondrocytes.  The applicant is of the belief that both issues cannot currently be addressed by a single new therapy. Therefore the proposed project will put forward separate solutions to both issues. The first theme of the project will determine whether freshly isolated (not culture expanded) infrapatellar fat pad derived cells, embedded in a hydrogel containing microbead-encapsulated growth factors, can used to engineer functional cartilage tissue. A component of this theme will involve magnetic microbead enrichment for cells with surface markers associated with highly chondrogenic cells. Theme 2 of the proposed project will explore an alternative therapy for cartilage defect repair.  Specifically, the objective is to tissue engineer in vitro a functional tissue with a zonal structure mimicking that of normal articular cartilage using mesenchymal stem cells. It is hypothesised that such a zonal structure can be generated by controlling the oxygen tension and mechanical environment within the developing tissue. The final theme of the project will be to determine if repairing high-load bearing cartilage defects using either tissue engineering therapies will result in significantly improved repair compared to ACI in a cartilage defect model.","1499770","2010-09-01","2015-08-31"
"STGDELUCIA2007","Galaxies through the cosmic ages: the role of primordial conditions and environmental effects","Gabriella De Lucia","ISTITUTO NAZIONALE DI ASTROFISICA","The observed properties of galaxies have long been known to depend on the environment in which they are located. The physical origin of the observed environmental trends is, however, still a subject of debate. Much of the argument centres on whether these trends are the end products of physical processes acting over the entire lifetime of a galaxy (the nurture hypothesis), or whether they are established at formation (the nature hypothesis). One element is often overlooked in this debate: according to the current paradigm for structure formation, dark matter collapses into haloes in a bottom-up fashion. Small objects form first and subsequently merge into progressively larger systems. As structure grows, galaxies join more and more massive systems, and so experience a variety of environments over their lifetimes. Thus both initial conditions and subsequent physical processes must play a role in shaping the observed properties of galaxies, and their variation with environment. Here I propose a detailed investigation of the relative roles of various physical processes, and of conditions at formation in determining the observed environmental trends. The work will exploit synergies between different theoretical methods, and will involve detailed comparisons with observational data from spectroscopic and photometric surveys at both low and high redshift. The results should provide the first quantitative estimate of the relative importance of nature and nurture in determining the observed environmental trends within the current standard picture of structure formation. In addition, results from this study will be of important guidance for interpreting data from ongoing and planned surveys, and will provide precious and timely tools for the preparation of key programs and surveys for planned or proposed space missions and large telescope instruments (e.g. ELT, JWST, Herschel, etc.).","750000","2009-02-01","2014-01-31"
"STIMULUS","Space-Time Methods for Multi-Fluid Problems on Unstructured Meshes","Michael Dumbser","UNIVERSITA DEGLI STUDI DI TRENTO","In this project we develop new algorithms for the solution of general nonlinear systems of time dependent partial differential equations (PDE) in the context of non-ideal magnetized multi-fluid plasma flows with thermal radiation. We will produce new high order schemes on unstructured tetrahedral meshes that are applicable to a rather general class of problems in general geometries, thus opening a wide range of possible applications in science and engineering. We will consider both, Eulerian methods on fixed grids and Lagrangian schemes on moving meshes to reduce numerical diffusion at material interfaces. A particular feature of our schemes is that they are high-order one-step methods based on local space-time predictors that allow using time-accurate local time stepping, i.e. each element runs at its own optimal time step. Even nowadays better than second order accurate 3D unstructured Eulerian methods are very rare, but there is still no better than second order accurate unstructured Lagrangian scheme available on general tetrahedral meshes. To develop these missing algorithms is the objective of our research project.
A very challenging application that we have in mind is inertial confinement fusion (ICF) which is highly relevant for modern society and its increasing need for clean and inexhaustible energy. It is believed that early ICF experiments in the 1970ies failed because they did not reach the necessary critical pressure and temperature due to hydrodynamic instabilities in the flow. In this project we propose to design algorithms for simulating ICF flows with billions of high order elements on up to 100,000 CPUs of modern supercomputers. We will also propose active control strategies based on adjoint equations to reduce the hydrodynamical instabilities. Hence this project aims at providing next-generation numerical modeling tools for a possible future scenario of clean civil energy production via ICF.","918000","2011-11-01","2016-10-31"
"STING","STING - A Soft Tissue Intervention and Neurosurgical Probe","Ferdinando Rodriguez Y Baena","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","Current trends in surgical intervention favour a minimally invasive (MI) approach, in which complex procedures are performed through increasingly small incisions. Significant technological advancements have been made in the area of endoluminal surgery, where natural orifices and accessible vessels are used to direct MI instruments to the target (e.g. endoscopes and endovascular catheters). In contrast, progress on the development of percutaneous instruments has been slow and new approaches need to be explored. Steerable needles and probes able to follow complex trajectories through soft tissue would have a significant impact on the effectiveness and safety of conventional MI procedures and especially on the development of new treatments which will follow on from advancements in medical imaging, tissue engineering and genetics.
This project aims to deliver a biologically-inspired system for MI surgery, capable of automatically steering towards and targeting specific soft tissue areas deep within the body. A total system suitable for clinical application will be investigated, but taken only to final prototype stage through laboratory trials. The system will be applicable to a range of soft tissue applications (e.g. brachytherapy, drug delivery, etc.), but key demonstrators will be in the areas of liver and neurosurgery. Every aspect of the system will be modelled, including the complex interaction between the probe and the surrounding soft tissue, with an aim to optimise the design for the two demonstrators and to develop a comprehensive research platform to aid future application-specific research on the bio-inspired design. As an adjunct to this work, which will broaden the range of viable future applications to the area of interventional imaging, MRI-compatibility will be addressed at every stage of the project to ensure that the overall system can operate in proximity of the scanner, while the probe will be suitable for operation from within the bore itself.","1499353","2011-01-01","2016-06-30"
"STOANDMULMODINBIO","Stochastic and Multiscale Modelling in Biology","Radek Erban","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","I will create a research team in applied mathematics which will work on development and analysis of methods for stochastic and multiscale modelling of biological systems. This research project is divided into three core areas: (A) development of stochastic simulation algorithms for reaction-diffusion processes; (B) analysis of (bio)chemical reaction systems using the chemical Fokker-Planck equation and multiscale  computational approaches; (C) understanding the collective behaviour of systems of interacting particles. Two postdoctoral research assistants (each position of 3 years duration) will work on the research questions of parts (A) and (B). Part (C) will be the work of one doctoral student. Important questions of accuracy and efficiency of existing and novel stochastic and multiscale modelling approaches will be addressed. In part (A), we will investigate the conditions under which different stochastic simulation algorithms for reaction-diffusion processes are equivalent and under which they differ. We will develop correct and efficient methods for coupling models with a different level of detail in different parts of the simulated domain. The research outputs will be of use to scientists outside mathematics, for example, to computational biologists and computational chemists. In part (B), we will investigate methods  for extracting useful information from stochastic models of chemical reaction networks. One approach will be based on the analysis and numerical solution of the chemical Fokker-Planck equation, another approach on running and processing short bursts of appropriately initialized stochastic simulation of the chemical system. In both cases, the applicability of numerical methods for solving higher-dimensional partial differential equations will be explored. In part (C), the doctoral student will study approaches for understanding the collective behaviour of systems of interacting particles, with applications to individual-based modelling of cells and animals.","624999","2009-10-01","2015-05-31"
"STOMAMOTOR","Stomatocyte Nanomotors: Programmed Supramolecular Architectures for Autonomous Movement","Daniela Wilson","STICHTING KATHOLIEKE UNIVERSITEIT","The main goal of this ERC proposal is to harness a completely new approach to constructing biocompatible nanomotors, using supramolecular assembly of amphiphilic block-copolymers for loading the engine and catalysis as the driving force for autonomous movement. Polymersomes assembled from amphiphilic block copolymers can be further re-engineered to perform a controlled shape transformation from a thermodynamically stable spherical morphology to a kinetically trapped stomatocyte structure with controlled opening. These stable structures can selectively entrap catalytically active nanoparticles within their nanocavity making their design ideal for nanoreactor applications. The decomposition of hydrogen peroxide by an entrapped catalyst has been shown to generate a rapid discharge of gases and consequently generate thrust and directional movement. The design of the loaded stomatocytes is a truly miniature monopropellant rocket engine in which the catalytically active nanoparticles are the motor, the hydrogen peroxide is the propellant and the controlled opening of the stomatocyte is the nozzle. Their unique shape allows for added capabilities, extra compartmentalization for loading efficiency, polymeric PEG surface for biocompatibility and entrapped particles for catalytic activity. The supramolecular approach to assembling the motor allows facile alteration of its constituent parts: motor, fuel and cargo to make it more suitable for biological applications (type of catalytic particles, surface modification for cellular uptake or suitable biofuels). The appropriate design of the motor with recognition sites on the surface can facilitate the recognition, isolation and transport of specific type of cells, or can navigate the payloads within the cell via chemotaxis. Besides their initial role to overcome random diffusion, these “ship-in-a-bottle” loaded stomatocytes open interesting possibilities for designing new targeted drug delivery and nanoreactor systems.","1500000","2012-09-01","2018-05-31"
"Strained2DMaterials","Unlocking new physics in controllably strained two-dimensional materials","Kirill Igorevich Bolotin","FREIE UNIVERSITAET BERLIN","""We will use strain engineering as an enabling tool to study previously inaccessible or hard-to-study phenomena in two-dimensional atomic crystals (2DACs: graphene, bilayer graphene, and monolayer transition metal dichalcogenides). In our first objective, we develop unique experimental tools to control and characterize mechanical strain in 2DACs. These are the distinguishing features of our approach: (i) The use of very low disorder suspended devices; (ii) Both uniform and controlled non-uniform strain will be induced; (iii) The level of strain will be precisely adjusted and determined in-situ during measurements. We will then use controllably-strained samples to study electrical, mechanical, thermal, and optical properties of 2DACs:

Application of strain in suspended graphene will be shown to control amplitudes and dispersion relation of flexural out-of-plane phonons (FPs), a mode unique to 2D and quasi-2D materials. We will demonstrate, for the first time, that FPs dominate electrical, thermal, and mechanical of suspended graphene. Moreover, we will show dramatic mechanical softening of graphene in the regime of weak strain, similar to """"entropic spring"""" behaviour seen in polymers.

We will engineer strain distributions in high-mobility suspended graphene devices that translate into near-constant """"pseudomagnetic field"""" and observe Quantum Hall-like quantization at zero external magnetic field.

Strain-induced changes in topology of the band structure of bilayer graphene will be shown to affect Quantum Hall states and the Berry phase.

Through strain engineering, we will controllably adjust - and even make spatially dependent - the band gap energy and binding energies of excitons in monolayer transition metal dichalcogenides (TMDCs). We will study complex interplay between and direct and indirect excitons and look for emergence of a new phase of matter, an excitonic insulator, in strained narrow-bandgap TMDC.
""","1997452","2015-11-01","2020-10-31"
"STREAMS","Measuring the Lumpiness of Dark Matter With Tidal Streams","Vasily Belokurov","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The two main characteristics of Dark Matter are its puzzling ubiquity and its omnipotent evasiveness. To create galaxies, stars and planets, we need Cold Dark Matter to enhance minuscule primordial density fluctuations and provide seeds for the ordinary matter to cluster around. However, in the last 30 years, the colossal international efforts to detect this alien substance have made little progress. Yet there is one strong and imminently testable prediction of the modern Cosmology: in the early Universe, Dark Matter starts collapsing first and ends up arranging itself into a hierarchy of invisible clumps of all sizes. Detecting these dark haloes through their gravitational effects is feasible with existing technology and quantifying their abundance will shed light on the nature of  Dark Matter.

The objective of this Proposal is to study the evolution of the stellar phase-space density in the nearby tidal streams to quantify the abundance of the dark matter clumps in the Galactic halo. To probe the granularity of the Milky Way gravitational potential we will discover, measure and identify progenitors of, new Milky Way stellar tidal streams, both using the data from all-sky imaging and spectroscopic surveys and by conducting our own observational campaign. To link the amplitude and the amount of streams' perturbations to the sub- halo mass function, we will develop models of streams’ orbital evolution and will investigate the process of stellar debris scattering with N-body simulations. The ultimate judgement as to the lumpiness of the dark matter locally will come from application of the techniques developed in this project to the revolutionary datasets of Gaia and LSST.","1494129","2013-01-01","2018-12-31"
"StrEnQTh","Strong Entanglement in Quantum many-body Theory","Philipp Hans-Jürgen HAUKE","RUPRECHT-KARLS-UNIVERSITAET HEIDELBERG","This project addresses a frontier of modern quantum physics, entanglement in strongly correlated many-particle systems. At present, despite its importance for fundamental phenomena and potential applications, many-body entanglement is poorly understood theoretically and eludes experimental investigations. Three fundamental challenges are blocking further progress: there are infinitely many classes of many-body entangled states, the calculation of real-time quantum dynamics is inherently difficult, and the quantification of many-particle entanglement remains a hard experimental challenge. 

StrEnQTh adopts a radically novel approach to force a breakthrough in each of these challenges, concentrating on specific targets motivated by next-generation AMO setups. 1. By designing a dedicated quantum resource theory, I will establish a novel framework for topological long-range entanglement. 2. By implementing crucial improvements on a tensor-network method, thermalization dynamics in gauge theories becomes tractable, especially hydrodynamization after heavy-ion collisions. 3. By exploiting the untapped potentials of time-reversing quantum dynamics and measuring high-order correlations, mixed-state entanglement becomes accessible. Further, by introducing a new paradigm of detection by dissipation, unequal-time correlators become available as a novel toolset for witnessing many-body entanglement. 
To achieve these goals, StrEnQTh builds on (i) my expertise at the interface of quantum optics and information with quantum many-body theory; (ii) previous works and preliminary results that minimize risks; (iii) fruitful synergies between the goals; (iv) a high versatility of the developed methods. 

The impact of this project will reach far beyond its immediate field. It will elucidate fundamental theoretical questions of relevance to strongly correlated matter at large, and it will deliver a new generation of detection tools that can find application in other platforms.","1499563","2018-11-01","2023-10-31"
"STRING-QCD-BH","String Theory, QCD and Black Holes","Iosif Lucian Razvan Bena","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","I intend to use string theory to understand gauge theories in four dimensions, both at strong and weak coupling. This includes describing strongly interacting gauge theories and their metastable vacua using their supergravity and string duals, as well as understanding gauge theories at intermediate coupling via integrability. It also includes using and developing new string-inspired twistor-space techniques to analyze QCD and N=1 amplitudes at one loop level. I also plan to thoroughly investigate the implications of string theory for the physics of black holes. In particular I want to construct and analyze black hole microstates -- smooth horizon-less solutions that are very similar to the black hole asymptotically, but differ from the black hole at the location of the horizon. Counting and understanding these microstates via the AdS-CFT correspondence will establish whether black holes should be thought of as ensembles of such microstate geometries.","652500","2010-01-01","2014-12-31"
"STRINGCOSMOS","String Cosmology and Observational Signatures","Jean-Luc Lehners","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","The big bang and its immediate aftermath remain one of the great mysteries in science, and it is likely that only an inter-disciplinary approach using both fundamental particle physics and cosmology will get us closer to understanding the origin of our universe. Currently the most promising framework for a theory of all particles and forces is string theory; however, it has been notoriously difficult to address cosmological issues within the theory. Only in the last few years has it been possible to construct  stringy  inflationary models as well as radically new string-inspired models, such as the cyclic universe.

This research project proposes to explore string cosmology by focusing on the characteristic elements of string theoretic models: string dynamics (typically leading to non-canonical kinetic terms and many relevant light fields), brane dynamics (inter-brane forces, their motion, collision and eventual stabilization), holographic approaches (in particular attempts at understanding the big bang using the AdS/CFT correspondence), violations of the null energy condition (ubiquitous in string cosmology, but not much explored to date) and intersecting brane constructions.

These characteristic elements are equally relevant for both inflation and the cyclic universe, and they will therefore be used to identify distinct observational signatures of the string theoretic cosmological models, for example with regard to gravitational waves or the statistics of primordial temperature fluctuations in the cosmic radiation left over from the big bang. Combined with the upcoming precision cosmological data from ground and satellite experiments, this research has the potential to significantly enhance our knowledge of both fundamental theory and cosmology and, therefore, to shed light on the question of the origin and fate of our universe.","1146240","2010-12-01","2015-11-30"
"STROFUNSCAFF","Strong, functional, tunable, self-assembling hydrogel scaffolds for regenerative medicine","Alvaro Mata Chavarria","QUEEN MARY UNIVERSITY OF LONDON","This work proposes the development of novel material and fabrication platforms to generate strong, tunable, and highly biomimetic nanofibrous hydrogel scaffolds with an unparalleled level of control of both signaling and mechanical properties. The break-through element is the combination of elastin-like polymers (ELPs) and self-assembling peptide amphiphiles (PAs) to create nanofibrous hydrogels with an unprecedented combination of strength, tenability, and bioactivity. The proposed work aims to provide solutions to the current main limitations of self-assembling hydrogels. In addition, it describes novel fabrication methods to create unique biomimetic environments. The work is divided into 2 work packages. The first Work Package (WP1) aims to develop two material platforms designed to combine the benefits of ELPs and PAs. The second Work Package (WP2) aims to develop scaffold fabrication platforms with unprecedented complexity and precision exhibiting defined hierarchical features and spatio-temporal control of physical and chemical signals designed for cartilage or disc therapies. All the scaffolds will be validated in vitro using human cells. This is a critical component for the generation of human-based models and more efficient regenerative therapies.","1492686","2013-08-01","2018-07-31"
"STROMATA","Micro-pyrites associated with organic material in ancient stromatolites: a new proxy attesting for their biogenicity","Johanna MARIN-CARBONNE","UNIVERSITE JEAN MONNET SAINT-ETIENNE","Identifying Archean fossil remains from the Earth’s early biosphere is ambitious and determining the biological origin and the associated metabolic pathways present in these fossils is one outstanding question in the bigger quest of how life evolved on Earth. Stromatolites and Microbially Induced Sedimentary Structures (MISS) are considered as one of the earliest evidence of Life in Earth’s history, and can be found from the Archean to the present time. Stromatolites are “attached laminated, sedimentary growth structure accretionary away from a point of initiation”, and their morphological comparison with actual structure prevail for assessing the microbial origin of ancient stromatolite in the geological record. However, experimental studies have shown that abiotic precipitation can also form structures with a similar morphology. Therefore stable isotope proxies have been used to identify past microbial metabolisms even if abiotic processes can also produce similar isotope composition. Therefore new biogenicity criteria are needed to be determined by studying modern and ancient stromatolites and by comparing them to abiotic experiment. Stromatolites and MISS contain submicrometer sulfides (pyrite) that can have recorded large isotopic variations, interpreted as reflecting the influence of various microbial metabolisms like microbial sulfate reduction and iron respiration. STROMATA proposes to define new criteria based on actual stromatolite and to test the earliest traces of life by studying in situ these nano-pyrites in various emblematic and well-characterized samples from the Archean. STROMATA will be the first far-reaching scientific in situ study of nano-pyrite in ancient (3.4 to 1.9 Ga) and modern microbial mats and stromatolites and will compare the results with experimentally produced abiotic pyrite. Due to the small scale of the pyrite, STROMATA will develop an original in situ approach by combining state of art techniques, SIMS, NanoSIMS, FEG-TEM, XANES.","1060250","2018-02-01","2023-01-31"
"STRONG","Strong Coupling Between Molecules and Vacuum Fields: New Molecular Properties","Karl Magnus Henrik Börjesson","GOETEBORGS UNIVERSITET","Chemistry has had profound impact on society during the last two centuries. From mass production of drugs and pigments, to the invention of plastics, and more recently with the introduction of molecular electronics. However, some basic physical laws govern possible utilizations. It is therefore of great importance to examine how to bend these laws, how to bypass them and by so doing open up new opportunities for novel applications.

A central physical property of the molecule is its ability to interact with light. Plant leaves are green because they absorb light. However, less known is that this light-matter interaction can be enhanced to the point where it is so strong so that the photon and molecule cannot be regarded as separate entities, but as a system with unique properties. So called strong coupling occurs when exchange of energy between light and matter is stronger than any dissipation process and it leads to the formation of hybrid states with new physical and chemical properties.

STRONG will use a chemical viewpoint to develop unique molecules optimized for strong light-matter interactions, and with these examine excited state processes of strongly coupled systems. My aim is to demonstrate that strong light-matter coupling enables selective manipulation of energy levels. By so doing I will allow for a singlet ground and first excited state, thus challenge Hund’s rule and change how the basic rules of electronic state energetics are envisioned. This enables channelling of all excitation energy, irrespectively of origin, through a singlet pathway, which is of great technological importance in organic electronics. Furthermore, I will use reversible oriented molecules to enhance the coupling and for the first time examine the relationship between orientation of molecules and strong light-matter coupling. Also the ability of light-matter interactions to increase order of an ensemble of molecules, which has profound technological applications, will be explored.","1500000","2018-02-01","2023-01-31"
"STRONG","Nanoscale magnetic and thermal imaging of strongly correlated electronic materials","Yonathan Anahory","THE HEBREW UNIVERSITY OF JERUSALEM","Strongly correlated electronic materials have phase diagrams that are intrinsically complex. Multiple, distinct, broken symmetry phases can occur simultaneously and the presence of these intertwined orders gives rise to spontaneously inhomogeneous electronic structures. Observing and characterizing these patterns is crucial to understanding the mechanisms that govern these electronic states. I propose to study these phases using a novel scanning SQUID microscope with single electron spin sensitivity and thermal sensitivity better than one millionth of a degree. The SQUID is mounted on a nanometric tip and has ~50 nm resolution. I will expand the experimental capabilities of this technique by improving the resolution to a few nm, and by enabling near field microwave microscopy.
Local magnetic probes are ideal for spatially resolving magnetic order and can also be used to probe local superconducting phase fluctuation since they generate local currents and thus local magnetic fields. However, the required resolution is of the order of a few nm, which is far beyond the capabilities of most local magnetic probes. While thermal microscopy provides information about dissipation mechanisms, which is relevant in high Tc superconductors (HTSC) above Tc where superconducting correlations are locally present, there is no technique that can perform thermal microscopy at low temperatures. The SQUID-on-tip will allow us to look at all the above-mentioned aspects. We propose to look at three types of systems (1) Observe local signatures of pair-density waves and other manifestations of broken time reversal symmetry in HTSC (2) Characterize the unconventional superconducting phase at the LaAlO3/SrTiO3 interface (3) Study the inhomogeneous magnetic phases at the LaMnO3/SrTiO3 interface. These measurements will provide significant contributions to the understanding of phenomena in strongly correlated materials such as superconductivity and its relation to other electronic order.","1997926","2019-08-01","2024-07-31"
"STRONG-Q","Strong single-photon radiation-pressure coupling for quantum optomechanics","Simon Gröblacher","TECHNISCHE UNIVERSITEIT DELFT","While quantum physics is one of the most successful theories in modern science, it remains a puzzle why we do not observe quantum effects in our macroscopic every-day life. Quantum theory in fact does not pose an inherent limit on the size and mass of a quantum system. The field of optomechanics has been established with the explicit goal of testing massive, macroscopic quantum states. Here, the radiation-pressure force is used to bring a mechanical oscillator into a quantum state by coupling it to light. Most current experiments are however limited by either the optical or mechanical quality of the mechanical system, which so far have prevented a true breakthrough.

I will use two-dimensional photonic crystals on silicon nitride membranes to build a new generation of optomechanical systems, consisting of arrays of mechanical oscillators with high reflectivity and unprecedented mechanical quality. This will allow me to finally enter the single-photon strong coupling (SPSC) regime. This regime, which to date is far beyond experimental reach of any state-of-the-art system, will open up the possibility of true quantum experiments involving macroscopic mechanical objects. Preliminary experiments reveal that we can fabricate membranes of unparalleled optical and mechanical quality necessary for realizing STRONG-Q.

A fundamentally new approach combined with our novel devices will allow me to access the SPSC regime. I will be able to study superradiance, single-photon blockade and non-Gaussian states, explore avenues for phonon lasing, and perform quantum experiments that were up to now impossible. As enabling technologies I will build an entangled-photon source and highly efficient photodetectors.

With my expertise in (single-photon) quantum optics and optomechanics, and with TU Delft’s infrastructure as a leading institute in quantum technologies and microfabrication, the ultimate goal of this project to bring quantum physics into the macroscopic world is within reach.","1499970","2016-03-01","2021-02-28"
"STRONGINT","The strong interaction at neutron-rich extremes","Achim Schwenk","TECHNISCHE UNIVERSITAT DARMSTADT","""""""The strong interaction at neutron-rich extremes"""" (STRONGINT) will investigate the structure of matter at the neutron-rich frontier in the laboratory and in the cosmos based on chiral effective field theory (EFT) interactions. Chiral EFT opens up a systematic path to investigate many-body forces and provides unique constraints for three-neutron and four-neutron interactions. We will for the first time explore the predicted many-body forces in neutron matter and neutron-rich matter. One milestone will be set by the development of a systematic power counting for neutron-rich matter. This will enable us to carry out diagrammatic approaches, and to develop ground-breaking nonperturbative Monte-Carlo calculations. Our results will strongly constrain the nuclear equation of state at the extremes reached in core-collapse supernovae and neutron stars. Based on the developments for neutron-rich matter, we will investigate spin correlations and develop a systematic description of neutrino-matter interactions, which can set the new standard for supernova simulations. Our pioneering studies have revealed new facets of three-body forces in neutron-rich nuclei, such as their role in determining the location of the neutron dripline in oxygen and in elucidating the doubly-magic nature of calcium-48. We will investigate the impact of chiral three-nucleon forces on key regions in the r-process path and develop a chiral EFT for valence-shell interactions. This will open new horizons for understanding the shell structure of nuclei. Another milestone will be set by the first calculation of neutrino-less double-beta decay based on chiral EFT interactions and consistent electroweak currents. The proposed interdisciplinary problems are essential for a successful and quantitative understanding of these big science questions.""","1495020","2012-09-01","2017-08-31"
"STRONGPCP","Strong Probabilistically Checkable Proofs","Irit Dveer Dinur","WEIZMANN INSTITUTE OF SCIENCE LTD","Probabilistically Checkable Proofs (PCPs) encapsulate the striking idea that verification of proofs becomes nearly trivial if one is willing to use randomness. The PCP theorem, proven in the early 90&apos;s, is a cornerstone of modern computational complexity theory. It completely revises our notion of a proof, leading to an amazingly robust behavior: A PCP proof is guaranteed to have an abundance of errors if attempting to prove a falsity. This stands in sharp contrast to our classical notion of a proof whose correctness can collapse due to one wrong step. An important drive in the development of PCP theory is the revolutionary effect it had on the field of approximation. Feige et. al. [JACM, 1996] discovered that the PCP theorem is *equivalent* to the inapproximability of several classical optimization problems. Thus, PCP theory has resulted in a leap in our understanding of approximability and opened the gate to a flood of results. To date, virtually all inapproximability results are based on the PCP theorem, and while there is an impressive body of work on hardness-of-approximation, much work still lies ahead. The central goal of this proposal is to obtain stronger PCPs than currently known, leading towards optimal inapproximability results and novel notions of robustness in computation and in proofs. This study will build upon (i) new directions opened up by my novel proof of the PCP theorem [JACM, 2007]; and on (ii) state-of-the-art PCP machinery involving techniques from algebra, functional and harmonic analysis, probability, combinatorics, and coding theory. The broader impact of this study spans a better understanding of limits for approximation algorithms saving time and resources for algorithm designers; and new understanding of robustness in a variety of mathematical contexts, arising from the many connections between PCPs and stability questions in combinatorics, functional analysis, metric embeddings, probability, and more.","1639584","2009-09-01","2016-06-30"
"STRUBOLI","Structure and Bonding at Oxide-Liquid Interfaces","Martin Sterrer","UNIVERSITAET GRAZ","""The understanding of interfacial chemistry requires knowledge of interface properties at the atomic scale. Surface science studies provided microscopic details from surfaces in vacuum environment and electrochemists followed up to show that similar details might be obtained from electrode-electrolyte interfaces. For mineral-solution interfaces, however, our knowledge is still almost exclusively based on macroscopic observations. With the current project we take one step further toward a fundamental understanding of structure and bonding at oxide-liquid interfaces. For this purpose we will study the properties of water at the oxide-aqueous solution interface and its dependence on the chemical nature of different adsorbates, pH, and electrical potential. The latter can be applied because we are using metal-supported, single-crystalline oxide thin films as substrates. A combination of solid-liquid in-situ scanning tunnelling microscopy and sum-frequency generation spectroscopy together with ultrahigh vacuum-based analytical methods allows us to analyze adsorbate structure and chemical nature of the interface in detail. The structure-forming ability of adsobates will be inferred from vibrational relaxation studies. Finally, vibrational energy transfer from water into the adsorbate will provide details about intermolecular coupling at the interface.""","1571154","2011-12-01","2016-11-30"
"STRUCMAGFAST","The Physics and Applications of Magnetic Guiding of Fast Electrons through Structured Targets","Alexander Patrick Lowell Robinson","SCIENCE AND TECHNOLOGY FACILITIES COUNCIL","When ultra-intense lasers interact with matter, extreme conditions are produced which cannot normally be accessed in terrestrial laboratories.  These extreme states of temperature, density, and pressure form the basis for laser fusion, compact particle accelerators, and well as the exploration of the cosmos through scaled laboratory experiments of astrophysical phenomena.

One feature of ultra-intense laser-solid interactions is the absorption of energy into high current relativistic electron beams.  If the flow or propagation of these beams through dense matter could be controlled then many new possibilities in laser fusion and other areas of high energy density physics would emerge.  Detailed control of  fast electron propagation has not usually been seen as possible due to the short time-scales and small spatial scales.

In previous work the PI showed that detailed control of fast electron flow might in fact be possible by using structured targets, where a guide path is defined by a more resistive material than the bulk material.  Fast electron flow will then lead to resistive self-generation of magnetic field around this region which is strong enough to confine and guide the fast electrons along this path.

Although promising, this concept still needs much work, and in particular physics relevant to longer (i.e. multi-ps) time-scales need to be included.  In this project we will develop a numerical tool that incorporates all the relevant physics, use this to elucidate the important physics of the multi-ps regime, and then use this knowledge for Fast Ignition inertial fusion and other high energy density physics applications.","576403","2012-10-01","2016-09-30"
"StructDyn","‘Filming’ excited state structural dynamics in photosynthesis and organic semiconductors","Sebastian Westenhoff","GOETEBORGS UNIVERSITET","I wish to record ‘atomic movies’ of the structural quantum response in photoexcited chromophores.

The structure of molecules relaxes as a response to photoexcitation. Coherent atomic relocations are thought to be important in photosynthesis and they ‘wire’ together distant electronic systems on conjugated polymer chains. The structural details of such atomic motions remain today elusive, because experimental techniques to visualize them are simply not available.

I will pioneer femtosecond time-resolved X-ray scattering at state-of-the-art European synchrotrons and free electron lasers to directly visualize the atomic details of such motions.

The specific objectives of my challenging research plan are to measure the excited state structure of conjugated molecules; to film solvent structural dynamics on femtosecond time-scales; to directly visualize the coherent structural photoresponse of photosynthetic proteins and conjugated polymers; and to find out how nature uses structural dynamics for directed energy transport in photosynthesis.

This research program builds on my strength in organic semiconductor photophysics and time-resolved X-ray scattering. I have pioneered a description of exciton energy transfer in conjugated polymers beyond the state-of-the-art and have mapped the complete photocycle of organic solar cells for the first time. Proving that I can successfully change research fields and produce high quality results, I have been the first to apply time-resolved wide angle X-ray scattering to the study of membrane protein structural dynamics.

Femtosecond time-resolved X-ray scattering is a new generic approach with applications in physics, chemistry and biology. My work will open new horizons in quantum chemistry, photophysics, and structural biology.","1690465","2012-02-01","2017-01-31"
"STRUCTURAL DYNAMICS","New science for European ps time-resolved Laue X-ray diffraction","Jasper J. Van Thor","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","I pursue the study of molecular reaction dynamics of biomolecules mostly in the non-coherent time-regime using spectroscopic and X-ray structural techniques. In recent years, breakthroughs in the field of pump-probe X-ray crystallographic structure determination of reaction intermediates of light-sensitive proteins have generated much excitement. These experiments provide insight of key Biological importance and marry structural observations with spectroscopic studies to understand molecular reactivity. My work focuses on the light-sensitive proteins in particular to provide structural dynamics understanding of photoreactions and Biological signalling. The European Synchrotron Radiation Facility is investing in this area, providing and integrating technology that has been developed for pump-probe experiments using the pink-Laue technique in new designs for an upgraded and advanced beamline at ESRF. Historically, research groups in the US have dominated this field and the recent commissioning of the Biocars pump-probe beamline 14-ID-B at APS which has capabilities similar to ID09B at ESRF shows that international competition is strong. While technical capabilities of these two sources have rapidly developed, there has been an obvious lag in applications for the methodology. So far, photolysis of heme proteins (myoglobin-CO and recently FixL) and a photoreceptor molecule called Photoactive Yellow Protein (PYP) have been targets for pump-probe experiments. Clearly it is of strong strategic importance for European science to compete in this area. I have invested four years to develop the Green Fluorescent Protein as a new application, focussing on both time-resolved Laue diffraction as well as ultrafast vibrational and visible spectroscopy to characterise structural dynamics and to establish excited state absorption properties and reactivity for optical pulse profile optimisation for providing sufficient optical penetration. Here, I request European support for establi","1000000","2008-12-01","2012-09-30"
"STRUCTURE","De novo structural elucidation of functional organic powders at natural isotopic abundance","Giulia MOLLICA","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Structure holds the key to many of the physical world’s most intriguing secrets. Diffraction from single crystals has revolutionized our knowledge of crystalline matter by providing atomic-scale images of countless samples and leading to landmark achievements in science. However, when crystals of sufficient dimensions cannot be grown, structure can hardly be retrieved using currently available methodologies. This hampers our understanding of the physico-chemical behavior of numerous samples, such as functional organic powders (FOP), hence precluding the design of new materials with tailored properties. Solid-state NMR (SSNMR) has the potential to be the key to access the structure of powders for applications in energy or pharmacy. However, the inherently low sensitivity of NMR constitutes the main barrier to retrieve valuable constraints such as interatomic distances and torsional angles from spin-spin couplings involving rare nuclei (e.g. C-13, N-15) on organic samples at natural isotopic abundance (NA), for which chemical shifts are certainly easier to access but less structurally relevant. The project will capitalize on Dynamic Nuclear Polarization (DNP) to enhance the sensitivity of SSNMR and obtain unique structural constraints on NA FOPs. Specifically: (i) intra and intermolecular distances, torsional/bond angles and H bonds will be measured for the first time via DNP SSNMR; (ii) together with powder X-ray data, these constraints will be integrated within modern computational algorithms to assist the generation of physically meaningful 3D structures with minimized risk of false positives. The protocol will be applied to time-resolved in situ/ex situ investigation of self-assembly to gain control into polymorph production. We will create an integrated experimental/in silico tool that will extend the proficiency of crystallography in de novo structure elucidation of FOPs of increasing complexity, opening new avenues in chemistry and materials science.","1499632","2018-01-01","2022-12-31"
"STUCCOFIELDS","Structure and scaling in computational field theories","Snorre Harald Christiansen","UNIVERSITETET I OSLO","""The numerical simulations that are used in science and industry require ever more sophisticated mathematics. For the partial differential equations that are used to model physical processes, qualitative properties such as conserved quantities and monotonicity are crucial for well-posedness. Mimicking them in the discretizations seems equally important to get reliable  results.

This project will contribute to the interplay of geometry and numerical analysis by bridging the gap between Lie group based techniques and finite elements. The role of Lie algebra valued differential forms will be highlighted. One aim is to develop construction techniques for complexes of finite element spaces incorporating special functions adapted to singular perturbations. Another is to marry finite elements with holonomy based discretizations used in mathematical physics, such as the Lattice Gauge Theory of particle physics and the Regge calculus of general relativity. Stability and convergence of algorithms  will be related to differential geometric properties, and the interface between numerical analysis and quantum field theory will be explored. The techniques will be applied to the simulation of mechanics of complex materials and light-matter interactions.""","1100000","2012-01-01","2016-12-31"
"SUBLINEAR","Sublinear algorithms for the analysis of very large graphs","Christian Sohler","TECHNISCHE UNIVERSITAT DORTMUND","Large graphs appear in many application areas. Typical examples are the webgraph, the internet graph, friendship graphs of social networks like facebook or Google+, citation graphs, collaboration graphs, and transportation networks.
The structure of these graphs contains valuable information but their size makes them very hard to analyze. We need special algorithm that analyze the graph structure via random sampling.
The main objective of the proposed project is to advance our understanding of the foundations of sampling processes for the analysis of the structure of large graphs. We will use the approach of Property Testing, a theoretical framework to analyze such sampling algorithms.","1475306","2012-12-01","2018-11-30"
"SUBLINEAR","Sublinear Algorithms for Modern Data Analysis","Mikhail KAPRALOV","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Designing efficient algorithms for fundamental computational tasks as well as understanding the limits of tractability has been the goal of computer science since its inception. Polynomial runtime has been the de facto notion of efficiency since the introduction of the notion of NP-completeness. As the sizes of modern datasets grow, however, many classical polynomial time (and sometimes even linear time) solutions become prohibitively expensive. This calls for sublinear algorithms, i.e. algorithms whose resource requirements are substantially smaller than the size of the input that they operate on.  

We propose to design a toolbox of powerful algorithmic techniques with sublinear resource requirements that will form the theoretical foundation of large data analysis, as well as develop a nuanced runtime, space and communication complexity theory to show optimality of our algorithms. Specifically, we propose to:

1. design an algorithmic toolkit for sublinear graph processing that will form the basis of large scale graph analytics;
2. design a new generation of  sublinear algorithms for signal processing that will become the method of choice for a wide range of applications;
3. develop a far-reaching set of techniques for lower bounding runtime, space and communication complexity of sublinear algorithms.

The  problems that we propose to solve are among the most fundamental algorithmic questions on the forefront of the rapidly developing algorithmic theory of large data analysis, which has been the focus of an extensive body of work in the research community. The algorithms and complexity theoretic results that we propose to design will cut at the core of fundamental computational problems and form the theoretical foundation of computing with constrained resources.","1473175","2018-03-01","2023-02-28"
"SUBLRN","Information-optimal machine learning","Elad Eliezer Hazan","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","The statistical and computational theory of learning is one of the prime achievements of computer science and engineering. This is evident both in terms of mathematical elegance of capturing intuitive notions rigorously as well as in terms of practical applicability: machine learning has effectively reshaped the way we use information.
In this proposal we tackle the very basic notions of learning. Learning theory traditional focuses on statistics and computation. We propose to add information to the characterization of learning: namely the research question we address is: how much information is necessary to learn a certain concept efficiently?
The crucial difference from classical learning theory is that traditionally statistical complexity was measured in terms of the number of examples needed to learn a concept. Our question is more finely grained: what if we are allowed to inspect only parts of a given example? Can we reduce the amount of information necessary to successfully learn important concepts? This question is fundamental in understanding learning in general and designing efficient learning algorithms in particular. We show how recent advancements in convex optimization for machine learning yields positive answers to some of the above questions: there exists cases in which much more efficient algorithms exist for learning practically important concepts. Our goal is to characterize learning from the viewpoint of the amount of information necessary to learn, to design new algorithms that access less information than current state-of-the-art and are consequently significantly more efficient. New answers for these fundamental questions will be a breakthrough in our understanding of learning at large with significant potential for impact on the field of machine learning and its applications.","1453802","2013-10-01","2018-09-30"
"SULIWA","Deeply Supercooled Liquid Water","Thomas Loerting","UNIVERSITAET INNSBRUCK","""Water is ubiquitous and so an understanding of water’s anomalous liquid state is crucial for such diverse fields as protein biochemistry, meteorology or astrophysics. A postulated first order phase transition between two distinct one-component liquids at low temperatures is believed to be the key to many riddles in contemporary science: a “fragile” liquid of high density and a “strong” liquid of low density. At higher temperatures the phase boundary might end in a speculative second critical point in supercooled water. Unfortunately it has not been possible so far to support/falsify these hypotheses with direct experiments because of fast crystallization of the liquid(s) in the relevant portion of the phase diagram, which is called """"no man's land"""". Therefore, experiments to test the hypothesis were previously done in the non-crystalline, solid state (“amorphous water”) at temperatures well below the """"no man's land"""". More than 20 years ago liquid-like relaxation was measured on heating glassy water at 1 bar to 136 - 150 K, i.e., to temperatures slightly below crystallization, which is still discussed controversially. Recently we managed to observe liquid-like properties on heating high density amorphous ice (HDA) under isobaric conditions at pressures up to 1 GPa above its glass-liquid transition at a temperature slightly below the """"no man's land"""" without observing significant crystallization. These findings open the exciting possibility to characterize (e.g., by dilatometry, thermal analysis and dielectric spectroscopy) deeply supercooled liquid water both at ambient and high pressure conditions and to check if water indeed shows a first order liquid-liquid phase transition between two distinct liquids. This will unravel the question how many liquids and how many corresponding amorphous states there are in water, and if VHDA discovered by us in 2001 shows a polyamorphic transition to HDA, or if it is simply annealed HDA.""","1389238","2008-08-01","2013-07-31"
"SUMOMAN","Supramolecular Cell Manipulation","Pascal Jonkheijm","UNIVERSITEIT TWENTE","Supramolecular chemistry and nanofabrication methods provide excellent prospect to construct reversible dynamic biological nanoplatforms employed for supramolecular cell manipulation (SUMOMAN) experiments. Making use of supramolecular chemistry is a rewarding task in developing functional materials and devices. Knowing the limitations involved in ordering proteins at different length scales will surely hasten the development of future applications, supramolecular nanobiology being the most prominent. The construction of synthetic supramolecular assemblies of proteins provides an excellent tool to fabricate organized bioactive components in the sub-micron regime at surfaces. Supramolecular nanobiology narrows the gap between chemical biology and bionanotechnology. The latter devises ways to construct molecular devices using biomacromolecules and it attempts to build molecular machines utilizing concepts seen in nature. In chemical biology new synthesis methods and strategies are developed and employed for the synthesis of compounds which are used as probes for the study of biological phenomena. Steadily improved synthetic procedures for site-specific modification of proteins have gained more control over structure and function of the proteins. However, applications of protein chips remain hampered by orientational and conformational aspects at the surface.
With the development of supramolecular bioactive nano-platforms on surfaces serving as a reversible dynamic interface to cells, the goal to study and manipulate cellular processes will come closer.
An innovative construction process of biological nanoarrays is proposed to study important fundamental aspects of cell biology. When such structured surfaces display a biological interface with nm resolution, a lengthscale inherently more relevant to biorecognition than microlengthscales, the communication through biomolecules with cellular receptors can be modulated with unprecedented spatial and temporal specificity.","1500000","2010-11-01","2015-10-31"
"SUNMAG","SUNMAG: Understanding magnetic-field-regulated heating and explosive events in the solar chromosphere","Jaime DE LA CRUZ RODRIGUEZ","STOCKHOLMS UNIVERSITET","This project represents a new powerful approach to a fundamental problem in solar physics: the heating of the solar chromosphere. The aim of SUNMAG is to identify the mechanisms that heat the chromosphere and characterize the energy flux that is being released into the outer layers of the Sun in active regions and flares. By investigating how the chromosphere regulates the energy and mass transport we can also contribute to an understanding of the heating of the corona and the acceleration of the solar wind. These goals are finally within reach thanks to the recent developments of numerical simulations and analysis codes for observations and, observationally, thanks to the very recent development of a new powerful instrument for imaging spectroscopy (CHROMIS) that allows to study the upper chromosphere at the highest possible spatial resolution. 

Chromospheric observations show that energy must be released at very small spatial scales (smaller than 100 km), and therefore, the intricate fine structuring of the magnetized chromosphere is the key to understand the heating and the mass loading of the outer atmosphere. The CHROMIS instrument at the Swedish 1m Solar Telescope (SST) is the only instrument in the world that allows observing the upper chromosphere at this spatial resolution, and rich spectral resolution. SUNMAG shall use observations from this instrument and a space-borne solar telescope (NASA's Interface Region Imaging Spectrograph, IRIS) to reconstruct time-dependent 3D empirical models of the solar chromosphere. I will use these models to characterize the spatio-temporal distribution of heating in the chromosphere of active regions and flares, and to investigate to what extent this heating can be related to the observed (changes in) the physical parameters. These results will be confronted with predictions from the foremost theoretical models to propose what physical mechanisms are most likely providing the required energy deposition.","1491967","2018-01-01","2022-12-31"
"SUPERBAD","Understanding high-temperature superconductivity from the foundations: Superconductivity as a cure of bad metallic behaviour","Massimo Capone","SCUOLA INTERNAZIONALE SUPERIORE DI STUDI AVANZATI DI TRIESTE","""The origin of """"high-temperature"""" superconductivity is definitely one of the most elusive topics in modern solid-state physics. The twenty years that followed the discovery of unprecentedly high critical temperatures in copper oxides have been dominated by a dychotomy between the """"standard""""; superconductors, normal metals in which electron-phonon interaction drives the superconducting pairing, and """"high-temperature""""; superconductors, complex compounds in which superconductivity is most likely of electronic or magnetic origin. Even if the number of proposals in this regard is enormous, we believe that at the present stage we can move one big step forward. The positive circumstances are the appearance of a new player in the field, the iron-based materials discovered in 2008, and the development of theoretical tools able to deal with the main physical ingredients of the different supeconducting materials. The aim of this project is to overcome the electron-phonon/electronic dualism in the """"glue"""", and prove that the key to high-temperature superconductivity is the anomaly of the normal state. More precisely, high-temperature superconductivity is the way in which the pathologies of anomalous metallic states are """"healed"""". In this project we will build a theoretical approach to study ab initio superconductors beyond Migdal-Eliashberg theory, namely a combination of Density Functional Theory and Dynamical Mean-Field Theory extended to the superconducting state. In this way we will put the above physical idea on solid ground, and we will first show that it is actually realized in the already known superconductors by comparing normal state and superconducting states.The results obtained in the process can be used in a second step to design new superconductors, either in the same families as the existing ones and in principle even in other yet unknown families.""","1000000","2009-10-01","2015-09-30"
"SUPERCONDUCTINGMOTT","UNCONVENTIONAL SUPERCONDUCTIVITY FROM A MOTT INSULATING PARENT MATERIAL","Suchitra Esther Sebastian","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","""The mystery of unconventional superconductivity is one that is yet to be solved after decades of research. Better superconductors will have a crucial role in improved energy efficient applications such as power storage and transmission. While the highest temperature superconductors to date are the copper oxide family of antiferromagnetic Mott insulators, the origin of unconventional superconductivity in these materials remains mysterious. Furthermore, there are strikingly few other examples where unconventional superconductivity emerges from Mott insulating materials. In this project we adopt a two-pronged approach to find unconventional superconductivity and potentially novel quantum spin liquid phases in new classes of Mott insulating materials, and to understand the nature of the normal state out of which superconductivity develops in the family of copper oxide superconductors. In the first part of the project, multiple spin-orbit coupled Mott insulating materials will be tuned to induce superconductivity and / or an unconventional quantum spin liquid phase, and a roadmap relating materials properties to emergent unconventional superconductivity developed. In the second part of the project, we will aim to better understand the normal state out of which superconductivity emerges in cuprate superconductors by a study of the nature of Fermi surface evolution from a the small Fermi surface in the underdoped regime to a large Fermi surface in the overdoped regime, potentially via a quantum critical point underlying the superconducting dome. Our findings are anticipated to have important implications for the creation of newer and better superconductors.""","1703253","2013-08-01","2018-07-31"
"SUPERCOOL","Superelastic Porous Structures for Efficient Elastocaloric Cooling","Jaka TUŠEK","UNIVERZA V LJUBLJANI","Cooling, refrigeration and air-conditioning are crucial for our modern society. In the last decade, the global demands for cooling are growing exponentially. The standard refrigeration technology, based on vapour compression, is old, inefficient and environmentally harmful. In the SUPERCOOL project we will exploit the potential of elastocaloric cooling, probably the most promising solid-state refrigeration technology, which utilizes the latent heat associated with the martensitic transformation in superelastic shape-memory alloys. We have already demonstrated a novel concept of utilizing the elastocaloric effect (eCE) by introducing a superelastic porous structure in an elastocaloric regenerative thermodynamic cycle. Our preliminary results, recently published in Nature Energy, show the tremendous potential of such a system. However, two fundamental challenges remain. First, we need to create a geometry of the superelastic porous structure (elastocaloric regenerator) to ensure sufficient fatigue life, a large eCE and rapid heat transfer. Second, we must have a driver mechanism that can effectively utilize the work released during the unloading of the elastocaloric regenerator. To succeed I am proposing a unique approach to design advanced elastocaloric regenerators with complex structures together with a driver mechanism with the force-recovery principle. We will employ a systematic characterization and bottom-up linking of all three crucial aspects of the elastocaloric regenerator, i.e., the thermo-hydraulic properties, the stability and the structural fatigue, together with a new solution for force recovery in effective drivers. Based on these theoretical, numerical and experimental results we will combine both key elements of our novel elastocaloric concept into a prototype device, which could be the first major breakthrough in cooling technologies for 100 years, providing greater efficiency and reduced levels of pollution, by applying a solid-state refrigerant.","1359375","2019-01-01","2023-12-31"
"SUPERFOAM","Structure-Property Relations in Aqueous Foam and Their Control on a Molecular Level","Björn Braunschweig","WESTFAELISCHE WILHELMS-UNIVERSITAET MUENSTER","Foams are of enormous importance as we find them in many technological relevant applications and food products. Foams as hierarchical materials are dominated by the arrangement and distri-bution of gas bubbles on a macroscopic scale, as well as by thickness and composition of lamella on a mesoscopic scale. Liquid-gas interfaces are, however, the building block of foam with over-whelming importance as their molecular properties easily dominate hierarchical elements on larger length scales. In order to formulate foam with specific properties, its structure must be controlled at the molecular level of a liquid-gas interface. Here, the molecular composition, molecular order and interactions such as electrostatics dominate, and thus must be addressed with molecular level probes that can provide access to both interfacial solvent and solute molecules. Specifically, mo-lecular structures of aqueous interfaces can be modified by adding different mixtures of surface active molecules such as proteins, surfactants and polyelectrolytes, and by adjusting electrolyte properties. This is achieved by varying pH, introducing ions at different ionic strengths as well as by changing viscosities. Such model systems will be characterized with nonlinear optical spectroscopy amongst other surface sensitive probes. The gained information will be used to deduce properties of structures on larger length scales such as lamella, bubbles in a bulk liquid - as a precursor of foam - and finally macroscopic foam. For each length scale, experiments will be performed to gain access to molecular buildings blocks at liquid-gas interfaces and their effects on other hierarchical elements. These experiments thus provide essential information on foam stability and bubble coalescence, they can be used to verify structure-property relationships and to advance our understanding of foam on a molecular basis.","1499875","2015-03-01","2020-02-29"
"SuperH","Discovery and Characterization of Hydrogen-Based High-Temperature Superconductors","Ion ERREA","UNIVERSIDAD DEL PAIS VASCO/ EUSKAL HERRIKO UNIBERTSITATEA","After the discovery of superconductivity at above 200 K in the hydrogen sulfide system, two clear conclusions can be drawn: i) there is lots of room for discovering new hydrogen-based high-temperature superconducting compounds, and ii) first-principles calculations can guide the discovery of these materials. In fact, the possibility of high-temperature superconductivity in the hydrogen sulfide system had been predicted before the experiment.

However, in order to be accurate and reliable for this type of compounds, first-principles calculations need to go far beyond the state-of-the-art to correctly incorporate the large quantum effects intrinsic to hydrogen atoms. Huge errors on the superconducting properties of materials are often obtained with state-of-the-art methods, misguiding experimental effort.

In this project we will develop a new method that will make first-principles calculations correctly incorporate such quantum effects and, thus, reach an unprecedented precision and accuracy.

With the use of the novel first-principles method we will characterize correctly the physical and chemical properties of hydrogen-based superconductors, aiming at understanding clearly why and when these materials become high-temperature superconductors. We will also investigate the possibility of high-temperature superconductivity at ambient pressure in this type of compounds. In the end of the project, we will focus our theoretical effort to the discovery of new high-temperature superconductors, focusing on hydrides, hydrogen-storage materials, and organic compounds.","1432500","2019-02-01","2024-01-31"
"SuPERPORES","Structure-performance relationships in porous carbons for energy storage","Celine MERLET","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Supercapacitors are of great interest as energy storage systems because they exhibit very high rates of charge/discharge, long cycle lifes, and they are made of cheap and light materials. These attractive properties arise from the electrostatic nature of the charge storage which results from ion adsorption in the electrode pores. Recently, it was demonstrated that ions can enter pores of sub-nanometer sizes leading to a huge increase of capacitance. This was an important breakthrough as the energy density of supercapacitors, relatively low compared to batteries, is what currently limits their application. 
The progress towards more powerful supercapacitors is limited by our incomplete understanding of the relation between their performance, in particular their capacitance and charging rate, and the complex structure of the porous carbon electrodes. To make progress we need a better fundamental understanding of the ion transport and electrolyte structure in the pores but we are lacking the experimental and theoretical methods to do so. 
The aim of SuPERPORES is to carry out a systematic multi-scale simulation study of supercapacitors. The use of combined molecular and mesoscopic simulations will allow us to calculate the capacitive and transport properties of a wide range of systems. Molecular simulations will be used to model ordered three-dimensional porous carbons. This will allow us to vary geometric descriptors, e.g. pore size and ion size, in a systematic way and obtain relevant microscopic information for the subsequent computational screening of porous carbons, achieved through very efficient lattice simulations. We will then be able to formulate design principles for a new, and much improved, generation of supercapacitors. The simulations will also provide other macroscopic properties, e.g. adsorption isotherms and pair distribution functions, which will be used to propose a new method to determine accurately the structure of disordered porous carbons.","1240318","2017-07-01","2022-06-30"
"SUPERSPEC","Three-dimensional spectral modelling of astrophysical transients : unravelling the nucleosynthetic content of supernovae and kilonovae","Anders JERKSTRAND","STOCKHOLMS UNIVERSITET","Determining the origin of the elements is a fundamental quest in physics and astronomy. Most of the elements in the periodic table are believed to be produced by supernovae and kilonovae. However, this has for decades been little more than a prediction from theory. Now, with a dramatically changing observational situation and new modelling capabilities, it is within our reach to determine the nucleosynthesis production and structure in these transients. To really see what supernovae and kilonovae contain, we must study their spectra in the later so called nebular phase when the inner regions become visible. This project is aimed at establishing the first picture of the origin of elements by determining the yields from supernovae and kilonovae using such analysis. To do this, new spectral synthesis methods need to be developed considering the necessary microphysical (ejecta chemistry, r-process physics, time-dependent gas state) and macrophysical (3D radiation transport) processes to obtain sufficient accuracy. These tools will then be applied to the first 3D explosion simulations of these transients now becoming available. When applied to the growing library of data emerging from automated surveys and follow-up programs, as well to the recent first kilonova observations, this will provide a breakthrough in our understanding of these transients. This development will not only allow a determination of cosmic element production, but also allow tests of theories for stellar evolution, nucleosynthesis, and explosion processes. This will in turn have fundamental impact on several fields of astrophysics such as population synthesis, galactic chemical evolution modelling, and understanding of mass transfer in the progenitor systems. It has a strong connection to recent detections of stellar-mass black holes and merging neutron stars by gravitational waves.","1500000","2019-08-01","2024-07-31"
"SUPERSTARS","Type Ia supernovae: from explosions to cosmology","Kate MAGUIRE","THE QUEEN'S UNIVERSITY OF BELFAST","Type Ia supernovae (SNe Ia) are the incredibly luminous deaths of white dwarfs in binaries. They play a vital role in chemical enrichment, galaxy feedback, stellar evolution, and were instrumental in the discovery of dark energy. However, what are the progenitor systems of SNe Ia, and how they explode remains a mystery. My recent work has concluded the controversial result that there may be more than one way to produce SNe Ia. As SN Ia cosmology samples reach higher precision, understanding subtle differences in their properties becomes increasingly important. A surprising diversity in white-dwarf explosions has also been uncovered, with a much wider-than-expected range in luminosities, light-curve timescales and spectral properties. A key open question is ‘What explosion mechanisms result in normal SNe Ia compared to more exotic transients?’

My team will use novel early-time observations (within hours of explosion) of 100 SNe Ia in a volume-limited search (<75 Mpc). The targets will come from the ATLAS and Pan-STARRS surveys that will provide unprecedented sky coverage and cadence (>20000 square degrees, up to four times a night). These data will be combined with key progenitor diagnostics of each SN (companion interaction, circumstellar material, central density studies). The observed zoo of transients predicted to result from white-dwarf explosions (He-shell explosions, tidal-disruption events, violent mergers) will also be investigated, with the goal of constraining the mechanisms by which white dwarfs can explode. My access to ATLAS/Pan-STARRS and my previous experience puts me in a unique position to obtain ‘day-zero’ light curves, rapid spectroscopic follow-up, and late-time observations. The data will be analysed with detailed spectral modelling to unveil the progenitors and diversity of SNe Ia. This project is timely with the potential for significant breakthroughs to be made before the start of the next-generation ‘transient machine’, LSST in ~2021.","1876496","2018-06-01","2023-05-31"
"SUPRACHEMBIO","Supramolecular Chemical Biology Modulation of Protein-Protein Interactions","Lucas Brunsveld","TECHNISCHE UNIVERSITEIT EINDHOVEN","This proposal aims to explore the combination of supramolecular chemistry with chemical biology. Supramolecular architectures are proposed for the modulation of protein-protein interactions. Two general themes are outlined; 1) supramolecular chemistry in the cell and 2) supramolecular multivalency. Supramolecular architectures are brought forward as excellent tools to control protein-protein interactions at the cellular level. Supramolecular elements, selectively attached to proteins, are outlined to control protein dimerization and localization. The introduction of different supramolecular elements, either in vitro or in vivo via specific labeling techniques, is part of the plan, including the introduction of supramolecular elements, whose interactions can be reversibly switched with light. With these systems, the temporal control over protein-protein interactions and protein localization can be addressed. Self-assembling multivalent architectures are proposed as optimal scaffolds and flexible systems for recognition and binding of cells. The assembly process of the supramolecular architecture will be steered via interplay with the characteristics of the cellular target and the environment, controlling size, shape and composition. This self-assembling multivalent platform will be applied to control or initiate interactions between different types of surfaces, such as cells, antigens and chips. The self-assembling scaffolds additionally provide an ideal platform to be combined with proteins, allowing the generation of ordered protein wires, with control over orientation and distances between proteins. The combination of supramolecular chemistry with chemical biology is envisioned to enable the modulation of protein-protein interactions and thus provide entries to this long standing fundamental challenge.","1250000","2008-07-01","2013-06-30"
"SUPRACHEMMED","Supramolecular Chemistry in Medicine:Towards Complex Molecular Biomaterials that are Indistinguishable from Nature","Patricia Yvonne Wilhelmina Dankers","TECHNISCHE UNIVERSITEIT EINDHOVEN","This ERC proposal aims to bridge the gap between supramolecular chemistry and regenerative medicine by defining a new area of ‘supramolecular medicine’ in which supramolecular chemistry will be used to solve medical and health problems. Understanding of tissues, cells, and the interactions occurring at the molecular level in natural systems is prerequisite to intervene in and stimulate processes in the body, in order to perform regenerative medicine. Because all processes taking place in our body are based on supramolecular interactions between molecules that are dynamic in nature and have certain on-off rates, we anticipate that biomaterials brought into the body should display this same dynamic behaviour, and should be able to adapt to the tissue they encounter. Therefore, in the field of regenerative medicine there is a need for new biomaterials that are supramolecular in nature and are indistinguishable from their natural counterparts.
Here we describe the design of supramolecular biomaterials that can be applied as synthetic extracellular matrices and synthetic cell-like microcapsules. By designing these synthetic systems as indistinguishably from nature we propose to get more insight in the processes occurring in nature. At a fundamental molecular level several molecules will be brought together to form bioactive complex molecular assemblies. Control over the introduction of bioactivity is necessary for selective interactions with specific cells and (parts of) tissues. These design principles will be applied to make different biomaterials that can be brought to the patient as synthetic extracellular matrix gel-like materials for stem cell incorporation, free-standing membranes for the development of bioartificial kidneys, and microcapsules specifically binding to the deteriorating peritoneal membranes of kidney patients on peritoneal dialysis.","1351744","2013-02-01","2018-07-31"
"SUPRACOP","Systems Chemistry Approach towards Semiconductive Supramolecular Copolymers with Homo- and Heterometallophilic Interactions","Gustavo FERNANDEZ HUERTAS","WESTFAELISCHE WILHELMS-UNIVERSITAET MUENSTER","Infinite one-dimensional structures with a metallic main chain of short metal-metal contacts have attracted considerable attention in the field of materials science for many decades due to their excellent optical properties and remarkable dichroism and electrical (semi)conductivity. These materials suffer, however, from decomposition prior to melting and low solubility and processability. The strategy of introducing alkyl side chains of different nature in the past two decades proved to be particularly successful towards better soluble materials or gels with implications in optoelectronics. However, this comes at the price of reduced bulk conductivities leading in some cases to electrical insulators due to the perturbation of the metal-metal contacts.

In this proposal, a Systems Chemistry approach will be introduced to create unprecedented supramolecular copolymers that are anticipated to exhibit: a) high solubility, reversibility and stability in organic solvents and water and, b) short metal contacts involving either positively and negatively charged metal ions of the same nature (Pt2+/Pt2-) or dissimilar metal centres (Pd(II)/Pt(II) and Ag(I)/Au(I)) with equivalent coordination geometry. To achieve this goal, ligands with an extended aromatic surface for pi-stacking supported by complementary non-covalent interactions have been selected to bring suitable metal ions in close proximity. This can be summarized in three approaches. 1) Optimization of the geometrical complementarity between the interacting ligands; 2) Introduction of hydrogen bonding and electrostatic complementarity between side groups, and 3) Exploiting weak interactions between geometrically equivalent electron rich and electron poor units. The extent of metal-metal interactions can be ultimately controlled by introducing suitable light switchable groups. 

This concept is expected to provide access to novel, highly-ordered materials with rich photophysical and semiconductive properties.","1493750","2017-05-01","2022-04-30"
"SUPRAFUNCTION","Supramolecular materials for organic electronics: unravelling the architecture vs. function relationship","Paolo Samorì","CENTRE INTERNATIONAL DE RECHERCHE AUX FRONTIERES DE LA CHIMIE FONDATION","SUPRAFUNCTION aims at mastering principles of supramolecular chemistry, in combination with top-down nanofabrication, to achieve a full control over the architecture vs. function relation in macromolecular materials for organic electronics, by analyzing and optimizing fundamental properties through which new capacities can emerge.
Highly ordered supramolecularly engineered nanostructured materials (SENMs) will be self-assembled from conjugated 1D/2D molecules, and ultra-stiff multichromophoric arrays based on poly(isocyanides). Their interfaces with chemically functionalized top-down/bottom-up nanofabricated electrodes and with dielectrics will be tailored to reach SENM energy barriers with height <0.1eV and interface roughness of 3-7Å. Multiscale characterization of SENMs, nanoelectrodes and various interfaces will be done by Scanning Probe Microscopies, ultraviolet photoelectron spectroscopy and other methods, especially to quantitatively study 3 relevant properties, viz charge injection at interfaces, charge transfer, and photoswitching current through a molecular material. Prototypes of nanowires and Field-Effect Transistors (FETs) will be fabricated especially focusing on (1) unravelling charge transport vs. charge injection, (2) the effect of photo-doping in electron acceptor-donor dyad based SENMs, and (3) novel photo-switchable FETs based on either (i) photo-responsive azobenzene SAMs chemisorbed on electrodes/dielectrics to reversibly modulate the charge injection at interfaces, or (ii) electroactive SENMs of dithienylethenes featuring extended conjugation in the side arms to promote a light tuneable p-p stacking among adjacent molecules, ultimately affecting the charge transport in stacks.
The generated knowledge will offer new solutions to nanoscale multifunctional organic based logic applications.","1500000","2011-04-01","2016-03-31"
"SUPRANET","Supramolecular Recognition in Dynamic Covalent Networks at Equilibrium and Beyond","Max VON DELIUS","UNIVERSITAET ULM","Over the past two decades, a branch of organic chemistry has emerged that breaks with the paradigm of synthesizing pure compounds and focusses instead on complex (macro)molecular networks akin to those found in nature. In this proposed project, we aim to address unmet challenges in supramolecular chemistry and systems chemistry by developing original dynamic reaction networks whose building blocks are capable of supramolecular (self-)recognition.

The first two objectives of SUPRANET focus on the use of dynamic covalent orthoester networks for the discovery of anion, cation and ion pair receptors, whose unique properties may pave the way towards the utilization of inorganic ions as drugs. For instance, we will develop self-assembled ion pair cages for the electro-neutral transport of medicinally relevant anions across phospholipid membranes. Our network approach will also allow us to “imprison” ionic guests for the first time in self-assembled receptors that could be used for the transport and controlled release of ions, even against osmotic pressure.

Objectives three and four of SUPRANET go beyond the equilibrium state and, as such, are relevant to the chemistry of life, in which key processes depend on dissipative steady states. The proposed reaction networks will feature biologically relevant ribose building blocks that are continuously assembled and disassembled by two different irreversible reactions, resulting in steady state mixtures of either RNA oligomers or ribose-derived vesicles. It is our hope that these studies will provide insights into open questions regarding the molecular origins of life, such as the non-enzymatic formation of RNA oligomers capable of self-recognition and the simultaneous emergence of compartmentalization and self-replication.

SUPRANET thus seeks to break new ground in both equilibrium and far-from-equilibrium dynamic networks and is equally motivated by applied and fundamental challenges.","1500000","2019-07-01","2024-06-30"
"SUPREL","""Scaling Up Reinforcement Learning: Structure Learning, Skill Acquisition, and Reward Shaping""","Shie Mannor","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","""Learning how to act optimally in high-dimensional stochastic dynamic environments is a fundamental problem in many areas of engineering and computer science. The basic setup is that of an agent who interacts with an environment trying to maximize some long term payoff while having access to observations of the state of the environment. A standard approach to solving this problem is the Reinforcement Learning (RL) paradigm in which an agent is trying to improve its policy by interacting with the environment or, more generally, by using different sources of information such as traces from an expert and interacting with a simulator. In spite of several success stories of the RL paradigm, a unified methodology for scaling-up RL has not emerged to date. The goal of this research proposal is to create a methodology for learning and acting in high-dimensional stochastic dynamic environments that would scale up to real-world applications well and that will be useful across domains and engineering disciplines.
We focus on three key aspects of learning and optimization in high dimensional stochastic dynamic environments that are interrelated and essential to scaling up RL. First, we consider the problem of structure learning. This is the problem of how to identify the key features and underlying structures in the environment that are most useful for optimization and learning. Second, we consider the problem of learning, defining, and optimizing skills. Skills are sub-policies whose goal is more focused than solving the whole optimization problem and can hence be more easily learned and optimized. Third, we consider changing the natural reward of the system to obtain desirable properties of the solution such as robustness, adversity to risk and smoothness of the control policy. In order to validate our approach we study two challenging real-world domains: a jet fighter flight simulator and a smart-grid short term control problem.""","1500000","2013-01-01","2017-12-31"
"SURFACE","The unexplored world of aerosol surfaces and their impacts.","Nonne PRISLE","OULUN YLIOPISTO","We are changing the composition of Earth’s atmosphere, with profound consequences for the environment and our wellbeing. Tiny aerosol particles are globally responsible for much of the health effects and mortality related to air pollution and play key roles in regulating Earth’s climate via their critical influence on both radiation balance and cloud formation. Every single cloud droplet has been nucleated on the surface of an aerosol particle. Aerosols and droplets provide the media for condensed-phase chemistry in the atmosphere, but large gaps remain in our understanding of their formation, transformations, and climate interactions. Surface properties may play crucial roles in these processes, but currently next to nothing is known about the surfaces of atmospheric aerosols and cloud droplets and their impacts are almost entirely unconstrained. My recent work strongly suggests that such surfaces are significantly different from their associated bulk material and that these unique properties can impact aerosol processes all the way to the global scale. Very few surface-specific properties are currently considered when evaluating aerosol effects on atmospheric chemistry and global climate. Novel developments of cutting-edge computational and experimental methods, in particular synchrotron-based photoelectron spectroscopy, now for the first time makes direct molecular-level characterizations of atmospheric surfaces feasible. This project will demonstrate and quantify potential surface impacts in the atmosphere, by first directly characterizing realistic atmospheric surfaces, and then trace fingerprints of specific surface properties in a hierarchy of experimental and modelled aerosol processes and atmospheric effects. Successful demonstrations of unique aerosol surface fingerprints will constitute truly novel insights into a currently uncharted area of the atmospheric system and identify an entirely new frontier in aerosol research and atmospheric science.","1499626","2017-03-01","2022-02-28"
"SURFARI","Arithmetic of algebraic surfaces","Matthias Schütt","GOTTFRIED WILHELM LEIBNIZ UNIVERSITAET HANNOVER","This research proposal concerns a fundamental problem in the theory of al-
gebraic surfaces which poses one of the most important challenges in order to
understand the inner structure of algebraic surfaces beyond the current state of
the art. Our research team will investigate in detail the structure of curves on
algebraic surfaces which is captured in the Neron-Severi group. In general, it
is a widely open problem to decide which shapes this group can take. By de-
signing innovative and unconventional approaches at the borderline of arithmetic
and geometry, we aim at groundbreaking results that will spark deep insights
into the inner structures of algebraic surfaces and lay cornerstones for future
investigations.","899847","2011-10-01","2016-09-30"
"SURFCOMP","Comparing and Analyzing Collections of Surfaces","Yaron Lipman","WEIZMANN INSTITUTE OF SCIENCE LTD","The proposed research program intends to cover all aspects of the problem of learning and analyzing collections of surfaces and apply the developed methods and algorithms to a wide range of scientific data.

The proposal has two parts:

In the first part of the proposal, we concentrate on developing the most basic operators comparing automatically pairs of surfaces. Although this problem has received
a lot of attention in recent years,
and significant progress has been made, there is still a great need for algorithms that are both efficient/tractable and come with guarantees
of convergence or accuracy. The main difficulty in most approaches so far
is that they work in a huge and non-linear search space to compare surfaces; most algorithms resort to gradient descent from an initial guess, risking to find only local optimal solution.
We offer a few research directions to tackle this problem based on the idea of identifying EFFICIENT search spaces that APPROXIMATE the desired optimal correspondence.

In the second part of the proposal we propose to make use of the methods developed in the first part to perform global analysis of, or learn, collections of surfaces. We
put special emphasis on ``real-world'' applications and intend to validate our algorithm on a significant collection, including data-sets such as biological anatomic data-sets and computer graphics' benchmark collections of surfaces. We propose to formulate and construct geometric structures on these collections and investigate their domain specific implications.","1113744","2012-09-01","2017-08-31"
"SURFINK","Functional materials from on-surface linkage of molecular precursors","Dimas Garcia de Oteyza Feldermann","FUNDACION DONOSTIA INTERNATIONAL PHYSICS CENTER","With the advent of self-assembly, increasingly high hopes are being placed on supramolecular materials as future active components of a variety of devices. The main challenge remains the design and assembly of supramolecular structures with emerging functionalities tailored according to our needs. In this respect, the extensive research over the last decades has led to impressive progress in the self-assembly of molecular structures. However, self-assembly typically relies on non-covalent interactions, which are relatively weak and limit the structure’s stability and often even their functionality. Only recently the first covalently bonded organic networks were synthesized directly on substrate surfaces under ultra-high-vacuum, whose structure could be defined by appropriate design of the molecular precursors. The potential of this approach was immediately recognized and has attracted great attention. However, the field is still in its infancy, and the aim of this project is to lift this new concept to higher levels of sophistication reaching real functionality. 
For optimum tunability of the material’s properties, its structure must be controlled to the atomic level and allow great levels of complexity and perfection. Complexity can be reached e.g. with hybrid structures combining different types of precursors. In this project, this hardly explored approach will be applied to three families of materials of utmost timeliness and relevance: graphene nanoribbons, porous frameworks, and donor-acceptor networks. Along the pursuit of these objectives, side challenges that will be addressed are the extension of our currently available chemistry-on-surfaces toolbox by identification of new reactions, optimized reaction conditions, surfaces, and ultimately their combination strategies. A battery of tools, with special emphasis on scanning probe microscopies, will be used to visualize and characterize the reactions and physical-chemical properties of the resulting materials.","1894723","2015-09-01","2020-08-31"
"SURFLINK","MOLECULAR CARPETS ON INSULATING SURFACES: RATIONAL DESIGN OF COVALENT NETWORKS","Sabine Maier","FRIEDRICH-ALEXANDER-UNIVERSITAET ERLANGEN NUERNBERG","Inspired by the possibility to create artificial, three-dimensional covalent organic frameworks, the overall aim of this project is to construct novel two-dimensional (2D), covalently-linked, organic networks in a bottom-up approach on insulating surfaces. 2D materials have unique properties suitable for many scientific and technological applications including nano-electronic devices and sensors. On-surface synthesis of covalent structures is mainly limited to metal surfaces, as controlled growth procedures of molecules on insulators are often hindered by the weak, unspecific interaction with the substrate. We will establish suitable concepts for the covalent linking of molecules on insulators by balancing the molecule-molecule and molecule-surface interactions. That will greatly advance the atomic-scale understanding of molecular structures on insulators. Specially designed molecular building blocks doped with heteroatoms will be used to create functional 2D networks with tunable electronic properties and nanometer-sized pores. Novel concepts will be developed to achieve high quality structures with long-range order; one of the great challenges in all covalently-linked structures. 

The SURFLINK project uses a surface science approach in ultra-high vacuum to understand the fundamental mechanisms and properties of covalently-linked networks at the atomic level. The covalent networks will be studied by high-resolution scanning probe microscopy and spectroscopy at the atomic-scale. We will determine the electronic properties of the novel nano-porous networks that can be tailored by their geometry. The functionalized pores included in the network will be studied with respect to their size and their prospects to adsorb guest molecules. The rational design of the networks proposed in the SURFLINK project has great potential for materials research and will ultimately result in the development of new materials with adjustable electronic properties.","1499994","2015-05-01","2020-04-30"
"SURFPRO","Tuning electronic surface properties by molecular patterning","Meike Stöhr","RIJKSUNIVERSITEIT GRONINGEN","""Inspired by the possibility to create an artificial electronic band structure through the interplay of a molecular nanoporous network with the surface state electrons of a metallic substrate (recently reported by us), the utilization of this new concept for controlling the electronic surface properties of a material as well as establishing understanding of the underlying principles for the observed behavior is the overall aim of this project. The modification of the electronic surface properties also affects the material properties in general, such as conductivity, surface catalysis properties and reflectivity. Thus, the proposed concept has great potential for materials research and will ultimately result in the development of new materials with adjustable electronic properties. Such materials will find applications in e. g. (nano)electronic devices or sensors.

The plan is to make use of supramolecular self-assembly and such, to fabricate nanoporous networks from specially designed molecular building blocks on either metallic substrates having a surface state or graphene. Since both the metallic substrates and graphene feature a quasi free 2D electron gas it is assumed that quantum confinement will appear in the pores of the network leading to confined states. Due to the coupling of these confined states, an artificial electronic band structure is expected to form. Moreover, in the case of graphene the opening of a band gap is expected to occur which is a prerequisite for the implementation of graphene in electronic devices.

With the help of scanning tunneling microscopy and photoelectron spectroscopy measurements the confinement properties of different nanoporous networks will be studied with respect to pore to pore distance, pore diameter, effect of the interplay between intermolecular and molecule substrate interactions, effect of trapping guest molecules in the pores and coupling strength between graphene and its support layer.""","1486061","2013-01-01","2017-12-31"
"SURFSPEC","Theoretical multiphoton spectroscopy for understanding surfaces and interfaces","Kenneth Ruud","UNIVERSITETET I TROMSOE - NORGES ARKTISKE UNIVERSITET","The project will develop new methods for calculating nonlinear spectroscopic properties, both in the electronic as well as in the vibrational domain. The methods will be used to study molecular interactions at interfaces, allowing for a direct comparison of experimental observations with theoretical calculations. In order to explore different ways of modeling surface and interface interactions, we will develop three different ab initio methods for calculating these nonlinear molecular properties: 1) Multiscale methods, in which the interface region is partitioned into three different layers. The part involving interface-absorbed molecules will be described by quantum-chemical methods, the closest surrounding part of the system where specific interactions are important will be described by classical, polarizable force fields, and the long-range electrostatic interactions will be described by a polarizable continuum. 2) Periodic-boundary conditions: We will extend a response theory framework recently  developed in our group to describe periodic systems using Gaussian basis sets. This will be achieved by deriving the necessary formulas, and interface our response framework to existing periodic-boundary codes. 3) Time-domain methods: Starting from the equation of motion for the reduced single-electron density matrix, we will propagate the electron density and the classical nuclei in time in order to model time-resolved vibrational spectroscopies.

The novelty of the project is in its focus on nonlinear molecular properties, both electronic and vibrational, and the development of computational models for surfaces and interfaces that may help rationalize experimental observations of interface phenomena and molecular adsorption at interfaces. In the application of the methods developed, particular attention will be given to nonlinear electronic and vibrational spectroscopies that selectively probe surfaces and interfaces in a non-invasive manner, such as SFG.","1498500","2011-09-01","2016-08-31"
"SUSCATCU3","Sustainable C-X and C-H Functionalization Catalyzed by Copper(III) Species","Xavier Ribas Salamana","UNIVERSITAT DE GIRONA","On the basis of recent PI’s findings, this project aims to take advantage of the Cu(I)/Cu(III) redox pair chemistry to perform a wide range of copper catalyzed organic transformations under very mild conditions, i.e. aryl-heteroatom cross coupling, aryl-halide exchange, aryl fluorination, and direct C-H functionalization reactions. The development of new sustainable methodologies alternative to Pd-based ones will make a tremendous impact into routine organic synthesis procedures and into selective late-stage modification of pharmaceuticals. Pd is the metal of choice in most of aryl-heteroatom coupling reactions but toxicity and intrinsic cost are serious drawbacks for production of drugs. Cu has become a real alternative to Pd due to low cost, low toxicity and continuously increasing efficiencies of Cu-catalyzed coupling reactions. However, the mechanistic details for Cu-based processes are still poorly understood and experimental conditions are far from sustainable. Fundamental Cu(I)/Cu(III) oxidative addition and reductive elimination steps are often invoked but remained unobserved until the recent well-defined aryl-Cu(III)-halide key species reported by the PI. An initial goal consists in the in-depth mechanistic comprehension of Cu(I)/Cu(III) redox steps in a series of electronically and structurally tuned aryl-halide model systems. A subsequent goal will be the exploration of tridentate pincer-like systems in aryl-heteroatom coupling reactions with simple aryl-halides, aiming to find milder and efficient catalysts by redirecting mechanistic pathways towards the stabilization of aryl-Cu(III) species. Exploitation of Cu(I)/Cu(III) redox chemistry in direct C-H functionalization will also be undertaken. Exploration of analogous M(I)/M(III) redox chemistry with Au and Ag will be performed in order to gain a complete mechanistic picture of these reactions mediated by coinage metals, building on the necessary knowledge to design future generations of catalysts.","1489780","2011-12-01","2017-11-30"
"SusDrug","Sustainable Approach to Drug Discovery","David SARLAH","UNIVERSITA DEGLI STUDI DI PAVIA","Modern drug discovery is facing critical challenges. Rapid advances in human biology are revealing new biomolecular targets and processes, for which existing chemical compound libraries can provide only limited success in the identification of novel bioactive agents. This deficiency has been attributed primarily to the relative lack of structural diversity within the libraries. The three-dimensional world of biological macromolecules has been continuously interrogated with generally similar planar, aromatic, and structurally simple compounds. Contemporary diversity-generating methods have never been implemented for the preparation of large libraries, as an increase in the number of diverse members requires a corresponding increase in the number of synthetic steps, or a continuous supply of different staring materials. This proposal details a strategy for developing a chemically sustainable diversification method, by tapping into our largest source of organic compounds: arenes.
The proposed research aims to develop new methods that can rapidly convert simple aromatic entities into highly functionalized, complex small molecules. By integration of this strategy with many different chemical operations, numerous distinctive and independent dearomative programs will generate a diverse set of multiplex small molecules. This simplicity-to-complexity approach will provide a practical platform for the rapid, controlled access to a functionally diverse set of compounds, ranging from anticancer to anti-infective agents. This research will also deliver methods for dearomative diversification of existing aromatic compound libraries to provide new members with unique physiochemical properties. Given the broad scope of possible dearomative programs that will be developed, and the vast amount of aromatic compounds accessible, this will ultimately provide a sustainable source of diverse molecules for the next generation of compound libraries.","1400000","2019-07-01","2024-06-30"
"SUSYBREAKING","Supersymmetry Breaking in String Theory","Joseph Conlon","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","""This project will study supersymmetry breaking directly within string theory. Most studies of supersymmetry breaking, in both phenomenology and cosmology, are performed using the crutch of four dimensional field theory. This project will raise the state of the art and remove this restriction.

I will develop tools to study supersymmetry breaking directly on the worldsheet and aim to do this for both phenomenological models of low-energy supersymmetry breaking and also for cosmological supersymmetry breaking, applicable  during the inflationary epoch in the early universe.

Once these tools are developed they will be applied to search for novel effects of string theory in the early universe and also to calculate the phenomenology of string models of low energy supersymmetry breaking.""","1027855","2012-10-01","2017-09-30"
"SVIS","Supervised Verification of Infinite-State Systems","Sharon Shoham Buchbinder","TEL AVIV UNIVERSITY","Modern society relies more and more on computing for managing complex safety-critical tasks (e.g., in medicine, avionics, economy). Correctness of computerized systems is therefore crucial, as incorrect behaviors might lead to disastrous outcomes. Still, correctness is a big open problem without satisfactory theoretical or practical solutions. This has dramatic effects on the security of our lives. The current practice in industry mainly employs testing which detects bugs but cannot ensure their absence. In contrast, formal verification can provide formal correctness guarantees. Unfortunately, existing formal methods are still limited in their applicability to real world systems since most of them are either too automatic, hindered by the undecidability of verification, or too manual, relying on substantial human effort. This proposal will introduce a new methodology of supervised verification, based on which, we will develop novel approaches that can be used to formally verify certain realistic systems. This will be achieved by dividing the verification process into tasks that are well suited for automation, and tasks that are best done by a human supervisor, and finding a suitable mode of interaction between the human and the machine. Supervised verification represents a conceptual leap as it is centered around automatic procedures but complements them with human guidance; It therefore breaks the classical pattern of automated verification, and creates new opportunities, both practical and theoretical. In addition to opening the way for developing tractable verification algorithms, it can be used to prove lower and upper bounds on the asymptotic complexity of verification by explicitly distilling the human's role. The approaches developed by this research will significantly improve system correctness and agility. At the same time, they will reduce the cost of testing, increase developers productivity, and improve the overall understanding of computerized systems.","1499528","2018-04-01","2023-03-31"
"SWARM","Empirical analysis and theoretical modelling of self-organized collective behaviour in three-dimensions: from insect swarms and bird flocks to new schemes of distributed coordination","Irene Rosana Giardina","CONSIGLIO NAZIONALE DELLE RICERCHE","Animal groups represent paradigmatic cases of self-organized collective behaviour, where global coordination arises from local rules of interaction between individuals. A major issue, both for theoretical studies and technological applications, is to understand how self-organization emerges within a system with distributed intelligence.
SWARM aims at providing new knowledge about self-organization and collective behaviour in 3D animal aggregations. To do that, SWARM will export concepts and methods from physics, and will integrate empirical work, data analysis and theoretical modelling. In particular, SWARM will:
i) Perform field experiments on large insect swarms and bird flocks and retrieve individual 3D coordinates and trajectories.
ii) Perform a statistical characterization of swarming/flocking behaviour; obtain information on the interactions  between group members, and on the rules followed by individuals.
iii) Develop empirically based models of 3D animal collective behaviour and design efficient, biologically inspired, algorithms of distributed coordination.","1124000","2010-11-01","2015-10-31"
"SWEETBULLETS","Sweet Theranostics in Bitter Infections - Seek and Destroy","Alexander Walter Titz","HELMHOLTZ-ZENTRUM FUR INFEKTIONSFORSCHUNG GMBH","Bacterial infections are now a global threat demanding novel treatments due to the appearance of resistances against antibiotics at a high pace. The ESKAPE pathogens are those with highest importance in the EU and chronic infections due to biofilm formation are a particular task. Noninvasive pathogen-specific imaging of the infected tissue is not clinically available. Its successful implementation will enable the choice of appropriate therapy and boost efficacy. Furthermore,
Gram-negative bacteria have a highly protective cellular envelope as an important resistance mechanism for drugs acting intracellularly, resulting in an alarmingly empty drug-pipeline.
To overcome this gap, I will establish Lectin-directed Theranostics targeting pathogens via their extracellular carbohydrate-binding proteins at the site of infection for specific imaging and treatment. This will be implemented for the highly resistant ESKAPE pathogen Pseudomonas aeruginosa through 3 different work packages.
WP1 Sweet Imaging: Design & conjugation of lectin-directed ligands to imaging probes, Optimization of ligand/linker, in vivo proof-of-concept imaging study.
WP2 Sweet Targeting: Delivery of antibiotics to the infection through covalent linking of lectindirecting groups. Employing different antibiotics, assessment of bactericidal potency and targeting efficiency. Manufacturing of nano-carriers with surface exposed lectin-directed ligands, noncovalent charging with antibiotics. In vitro and in vivo targeting.
WP3 Sweet SMART Targeting: Conjugates as SMART drugs: specific release of anti-biofilm lectin inhibitor and drug cargo upon contact with pathogen, development of linkers cleavable by pathogenic enzymes.
SWEETBULLETS will establish fundamentally novel lectin-directed theranostics to fight these deleterious infections and provide relief to nosocomially infected and cystic fibrosis patients. It is rapidly extendable towards other ESKAPE pathogens, e.g. Klebsiella spp..","1499551","2017-02-01","2022-01-31"
"SWIFT","Surface Plasmon-Based Wifi for Nanoscale Optical Information Transport - SWIFT","Alexandre Yves Jean Bouhelier","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""This proposal focuses on the design, fabrication, characterization and optimization of novel groundbreaking communication nano-devices. SWIFT proposes resolutely innovative concepts adopting metal-based optical nano-antennas as a disruptive technological vehicle. Nanoscale electronics and photonics exploit novel fascinating physical phenomena and are among the most promising research areas for providing functional nano-components for data transfer and processing. The aim of this proposal is to interface these two device-generating technologies to create the first electrically-driven nanoscale optical antenna transceiver. The concept will enable electron/photon transduction at the nanoscale by a unique surface plasmon-assisted metal-based design, a significant leap at the forefront of research in nanoelectronics and nanophotonics. SWIFT proposes a series of fundamental advances motivated by application-driven perspectives that will push the burgeoning field of optical antenna to a new area. Deploying optical antenna transceivers enables a paradigm shift in optical interconnects and communication at ultimate device densities through the following innovations:
• Development a whole new class of plasmon-assisted transducing optical functional nanodevices.This unique concept addresses the development for ultracompact nanocomponents.
• Prototyping self-sustained plasmonic in/out electrical ports on SPP waveguiding platforms, removing thus complex optical interfacing that cannot be miniaturized.
• Pioneering a technological breakthrough enabling nanoscale wireless broadcasting of optical information.
• Using these functionalities, we will prospect new research directions by proviing a unique ground for (i)  generating ultrafast electron surges in an integrated electronic layout enabling ultrafast transport studies in molecular electronics and (ii) for  realizing ultrasmall THz sources enabling thus penetration of THz technology at the nanometer-scale.""","1495289","2013-02-01","2018-01-31"
"SwitchProteinSwitch","Engineering protein switches: sensors and regulators for biology and diagnostics","Maarten Merkx","TECHNISCHE UNIVERSITEIT EINDHOVEN","Proteins that switch between distinct conformational states are ideal to monitor and control molecular processes within the complexity of living cells. Inspired by the modular design of natural signaling proteins, we are pursuing a true engineering approach towards the development of protein switches which we call ‘plug-and-play’ protein engineering. In this approach we explore generic strategies to translate molecular recognition in an input protein domain directly into a readable signal generated by an output protein domain.
A comprehensive research program is proposed that explores 4 new design concepts for protein switches:
1. FRET 3.0. A generic design concept for genetically encoded fluorescent sensors that does not depend on ligand-induced conformational changes in a receptor domain.
2. FRET-bodies. Integration of FRET sensors and antibody technology. Libraries of FRET sensor proteins displaying CDR3-type loops on the donor and acceptor domains are displayed on yeast, allowing efficient high-throughput screening using FACS.
3. ELISA in solution. An innovative and generic approach to translate antibody binding directly into an enzymatic activation step is proposed that takes advantage of the unique structural properties of antibodies
4. Light-responsive protein switches. Ligand binding proteins will be developed whose affinity can be reversibly controlled by photoresponsive protein domains.
An integral aspect of the research program is to combine rational design and directed evolution. Apart from developing generic engineering concepts for specific application areas, modeling tools will be developed that allow quantitative analysis and prediction of conformational stabilities for modular protein switches. The availability of these robust and generally applicable engineering strategies will proof beneficial to many areas of life sciences, providing essential tools for intracellular imaging, synthetic biology, and molecular diagnostics.","1999883","2012-01-01","2016-12-31"
"SYLO","Spin dynamics and transport at the quantum edge in low dimensional nanomaterials","Ferenc Simon","BUDAPESTI MUSZAKI ES GAZDASAGTUDOMANYI EGYETEM","Sustainable development in information technology calls for an ever increasing information processing and storage capability. A promising route to maintain exponential growth capability, i.e. to keep on the Moore's roadmap, is to turn to the electron spins as information carriers rather than their charge. This field, spintronics, has enormous potential whose exploitation requires solid knowledge in the fundamentals of spin dynamics and spin transport. Herein, novel nanomaterials are suggested for spintronics purposes, such as graphene and single-wall carbon nanotubes (SWCNTs). These, fundamental two- and one-dimensional carbon allotropes are promising candidates for such purposes, carbon being a light element with a low spin-orbit coupling which results in a long spin coherence. There are several fundamental open issues, e.g. the dominant spin orbit coupling mechanism in graphene, whether bulk electron spin resonance can be observed for this material, and the length of the spin diffusion length. For SWCNTs, the ground state of isolated metallic tubes is known to be the Tomonaga-Luttinger liquid (TLL), which greatly limit the spin coherence, but it is at present open whether this state is destroyed when an ensemble of interacting metallic tubes is studied. The decay time and spin symmetry of optical excitations (excitons) in semiconducting SWCNTs is yet unknown.

Our goal is to pursue electron spin resonance in graphene and carbon nanotubes and to perform optically detected magnetic resonance in carbon nanotubes. We will commission a magnetoptical spectrometer with a substantial added value.

The expected results are characterization of spin transport capabilities of these materials and understanding of the spin decoherence mechanisms. The PI leads magnetic resonance studies of these materials, shown by his more than 300 citations to this field (the total being over 470) and his 15 Physical Review Letters papers in this field (of which for 9 he is main Author).","1230000","2010-11-01","2015-10-31"
"SYMCAR","Symbolic Computation and Automated Reasoning for Program Analysis","Laura Kovacs","TECHNISCHE UNIVERSITAET WIEN","Individuals, industries, and nations are depending on software and systems using software. Automated approaches are needed to eliminate tedious aspects of software development, helping software developers in dealing with the increasing software complexity. Automatic program analysis aims to discover program properties preventing programmers from introducing errors while making changes and can drastically cut the time needed for program development.
This project addresses the challenge of automating program analysis, by developing rigorous mathematical techniques analyzing the logically complex parts of software. We will carry out novel research in computer theorem proving and symbolic computation, and integrate program analysis techniques with new approaches to program assertion synthesis and reasoning with both theories and quantifiers. The common theme of the project is a creative development of automated reasoning techniques based on our recently introduced symbol elimination method. Symbol elimination makes the project challenging, risky and interdisciplinary, bridging together computer science, mathematics, and logic.
Symbol elimination will enhance program analysis, in particular by generating polynomial and quantified first-order program properties that cannot be derived by other methods. As many program properties can best be expressed using quantified formulas with arithmetic, our project will make a significant step in analyzing large systems. Since program analysis requires reasoning in the combination of first-order logic and theories, we will design efficient algorithms for automated reasoning with both theories and quantifiers. Our results will be supported by the development of world-leading tools supporting symbol elimination in program analysis.
Our project brings breakthrough approaches to program analysis, which, together with other advances in the area, will reduce the cost of developing safe and reliable computer systems used in our daily life.","1500000","2016-04-01","2021-03-31"
"SYMPLECTIC","Symplectic Measurements and Hamiltonian Dynamics","Yaron Ostrover","TEL AVIV UNIVERSITY","Symplectic geometry combines a broad spectrum of interrelated disciplines lying in the mainstream of modern mathematics. The past two decades have given rise to several exciting developments in this field, which introduced new mathematical tools and opened challenging new questions. Nowadays symplectic geometry reaches out to an amazingly wide range of areas, such as differential and algebraic geometry, complex analysis, dynamical systems, as well as quantum mechanics, and string theory. Moreover, symplectic geometry serves as a basis for Hamiltonian dynamics, a discipline providing efficient tools for modeling a variety of physical and technological processes, such as orbital motion of satellites (telecommunication and GPS navigation), and propagation of light in optical fibers (with significant applications to medicine).

The proposed research is composed of several innovative studies in the frontier of symplectic geometry and Hamiltonian dynamics, which are of highly significant interest in both fields. These studies have strong interactions on a variety of topics that lie at the heart of contemporary symplectic geometry, such as symplectic embedding questions, the geometry of Hofer’s metric, Lagrangian intersection problems, and the theory of symplectic capacities. 
My research objectives are twofold. First, to solve the open research questions described below, which I consider to be pivotal in the field. Some of these questions have already been studied intensively, and progress toward solving them would be of considerable significance. Second, some of the studies in this proposal are interdisciplinary by nature, and use symplectic tools in order to address major open questions in other fields, such as the famous Mahler conjecture in convex geometry. My goal is to deepen the connections between symplectic geometry and these fields, thus creating a powerful framework that will allow the consideration of questions currently unattainable.","1221921","2015-03-01","2020-02-29"
"SYMPTEICH","Towards symplectic Teichmueller theory","Ivan Smith","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","""Over the last decade, homological algebra has entered symplectic topology, largely thanks to the appearance of Fukaya categories in homological mirror symmetry. Applications of these new methods and ideas are still scarce. We propose a fundamentally new approach to studying symplectic dynamics, by studying the action of the symplectic mapping class group on the complex manifold of stability conditions on its Fukaya category. This can be seen as a first attempt to generalise classical Teichmueller theory to higher-dimensional symplectic manifolds. Many invariants arising in low-dimensional topology, including Khovanov cohomology for knots, are governed by the Fukaya categories of associated moduli spaces. We propose an """"uncertainty principle"""" in topology, in which these invariants are intrinsically constrained by rigidity of this underlying categorical structure. Besides applications in topology, this suggests a framework for studying the sense in which topological complexity is the shadow of dynamical complexity.""","650000","2008-07-01","2013-06-30"
"SYNINTER","Smart interrogation of the immune synapse by nano-patterned and soft 3D substrates","Kheya Sengupta","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""We aim to design smart substrates and suitable detection techniques to understand better the dynamics and spatial organization found in the immunological synapse, with the ultimate goal of developing new diagnostic tools for sensitive detection of immune deficiency diseases that arise from faulty adhesion. The immunological synapse (IS), formed at the interface between a T-lymphocyte and an antigen presenting cell, has been the target of intense multidisciplinary research in the last decade. Studies point to a crucial role for adhesion mediated by protein clusters for the stability and activity of the synapse. However, even the cluster size - micro or nano scale - remains contentious. Furthermore, while in vivo, the synapse is formed in a soft 3D environment, most in vitro experiments are on hard 2D surfaces. Clearly, one way to probe how the micro/nano environment of the T-cell influences the IS is by interrogating it with artificial substrates that are soft, three dimensionally structured and exhibit motifs down to the cluster length-scale. We shall develop 3D and soft polymeric structures with controlled placement of adhesion molecules and antigens on a single molecule level. The structure, assembly and signalling for stable as well as dynamic IS, on such substrates, will be investigated. Mechano-transduction at IS will be probed by using soft substrates of tunable Youngs modulus. Advanced optical techniques will be developed for quantitative and dynamic mapping of proteins and the cell-cell interface topography. Quantitative reflection interference contrast microscopy, will permit characterization of adhesion of native cells without the need of a special labelling strategy. Our advanced substrates and observation techniques will open up new ways to probe inter-cellular adhesion in general and the immunological synapse in particular. The acquired knowledge will be used for fabricating a cell sensor device for diagnosing T-cell pathology.""","1133565","2013-01-01","2017-12-31"
"SynPhos","Highly-Reactive (Regenerative) Phosphorus Building Blocks -
New Concepts in Synthesis","Jan J. Weigand","TECHNISCHE UNIVERSITAET DRESDEN","Fundamental research is key important to allow us to find new, economically and ecologically attractive ways to meet current challenges. The current depletion of available phosphorus resources is a concern. Unlike oil, which is lost once used, phosphorus can be recovered and used over and over again or at least transformed into other compounds of chemical use. The intention of the present research proposal is to contribute to the field of synthetic chemistry both, inorganic and organic,  by identifying and developing highly-reactive phosphorus building blocks that can be potentially regenerated. We would like to enter new avenues of phosphorus chemistry which will address fundamental questions and develop new applications. Using novel and powerful phosphorus reagents, new concepts for more efficient, selective and sustainable synthetic procedures will be developed. We also seek greener and more efficient processes and, whenever possible, to recover the phosphorus after the reaction. Phosphorus-based compounds will also be used in the recovery of industrial waste by-products such as phosphane oxides and depleted UF6, and will therefore have a positive impact on certain chemical industries and the environment.

With our proposal we aim to address the following aspects:

1) Synthesis of N-heterocyclic pnictanes (Pn = P, As, Sb, Bi with emphasis on P) and related cationic derivatives
2) Exploration of N-heterocyclic phosphanes as reagents in organic synthesis.
3) Preparation of catenated, branched and cyclic polyphosphanes via a novel method for P–P bond     formation – access to
novel polyphosphorus-based ligand systems.
4) Alternative processes for the recycling of UF6 and related derivatives to access novel precursors for uranium chemistry
and to recover fluorine based products of high economic value.","1422000","2013-03-01","2018-02-28"
"SYNTECH","Synthesis Technologies for Reactive Systems Software Engineers","Shahar Maoz","TEL AVIV UNIVERSITY","The design and development of open reactive systems, which compute by reacting to ongoing stimuli from their environment, and include, for example, mobile applications running on smart phone devices, web-based applications, industrial robotic systems, embedded software running on chips inside cars and aircraft, etc., is a complex and challenging task. Despite advancement from low-level assembly languages to higher-level languages with powerful abstraction mechanisms, and the use of automated testing and formal verification, reactive systems software development is still a mostly manual and error-prone iterative activity of coding and debugging.
A fundamentally different alternative approach to reactive systems development is synthesis, the automatic creation of correct-by-construction software from its specification.  Synthesis has the potential to transform the way open reactive systems software is developed, making the process more effective and productive, and making its results more reliable and usable. However, while important advancements have been recently made on the algorithmic aspects of synthesis, no work has yet taken advantage of these achievements to change software engineering practices from “program centric” to “specification centric”. No effective end-to-end means to use synthesis are available to engineers, and the potential revolutionary impact of synthesis on the engineering of reactive systems software is far from being fully explored. 
 The proposal targets four objectives: a new, rich specification language, tailored for synthesis and for use by software engineers; a set of new methods for specification centric development; tool implementations in ‘killer app’ application domains; and systematic evaluation with engineers. 
The research aims to unleash and evaluate the potential of synthesis to revolutionize reactive systems software development and to open the way for new directions in software engineering research and practice.","1477000","2015-04-01","2020-09-30"
"T2KQMUL","Study of Neutrino Oscillations at T2K","Francesca Di Lodovico","QUEEN MARY UNIVERSITY OF LONDON","Neutrinos are among the most intriguing, least understood and probably most abundant particles in the Universe. The most exciting discovery of the last decade is that different neutrino types do change their identity as they propagate, suggesting that they are not strictly massless as required by the Standard Model of Particle Physics. Improving our understanding of neutrinos, which is one of the main priorities of the European strategy for particle physics, has the potential to unravel some of the deepest and long-standing issues of particle and astrophysics, such as the origin of matter, the origin of heavy elements, the nature of dark energy, etc. In this project, I aim to exploit data recorded by the T2K experiment in Japan, resulting from the highest intensity muon neutrino beam in the world. T2K will start taking data in 2009 and will run for 10 years, with an upgrade after 5 years. It is an international collaboration of 62 institutions, with European countries undertaking a critical and strategic role in its design and construction. I aim to utilise a new method, which exploits the excellent T2K charged and neutral particle identification, to look for evidence of oscillations of muon into electron neutrinos, which I believe is the most exciting measurement to be made in the near future. A non-zero oscillation value could hint at CP violation in the neutrino sector, which could explain the current matter-antimatter asymmetry in the Universe. In addition, I intend to search for the presence of sterile neutrinos, which are candidates for dark matter in the Universe. I also intend to improve the neutral current pi0 cross-section measurement, which is an important background in neutrino oscillations. The project will conclude with a global model-dependent fit to the existing world-wide measurements in neutrino physics. To carry out this project, I request funds for a Research Associate, two PhD students, my research time, travel and computing equipment.","950000","2009-03-01","2014-12-31"
"TagIt","A Minimal-Tag Bioorthogonal Labelling Approach to Protein Uptake, Traffic and Delivery","Gonçalo- Jose Bernardes","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The ability to probe dynamic cellular events that involve disease-associated proteins is limited, to a large extent, by the lack of development of a strategy that can use small coupling partners to react in chemoselective fashion with rapid kinetics that does not interfere with biological function(s) and localisation. In this application, I describe a conceptually new bioorthogonal-labelling approach that combines the introduction of a minimal non-canonical amino acid with chemoselective reactions, which display rapid kinetics, to label proteins in live cells. The small size of the new alkene-tagged amino acids, which will be genetically encoded, should not interfere with the protein’s innate structure, function(s) or localisation. Site-selective bioorthogonal labelling will be achieved through the use of a new photo-triggered [2+2] cycloaddition reaction with an alkene-bearing fluorophore and the known inverse-electron-demand Diels-Alder reaction with a fluorogenic tetrazine. Although the former offers potentially improved spatial and temporal resolution, the latter allows for turn-on fluorescence. The proposed new methodology will be applied in the context of a key cytokine involved in cancer progression. The ability to label this cytokine with minimal perturbation of its structure, function(s) and localisation will enable monitoring of its internalisation and intracellular trafficking pathways in cells that overexpress its receptor. In doing so, new insight into cancer biology will be generated that will inform the construction of safer, selective and more efficient protein-drug conjugates for targeted cancer treatment. The concept proposed here is designed to be generally applicable to label and study disease-associated proteins that are difficult to access by means of conventional protein-labelling methods and constitute the first integrated, interdisciplinary approach for the development of protein drug-conjugates.","1499106","2016-10-01","2021-09-30"
"TAIAC","Breaking the paradigm: A new approach to understanding and controlling combustion instabilities","Nicholas Worth","NORGES TEKNISK-NATURVITENSKAPELIGE UNIVERSITET NTNU","It is well known that current and future low-emission combustion concepts for gas turbines are prone to thermoacoustic instabilities. These give rise to large pressure fluctuations that can drastically reduce the operable range and threaten the structural integrity of stationary gas turbines and aero engines. In the last 6 years the development of laboratory-scale annular combustors and high-performance computing based on Large Eddy Simulations (LES) have been able to reproduce thermoacoustic oscillations in annular combustion chambers, giving us unprecedented access to information about their nature.

Until now, it has been assumed that a complete understanding of thermoacoustic instabilities could be developed by studying the response of single axisymmetric flames. Consequently stability issues crop up far into engine development programmes, or in service, because we lack the knowledge to predict their occurrence at the design stage. However, the ability to experimentally study thermoacoustic instabilities in laboratory-scale annular combustors using modern experimental methods has set the stage for a breakthrough in our scientific understanding capable of yielding truly predictive tools.

This proposal aims to break the existing paradigm of studying isolated flames and provide a step change in our scientific understanding by studying thermoacoustic instabilities in annular chambers where the full multiphysics of the problem are present. The technical goals of the proposal are: to develop a novel annular facility with engine relevant boundary conditions; to use this to radically increase our understanding of the underlying physics and flame response, paving the way for the next generation of predictive methods; and to exploit this understanding to improve system stability through intelligent design. Through these goals the proposal will provide an essential bridge between academic and industrial research and strengthening European thermoacoustic expertises.","1929103","2016-09-01","2021-08-31"
"Tamed Cancer","Personalized Cancer Therapy by Model-based Optimal Robust Control Algorithm","Levente Adalbert Kovács","OBUDAI EGYETEM","Imagine if tumor growth would be reduced and then kept in a minimal and safe volume in an automated manner and in a personalized way, i.e. cancer drug would be injected using a continuous therapy improving the patient’s quality of life.
By control engineering approaches it is possible to create model-based strategies for health problems. Artificial pancreas is an adequate example for this, where by continuous glucose measurement device and insulin pump it is possible to improve diabetes treatment. Gaining expertise from this problem, the current proposal focuses on taming the cancer by developing an engineering-based medical therapy.
The interdisciplinary approach focuses on modern robust control algorithm development in order to stop the angiogenesis process (i.e. vascular system development) of the tumor; hence, to stop tumor growth, maintaining it in a minimal, “tamed” form. This breakthrough concept could revitalize cancer treatment. It is the right time to do it as some investigations regarding tumor growth modeling have been already done; now, it should be refined by model identification tools and validated on animal trials. The benefit of robust control was already demonstrated in artificial pancreas; hence, it could be adapted to cancer research. The result could end with a personalized healthcare approach for drug-delivery in cancer, improving quality of life, optimizing drug infusion and minimizing treatment costs. This interdisciplinary approach combines control engineering with mathematics, computer science and medical sciences.
As a result, the model-based robust control approach envisage refining the currently existing tumor growth modeling aspects, design an optimal control algorithm and extend it by robust control theory to guarantee its general applicability. Based on our research background, validation will be done first in a manually controlled way, but then in an automatic mode to propose it for further human investigations.","1015900","2016-07-01","2021-06-30"
"TAPEASE","Theory and Practice of Advanced Search and Enumeration","Petteri Samuel Kaski","AALTO KORKEAKOULUSAATIO SR","Computer science is permeated with canonical hard problems that, a priori, have a fundamentally combinatorial structure, such as the graph coloring problem, the Steiner tree problem, the Hamiltonian cycle problem, the k-clique problem, and so forth. Accordingly, it would perhaps be quite reasonable to expect that currently the asymptotically fastest solution techniques would rely on the canonical combinatorial algorithms toolbox, such as carefully tailored combinatorial (backtrack/branching) search and case-by-case analysis, combined with, say, advanced data structures.
However, this is not the case.

Indeed, currently the fastest known exact/parameterized algorithms for each of the aforementioned problems (and beyond) rely on a mixed bag of advanced _algebraic_ techniques ranging from fast matrix multiplication to sieving e.g. via Möbius inversion and polynomial identity testing. This, in essence, signals that the development of systematic algorithmic principles and tools to cope with exponential-sized combinatorial spaces associated with hard search and enumeration problems is rather in its infancy. The proposed project aims to improve our understanding how such spaces can be systematically transformed and filtered using advanced algebraic and combinatorial techniques.
The results of the project are of foremost interest in fundamental research in improving our understanding of computation, but potential exists also for breakthroughs that affect the computing practice, for example in connection with specific canonical tasks such as matrix multiplication or frontier applications such as motif problems in bioinformatics.","1145078","2014-02-01","2019-01-31"
"TAQ","Taming non-equilibrium quantum systems","Jens Eisert","FREIE UNIVERSITAET BERLIN","Complex quantum systems out of equilibrium are not only at the basis of some of the most intriguing puzzles in physics, they also allow for new applications in quantum technologies. Equipped with a portfolio of innovative methods, this proposal makes a concerted and focused effort to tackle some of the difficult questions on strongly correlated systems out of equilibrium. At the same time it suggests new modes of quantum technologies by intrinsically exploiting notions of openness and non-equilibrium. The proposed work will be structured according to four methodologically intertwined objectives: 1. A fresh attempt will be made at solving long-standing questions of equilibration and thermalization of interacting quantum many-body systems. Precise conditions will be given under which thermalization provably does, or does not, happen. 2. It will be shown that notions of exactly timed, controlled and protected quantum information processing are possible not despite of, but because of quantum many-body systems undergoing non-equilibrium dynamics and dissipation. This research will employ innovative and highly unorthodox Markov chain mixing tools, applied to the quantum domain for the first time. 3. Based on new insights on the entanglement structure of systems in non-equilibrium, new algorithms will be proposed that promise to overcome the road block of numerically simulating long time dynamics being prohibitively difficult, and which are suitable for simulating the evolution of quantum fields. 4. Experimental progress in non-equilibrium dynamics will slow down dramatically unless new probing techniques are developed. New paths will be taken to achieve efficient dynamical system identiﬁcation, based on novel paradigms of quantum compressed sensing. This high-risk, high-gain research promises truly ground breaking results on long-standing fundamental problems as well as on innovative applications in quantum technologies.","1234020","2013-02-01","2019-01-31"
"TDMET","Time-resolving electron dynamics in molecules by time-dependent many-electron theory","Lars Bojer Madsen","AARHUS UNIVERSITET","The interaction of atoms and molecules with new light sources such as attosecond and free-electron lasers is under strong current experimental investigation. Within the next few years the interest will shift from relatively simple systems with a few atoms and electrons to bigger systems with many atoms and may electrons. The aims will be to study time-resolved dynamics and chemical reactions on the natural timescales for these processes. To fulfill this ambitious goal, there will be a strong need for the development of new theory to guide the experiments and to analyze and understand the results. Currently there is no satisfactory theory in this research area that can treat more than the nonperturbative response of a single electron in a model potential. It is the purpose of the present project to develop such theory.","1330305","2011-12-01","2016-11-30"
"TDRFSP","Time-Domain RF and Analog Signal Processing","Robert Staszewski","UNIVERSITY COLLEGE DUBLIN, NATIONAL UNIVERSITY OF IRELAND, DUBLIN","""One of the most important developments in the communication microelectronics in the last decade was the invention and popularization of “Digital RF”. It transforms the radio frequency (RF) analog functionality of a wireless transceiver into digitally-intensive implementations that operate in time-domain. They are best realized in mainstream nanometer-scale CMOS technologies and easily integrated with digital processors. As a result, RF transceivers based on this new approach now enjoy significant benefits. Consequently, the RF transceivers based on this architecture are now the majority of the 1.5 billion mobile handsets produced annually.

The invention and development of “Digital RF” was pioneered in the last decade by this applicant at Texas Instruments in Dallas, Texas, USA. Despite having published over 130 scientific papers,  that industrial research focus has been mainly limited to the highest volume segment of the wireless communications market: low-cost GSM/EDGE cellular phones and Bluetooth radios. Unfortunately, that low-cost low-data-rate market segment has already reached the saturation. The fastest growing segments of the wireless communications are now: high-data-rate “smart phones”, ultra-low-power wireless sensor network devices, antenna-array and millimeter-wave transceivers, where the original “Digital RF” approach could not be readily exploited.

The goal of this proposal is to revisit and exploit the fundamental theory of the time-domain operation of RF and analog circuits. This way the broad area of the wireless communications, as well as analog and mixed-signal electronics in general, can be transformed for the ready realization in the advanced CMOS technology. This is expected to revolutionize the entire research field to even a larger extent than the “Digital RF” breakthrough in low-cost low-data-rate radios pioneered by this applicant in the last decade.""","1497000","2012-09-01","2017-08-31"
"TDSTC","Time Dependent String Theories and Cosmology","Nissan Itzhaki","TEL AVIV UNIVERSITY","Great progress was made in recent years on two different fronts. Experimentally we have improved our understanding of cosmology to the level that we now have a standard model of cosmology. Perhaps the most exciting aspect of this model is the realization that most of the energy in the universe is made out of dark matter and dark energy which are not well understood. On the theoretical front string theory has been developed to the level that we now understand many non-perturbative aspects of the theory. This progress offers new ways of making contact between string theory and our four dimensional real world in general and cosmology in particular. The objective of this proposal is to take advantage of this progress and to improve our understanding of string theory in time dependent situations. We are hopeful that such a progress could lead to a {\it precise} realization in string theory of the standard model of cosmology in general, and of dark matter, dark energy and inflation in particular.","441116","2008-09-01","2012-08-31"
"TEMPORE","Self-Regulating Porous Nano-Oscillators: from Nanoscale Homeostasis to Time-Programmable Devices","Marco FAUSTINI","SORBONNE UNIVERSITE","Living systems exhibit unique autonomous behaviors such as homeostasis, self-regulation or spontaneous oscillations, not existing in conventional materials. Designing artificial systems with life-like functionalities is a long-standing challenge in chemistry and material science. This groundbreaking research field has been developed exclusively at the molecular and supramolecular level, through chemical self-regulation based on interconnected networks of reactions in solution.

In this project, I will explore a conceptually new and different approach based on interconnected nanomaterials in open atmosphere; I will design a new family of autonomous systems, called porous Nano-Oscillators, exhibiting a “physical” self-regulation mechanism at the nanoscale. To do so, I will engineer nanoparticles, nanoporous materials and light in a very specific way in order to activate artificial feedback loops; self-oscillatory behavior will be time-programmed by exploiting the sorption dynamics of the nanoporous materials.

I will exploit a multidisciplinary approach based on nanochemistry, nanofabrication and optics to fabricate isolated and groups of nano-oscillators and to investigate their dynamic behaviors. By analogy with cells, communication, synchronization and collective response will be investigated by a new methodology able to describe the spatiotemporal evolutions of self-oscillating nano-objects in controlled environments. Themo-optical simulations will support the experimental work by providing thermodynamic and kinetic guidelines. 

Inspired by examples from nature, I will provide proof-of-concept of time-programmable, autonomous devices, working in open atmosphere with unprecedented functionalities.","1496225","2018-10-01","2023-09-30"
"TENSORNETSIM","Accurate simulations of strongly correlated systems with tensor network methods","Philippe- Roger Corboz","UNIVERSITEIT VAN AMSTERDAM","This project aims to achieve ground-breaking advances in the biggest challenge in computational condensed matter physics: the accurate simulation of strongly correlated systems (SCS), which give rise to fascinating phenomena such as high-Tc superconductivity (HTSC), quantum spin liquids with topological order, and other exotic phases of matter. While for one-dimensional (1D) systems tremendous progress has been made thanks to the famous density-matrix renormalization group (DMRG) method, in higher dimensions accurate methods have been lacking for many years. Recently, I achieved several major breakthroughs with tensor network methods, which can be seen as an extension of DMRG to higher dimensions. I was able to show that these methods outperform previous state-of-the-art approaches for several challenging models, making it clear that these methods will play a pivotal role in understanding SCS in 2D, in the same way as DMRG has revolutionized the study of 1D systems.

The goal of this project is to build upon these breakthroughs and to develop the next generation of tensor network methods to simulate relevant open problems in SCS with an unprecedented accuracy, in order to make substantial progress in understanding the physics of these systems. 

Milestones of this project include:
- Development and improvement of 2D tensor network methods for finite temperature simulations, the computation of excitation spectra, and the classification of topological states, and pioneering work with tensor networks for 3D systems.
- Accurate simulation of the 2D Hubbard model to answer the longstanding question if it captures the essential features of HTSC. 
- Study of extended Hubbard models to understand which ingredients are really essential for HTSC.
- Prediction of novel phases of matter in SU(N) systems, relevant for experiments on ultra-cold atoms in optical lattices. 
- Cutting-edge simulations for the quantitative understanding of 2D and 3D frustrated materials.","1499909","2016-09-01","2021-08-31"
"TEPESS","Technologies and psychophysics of spatial sound","Ville Pulkki","AALTO KORKEAKOULUSAATIO SR","Spatial audio is a field, which investigates technologies to capture and reproduce sound in a way that the spatial properties of it are either preserved or modified depending on application. For example, modern surround sound techniques try to reproduce the sound scene perceived by a human listener in the same way than in the original occasion. The principal investigator (PI) has been able to develop a number of technologies in spatial audio field and to transfer them to the industry. The project would have two work packages, one concentrating on development of technology (WP1) and the other on perceptual studies (WP2). The perceptual studies are assumed to help technology development, and new technologies are assumed to reveal new phenomena in perception. The main issue for WP1 is the development of generic audio format. In future all music records and movie audio tracks are targeted to be in this format, which would be suitable for listening with any loudspeaker setup and also with headphones, always with optimal spatial and timbral quality. The development of the format is based on a technique by the PI, which is extended in this work for enhanced playback over loudspeakers and over headphones. Also, new techniques are developed for sound input from different types of microphones and from existing audio formats. The perceptual issues studied in WP2 would be the functioning of spatial hearing with wide sources and complex sound scenarios, together with computational modeling of brain mechanisms devoted to binaural hearing. The crossmodal effects between vision and auditory systems would also be investigated in the anechoic chamber specially equipped for spatial sound research. As the final task, the perceptual quality of developed generic audio format in different listening scenarios would be evaluated with subjective and objective tests.","1879458","2009-09-01","2014-08-31"
"TeraGaN","GaN Quantum Devices for T-Ray Sources","Eva Maria Monroy Fernandez","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","T-rays, often called terahertz radiation or submillimeter waves, are loosely defined as the wavelengths from 30 µm to 1,000 µm, or the frequencies from 10 THz to 300 GHz. This non-ionizing radiation appears as a harmless alternative to x-rays in medical, biological and security screening. Current solutions in terms of coherent sources of T-rays either require cryogenic temperatures or are relatively bulky equipments based on optically-pumped materials. The solid-state recourse consisting of GaAs-based quantum cascade lasers presents an intrinsic limitation in operation temperature: The low energy of the longitudinal-optical (LO) phonon in arsenide compounds hinders laser emission beyond 180 K at 4 THz, and forces operation below the liquid nitrogen temperature (< 70 K) for frequencies below 1 THz. Overcoming this limitation requires a technology revolution through introduction of a new material system. This project aims at exploring a novel semiconductor technology for high-performance photonic devices operating in the T-ray spectral region. The advanced materials that we will investigate consist of nitride-based [GaN/Al(Ga,In)N] superlattices and nanowires, where we can profit from unique properties of III-nitride semiconductors, namely the large LO-phonon energy and the strong electron-phonon interaction. Our target is to adapt the quantum cascade design and fabrication technology to these new materials, characterized by intense internal polarization fields. Our project aims at pushing intersubband transitions in this material family to unprecendently long wavelengths, in other to cover the whole T-ray spectral gap with coherent solid-state sources operating at room temperature and above.","1627236","2012-01-01","2017-06-30"
"TERAMIX","Study of Novel Low Noise Superconducting Mixers for Terahertz Radio Astronomy","Sergey Cherednichenko","CHALMERS TEKNISKA HOEGSKOLA AB","""Terahertz heterodyne receivers are valuable tools for molecular gas spectroscopy both for space (radioastronomy, planetary science) and terrestrial applications. They provide both high resolution spectral data, as well as broad bandwidth line survey data. Due to the progress in device physics, such receivers can now reach several THz. At such high radio frequencies, neither electronic nor photonic approaches for THz detectors work, but rather a combination of both is required. Superconducting devices have proven to provide sensitivity levels close to the quantum limit, hf/k. Superconducting Hot- Electron Bolometers (HEB) based on ultrathin NbN and NbTiN films are currently the only devices which are used as mixers for frequencies above 1.2THz (SIS mixer limit). However, their speed (i.e. the instantaneous bandwidth) is limited by the finite electron energy relaxation rate, of 40-100 ps. It corresponds to the bandwidth of maximum 4-5GHz. Such applications in radio astronomy as extragalactic spectroscopy, molecular line survey require this bandwidth to be doubled to say at least. In this project we will investigate response rate in ultra thin MgB2 superconducting films. Preliminary investigation measured the electron-phonon interaction time as short as 1ps. Our recent data, point out on the response rate being limited by the phonon dynamic in the thick films. We will develop technology for ultrathin MgB2 film deposition, and processing THz nanobolometers. The response rate will be investigated with regards to the film parameters. In particularly, the phonon diffusion in superconducting nanobolometers will be studied in order to enhance the instantaneous bandwidth of MgB2 mixers. We estimate that the bandwidth of the novel THz detectors will be at least doubled compared to the existing once, providing completely new functionalities for THz radio astronomical receivers.""","1497775","2012-10-01","2017-09-30"
"TerAqua","Compact and powerful strong-field terahertz light source for exploring water in new regimes","Clara SARACENO","RUHR-UNIVERSITAET BOCHUM","Ultrafast laser driven sources of few-cycle THz pulses have progressed from being specialized systems to widespread laboratory tools. This has enabled THz time-domain spectroscopy to emerge as a powerful technique for time-resolved studies of the dynamics of many fundamental constituents of matter. Recent progress in the generation of few-cycle THz pulses with electric field strengths exceeding several MV/cm, has allowed nonlinear THz spectroscopy to open new exciting fields of research.
However, many fields suffer from a persistent lack of table-top THz sources combining high field strength and high repetition rate (i.e. high average power). So far, these parameters can only be achieved by costly and restrictive accelerator facilities. In most cases, scientists will either abandon research lines where high pulse energy is required or operate at low repetition rate (typically <1 kHz). This imposes severe limits on the explored parameter space in many measurements, for which minuscule average powers result in very long integration time and low signal-to-noise ratio.
My objective is to fill this gap by achieving table-top strong-field THz sources with watt-level average power. The key feature is to leverage existing THz generation techniques to higher efficiency by driving them inside the resonator of a high-power ultrafast thin-disk oscillator, operated in the mid-infrared (2 µm) spectral region.
This source will have impact in various fields of physics, physical chemistry, engineering, biology and medicine. I envision to apply this source in physical chemistry, to gain understanding of the role of water as “the solvent of life”. THz spectroscopy is perfectly suited to study aqueous samples, however, so far, most THz studies in liquid phase have been performed in the linear domain, and in a restricted parameter space due to the inadequacy of the sources. Our source will enable us to expand these studies into the nonlinear domain, which was previously impossible.","1415417","2019-04-01","2024-03-31"
"TERATOMO","Near-field Spectroscopic Nanotomography at Infrared and Terahertz Frequencies","Rainer Hillenbrand","Asociacion - Centro de Investigacion Cooperativa en Nanociencias - CIC NANOGUNE","Fundamental understanding and engineering of composite materials, biological structures and building
blocks for electrical and optical devices of nanoscale dimensions necessitate the availability of advanced
microscopy tools for mapping their local chemical, structural and free-carrier properties. But while optical
spectroscopy, particularly in the infrared (IR) and terahertz (THz) frequency range, has tremendous merit in
measuring such properties optically, the diffraction-limited spatial resolution has been preventing IR and
THz microscopy applications for the longest time to be used in nanoscale materials and device analysis, bioimaging,
industrial failure analysis and quality control.
During the last years we pioneered the field of IR and THz near-field microscopy, which allows twodimensional
(2D) spectroscopic IR and THz imaging of a sample surface with nanoscale spatial resolution,
independent of the wavelength. Key achievements of our work are the nanoscale resolved near-field mapping
of chemical compositions of polymer blends, mechanical strain fields in ceramics and free-carrier
concentrations in doped semiconductor transistors.
The core objective of this proposal is to develop a three-dimensional (3D) spectroscopic imaging method in
a wide spectral range between infrared (IR) and terahertz (THz) frequencies with nanoscale spatial
resolution, a method that does not and not even nearly exist today. Our approach will be based on scatteringtype
scanning near-field optical microscopy (s-SNOM), even though s-SNOM is generally considered to be a
surface mapping technique. Instead of scanning the surface, it is proposed to scan a volume above the sample
surface. By using appropriate reconstruction methods, the three-dimensional structure of the sample volume
below the sample surface could be obtained in principle. We recently conducted a theoretical study, which
confirmed the fundamental feasibility of this novel approach that shall be experimentally realized within this
proposal.
The proposed method of IR and THz nanotomography could become a new paradigm in nanoscale optical
imaging. Near-field nanotomography will have the potential to open new and even unexpected avenues for
optical characterization throughout all nanosciences, such as non-invasive, chemical identification of single
(biological) nanoparticles in complex 3D-nanostructures or the measurement of the local free-carrier
concentration and mobility in semiconductor nanowires or devices with 3D-architecture.","1455600","2010-11-01","2015-10-31"
"TERI","Teaching Robots Interactively","Jens KOBER","TECHNISCHE UNIVERSITEIT DELFT","Programming and re-programming robots is extremely time-consuming and expensive, which presents a major bottleneck for new industrial, agricultural, care, and household robot applications. My goal is to realize a scientific breakthrough in enabling robots to learn how to perform manipulation tasks from few human demonstrations, based on novel interactive machine learning techniques.
Current robot learning approaches focus either on imitation learning (mimicking the teacher’s movement) or on reinforcement learning (self-improvement by trial and error). Learning even moderately complex tasks in this way still requires infeasibly many iterations or task-specific prior knowledge that needs to be programmed in the robot. To render robot learning fast, effective, and efficient, I propose to incorporate intermittent robot-teacher interaction, which so far has been largely ignored in robot learning although it is a prominent feature in human learning. This project will deliver a completely new and better approach: robot learning will no longer rely on initial demonstrations only, but it will effectively use additional user feedback to continuously optimize the task performance. It will enable the user to directly perceive and correct undesirable behavior and to quickly guide the robot toward the target behavior. In my previous research I have made ground-breaking contributions to the existing learning paradigms and I am therefore ideally prepared to tackle the three-fold challenge of this project: developing theoretically sound techniques which are at the same time intuitive for the user and efficient for real-world applications.
The novel framework will be validated with generic real-world robotic force-interaction tasks related to handling and (dis)assembly. The potential of the newly developed teaching framework will be demonstrated with challenging bi-manual tasks and a final study evaluating how well novice human operators can teach novel tasks to a robot.","1499894","2019-02-01","2024-01-31"
"TERIFIC","Targeted Experiment to Reconcile Increased Freshwater with Increased Convection","Eleanor FRAJKA-WILLIAMS","UNITED KINGDOM RESEARCH AND INNOVATION","The ocean Meridional Overturning Circulation (MOC) is  responsible for poleward heat transport, and deep storage of heat and carbon.  Climate models generally predict that a slowdown of the MOC will occur this century, with dramatic regional and global climate changes,  but how the slowdown will occur is subject to debate.  It is widely recognised that convection -- the downward limb of the MOC -- is sensitive to freshwater fluxes, and recent investigations have suggested that the increased melt from the Greenland Ice Sheet has already reduced convection.  Yet in 2015, convection returned, and was anomalously strong.  Despite the expectation that the MOC is sensitive to freshwater forcing, we do not understand the processes by which  freshwater inputs influence the MOC.

The two key gaps in our understanding are: how freshwater reaches the regions of deep convection, and the relative importance of freshwater to convection and restratification. Numerical simulations give conflicting results on  freshwater pathways, and convective parameterisations neglect small-scale restratifying processes. Traditional observational approaches cannot  capture the spatial and temporal variability of these processes  without inordinate cost.

TERIFIC addresses these gaps in understanding through new observations, leveraging recent advances in small-scale electronics to  deploy large numbers of mini-drifters on the shelves of Greenland, and employing subsurface and surface autonomous platforms to characterise the balance of processes controlling convection and restratification. The analysis of these observations will answer fundamental questions about how freshwater reaches and affects the regions of deep convection, and enable a critical ground truth of numerical simulations of these processes for climate models.","1999909","2018-11-01","2023-10-31"
"TERPENECAT","Bridging the gap between supramolecular chemistry and current synthetic challenges: Developing artificial catalysts for the tail-to-head terpene cyclization","Konrad TIEFENBACHER","UNIVERSITAT BASEL","Nature is a rich source of biologically active molecules, among which the largest and most diverse group of natural products are terpenes. Essential drugs like the cancer medication taxol/paclitaxel or the malaria drug artemisinin belong to the terpene family. They are efficiently formed in nature through a so-called tail-to-head terpene cyclization. Chemists are not able to mimic this process with man-made catalysts. This proposal aims at closing this significant research gap by utilizing supramolecular chemistry. Learning how to design such complex catalysts will not only enable us to mimic natural enzymes, but to enter uncharted territory of terpene chemistry.

The main objective is the development of selective catalysts for terpene cyclizations. This certainly poses the greatest challenge within this proposal. Therefore, two independent work packages were devised to tackle this challenge. A novel class of self-assembled catalysts will be developed which are able to control the conformation of the substrate, thereby allowing for selectivity in the cyclization process. The active site of these catalysts can be modified to selectively produce the desired terpene product. Additionally, dynamic covalent chemistry will be employed to construct covalent catalyst structures.

As the second objective, this proposal aims to greatly expand the natural variety of terpenes by utilizing unnatural terpene cyclization precursors. Utilizing the catalysts developed from objective 1, unprecedented artemisinin drug derivatives, which are not accessible via other routes, will be synthesized.

This project will provide catalysts which are able to predictably constrain the conformation of the substrate. Such control is not possible with state-of-the-art catalyst systems. Therefore, I anticipate that this project will open up new horizons in the fields of catalysis and organic synthesis.","1500000","2016-11-01","2021-10-31"
"TERRA","375 Million Years of the Diversification of Life on Land: Shifting the Paradigm?","Richard James Butler","THE UNIVERSITY OF BIRMINGHAM","Life on land today is spectacularly diverse, representing 75–95% of all species on Earth. However, it remains unclear how this extraordinary diversity has been acquired across deep geological time. This research project will address this major knowledge gap by reassessing the dominant paradigm of terrestrial diversification, an exponential increase in diversity over the last 375 million years, using the rich and well-studied fossil record of tetrapods (four-limbed vertebrates) as an exemplar group. Previous analyses of tetrapod diversification have been based on an outdated and problematic dataset that is likely to artificially inflate apparent diversity towards the present day. A major new dataset will be assembled, detailing the spatial and temporal distribution of terrestrial tetrapods across their entire fossil record in unprecedented detail. These data will be analysed using the latest approaches to sampling-standardisation in order to generate completely novel, rigorous curves of diversification through time. These will be compared within a cutting-edge statistical framework to alternate diversification models, as well as to changes in rock record sampling, global environments (e.g. sea level and atmospheric composition) and marine diversity. These comparisons will allow us to address the following key questions: (i) Does terrestrial diversification follow an exponential pattern over the last 375 million years? (ii) Is the terrestrial fossil record as complete as the marine fossil record? (iii) Are long-term patterns of terrestrial diversification driven by physical changes in the Earth system such as climate change? (iv) Did marine and terrestrial biodiversity follow similar trajectories across geological time? (v) How severely did mass extinction events impact upon terrestrial tetrapod diversification? Our work will establish a new, rigorous paradigm for the long-term pattern of terrestrial diversification, and test and identify its drivers.","1495063","2015-07-01","2020-06-30"
"TESLA","Living on the Edge: Tunable Electronics from Edge Structures in 1D Layered Materials","Sonia Conesa Boj","TECHNISCHE UNIVERSITEIT DELFT","One of the driving forces of the ongoing nanotechnology revolution is the ever-improving ability to understand and control the properties of quantum matter even down to the atomic scale. Key drivers of this revolution are layered materials like transition metal dichalcogenides (TMD). The realisation of novel TMD-based electronic devices relies heavily on understanding the relation between structural and electrical properties at the nanoscale. Crucially, one-dimensional (1D) TMDs have been predicted to exhibit striking functionalities including metallic edge states, ferromagnetic behaviour, and mobilities that are not suppressed as compared to their 2D counterparts. Indeed, in the 1D nanoscale limit, the lateral edges of TMDs become dominant, opening novel opportunities to tune edge-induced electrical properties leading to i.e. enhanced charge carrier mobility. 
However, these predictions for novel phenomena in 1D TMDs lack experimental verification, due to the challenge in accessing the relevant information at the nanoscale. I propose to unravel the interplay between structural and electrical edge-induced properties by exploiting recent breakthroughs in electron microscopy (EM) allowing simultaneous unprecedented spatial and spectral resolution. I will focus on MoS2 nanoribbons, and use electron-energy loss spectroscopy to map the electronic properties at the nanometer-scale. Beyond the optimization of EM for 1D TMD characterization, I will investigate semiconducting-to-metal and ferromagnetic transitions by realising controllable edge structures. I have an extensive track record in pushing the frontier of EM characterization and growing nanostructures. I recently demonstrated the feasibility of pinning down the interplay between structure and electronic properties at the edges of 2D MoS2. This proposal will provide input towards novel quantum technologies for developing low-energy-consumption tunable electronics, efficient signal processing and quantum computation.","1499475","2019-01-01","2023-12-31"
"Tesseract","Best Curved Adapted Meshes for Space-Time Flow Simulations","Xevi Roca","BARCELONA SUPERCOMPUTING CENTER - CENTRO NACIONAL DE SUPERCOMPUTACION","The accuracy obtained in wind tunnel aerodynamic and aero-acoustic measurements is extremely demanding and it still challenges our current simulation technologies. It mainly challenges the capabilities of current mesh generation technologies used in flow simulations. The ground-breaking TESSERACT project addresses the challenge of studying how to generate computational meshes that enable the ability to obtain computer flow simulations that beat the predictive capabilities of the wind tunnel experiments for a fixed accuracy, cost, and time scale. These important challenges correspond to capabilities that have been considered essential to fulfil the European strategic goals of future transportation. The main objective is to generate optimal quality curved adapted meshes for space-time flow simulations by addressing the following ambitious and beyond the state of the art 4-dimensional meshing research objectives: curved geometry representation and approximation, mesh quality measures, adapted mesh resolution, and space-time flow simulation. This is a high risk project since it tackles meshing objectives in 4D while lower dimension versions of these issues have not yet been fully solved. However, providing the foundations and the methods to improve current space-time meshing algorithms will suppose a high gain in the field of computational and aerospace engineering. This is so since in the near future, it will be of major importance to conduct accurate, robust, and efficient parallel in space-time adapted flow simulations that exploit the computational power of the exascale super-computing facilities to come. To enhance the feasibility of the project, the scientific approach considers different novel approaches to reach the same objectives and therefore, bear in mind the high-risk / high-gain nature of this 4D meshing project.","1478135","2017-04-01","2022-03-31"
"TFPA","Study of Terahertz Focal Plain Arrays","Andrei Barychev","STICHTING SRON NETHERLANDS INSTITUTE FOR SPACE RESEARCH","The Terahertz frequency band is largely unexplored both for astronomical and for ground based applications. Over the recent years significant progress has been made in developing coherent detection techniques applicable at these higher frequencies, which enabled the building of array-receivers. When located in a focal plane a telescope such novel systems allow for a manifold increase in performance. However, the development of coherent detector focal plane arrays is still lagging behind. For incoherent bolometric arrays, a novel kinetic inductor detector (KID) has been recently proposed. It makes it possible to multiplex focal plane arrays with many thousands of pixels in very efficient and cost effective way. Backend technologies for both coherent and KID focal plane array will be based on Fourier Transform Spectrometer digital technique, which has demonstrated impressive progress during last few years and ready for large scale arrays. We propose to develop an advanced side band separating mixer technology for focal plane arrays in combination with research directed to increase the operation frequency of superconductor-insulator-superconductor technology by studying new superconducting materials. This research will be combined with development of many kilo pixel KID array. This inter-disciplinary research will enable building large focal plane arrays for use in astronomy and other Terahertz applications.","900000","2009-11-01","2014-10-31"
"TGASS","Topological, Geometric and Analytical Study of Singularities","Javier Fernandez De Bobadilla","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","Singularity theory is a transversal research subject in which many different techniques converge (algebraic, analytical, geometric, topological), with different perspectives (local, global), but with a very strong interconnection. Our research involves the study of analytic singularities from different perspectives. Specific topics are the vanishing cohomology of singularities with its D-module and Hodge structure, the topology of the Milnor fibre, the embedded topology of the link, topological equisingularity, versal deformations and their base spaces with additional structure (such as Frobenius manifolds), invariants of surface singularities, rational cuspidal curves, global polynomial mappings, hyperesolutions and descent categories. More concretely we intend to work at the following topics and their interactions: 1.- Study a new class of non-isolated singularities introduced by the applicant which already had striking applications. Study the sheaf of vanishing cycles of non-isolated singularities. 2.- Develop a new program towards characterisation of topological equisingularity and apply it to the study of global polynomial mappings. Study several old fundamental questions in topological equisingularity such as the µ-constant problem. 3.- Study some conjectures on rational cuspidal plane curves, Q-acyclic open surfaces and normal surface singulatities with rational homology sphere links, via algebra-geometric and topological techniques. 4.- Study specific problems on configurations of varieties and their complements 5.- Study moduli of polynomial maps and algebraic endomorphisms. 6.- Study descent categories and applications to D-modules, Hodge Theory and Singularity Theory.","630000","2008-11-01","2013-10-31"
"THAWSOME","THAWing permafrost: the fate of Soil Organic Matter in the aquatic Environment","Jorien Elisabeth Vonk","STICHTING VU","As the Arctic permafrost region warms, its large organic carbon (OC) pool becomes vulnerable to decomposition. This generates greenhouse gases (GHG) that in turn fuel increased surface warming: the permafrost carbon feedback. Higher temperatures will jump-start the coupling between the carbon and hydrological cycle, allowing for the introduction of previously frozen OC pools in aquatic systems. This lateral, or horizontal, aquatic flux remains largely unknown in contrast to the relatively well-studied vertical flux, GHG emission on land. 

Horizontal OC release either occurs via gradual thaw, slowly leaching OC into aquatic systems, or abrupt thaw, where ground-ice melt causes destructive surface collapse and slumping of OC into aquatic systems. Both types of thaw facilitate decomposition of OC (generating GHG) but also re-bury OC into sediments (sequestering OC). The relative importance of decomposition versus burial is unknown. 

For THAWSOME, I have developed a multi-scale approach combining detailed process-based field studies with up-scaling techniques on multiple levels: (i) observational, using large Arctic rivers as natural integrators, (ii) numerical, using a coupled hydrological-biogeochemical model, and (iii) spatial, using GIS-based analysis. For the first time, decomposition of particulate OC from permafrost will be quantified with a recently developed incubation method. Burial rates of permafrost OC will be assessed through molecular isotope analyses of both sources (river OC) and sinks (sediment OC) across the land-to-shelf route. 

THAWSOME will generate critically needed quantitative data on the amount of decomposition versus burial of permafrost OC, as well as qualitative insights into the processes that control this. This will allow a true coupling of the carbon and hydrological cycle into the 'boundless Arctic carbon cycle', and integration of horizontal OC release into estimates of the impact of the permafrost carbon feedback on global climate.","1500000","2016-12-01","2021-11-30"
"THE MR CHALLENGE","Expanding the horizons of magnetic resonance in sensitivity, imaging resolution, and availability","Aharon Blank","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","""We propose to develop and implement advanced magnetic resonance detection and micro-imaging techniques that will benefit many biophysical, chemical, physical, and medical applications. Magnetic resonance (MR) is one of the most profound observation methods in science. MR includes Nuclear Magnetic Resonance (NMR) and Electron Spin Resonance (ESR). It has a variety of applications ranging from chemical structure determination to medical imaging and quantum computing. From a scientific standpoint, MR was the main focus of at least seven Nobel prizes in physics, chemistry, and medicine. From an industrial standpoint, MR is a multibillion industry focused on a range of medical (MRI) and chemical applications (MR spectrometers). Despite the fact that magnetic resonance was discovered more than 60 years ago, there is still plenty of room for new methodologies and applications. This research will confront some of the most challenging issues that this field has yet to offer, which also contain the greatest potential benefits. This is what we call “The MR Challenge”. We will focus on three key MR issues: sensitivity, image resolution, and affordability. Our first goal is to substantially improve the sensitivity of MR spectroscopy and the resolution of MR micro-imaging. We will put most of our efforts on ESR spectroscopy and on the detection of NMR information through an ESR signal (ENDOR). At ambient conditions our goal is to achieve a sensitivity of ~10^4 electron spins and a resolution of 1 micron; at low temperatures we will approach single electron spin sensitivity and image resolution as high as 10nm. In terms of affordability, our goal is to introduce a small probe that is capable of acquiring NMR spectra from samples located outside the magnet (an """"ex-situ"""" probe). We will also design and construct a new family of hand-held 3D NMR imaging probes. The new capabilities would be applied in the field of single cell imaging and biophysics, materials science, and medicine.""","1250000","2008-08-01","2013-07-31"
"THEMISS","Thermal Evolution Modeling of Icy objects in the Solar System","Aurelie GUILBERT-LEPOUTRE","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Comets can be used as tracers of the conditions prevailing during the formation of the solar system. We have been studying them for decades, yet we still have not answered this foreground question: how do comets work? To answer that question is to understand which of comets properties are actually relevant to characterize the early solar system, and how primitive comets really are. Icy objects in the solar system are stored in different reservoirs, where they evolve very slowly owing irradiation, collisions and thermal processing. When they enter the inner solar system, they become comets, i.e. objects loosing mass. If comets are deemed very primitive due, for example, to their high content in very volatile species, some observations including ESA/Rosetta’s, have brought us a conundrum. Indeed, some comet properties indicate that they could have suffered from a long-term processing, which lead their basic properties (like shape, composition or size) to evolve significantly since the time they were formed. This proposal will explore the thermal processing of comets from their storage in the Oort Cloud, the Kuiper Belt and the Main Belt, and the thermally-induced variations in their physical and chemical characteristics, in order to understand whether such effects were important for shaping comets as we observe them today. Based on observational constraints obtained both from the ground and the latest space missions, an unprecedented modeling effort will be undertaken to evaluate under which conditions comets can preserve pristine material, what the long-lasting effects of thermal processing are for the various comet populations, and provide tools for deciphering between primitive properties and properties affected by evolution. Finally, this work will be shared via a web application, allowing the community to work from this foundation to prepare the future of our field, including the next generation of space missions exploring comets from sample returns.","1494494","2019-02-01","2024-01-31"
"THEOFUN","Theoretical studies on the functionalisation of metal surfaces with organic and biological complexes under electrochemical conditions","Timo Jacob","UNIVERSITAET ULM","The aim of this project is to understand the mechanisms behind the functionalisation of metal surfaces with organic and biological
complexes under realistic electrochemical conditions. Focusing on low-index surfaces of gold and platinum, which are also the
electrodes in corresponding experiments, we will use a series of theoretical methods applicable for different time- and lengthsscales
to investigate the geometry and electronic properties of different complexes attached to these electrodes as function of the
surrounding (e.g. electrolyte) and the environmental conditions: temperature, pressure/concentrations, and electrode potential. As
complexes we will consider small organic molecules such as 4-mercaptopyridine, 4-ATP, or alkane-chains of variable length, as
well as biological complexes, i.e. DNA-sequences.
Within the first step we will establish a deeper understanding of how these complexes interact with the metal electrodes and how
adlayer structures can be manipulated by applying specific temperature, pressure, or potential-conditions. Since the intermolecular
interactions are rather weak, the presence of the external electrode potential could lead to drastic changes of the interfacial
morphology. In this respect, particular attention will be spend to the highly-reversible folding and unfolding of DNA-sequences,
which has recently had been realized experimentally.
Based on thus functionalized electrode surfaces, we will investigate their potential as templates for growing nanoparticles of
desired size and shape, which would allow for bridging the gap between well-defined single crystal surfaces and nanoparticles. It
is now a matter of establishing the predictive capacity for these methods, an expansive process that itself will open new doors of
research.","1409400","2011-04-01","2016-03-31"
"TheoryDL","Practically Relevant Theory of Deep Learning","Shai Shalev-shwartz","THE HEBREW UNIVERSITY OF JERUSALEM","One of the most significant recent developments in applied machine learning has been the resurgence of ``deep learning'', usually in the form of artificial neural networks.  The empirical success of deep learning is stunning, and deep learning based systems have already led to breakthroughs in computer vision and speech recognition. In contrast, from the theoretical point of view, by and large, we do not understand why deep learning is at all possible, since most state of
the art theoretical results show that deep learning is computationally hard.

Bridging this gap is a great challenge since it involves proficiency in  several theoretic fields (algorithms, complexity, and statistics) and at the same time requires a good understanding of real world practical problems and the ability to conduct applied research. We believe that a good theory must lead to better practical algorithms. It should also broaden the applicability of learning in general, and deep learning in particular, to new domains. Such a practically relevant theory may also lead to a fundamental paradigm shift in the way we currently analyze the complexity of algorithms.

Previous works by the PI and his colleagues and students have provided novel ways to analyze the computational complexity of learning algorithms and understand the tradeoffs between data and computational time. In this proposal, in order to bridge the gap between theory and practice, I suggest a departure from worst-case analyses and the development of a more optimistic, data dependent, theory with ``grey'' components.  Success will lead to a breakthrough in our understanding of learning at large with significant potential for impact on the field of machine learning and its applications.","1342500","2016-02-01","2021-01-31"
"ThermoDynaMix","Dynamics and Thermodynamics in Mixed Dimensions","Frédéric Chevy","ECOLE NORMALE SUPERIEURE","In ThermoDynamix, we explore experimentally and theoretically the physics of ultracold mixtures of particles evolving in a different number of dimensions. We will realize experimentally a mixed-dimensional system by laser trapping an ultracold mixture of fermionic 6Li and 40K atoms. By tuning the wavelength of the trapping laser, we will create a potential that will almost not affect Lithium, and at the same time will confine strongly the Potassium atoms in one, two or three dimensions to freeze their motion along these directions. Experimental investigations will be supplemented by a comprehensive theoretical study of the system, using both analytical and numerical tools, in particular Diagrammatic Quantum Monte-Carlo simulations.","1482000","2012-09-01","2017-08-31"
"THERMOS","The protein thermostability: same activity, different working temperature. A water problem? A rigidity/flexibility trade-off?","Fabio Sterpone","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The proteins from thermophilic organisms are the objects of the present study. Here it is specifically proposed a study on the microscopic origin of proteins thermostability using a multi-computational approach. The multi-methodological strategy is a powerful tool for exploring this issue since it allows an investigation at many different levels of molecular details. Neutron Scattering experiments will complement the in silico investigation.
The present study will tackle the issue of thermostability under a new light by explicitly focusing on the role of hydration water and by carefully selecting homologues proteins from mesophilic, thermophilic and hyperthermophilic organisms as cases of study.
I will investigate how the chemical composition of a protein surface, the distribution of charged, polar and hydrophobic amino acids, could be tuned in order to increase/reduce thermal resistance of the hydration layer and of the protein matrix. I will examine whether thermostability correlates to the flexibility or the rigidity of the protein matrix and/or of its hydration skin.  I will study in details how the catalytic activity of enzymes is affected by the dynamics of the protein at extreme temperatures.
The theoretical study will be supported by Neutron Scattering experiments gaining key knowledge on the structure and dynamics of hydration water and on the dynamics of proteins in the nanosecond time scale.
Nowadays the possibility to design functional thermostable proteins is strategic for expanding the use of enzymes in industrial processes and in biotechnology. The study of the coupling between hydration water and protein surface could pave the way for the computer-aided engineering of thermostable proteins.","1225000","2011-05-01","2016-04-30"
"ThermoTex","Woven and 3D-Printed Thermoelectric Textiles","Christian Müller","CHALMERS TEKNISKA HOEGSKOLA AB","Imagine a world, in which countless embedded microelectronic components continuously monitor our health and allow us to seamlessly interact with our digital environment. One particularly promising platform for the realisation of this concept is based on wearable electronic textiles. In order for this technology to become truly pervasive, a myriad of devices will have to operate autonomously over an extended period of time without the need for additional maintenance, repair or battery replacement. The goal of this research programme is to realise textile-based thermoelectric generators that without additional cost can power built-in electronics by harvesting one of the most ubiquitous energy sources available to us: our body heat.

Current thermoelectric technologies rely on toxic inorganic materials that are both expensive to produce and fragile by design, which renders them unsuitable especially for wearable applications. Instead, in this programme we will use polymer semiconductors and nanocomposites. Initially, we will focus on the preparation of materials with a thermoelectric performance significantly beyond the state-of-the-art. Then, we will exploit the ease of shaping polymers into light-weight and flexible articles such as fibres, yarns and fabrics. We will explore both, traditional weaving methods as well as emerging 3D-printing techniques, in order to realise low-cost thermoelectric textiles.

Finally, within the scope of this programme we will demonstrate the ability of prototype thermoelectric textiles to harvest a small fraction of the wearer’s body heat under realistic conditions. We will achieve this through integration into clothing to power off-the-shelf sensors for health care and security applications. Eventually, it can be anticipated that the here interrogated thermoelectric design paradigms will be of significant benefit to the European textile and health care sector as well as society in general.","1500000","2015-06-01","2020-05-31"
"ThforPV","New Thermodynamic for Frequency Conversion and Photovoltaics","Carmel Rotschild","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","""The Shockley Queisser (SQ) limits the efficiency of single junction photovoltaic (PV) cells and sets the maximum efficiency for Si PV at about 30%. This is because of two constraints: i. The energy PV generates at each conversion event is set by its bandgap, irrespective of the photon’s energy. Thus, energetic photons lose most of their energy to heat. ii. PV cannot harness photons at lower energy than its bandgap. Therefore, splitting energetic photons, and fusing two photons each below the Si bandgap to generate one higher-energy photon that match the PV, push the potential efficiency above the Shockley Queisser limit. Nonlinear optics (NLO) offers efficient frequency conversion, yet it is inefficient at the intensity and the coherence level of solar and thermal radiation. 
Here I propose new thermodynamic concepts for frequency conversion of partially incoherent light aiming to overcome the SQ limit for single junction PVs. Specifically, I propose entropy driven up-conversion of low energy photons such as in thermal radiation to emission that matches Si PV cell. This concept is based on coupling """"hot phonons"""" to Near-IR emitters, while the bulk remains at low temperature. As preliminary results we experimentally demonstrate entropy-driven ten-fold up-conversion of 10.6m excitation to 1m at internal efficiency of 27% and total efficiency of 10%. This is more efficient by orders of magnitude from any prior art, and opens the way for efficient up-conversion of thermal radiation. 
We continue by applying similar thermodynamic ideas for harvesting the otherwise lost thermalization in single junction PVs and present the concept of """"optical refrigeration for ultra-efficient PV"""" with theoretical efficiencies as high as 69%. We support the theory by experimental validation, showing enhancement in photon energy of 107% and orders of magnitude enhancement in the number of accessible photons for high-bandgap PV. This opens the way for disruptive innovation in photovoltaics""","1500000","2015-07-01","2020-06-30"
"THREEDSURFACE","Three-Dimensional Surface Nano-Patterning: Concepts, Challenges and Applications","Yong Lei","TECHNISCHE UNIVERSITAET ILMENAU","Multifunctional surface nano-patterns on substrates are the foundation of semiconductor nano-devices. There is a major shortcoming of the existing surface nano-patterning techniques - in fact almost all synthesized surface patterns are two-dimensional (2-D) planar structures with low aspect ratio. Thus one of the most attractive advantages of nanomaterials, an extremely large surface area, is missing in the existing 2-D surface nano-patterns. This largely limits the application potential of surface nanostructures on semiconductor devices. In this project, a new concept of three-dimensional (3-D) nano-patterning is proposed. Using this multi-functional 3-D surface nano-patterning technique, large-scale surface patterns of well-defined one-dimensional (1-D) nanostructures can be synthesized by different fabrication strategies. The realization of the 3-D surface nano-patterning will not only retain the attractive features of the conventional 2-D surface nano-patterning (e.g. high patterning density), but also bring back one of the basic advantages of nanomaterials, i.e. an extremely large surface area. Using an innovative addressing system proposed in this project, it is possible to investigate and analyze the properties of an individual unit within a regular surface nanostructure array and the coupling interaction between the adjacent units. By integrating these data, the properties of the whole ensembles could be obtained. This bottom-up investigation might pave the way to a complete property tuning based on the structural design of surface 1-D nanostructures. The large-scale 1-D surface nano-patterns with well-defined structures have broad application potentials for different high-performance and property-controllable nano-devices.","1398600","2009-09-01","2015-06-30"
"THUNDEEP","A Theory for Understanding, Designing, and Training Deep Learning Systems","Ohad SHAMIR","WEIZMANN INSTITUTE OF SCIENCE LTD","The rise of deep learning, in the form of artificial neural networks, has been the most dramatic and important development in machine learning over the past decade. Much more than a merely academic topic, deep learning is currently being widely adopted in industry, placed inside commercial products, and is expected to play a key role in anticipated technological leaps such as autonomous driving and general-purpose artificial intelligence. However, our scientific understanding of deep learning is woefully incomplete. Most methods to design and train these systems are based on rules-of-thumb and heuristics, and there is a drastic theory-practice gap in our understanding of why these systems work in practice. We believe this poses a significant risk to the long-term health of the field, as well as an obstacle to widening the applicability of deep learning beyond what has been achieved with current methods.

Our goal is to tackle head-on this important problem, and develop principled tools for understanding, designing, and training deep learning systems, based on rigorous theoretical results. 

Our approach is to focus on three inter-related sources of performance losses in neural networks learning: Their optimization error (that is, how to train a given network in a computationally efficient manner); their estimation error (how to ensure that training a network on a finite training set will ensure good performance on future examples); and their approximation error (how architectural choices of the networks affect the type of functions they can compute). For each of these problems, we show how recent advances allow us to effectively approach them, and describe concrete preliminary results and ideas, which will serve as starting points and indicate the feasibility of this challenging project.","1442360","2018-09-01","2023-08-31"
"THz-FRaScan-ESR","THz Frequency Rapid Scan – Electron Spin Resonance spectroscopy for spin dynamics investigations of bulk and surface materials (THz-FRaScan-ESR)","Petr NEUGEBAUER","VYSOKE UCENI TECHNICKE V BRNE","Current high frequency electron spin resonance (HFESR) instruments suffer from the disadvantages of being limited to a single frequency and to tiny sample volumes. The study of spin dynamics at frequencies beyond a few hundred gigahertz is currently prohibitively difficult. These limitations are now preventing progress in dynamic nuclear polarization (DNP) and preclude the implementation of zero-field quantum computing. In order to revolutionize sensitivity in nuclear magnetic resonance (NMR) and magnetic resonance imaging (MRI) by means of DNP techniques allowing to watch in real time molecular interactions or even to monitor how sophisticated systems (ribosomes) work, the HFESR methods have to be substantially improved. I will develop a novel and worldwide unique technique called broadband terahertz frequency rapid scan (FRaScan) ESR. I intend to implement this method into a working prototype, which will seamlessly span the entire frequency range from 100 GHz to 1 THz, and allow spin dynamics investigation of large samples. This revolutionary new concept based on rapid frequency sweeps will remove all the restrictions and limitations of conventional HFESR methods used nowadays. It will enable for the first time multi-frequency studies of quantum coherence also in zero magnetic field. It will lead to substantial increases in sensitivity and concurrent decrease of measurement time, thus allowing more efficient use of resources. Finally, the method will allow identification of novel DNP signal enhancement agents, ultimately leading to a step change improvement of the MRI method. It will drastically shorten MRI scan times in hospitals, greatly enhancing patient comfort together with significantly better and precise diagnoses. The method will lead to zero field quantum computers with computation power which will be never reached with conventional technology. In summary it will lead to impacts far beyond the scientific community.","1999874","2018-01-01","2022-12-31"
"THZ-PLASMON","Ultra-fast control of THz plasmon polariton resonances","Jaime Gómez Rivas","STICHTING NEDERLANDSE WETENSCHAPPELIJK ONDERZOEK INSTITUTEN","Active control of the optical properties of materials represents one of the most fundamental aspects of photonics. It is crucial to deepen our understanding on light-matter interactions, and for the development of novel technologies. Of special importance is the interaction of light with metal surfaces, leading to surface plasmon polaritons (SPPs). SPPs can give rise to giant local field enhancements in subwavelength volumes. Thanks to recent developments in plasmonics, electromagnetic fields can nowadays be controlled in subwavelength volumes and on ultra-fast time scales. However, research has been limited to optical and near-IR frequencies. The lack of studies at lower frequencies originates from the weak confinement of SPPs at these frequencies. Our objective is to overcome these limitations, demonstrating for the first time ultra-fast control in subwavelength dimensions of plasmonic resonances at THz frequencies. In particular, we plan to control localized surface plasmon polaritons (LSPPs), arising from the coherent oscillation of charges in particles. For this purpose, we will fabricate semiconductor particles supporting LSPPs, which will be controlled optically at very low fluences. We will show that the spatial distribution of giant field enhancements can be tuned with unprecedented accuracy by demonstrating its optical switching by several orders of magnitude. Moreover, we will study for the first time the active coupling of plasmonic resonances in periodic arrays of semiconductor particles. These arrays will constitute the plasmonic analogue to phased array antennas. The project will open new horizons in fundamental research, as well as in applications such as THz modulators and sensors.","1492800","2011-06-01","2016-05-31"
"TIME-BRIDGE","Time-scale bridging potentials for realistic molecular dynamics simulations","Blazej Tadeusz Grabowski","MAX PLANCK INSTITUT FUR EISENFORSCHUNG GMBH","The possibility to produce materials with ultra-strengths could revolutionize materials design. Since 80 years ultra-strength materials are known to exist only theoretically. Now, new experiments show that traditional believe can be overcome by nanostructured design. Yet, while selected experiments point towards this scientifically fascinating and technologically important possibility (e.g., for advances in structural and functional materials), further progress crucially relies on insight from theoretical simulations. The most successful simulation tool is molecular dynamics. Recent advances in hardware allow to tackle trillions of atoms making a comparison with nano-experiments almost possible. The nagging problem is, however, a huge time-scale gap of up to ten orders of magnitude and none of the presently available approaches is able to cope with this discrepancy.
TIME-BRIDGE aims at solving the time-scale problem by borrowing a concept well known and developed in the field of first-principles simulations: the pseudopotential ansatz. In first principles simulations a similar time scale gap exists between slow and fast moving electrons. The solution is to capture the effect of the fast electrons only effectively within a pseudopotential while retaining the motion of slow electrons important for chemical bonding. An equivalent pseudopotential ansatz is envisioned to be applicable to the fast thermal motion of atoms, the origin of the time scale problem. Capturing the thermal motion in an effective potential will allow to simulate the relevant mechanical processes occurring on microsecond and second time scales. In TIME-BRIDGE high risk and high gains apply: the physics of electrons is distinct from the atomic motion possibly making the pseudopotential ansatz non-transferable, but—based on PI’s distinguished expertise and his recent methodological advancements—a route to bridge the fundamental time scale gap might arise.","1499375","2015-07-01","2020-06-30"
"TimePROSAMAT","Time-Programmed Self-Assemblies and Dynamic Materials","Andreas Jean Leopold Walther","ALBERT-LUDWIGS-UNIVERSITAET FREIBURG","""TimeProSAMAT aims to introduce concepts to program the time domain of self-assembled systems and materials in CLOSED systems under non-equilibrium conditions by controlling the kinetics of assembly and disassembly pathways via (i) modulating the surrounding by feedback systems, (ii) dissipative structure formation and (iii) active structural feedback. After reaching a fundamental understanding on a SA level, we want to capitalize on these enabling SA concepts by providing entirely new and original approaches to dynamic soft materials with internally encoded self-regulation features (similar to a self-destruction mechanism), opening doors to active functionalities and adaptive properties beyond what classical responsive equilibrium SA can offer.
Read more about such concepts in the 10th year anniversary issue of Soft Matter: """" Approaches to program the time domain of self-assemblies"""" Soft Matter, 2015,11, 7857-7866""","1499813","2016-11-01","2021-10-31"
"TIMER","TIME-Resolved Spectroscopy of Nanoscale Dynamics in Condensed Matter Physics","Claudio Masciovecchio","ELETTRA - SINCROTRONE TRIESTE SCPA","The main goal of this project is the development of an inelastic photon scattering spectroscopy that will benefit from the advent of the new generation of ultra bright light sources. The current construction of the free electron laser FERMI by Sincrotrone Trieste, is expected to make available VUV photon pulses with unique characteristics and therefore calls for the development of the proposed technique in Trieste. The idea is to extend the standard transient grating spectroscopy towards higher energies in a way such that nanoscale dynamics may be investigated. This would be extremely interesting to better understand the physics of disordered systems since it will make accessible the mesoscopic kinematic region that cannot be explored by the use of visible laser or synchrotron based instruments. Transient grating experiments at the nanoscale could also allow sensitive probing of interfaces and extremely thin films, as well as heat transport and correlations in nanostructured materials. ERC funding will allow the PI transition from his current position as junior beam line scientist at the Elettra facility, to an independent research group leader status at the FERMI laboratory for conducting experiments that will be relevant to many research fields at the frontier in physics and chemistry.","1792800","2008-06-01","2014-05-31"
"TMCS","Topological Matter and Crystal Symmetry: From Microscopic Structure to Phenomenology","Siddharth ASHOK PARAMESWARAN","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","This ERC project will build a wide-ranging theory of strongly-correlated topological states of matter in three dimensions, via analytical, numerical, and phenomenological approaches. It will use non-symmorphic crystal symmetry as an organizing principle to identify systems that are good candidates to host fractionalized states of matter. Via slave-particle mean-field theories used in concert with symmetry analysis, it will provide a systematic classification of different possible spin-charge-separated, topologically ordered, and broken-symmetry states in correlated Mott insulators and heavy-fermion materials.  This mean-field study of model Hamiltonians will be wedded to a sophisticated new variational tensor-network scheme for simulating physically-realistic systems. Separately, an analytical classification of gapless ‘U(1)’ quantum spin liquids with emergent photon excitations will be implemented. Variational trial wavefunctions will also be developed to access a new class of interacting 'topological quantum paramagnets’ with gapless edge states. 

The symmetry analysis will be coupled to two phenomenological studies. One will examine unconventional surface state properties of topological semimetals, and extend these to the interacting regime. Another will develop a spectroscopic theory for topological matter with symmetry, leveraging results from the parton approach where possible. Experimental input from studies of nematic quantum Hall states and photoemission studies of Weyl semimetals will provide feedback to this effort. 

A final thrust of activity will focus on newly-proposed fracton states of matter not captured by usual theories of topological order, and will employ both analytical parton techniques and numerical quantum Monte Carlo simulations. 

At its close, this project will deliver a dramatically altered understanding of three dimensional topological phases and provide a new class of analytical and numerical tools as a platform for future studies.","1499622","2019-01-01","2023-12-31"
"TMHA","Transversal Multilinear Harmonic Analysis","Jonathan Bennett","THE UNIVERSITY OF BIRMINGHAM","This proposal consists of two intimately related programmes. The aim of Programme 1 is to make major contributions to the celebrated restriction theory for the Fourier transform and combinatorial problems of Kakeya-type using emerging multilinear techniques. The aim of Programme 2 is to develop a multilinear perspective on a much broader family of curvature-related problems in harmonic analysis, including important classes of Radon-like transforms that arise naturally in the theory of dispersive partial differential equations.

The specific objectives represent major challenges at the emerging frontiers of harmonic analysis with a variety of disciplines, including geometric analysis (encompassing heat-flow methods and convex geometry), affine geometry and algebraic topology.","1042293","2012-10-01","2018-09-30"
"TOI","Theoretical Foundations and Observational Tests of Inflationary Cosmology","Daniel Baumann","UNIVERSITEIT VAN AMSTERDAM","Observations of the temperature fluctuations in the cosmic microwave background (CMB) have transformed cosmology into a quantitative science.  This revolution has led to a detailed understanding of the geometry and composition of the universe. However, the microphysical origin of the CMB fluctuations remains a mystery. Although quantum-mechanical fluctuations during a early period of inflationary expansion provide a remarkably successful description of the data, a concrete realization of inflation in a fundamental theory of particle physics remains elusive.  Moreover, it seems almost certain that the inflationary era involved physics beyond the Standard Model. This is both a challenge and a fantastic opportunity to learn about physics at the highest energy scales from cosmological observations.

With the ERC grant I plan to form a group at Cambridge University that will explore both the theoretical foundations and the observational consequences of inflation.  A key question that a satisfactory microscopic theory of inflation should answer is why the mass of the inflaton field is much smaller than its natural value. We will search for answers to this question both from the bottom up in effective field theory (EFT) and from the top down in string theory.  We will furthermore initiate the first systematic study of large-field inflation, thereby providing the theoretical foundation for inflationary models with observable gravitational waves.  To make contact with observations we will then develop an EFT for the inflationary fluctuations as Goldstone bosons of spontaneously broken time-translations. We will compare the predictions of that theory to the latest CMB observations.  Finally, we will use the data to pursue two of the central signatures of the physics of inflation: gravitational waves and non-Gaussianity.","1493535","2011-10-01","2016-09-30"
"TOP","Towards the Bottom of the Periodic Table","Kristina KVASHNINA","HELMHOLTZ-ZENTRUM DRESDEN-ROSSENDORF EV","Actinide and lanthanide chemistry is currently experiencing a renaissance, due to the prospects of obtaining novel materials relevant for applications in chemical synthesis, electronics, materials science, nanotechnology, biology and medicine. Most of the fascinating properties of the lanthanide and actinide materials are related to the partially filled 4f/5f valence shell and in contrast to the rest of the periodic table, are poorly understood.  This includes the surprising reactivity, magnetic and crystal structure properties and, the rather unpredictable, covalent or ionic nature of their bonds. It is now possible to study the chemistry of the f-block elements using state-of-the-art techniques that were not available before. Two new synchrotron-based techniques, high energy resolution fluorescence detection (HERFD) X-ray absorption spectroscopy and resonant inelastic X-ray scattering (RIXS), can now provide unprecedented detailed information on processes such as the electron-electron interactions, hybridization between molecular orbitals, the nature of their chemical bonding, and the occupation and the degree of the f-electron localization. Therefore, I propose to apply these cutting-edge techniques to advance the fundamental understanding of the lanthanide and actinide nanoparticles, an outstanding problem in materials science, chemistry and environmental science technology. The research will be conducted at the European Synchrotron (ESRF), at the Rossendorf Beamline (ROBL) dedicated to actinide science, where we recently installed a novel X-ray emission spectrometer with ground-breaking detection limits. The experimental work will be complemented by electronic structure calculations. The combined experimental and theoretical data will provide an essential knowledge of lanthanide and actinide chemistry, significant for topics of high societal relevance, like green chemistry, renewable energy and catalysis.","1499625","2018-01-01","2022-12-31"
"TopDyn","Probing topology and dynamics in driven quantum many-body systems","Mark Rudner","KOBENHAVNS UNIVERSITET","If the twentieth century was about discovering the basic laws of quantum mechanics, then the twenty first century will be about pushing them to new frontiers and learning how to control them. Condensed matter systems are predicted to host many intriguing and potentially useful quantum phenomena, though materials where they can be realized are rare. This motivates me to seek alternative routes for their realization, and to find new means for controlling quantum many-body systems.

In this project I aim to provide a deeper and broader theoretical understanding of quantum dynamics in driven many-body systems, and to expose new routes for experimental investigation. As a major research theme, my team will investigate possibilities for using time-dependent fields to realize topological phenomena through strong driving. The theoretical description and realization of such phenomena is a multifaceted problem that will serve as a vehicle for elucidating many general aspects of driven quantum dynamics that are relevant on an even broader scale.

To achieve my broad goals I propose an ambitious work plan, organized into three interrelated work packages focused on: 1) characterizing, 2) realizing, and 3) probing the static, dynamic, and topological properties of driven quantum systems. In some cases we will study conceptually pure, minimal models, designed to illustrate the interplay between driving and interactions. We will also investigate realistic, experimentally-motivated models, seeking to understand the key factors and processes that govern the realization of topological phenomena in driven systems, and how to control them. In addition, we will study non-equilibrium probes of correlated systems, focusing on using the nuclear spin environments of electronic systems to probe and control the systems' magnetic properties.  Through each of these tracks we will gain valuable new insight into the nature and dynamics of quantum many-body systems, far from equilibrium.","1205000","2016-02-01","2021-01-31"
"TopFront","Expanding the Topological Frontier in Quantum Matter: from Concepts to Future Applications","Netanel Hanan Lindner","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","Topological phases arise from a fascinating interplay between quantum mechanics and many-body physics. They exhibit an abundance of extraordinary properties, such as protected edge and surface modes, exotic particle statistics, and non-local correlations. These make them not only scientifically stimulating, but also appealing for ground-breaking future applications, such as quantum computing using non-Abelian systems. Their subtle nature often renders them hard to study theoretically, and even more so to detect and control experimentally. To date, only a small subset of them has been accessed in experiments. The purpose of this research program is to expand the scope of possible realizations of topological quantum matter, and to develop methods to detect, control and manipulate them. Two main research directions will be considered. The first will focus on utilizing defects to synthesize new non-Abelian systems. We will study the mathematical theory describing the defects, starting from microscopic considerations and aiming to achieve a unifying mathematical framework. New non-Abelian phases arising in networks of coupled defects will be explored. Protocols for controlling non-Abelian anyons and zero modes will be developed and optimized, aiming to minimize errors arising from imperfections in physical implementations. The second direction will explore the exciting possibility of inducing topological behaviour in non-equilibrium systems. Periodically driven systems, such as matter interacting with light, can exhibit anomalous topological phenomena with no analogue in static systems, which we intend to reveal and classify. We will study the unique many body physics arising from the interplay of topological Bloch-Floquet band structures, inter-particle interactions, and coupling to the environment. Finally, for both research directions we will consider possible experimental realizations in a variety of solid state and cold atom systems along with designated probes.","1500000","2015-09-01","2020-08-31"
"TopInSy","Novel phases of matter emerging from topology, interactions, and symmetries","Benjamin Beri","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The interplay of symmetry and topology can give rise to fascinating new forms of matter. Prime examples are topological insulators and topological superconductors with Majorana fermions, discoveries which resulted from a breakthrough extension of noninteracting band theory to include symmetry and topology. This proposal aims to go beyond this noninteracting paradigm and explore systems where strong interactions, symmetry, and topology conspire to give rise to new strongly correlated phases of matter. The broad scientific goals are to theoretically obtain (i) tangible microscopic models and (ii) predictions on the clearest experimental signatures of these novel symmetry protected topological phases (SPTs). The project will consider both fermionic and bosonic systems, where in the latter case even the most direct analogues of topological insulators require strong interactions, as mere band theory is inapplicable due to the absence of Pauli exclusion. We will examine systems with various interactions and symmetries (e.g., time-reversal, particle number conservation, spin-rotation, inversion and other crystalline symmetries), aiming to deliver results with ramifications in all three dimensionalities. Characteristic aspects of our approach include, for microscopics, the emphasis on well controlled analytical methods to expose physically transparent links to phenomenology; and, for experimental signatures, exploring and utilising novel strong correlations emerging in hybrids formed of SPTs and their probes, which would be absent in either of the probed systems or the probes alone. Accomplishing our research objectives will fill a major scientific gap by bridging the findings of recent abstract mathematical classifications of possible SPTs, and the tremendous experimental progress both in the solid-state and with ultracold atoms on topological systems that may already provide the fundamental building blocks for physical realisations.","1300000","2016-09-01","2021-08-31"
"TOPIOS","Tracking Of Plastic In Our Seas","Erik VAN SEBILLE","UNIVERSITEIT UTRECHT","The amount of plastic in our ocean is exponentially growing, with recent estimates of more than 5 million metric tonnes of plastic reaching the ocean each year. This plastic infiltrates the ocean food chain and thus poses a major threat to marine life. However, understanding of plastic movement and its budget in the ocean is inadequate to fully establish its environmental impact, prompting the EU and G7 to recently make marine litter a top science priority.
It is now recognised that the amount of plastic entering our ocean is several orders of magnitude larger than the estimates of floating plastic on the surface of the ocean. More than 99% of plastic within our ocean is therefore ‘missing’.
This project will make breakthroughs towards closing the plastic budget by creating a novel comprehensive modelling framework that tracks plastic movement through the ocean. Building on well-established previous work to follow generic water parcels through hydrodynamic ocean models, this project will modify these ‘virtual’ parcels to represent pieces of plastic by, for the first time, simulating fragmentation, sinking, beaching, wave-mixing and ingestion by biota. 
The new parameterisations that underpin this modelling will be based on field data and new coastal flume wave tank lab experiments. The simulated plastic particles will be tracked within state-of-the-art hydrodynamic ocean models, in order to compute maps of pathways and transports around our oceans and on coastlines and in biota. This numerical modelling will be used to evaluate a broad suite of scenarios and test hypotheses, including where the risk to marine biota is greatest. 
The results from this project will inform policymakers and the public on which countries, for example, are responsible for which part of the plastic problem, crucial for mitigation and legal frameworks. It will also inform engineers on where and how to best invest resources in mitigating the problem of plastic in our ocean.","1484760","2017-04-01","2022-03-31"
"TOPO-NW","VISUALIZATION OF TOPOLGICAL STATES IN PRISTINE NANOWIRES","Haim Beidenkopf","WEIZMANN INSTITUTE OF SCIENCE","Topological phases of matter have been at the center of intense scientific research. Over the past decade this has led to the discovery of dozens of topological materials with exotic boundary states. In three dimensional topological phases, scanning tunneling microscopy (STM) has been instrumental in unveiling the unusual properties of these surface states. This success, however, did not encompass lower dimensional topological systems. The main reason is surface contamination which is disruptive both for STM and for the fragile electronic states. We propose to study topological states of matter in pristine epitaxial nanowires by combining growth, fabrication and STM, all in a single modular ultra-high vacuum space. This platform will uniquely allow us to observe well anticipated topological phenomena in one dimension such as the Majorana end-modes in semiconducting nanowires. On a broader view, the nanowire configuration intertwines dimensionality and geometry with topology giving rise to novel topological systems with high tunability. A vivid instance is given by topological crystalline insulator nanowires in which the topological symmetry protection can be broken by a variety of perturbations. We will selectively tune the surface states band structure and study the local response of massless and massive surface Dirac electrons. Tunability provides a higher degree of control. We will utilize this to realize topological nanowire-based electronic and spintronic devices such as a Z2 pump and spin-based Mach-Zehnder interferometer for Dirac electrons. The low dimensionality of the nanowire alongside various singularities in the electronic spectra of different topological phases enhance interaction effects, serving as a cradle for novel correlated topological states. This new paradigm of topological nanowires will allow us to elucidate deep notions in topological matter as well as to explore new concepts and novel states, thus providing ample experimental prospects.","1750000","2016-01-01","2020-12-31"
"Topo2DEG","Topological states in superconducting two-dimensional electron gases","Fabrizio Nichele","IBM RESEARCH GMBH","I will experimentally investigate hybrid superconductor/semiconductor devices for realizing novel topological states of matter, with interest both in fundamental physics and quantum computing applications. Common denominator of the proposed experiments is a regime where the characteristic energy scales of the system, namely Fermi energy, spin orbit interaction correction, superconducting gap and Zeeman splitting are comparable to each other, resulting in unique and mostly uncharted physical territories. Differently from the most widespread use of semiconductor nanowires coupled to superconductors, I will employ novel hybrid two-dimensional electron gases (2DEGs) where the superconductor is grown in-situ and matched to the semiconductor lattice. This novel system was mainly developed by the team I supervise, during the last two years. Compared to the conventional nanowire-based approach, hybrid 2DEGs are readily available, characterized by very low disorder and more amenable to complex sample designs. The work will focus on: 1) Taking full advantage of the planar geometry to study spatial and non-local properties of individual Majorana wires, as well as branched geometries. These experiments will constitute critical tests to establish if the commonly observed zero bias peaks are indeed associated with Majorana modes and pave the way to complex networks of interacting Majorana wires, a requirement for quantum computing. 2) Studying topological phenomena in multi-terminal Josephson junctions (JJs), with particular emphasis on tuning the superconducting phase difference across electrodes pairs. Topological JJs offer a new and possibly advantageous path forward to create and manipulate Majorana modes not explored up to date, including the possibility to reach the topological regime for vanishing small external magnetic fields, useful for applications. Success of the proposal will constitute a key step forward towards topological quantum computing.","1999916","2019-03-01","2024-02-29"
"TopoCold","Manipulation of topological phases with cold atoms","Nathan GOLDMAN","UNIVERSITE LIBRE DE BRUXELLES","Topological states of matter constitute one of the hottest disciplines in quantum physics, demonstrating a remarkable fusion between elegant mathematical theories and technological applications. However, solid-state experiments only provide a limited set of physical systems and probes that can reveal non-trivial topological order. It is thus appealing to seek for alternative setups exhibiting topological properties. Cold atoms in optical lattices constitute an instructive and complementary toolbox, being extremely versatile, clean and controllable. In fact, cold-atom theorists and experimentalists have recently developed new tools providing the building blocks for the exploitation of topological atomic gases.

TopoCold will propose realistic optical-lattice setups hosting novel topologically-ordered phases, based on those technologies that are currently developed in cold-atom experiments. The central goal of the project consists in identifying unambiguous manifestations of topological properties that are specific to the cold-atom framework. We will establish concrete methods to experimentally visualize these signatures, elaborating efficient schemes to detect the unique features of topological phases using available manipulation and imaging techniques. This central part of the TopoCold project will deepen our understanding of topological phenomena and guide ongoing experiments. We also plan to elaborate simple protocols to exploit topological excitations, based on the great controllability of atom-light coupling methods. Moreover, by tailoring the geometry and laser-coupling of optical-lattice setups, we will explore topological systems that are not accessible in solid-state devices. Finally, we will study the properties of topological phases that arise in the strongly-correlated regime of atomic gases. TopoCold will build a bridge between several communities, deepening our knowledge of topological phases from an original and interdisciplinary perspective.","1038039","2017-02-01","2022-01-31"
"TOPODY","Exploring topological matter with atomic Dysprosium","Sylvain NASCIMBENE","ECOLE NORMALE SUPERIEURE","Recently, topology stepped in an increasing number of areas in physics, including via the concept of topological phases of matter. In strongly interacting systems, topological phases may exhibit intricate quantum entanglement between their constituents, leading to fascinating physical properties, such as the emergence of anyons. Since the discovery of fractional quantum Hall states in 1982, the scientific community is awaiting further experimental advances: new types of strongly correlated states, observation/manipulation of anyons. In this field, ultracold atoms promise novel approaches with a specific degree of control. Yet, despite the recent creation of weakly interacting topological states with atomic gases, reaching the strongly correlated regime, with long-range quantum entanglement, remains an open challenge.
In TOPODY, I will use a novel approach to produce strongly correlated topological states with microscopic samples of atomic Dysprosium. The envisioned laboratory experiment will combine state-of-the-art techniques, such as single-atom detection or laser-induced spin-orbit coupling. It will allow preparing quantum gases subjected to artificial gauge fields beyond the previously accessible regimes. The project will focus on realizing two paradigmatic physical systems – the Laughlin state and a topological superfluid. 
1. We will create the Laughlin state by injecting a controlled amount of angular momentum using optical transitions with finely-shaped laser beams. We will infer distinctive features of the Laughlin state – incompressibility and atom anti-bunching – from the distribution of atom positions. 
2. We will produce strongly interacting Fermi gases in one dimension and subjected to a spin-orbit coupling, leading to a topological superfluid state. The topology will manifest itself by the presence of Majorana bound states, which are quantum states delocalized between the two ends of the system, and accessible using quasi-particle spectroscopy.","1500000","2018-01-01","2022-12-31"
"Topolectrics","Emergence of Topological Phases from Electronic Interactions","Ronny Thomale","JULIUS-MAXIMILIANS-UNIVERSITAT WURZBURG","Topological properties of a quantum state of matter describe global signatures which are invariant under weak local perturbations of the system. Such scenarios constitute a major branch of contemporary condensed matter physics and exhibit remarkable phenomena such as quantized edge channels of a bulk-incompressible phase as well as fractionalized quantum numbers and statistics of quasiparticles. In the TOPOLECTRICS ERC starting grant research plan, we investigate topological quantum phases which result from electronic interactions. The key objective is to provide a rigorous link between bare electronic models and low energy effective models hosting emergent topological quantum phases. This task is approached from two perspectives. First, in a top-down approach, renormalization group schemes are developed and employed for weakly and strongly interacting electrons to derive effective models from realistic bare electronic scenarios. We investigate how Fermi liquids can host topological phases such as Chern insulators, topological insulators, and topological superconductors as well as how Mott phases of frustrated magnets can drive the system into a spin liquid regime. The central goal is to identify the crucial parameters which control the stabilization of such phases and to provide a direct feedback for experimental setups. Second, in a bottom-up approach, entanglement spectrum analysis along with reverse Hamiltonian model building is applied to topologically ordered quantum states such as fractional quantum Hall states, spin liquids, spin chain states, as well as fractional Chern and fractional topological insulators. The aim is to develop effective models hosting such topological quantum states and to reconnect them with bare electronic models as well as to investigate key experimental signatures. Ultimately, the goal is to fuse both approaches to develop a microscopically substantiated procedure to identify electronic topological quantum phases in nature.","1307503","2013-11-01","2018-10-31"
"TopoMat","Topological insulators: computational exploration of emerging electronic materials","Oleg Yazyev","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","""Topological insulators are the recently discovered class of materials that have a bulk electronic band gap, but also exhibit conducting surface states as a result of non-trivial band structure topology induced by strong spin-orbit interactions. These peculiar surface states are characterized by a number of novel properties such as graphene-like band dispersion and helical spin textures with no spin degeneracy which makes them immune to scattering. Due to these extraordinary properties, topological insulators are expected to find numerous technological applications, in particular in electronics and its next-day extensions - spintronics and quantum information processing. The project aims at exploring these novel electronic materials in silico; that is, by performing computer simulations using numerical methods of various levels of complexity. The global objective of the proposed research plan is to provide the theoretical support required to build a link between the fundamental physical properties of topological insulators and their prospective technological application by investigating materials science, chemistry and device-related aspects of these materials. In particular, the following three research directions will be explored: (i) understanding the structure-property relations and rational design of new materials with topologically non-trivial electronic structures, (ii) modeling realistic """"second generation"""" topological insulators (bismuth chalcogenides Bi2Se3, Bi2Te3, and related materials), and (iii) investigating magnetic and transport properties of topological insulators and their interfaces related to the device applications. The proposed interdisciplinary program intends to make a strong impact in basic research and future technologies, and will eventually promote the rapidly expanding field of topological insulators in European science.""","1643998","2012-09-01","2017-08-31"
"TOPOPLAN","Topographically guided placement of asymmetric nano-objects","Armin Wolfgang Knoll","IBM RESEARCH GMBH","""The controlled synthesis of nanoparticles in the form of spheres, rods and wires has led to a variety of applications. A much wider spectrum of applications e.g. in integrated devices would be available if a precise placement and alignment relative to neighbouring particles or other functional structures on the substrate is achieved. A potential solution to this challenge is to use top-down methods to guide the placement and orientation of nanoparticles. Ideally, a precise orientation and placement is achieved for a wide range of particle shapes, a so far unresolved challenge.

Here we propose to generate a tunable electrostatic potential minimum by exploiting double-layer potentials between two confining surfaces in liquid. The shape of the potential is determined by the local three-dimensional topography of the confining surfaces. This topography can be precisely tailored using the patterning technology that has been developed in our research group. The potential shape can be adapted to fit to a wide range of particle shapes. The trapping energies exceed the thermal energies governing Brownian motion and trap and orient particles reliably. After trapping, the particles are transferred in a subsequent step onto the substrate by external manipulation.

The separation of the trapping and placement steps has several unique advantages over existing strategies. High aspect ratio structures or fragile pre-assembled structures like nanoparticles linked by DNA strands can be pre-aligned in the trapping field and placed in the desired geometry. For applications like the placement of quantum dots into high fidelity cavities, the trapped particles can be examined optically and repelled if the spectral properties do not match. In particular the precise positioning of nanowires is promising to build up complex circuits for (opto-)electronic applications. Additionally, the trapping and placement processes proceed in parallel and high throughput values can be achieved.""","1496526","2012-10-01","2017-09-30"
"TOPOQDot","A bottom-up topological superconductor based on quantum dot arrays","Eduardo Jian Hua LEE","UNIVERSIDAD AUTONOMA DE MADRID","The realization of a topological superconductor (TS) and of Majorana fermion (MF) quasiparticles promises to open new avenues towards decoherence-robust topological quantum computing. However, further developments in this direction, including the investigation of their topological properties, have been hindered by the lack of a fully conclusive demonstration. In setups based on 1D nanostructures, e.g. semiconductor nanowires and magnetic adatom chains, this is linked to the difficulty to unambiguously assign the main reported signatures, a zero-bias peak in Andreev conductance, to MF modes. This proposal aims to overcome these limitations by exploring an alternative approach in which a 1D TS is built from the bottom up. In particular, arrays of proximity-coupled semiconductor quantum dots (QDs) will be explored as a platform for emulating the Kitaev chain. Such an approach offers the advantage that the evolution of subgap states into MF modes is followed during the assembly of the TS, thereby providing conclusive evidence of their realization. It also enables to controllably adjust the chain parameters to their optimal values  where the topological array is most robust. 
Recent work from the PI addressed in detail the single QD limit of these arrays, where important milestones have been reached. These include the demonstration of spin-polarized subgap states that are the atomic precursors of topological chains and MF modes, and of the precise electrical control over the parameters of proximity-coupled QDs. Here, the PI aims to take the subsequent steps and study the assembly of these building blocks into a 1D TS. The main goals will be to:
i) study the hybridization of subgap states into molecular levels as well as the spinless superconducting pairing in double QDs. This will shed light on the mechanisms involved in the formation of arrays;
ii) show conclusive signatures of MF quasiparticles in a topological triple QD array;
iii) study properties of MF modes.","1750625","2017-06-01","2022-05-31"
"TOPP","Topological phononics through nano-optomechanical interactions","Ewold VERHAGEN","STICHTING NEDERLANDSE WETENSCHAPPELIJK ONDERZOEK INSTITUTEN","The concept of topology has proven very powerful in predicting and explaining a range of intriguing phenomena for the behaviour of electrons in solid-state materials, including the existence of topological phases that offer robust transport with unusual characteristics. This program aims to create and study topological behaviour for sound in systems that couple light and mechanical motion through radiation pressure. I will realize on-chip nano-optomechanical systems where many optical and mechanical modes interact. Through this interaction, suitable laser illumination will induce phononic transport with properties that are not found in any natural material. In particular, they will establish topologically nontrivial acoustic phases that we will create and explore in space and time using a new optical measurement technique. The induced behaviour crucially depends on the extreme coupling between optical fields and nanomechanical resonators when both are confined at the nanoscale. I aim to study three manifestations of topological behaviour: (1) The creation of an acoustic quantum Hall phase, in which topologically protected unidirectional sound propagation should emerge, in edge modes that are robust against disorder. (2) The formation of a new type of topological insulator arising from optomechanical nonlinearity, supporting topological acoustic solitons with unusual dynamics. (3) The breaking of parity-time symmetry in mechanical systems that undergo both optical damping and amplification, a phase transition with unique topological properties.","1500000","2018-01-01","2022-12-31"
"ToRH","A Theory of Reliable Hardware","Christoph Peter LENZEN","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","The complexity of computing hardware keeps increasing at a rapid pace, with no end in sight. With growing number of components and ongoing miniaturization, hardware engineers struggle with an increasing number and variety of faults, yet must guarantee correct operation of the system as a whole. This is hindered by a concurrent increase in design complexity, i.e., the trend of integrating ever more diverse circuits into larger systems, which results in a higher complexity of verifying the correctness of a given design -- especially when providing for the possibility that some components may fail.

The goal of this project is to develop a holistic mathematical approach to modeling fault-tolerant circuits and demonstrate its usefulness in practice. Such a framework will exhibit several advantages over the present methods, which are largely based on simulation and experimentation: 

(i)   mathematical proofs offer parametrized guarantees, which implies that the derived building blocks can easily be re-used in varying configurations and translated to different technologies;
  
(ii)  it permits statements about general fault types, entailing that claimed properties do not rely on specific fault behavior (which depends on operational parameters and technology); and
  
(iii) abstract, parametrized reasoning enables to design and optimize for long-term scalability.

While this approach to fault-tolerance has been successfully applied in the area of distributed computing for decades, transferring it to low-level hardware design introduces new obstacles, such as very limited computational capabilities of the basic components and the potential for metastability. Overcoming these challenges will pave the way for highly dependable and scalable systems, and thus help in further sustaining the exponential growth in available computing power commonly referred to as Moore's Law.","1472000","2017-08-01","2022-07-31"
"TOROS","A Theory-Oriented Real-Time Operating System for Temporally Sound Cyber-Physical Systems","Björn BRANDENBURG","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","""The TOROS project targets the challenge of implementing safety-critical cyber-physical systems (CPSs) on commodity  multicore processors such that their temporal correctness can be certified in a formal, trustworthy manner.
While today  it is in principle possible to construct a CPS in a temporally sound way, in practice   this rarely happens because, with the current real-time foundations, the prerequisite investments in time, expertise, and resources are prohibitive. 

This situation is caused in large parts by three fundamental shortcomings in the design of state-of-the-art real-time operating systems (RTOSs) and the applicable timing analyses: (i) current RTOSs expose primarily low-level mechanisms that suffer from accidental unpredictability, i.e., mechanisms that require too much expertise to be used and composed in a temporally sound way;  (ii)  most analyses rely on  idealized worst-case execution-time assumptions that realistically cannot be satisfied on commodity multicore platforms; and (iii) the available real-time theory depends on often complex and tedious proofs, and cannot always  be trusted to be sound. 
As a result,  formal timing analysis is rarely relied upon in the certification of CPSs in reality, and instead
  the use of ad-hoc, unsound """"safety margins"""" prevails. 

The TOROS project seeks to close this gap by moving the RTOS closer to analysis, the analysis closer to reality, and by ensuring that the analysis can be trusted. 
Specifically, the TOROS project will
 1. introduce a radically new, theory-oriented RTOS that by design ensures that the temporal behavior of any workload can be analyzed (even if the application developer is unaware of the relevant theory), 
 2. develop a matching novel timing analysis that allows for below-worst-case provisioning with analytically sound safety margins  that yields meaningful probabilistic response-time guarantees, and
 3. mechanize and verify all supporting timing analysis with the Coq proof assistant.""","1499813","2019-01-01","2023-12-31"
"TOSSIBERG","Theory of Stein Spaces in Berkovich Geometry","Jérôme, Jacques, René Poineau","UNIVERSITE DE CAEN NORMANDIE","Complex Stein spaces may be thought of as analytic analogues of the affine schemes of algebraic geometry. They may be characterized in several manners: using convergence of holomorphic functions, topological properties or potential-theoretic properties, for instance. Especially useful for applications is the fact that their coherent cohomology vanishes. Despite the crucial importance of this theory in complex analytic geometry, its p-adic counterpart has hardly been sketched.
 In the setting of Berkovich geometry (one among the several notions of p-adic geometry), recent developments have enabled to get a fine understanding of the topology of the spaces (work of Berkovich and Hrushovski-Loeser) and to define the basic tools of potential theory (work of Baker-Rumely, Thuillier, Boucksom-Favre-Jonsson and Chambert-Loir-Ducros). The conditions for a comprehensive study of p-adic Stein spaces are now met; this will be our first goal. The theory will then be used to investigate envelopes of holomorphy and meromorphy. As an application, I plan to derive rationality criteria for power series over function fields.
 The second part of the project is devoted to the theory of Stein spaces for Berkovich spaces over rings of integers of number fields (where all the places appear on an equal footing). Those spaces have hardly been studied and only a very small part of the usual analytic machinery is available in this setting. Here, my main goal will consist in proving the basic and fundamental fact that relative polydisks are Stein spaces (in the cohomological sense). This will allow a deeper investigation of rings of convergent arithmetic power series (i.e. with integral coefficients) and will lead up to properties related to commutative algebra but also to the inverse Galois problem. Knowing that the coherent cohomology of polydisks vanishes also opens the road towards computing global cohomology groups for projective analytic spaces over ring of integers of number fields.","1153750","2015-07-01","2020-06-30"
"TOTAL","Technology transfer between modern algorithmic paradigms","Marek Adam Cygan","UNIWERSYTET WARSZAWSKI","The two most recognized algorithmic paradigms of dealing with NP-hard problems in theoretical computer science nowadays are approximation algorithms and fixed parameter tractability (FPT). Despite the fact that both fields are by now developed, they have grown mostly on their own. In our opinion the two fields have critical mass allowing for synergy effects to appear when using techniques from one area to obtain results in the other, which is the main theme of the project.
Furthermore, practical effectiveness of randomized local search heuristics is a not yet understood phenomenon. We believe that substantial increase of our understanding of superpolynomial running times in recent years should allow for rigorous proofs of parameterized and approximation results obtained by those methods.
Based on our experience with parameterized complexity and approximation algorithms we propose three research directions with potential of solving major long-standing open problems in both areas with the following objectives.
(a) Transfer from Approximation to FPT: use the PCP theorem to prove lower bounds for exact parameterized algorithms under the Exponential Time Hypothesis and take advantage of extended formulations in FPT branching algorithms.
(b) Transfer from FPT to Approximation: utilize parameterized algorithms tools in local-search-based approximation algorithms and explore the potential of proving new inapproximability results based on the Exponential Time Hypothesis.
(c) Rigorous Analysis of Practical Heuristics: unravel the practical effectiveness of randomized local search heuristics by proving their parameterized and approximation properties, when given subexponential or even moderately exponential running time.
Our objectives lie in the frontier of fixed parameter tractability and approximation areas. Complete resolution of our goals would dramatically change our understanding of both areas, with further consequences in other disciplines within computer science.","1417625","2016-04-01","2021-03-31"
"TQFT","The geometry of topological quantum field theories","Katrin Wendland","ALBERT-LUDWIGS-UNIVERSITAET FREIBURG","The predictive power of quantum field theory (QFT) is a perpetual driving force in geometry. Examples include the invention of Frobenius manifolds, mixed twistor structures, primitive forms, and harmonic bundles, up to the discovery of the McKay correspondence, mirror symmetry, and Gromov-Witten invariants. Still seemingly disparate, in fact these all are related to topological (T) QFT and thereby to the work by Cecotti, Vafa et al of more than 20 years ago.  The broad aim of the proposed research is to pull the strands together which have evolved from TQFT, by implementing insights from mathematics and physics. The goal is a unified, conclusive picture of the geometry of TQFTs. Solving the fundamental questions on the underlying common structure will open new horizons for all disciplines built on TQFT. Hertling’s “TERP” structures, formally unifying the geometric ingredients, will be key. The work plan is textured into four independent strands which gain full power from their intricate interrelations.  (1) To implement TQFT, a construction by Hitchin will be generalised to perform geometric quantisation for spaces with TERP structure. Quasi-classical limits and conformal blocks will be studied as well as TERP structures in the Barannikov-Kontsevich construction of Frobenius manifolds.  (2) Relating to singularity theory, a complete picture is aspired, including matrix factorisation and allowing singularities of functions on complete intersections. A main new ingredient are QFT results by Martinec and Moore.  (3) Incorporating D-branes, spaces of stability conditions in triangulated categories will be equipped with TERP structures. To use geometric quantisation is a novel approach which should solve the expected convergence issues.  (4) For Borcherds automorphic forms and GKM algebras their as yet cryptic relation to “generalised indices” shall be demystified: In a geometric quantisation of TERP structures, generalised theta functions should appear naturally.","750000","2009-01-01","2014-06-30"
"TRACE","Tephra constraints on rapid climatic events","Siwan Manon Davies","SWANSEA UNIVERSITY","Little has challenged our understanding of climate change more so than the abruptness with which large-scale shifts in
temperature
occurred during the last glacial period. Atmospheric temperature jumps of 8-16°C, occurring within decades over Greenland,
were closely
matched by rapid changes in North Atlantic sea surface temperatures. Though these climatic instabilities are
well-documented in various
proxy records, the causal mechanisms of such short-lived oscillations remain poorly understood. Two hypotheses have been
proposed:
one relating to the behaviour of the ocean circulation and the other to the dynamics of the atmosphere. Testing these
hypotheses,
however, is severely hampered by dating uncertainties that prevent the integration of proxy records on common timescales.
As a result
unravelling the lead/lag responses (hence cause and effect) between the Earth’s climate components is currently beyond our
reach.
TRACE will exploit a powerful new approach whereby microscopic traces of volcanic events are employed to precisely
correlate proxy
records from the North Atlantic region to assess the phasing relationships between the atmosphere and the ocean during
these rapid
climatic events. Volcanic layers have the unique advantage of representing fixed time-lines between different proxy records.
This
correlation tool has experienced a considerable step-change in recent years, with invisible layers of volcanic ash traced over
much wider
geographical regions than previously thought. What is more, recent work has identified new, previously unknown eruptions -
several of
which coincide with the rapid climatic jumps imprinted in the proxy records. Thus tephra isochrones represent (perhaps the
only)
independent constraints for resolving past events on decadal timescales.","1471117","2011-09-01","2017-02-28"
"TRAM","Transport at the microscopic interface","Rob Gerhardus Hendrikus Lammertink","UNIVERSITEIT TWENTE","The research objective is to study and model membrane separation processes at the microscopic scale. I propose to exploit microfluidic platforms that contain a certain membrane separation challenge to be studied; i.e. biofouling, overlimiting current, and concentration polarization. Each challenge will be the topic for an individual PhD student.","1500000","2012-09-01","2017-08-31"
"TRAM3","Traffic Management by Macroscopic Models","Paola Goatin","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","We propose to investigate traffic phenomena from the macroscopic point of view, using models derived from fluid-dynamics consisting in hyperbolic conservation laws. In fact, even if the continuum hypothesis is clearly not physically satisfied, macroscopic quantities can be regarded as measures of traffic features and allow to depict the spatio-temporal evolution of traffic waves.
Continuum models have shown to be in good agreement with empirical data. Moreover, they are suitable for analytical investigations and very efficient from the numerical point of view. Therefore, they provide the right framework to state and solve control and optimization problems, and we believe that the use of macroscopic models can open new horizons in traffic management.
The major mathematical difficulties related to this study follow from the mandatory use of weak (possibly discontinuous) solutions in distributional sense. Indeed, due to the presence of shock waves and interactions among them, standard techniques are generally useless for solving optimal control problems, and the available esults are scarce and restricted to particular and unrealistic cases. This strongly limits their applicability.
Our scope is to develop a rigorous analytical framework and fast and efficient numerical tools for solving optimization and control problems, such as queues lengths control or buildings exits design. This will allow to elaborate reliable predictions and to optimize traffic fluxes. To achieve this goal, we will move from the detailed structure of the solutions in order to construct ad hoc methods to tackle the analytical and numerical difficulties arising in this study. The foreseen applications target the sustainability and safety issues of modern society.","809000","2010-10-01","2016-03-31"
"TRANSCALE","Reconciling Scales in Global Seimology","Thomas BODIN","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","For more than 30 years, seismologists have used seismic waves to produce 3D images of the structure of the Earth. Despite many successes, a number of key questions still remain, which are of the uttermost importance to understand plate tectonics. What is the nature of the Lithosphere-Asthenosphere Boundary? What is the structure and history of the continental lithosphere? 

The problem is that different seismic observables sample the Earth at different scales; they have different sensitivity to structure, and are usually interpreted separately. Images obtained from short period converted and reflected body waves see sharp discontinuities, and are interpreted in terms of thermo-chemical stratification, whereas seismic models constructed from long period seismograms depict a smooth and anisotropic upper mantle, and are usually interpreted in terms of mantle flow. However, sharp discontinuities may also produce effective anisotropy at large scales, and only a joint interpretation of different frequency bands can allow to fully localizing the patterns of deformation in the mantle. 

The proposed work consists in developing and applying an entirely new approach to geophysical data interpretation, where different data types sampling the Earth at different scales are jointly embraced into a single Bayesian procedure. This proposal focuses on theoretical, algorithmic and computational advances needed for a new generation of tomographic models. We will use the large amount of data available in North-America (surface wave measurements, scattered body waves, SKS splitting measurements) to produce a multiscale model under North-America, depicting both discontinuities and anisotropy. This will allow us to answer some crucial questions about the structure and evolution of Earth. We will also produce a first fully Bayesian global Earth model by jointly inverting normal modes, surface and body wave observations.","1498750","2017-04-01","2022-03-31"
"TRANSDESIGN","Design of Phase Transition Kinetics in Non-Equilibrium Metals","Stefan POGATSCHER","MONTANUNIVERSITAET LEOBEN","The first technological use of non-equilibrium phase transitions in metals for designing properties is documented as ~800 BC, but the time has come to make a leap forward. Nearly all classes of materials show non-equilibrium phase transitions. Understanding how fast these transitions occur is a key question in materials science. In metals, kinetics is connected to diffusion via atomic lattice vacancies. However, there is no universal sound and predictive physical understanding of the kinetics under non-equilibrium situations so far, because theory cannot be verified experimentally. The in-situ measuring of non-equilibrium vacancy evolution is not possible at industrially relevant and controlled high thermal rates today. Moreover, direct microscopic observation of vacancies in bulk metals has not yet been achieved.
The development of unique strategies for in-situ measuring of non-equilibrium vacancy evolution and the microscopic observation of atomic lattice vacancies and their motion will be the main breakthroughs of TRANSDESIGN. Observing non-equilibrium vacancy annihilation via ultrafast chip calorimetry offers unique advantages and will be a ground-breaking step to understand non-equilibrium diffusion. Moreover, this will also establish novel chip calorimetry as a standard in thermal analysis of relevant metals.
Within TRANSDESIGN we utilize high image contrast solutes, which trap vacancies, as markers for an identification of vacancies via field ion and scanning transmission electron microscopy. This unique strategy will enable the observation of “vacancies at work” in the bulk of metals.
The project will close longstanding experimental-theoretical gaps with significant impact on the optimization and design of new kinetically driven processes and products in the field of metallurgy. However, the fundamentals gained within TRANSDESIGN are universal and will significantly contribute to the advancement of the European competence in materials science.","1499679","2018-02-01","2023-01-31"
"TRANSPORET","Transmembrane Molecular Machines","Scott Cockroft","THE UNIVERSITY OF EDINBURGH","This research programme is dedicated to the development of rationally designed transmembrane molecular machines. Proteins that undergo nanomechanical transitions to facilitate transmembrane communication, or that operate as transmembrane pumps, rotary motors, and molecular transporters are ubiquitous in nature. Despite much progress in the development of solid-state and chemical nanomechanical devices such as molecular rotors, walkers and logic devices, no such systems have been developed that operate across lipid membranes.

Thus, by exploiting our knowledge of biological and synthetic supramolecular components we seek to construct some of the first ever transmembrane molecular machines built to man-made specifications. Mirroring the operation of biological transmembrane molecular machines, compartmentalisation facilitates the construction of nanomechanical devices that can be driven by electrochemical gradients, or those that operate in the reverse sense by turning over chemical fuels to establish non-equilibrium conditions. We intend to demonstrate these principles by combining nanopore-based techniques and DNA-recognition processes to assemble a range of membrane-spanning nanomechanical devices that can be operated, controlled and monitored down to single-molecule levels:

Project 1 – Transmembrane logic & signalling on the single-molecule level.
Project 2 – A transmembrane transporter.
Project 3 – A transmembrane reciprocating pump.
Project 4 – A transmembrane rotary motor.","1499780","2013-09-01","2018-08-31"
"TRAPLAB","Lab Based Searches for Beyond Standard Model Physics Using Traps","Guy RON","THE HEBREW UNIVERSITY OF JERUSALEM","In this project I will measure a critical constant (beta-nu correlation) of the standard model to a precision of at least 0.1%, an order of magnitude improvement over the state of the art. The project will provide a platform for beyond standard-model (BSM) explorations, based on modern atom/ion trapping and a new accelerator facility.

High precision measurements of beta decay correlations in trapped radioactive atoms and ions are one of the most precise tools with which to search for BSM physics. The recently published US National Science Advisory Council 2015 Long Range Plan states: ``Measurements of the decays of neutrons and nuclei provide the most precise and sensitive characterization of the charge-changing weak force of quarks and are a very sensitive probe of yet undiscovered new  forces. In fact, weak decay measurements with an accuracy of 0.1% or better provide a unique probe of new physics at the TeV energy scale``. Ne and He isotopes are particularly attractive due to calculable SM values, high sensitivity to several manifestations of BSM physics, ease of production, and lifetimes in the useful range for such experiments. 

This program combines a Magneto-Optical Trap (MOT) and an Electrostatic Ion Beam Trap (EIBT) to perform a high-precision, competitive, measurement of correlations in the decay of such nuclei. The MOT program focuses on the neon isotopes, where existing measurements are of insufficient quality, and have unique sensitivities to aspects of BSM physics. The EIBT program focuses on measurements using 6He (where a comparison with existing measurements is of great import) and the aforementioned neon isotopes, allowing a direct comparison between the two systems within the same facility (a unique worldwide capability). The combination of these methods will allow an extraction of the beta-nu coefficient to the 0.1% level, making this proposal a forerunner in the field, which will provide a leap-step in the current set of world data.","1297813","2016-12-01","2021-11-30"
"TRAPSENSOR","High-Performance Mass Spectrometry Using a Quantum Sensor","Daniel Rodríguez Rubiales","UNIVERSIDAD DE GRANADA","Mass spectrometry is one of the most important, essential and basic techniques in modern science. This is because the mass of a fundamental particle is a fundamental property of the particle itself, or, in a composite quantum mechanical system such as an atom, the mass is the sum of the masses of all its building blocks minus the binding energy between those constituents. The binding energy reflects all physical forces acting in such a quantum system.
The most-advanced instrument for high-accuracy mass determinations is the Penning trap using the fundamental techniques of cooling and storing. The most highly developed Penning trap presently at hand needs still a drastic improvement and ground-breaking ideas in order to achieve the two scientific goals of the present grant application: (i), determination of the Q-value of the decay 187Re to 187Os with an accuracy of delta_m/m = 10^(-11) as required for the MARE I campaign aiming at a determination of the mass of the electron antineutrino via a very careful determination and analysis of the beta spectrum; (ii), measurement of the masses of superheavy nuclei (Z less or equal to 118) produced by hot fusion process enabling a clear assignment of the proton number to the different isotopes (and by this also the naming of the elements discovered by hot fusion) and making possible tests of state-of-the-art nuclear theories.
A novel method, called quantum sensor, is proposed to measure the mass of a single ion with ultimate accuracy and unprecedented sensitivity while it is stored and cooled in a trap. The new device consists of a single calcium ion as sensor, which is laser-cooled to mK temperatures and stored in a trap connected to the trap for the ion under study by a common endcap. The motion of the ion under investigation is coupled to the sensor ion by the image current induced in the common endcap and observed through its fluorescence light. In this way the detection of phonons is upgraded to a detection of photons.","1499280","2011-11-01","2017-07-31"
"TREND","Transparent and flexible electronics with embedded energy harvesting based on oxide nanowire devices","Pedro CANDIDO BARQUINHA","NOVA ID FCT - ASSOCIACAO PARA A INOVACAO E DESENVOLVIMENTO DA FCT","The Internet of Things is shaping the evolution of information society, requiring an increasing number of objects with embedded electronics, sensors and connectivity. This spurs the need for systems where summing to performance and low cost, multifunctionality has to be assured. In this context, TREND aims to take transparent electronics into as-of-yet unexplored levels of integration, by combining on flexible substrates transparent and high-speed nanocircuits with energy harvesting capabilities, all based on multicomponent metal oxide nanowires (NWs). For this end, sustainable and recyclable materials as ZnO, SnO2, TiO2 and Cu2O will be synthesized in different forms of heterostructured NWs, using low-temperature and low-cost solution processes. For precise positioning, NWs will be directly grow on flexible substrates using seed layers patterned by nanoimprint lithography. This will be crucial for integration in different nanotransistor structures, which will be combined into digital/analog nanocircuits following planar and 3D approaches. Energy will be provided by piezoelectric nanogenerators with innovative structures and materials. Final platform of nanocircuits+nanogenerators will make use of NW interconnects, bringing a new dimension to the systems-on-foil concept.
The research will be carried out at FCT-UNL, in a group pioneering transparent electronics. My PhD on oxide materials/devices and proven expertise on circuit integration, oxide nanostructure synthesis and nanofabrication/characterization tools will be a decisive contribute to the implementation of the proposal. TREND is an ambitious multidisciplinary project motivating advances in materials science, engineering, physics and chemistry, with impact extending from consumer electronics to health monitoring wearable devices. By promoting new ideas for practical ends, it will contribute to place Europe in the leading position of such strategic areas, where sustainability and innovation are key factors.","1500000","2017-01-01","2021-12-31"
"TREX","Novel Developments in Time Projection Chambers (TPCs) for Rare Event Searches in Underground Astroparticle EXperiments","Igor Garcia Irastorza","UNIVERSIDAD DE ZARAGOZA","Gaseous detectors like TPCs are seldom used in Rare Event searches (like those looking for Dark Matter particles, Neutrinoless Double Beta Decay or other neutrino experiments) due mainly to the difficulty in reaching target masses large enough to be sensitive to the extremely low counting rates expected. However, very promising recent advances in TPC readouts based on micropatterns (e.g. the Micromegas concept) may overcome those limitations, making the application of TPCs to this kind of experiments not only viable, but competitive. The first order discoveries that await us in double beta or dark matter experiments will require the identification of the signal events with an unmistakable signature like the one only offered by gaseous detectors. The proposed research aims at the exploration of these new concepts from the point of view of low background techniques, and at their development up to the point of application to next-generation high-discovery-potential experiments in this field. The main practical objective is the creation and consolidation, in the host institution, of a core of technological and scientific expertise on TPCs for low background applications. In addition, the host group is currently responsible for the maintenance and operation of the new Canfranc Underground Laboratory (LSC). The proposed development will have high impact regarding future experiments in the field. In particular, they will find direct application in several starting initiatives in which the applicant has a leading role: the recently proposed Neutrino Xenon TPC project (NEXT) for double beta decay at Canfranc, the Cosmology with Nuclear Recoils (CYGNUS) network to explore the directional signal of Dark Matter particles and, more generically, the CERN-based RD-51 technological collaboration recently approved to promote developments on micropattern detectors. It will certainly have practical applications beyond fundamental science.","1223776","2009-12-01","2015-11-30"
"TRICEPS","Time-resolved Ring-Cavity-Enhanced Polarization Spectroscopy: Breakthroughs in measurements of a) Atomic Parity Violation, b) Protein conformation and biosensing and c) surface and thin film dynamics","Theodore Peter Rakitzis","FOUNDATION FOR RESEARCH AND TECHNOLOGY HELLAS","Polarimetry is a crucial tool in both fundamental and applied physics, ranging from the measurement of parity nonconservation (PNC) in atoms, to the determination of biomolecule structure, and the probing of interfaces. These measurements tend to be extremely challenging as the change of the polarization of light is usually extremely small; typical differences in polarization states are of the order of 10^-5 to 10^-8. Current experimental techniques often require acquisition times of the order of seconds or, in the case of PNC, even many days, limiting the possibilities of time-resolved measurements. Here, I propose to develop optical-cavity-based techniques which will enhance measurements of the polarization sensitivity and/or the time-resolution by 3-6 orders of magnitude. Preliminary data from prototypes and feasibility studies are presented. I propose to demonstrate how these breakthroughs will revolutionize polarimetry, by addressing some of the most important multidisciplinary problems in fundamental physics, biophysics, and material science: a) Testing the limits of the Standard Model with atomic PNC measurements. Current PNC experiments, and more importantly theory, for cesium atoms are limited to precision of about 0.5%. The novel and robust experimental technique I am proposing here affords 4 orders-of-magnitude higher sensitivity, thus giving access to lighter atoms, where the theory can be better than 0.1%, for the most stringent test of the Standard Model, while seeking new physics. b) The measurement of protein folding dynamics. Highly sensitive time-resolved spectroscopic ellipsometry, providing novel dynamical information on protein folding: nanosecond resolved, position measurements of functional groups of surface proteins, which map out the time-dependent protein structure. c) Determination of thin film thickness and surface density with nanosecond resolution, for the study of processes such as laser ablation and polymer growth.","909999","2009-01-01","2014-12-31"
"TRIPLE","Three Indirect Probes of Lyman continuum LEakage from galaxies","Anne VERHAMME","UNIVERSITE DE GENEVE","Cosmic reionization corresponds to the period in the history of the Universe during which the predominantly neutral intergalactic medium was ionised by the emergence of the first luminous sources.  Young stars in primeval galaxies may be the sources of reionization, if the ionising radiation, called Lyman continuum (LyC), that they produce can escape their interstellar medium: the escape fraction of LyC photons from galaxies is one of the main unknowns of reionization studies. This ERC project will contribute to answer this question, by computing from simulated galaxies three indirect diagnostics of LyC leakage that were recently reported in the literature, and comparing the virtual observables with the direct escape of LyC photons from simulated galaxies, and with observations. The first diagnostic for LyC leakage relates the escape of the strongly resonant Lyman-alpha radiation from galaxies to the LyC escape. It was proposed by the PI (Verhamme et al. 2015), and recently validated by observations (Verhamme et al. 2016). The second diagnostic proposes that the strength of Oxygen lines ratios can trace density-bounded interstellar regions. It was the selection criterion for the successful detection of 5 strong Lyman Continuum Emitters from our team (Izotov 2016a,b). The third diagnostic relates the metallic absorption line strengths to the porosity of the absorbing interstellar gas in front of the stars. The increasing opacity of the intergalactic medium with redshift renders direct LyC detections impossible during reionisation. Indirect methods are the only probes of LyC leakage in the distant Universe, and the proposed diagnostics will soon become observables at the redshifts of interest with JWST. They have passed the validation tests, it is now urgent to calibrate these indicators on state-of-the art simulations of galaxy formation. This is the main objective of the proposed project.","1500000","2018-03-01","2023-02-28"
"TROCONVEX","Turbulent Rotating Convection to the Extreme","Rudie Peter Jacobus Kunnen","TECHNISCHE UNIVERSITEIT EINDHOVEN","The aim of this project is to understand the processes involved in heat and energy transfer of geostrophic turbulent convection, to identify the structure of this seemingly featureless flow, and to model its global convective heat transfer based on these new insights. The geostrophic regime of turbulent rotating convection is relevant for many geophysical and astrophysical flows. The flow behaviour in this regime displays significant and unexpected differences with the traditionally studied regime, making extrapolations impossible. Heat-transfer models of geophysical and astrophysical flows are an essential part of assessing their energy balance for e.g. climate modelling. Geostrophic turbulent convection is characterised by combined strong thermal forcing and rapid rotation, making it hard to replicate in experiments and computations.We propose a revolutionary experiment capable of covering an unprecedented part of this new regime. Heat-flux measurements and optical diagnostics of the flow using stereoscopic particle image velocimetry are featured. The experiment is complemented with highly optimised parallel computations to gain access to additional flow information in parts of the parameter range. The focus is on the small-scale energy transfer processes and the influence of presence or absence of (Ekman-type) boundary layers, both of which are decisive for the flow organisation into structures and subsequently for the global heat transfer. We will characterise the heat transfer and energy cascade processes in the flow as a function of the governing parameters quantifying buoyant forcing and rotation. We will address open questions on the heat-flux scaling of geostrophic convection and its dependence on the types of coherent structures being formed, effects of boundary-layer dynamics and energy cascade mechanisms in the flow. These insights will allow us to model the heat flux, a crucial result for the understanding of convection in geo- and astrophysics.","1804544","2016-04-01","2021-03-31"
"Trop-ClOC","Quantifying the impact of Tropospheric Chlorine Oxidation Chemistry","Peter Martin EDWARDS","UNIVERSITY OF YORK","This project addresses a fundamental and yet highly unconstrained question in atmospheric chemistry: What is the impact of atomic chlorine on the composition of the troposphere?
Gas phase oxidants control the concentrations of important climate and air pollutants such as methane, ozone and particles. Accurate representation of oxidation chemistry in computational models is paramount to our ability predict and understand past, present and future changes to the Earth system. Over recent decades there have been continual suggestions that the chlorine atom may be a significant tropospheric oxidant, but a lack of observations capable of constraining its chemistry mean that its role remains highly uncertain. Without these underpinning observations, our understanding of atmospheric oxidation and thus our ability to develop effective and timely policies to address air quality and climate change is compromised.
This project will provide a step-change in our understanding of atmospheric chlorine chemistry. Capitalising on recent technology advances, two innovative instruments capable of definitively constraining chlorine atom sources and sinks will be developed. These instruments will be deployed at three chemically contrasting locations (Cape Verde, coastal Netherlands and central Germany), generating the first comprehensive dataset on tropospheric chlorine atom production and loss. These data will be used to challenge the state-of-the-science representation of chlorine chemistry in atmospheric chemistry models. Ultimately this work will advance our understanding of the fundamental chemistry occurring in the atmosphere and help to direct developments in the next generation of air quality and climate models.","1651508","2019-02-01","2024-01-31"
"TRUE DEPTHS","deTeRmine the trUe dEpth of DeEp subduction from PiezobaromeTry on Host –inclusions Systems","Matteo ALVARO","UNIVERSITA DEGLI STUDI DI PAVIA","Subduction of one tectonic plate below another is the primary cause of catastrophic geological events such as earthquakes and explosive volcanism that directly impact thousands of kilometers of coastal and mountain areas located on convergent margins.  Real-time geophysical or seismic data only provide static snapshots of these subduction zones today. Therefore, quantitative understanding of the rates and true depths of subduction can only be achieved by determining the pressure-temperature-time-depth histories of Ultra-High-Pressure Metamorphic (UHPM) rocks that have been subducted to pressures greater than 3 GPa and subsequently exhumed. Conventional mineral thermo-barometry is severely challenged in UHPM terraines and thus the mechanisms attending the downwards transport of crustal material, and its return back to the Earth’s surface (exhumation), are still a matter of vigorous debate.
The TRUE DEPTHS project will develop X-ray diffraction analysis of the anisotropic elastic interactions of inclusion minerals trapped inside host minerals. I will develop non-linear elasticity theory to provide a method that will be uniquely able to determine whether significant deviatoric stresses are recorded by UHPM rocks. By applying this method to samples from carefully selected field areas, I will be able to determine if metamorphic phase equilibria represent the true depths of UHPM, in which case subduction to depths in excess of 90 km must occur. Alternatively, quantitative measurements of large deviatoric stresses could indicate that tectonic over-pressure can account for the observed phase equilibria, thus not requiring deep subduction. If overpressurized domains are present in tectonically thickened lithosphere, they may represent a driving force for stress release leading to earthquakes. The results will provide new constraints on earthquake triggering mechanisms and how the styles of subduction and its detailed mechanisms have evolved over Earth’s history.","1697500","2017-06-01","2022-05-31"
"TrueBrainConnect","Advancing the non-invasive assessment of brain communication in neurological disease","Stefan HAUFE","CHARITE - UNIVERSITAETSMEDIZIN BERLIN","Pathological communication between different brain regions has been implicated in various neurological disorders. However, the computational tools for assessing such communication from neuroimaging data are not sufficiently developed. The goal of TrueBrainConnect is to establish brain connectivity analysis using non-invasive electrophysiology as a practical and reliable neuroscience tool. To achieve this, we will develop novel signal processing and machine learning techniques that address shortcomings in state-of-the-art reconstruction and localization of neural activity from sensor data, the estimation of genuine neural interactions, the prediction of external (e.g., clinical) variables from estimated neural interactions, and the interpretation of the resulting models. These techniques will be thoroughly validated and then made publicly available. We will use the TrueBrainConnect methodology to characterize the neural bases underlying dementia and Parkinson's disease (PD), two of the most pressing neurological health challenges of our time. In collaboration with clinical experts, we will address practically relevant issues such as how to determine the onset of 'freezing' episodes in PD patients, and how to detect different variants and precursors of dementia. The outcome of TrueBrainConnect will be a versatile methodology allowing researchers, for the first time, to reliably estimate and anatomically localize important types of interactions between different brain structures in humans within known confidence bounds. The proposed clinical applications will improve our understanding of the studied diseases and will lay the foundation for the development of novel diagnostic markers for these diseases.","1499875","2019-01-01","2023-12-31"
"TSuNAMI","Trans-Spin NanoArchitectures: from birth to functionalities in magnetic field","Jana KALBACOVA VEJPRAVOVA","UNIVERZITA KARLOVA","Control over electrons in molecules and periodic solids can be reached via manipulation of their internal quantum degrees of freedom. The most prominent and exploited case is the electronic spin accommodated in standalone spin units composed of 1 – 10^5 of spins. A challenging alternative to the spin is the binary quantum degree of freedom, termed pseudospin existing e.g. in two-dimensional semiconductors. The aim of the proposed research is to build prototypes of trans-spin nano-architectures composed of at least two divergent spin entities, the TSuNAMIes. The spin entities of interest correspond to single atomic spin embedded in spin crossover complexes (SCO), molecular spin of molecular magnets (SMM), superspins of single-domain magnetic nanoparticles (SuperS) and pseudospins in two-dimensional transition metal dichalcogenides (PseudoS). Ultimate goal of the project is to identify a profit from trans-spin cooperation between the different spin entities coexisting in a single TSuNAMI. Influence of external static and alternating magnetic fields on the elementary spin state, unit cell magnetic structure, long-range magnetic order, mesoscopic spin order, spin relaxations and pseudospin state mirrored in essential fingerprints of the spin units and their ensembles will be explored using macroscopic and microscopic in situ and ex situ probes, including Raman and Mössbauer spectroscopies in magnetic field.  Within the proposed high-risk/high-gain trans-spin strategy, we thus expect: 1. Enhancement of magnetic anisotropy in SMM-SuperS with enormous impact on cancer therapy using magnetic fluid hyperthermia, 2. Control over SCO via coupling to giant classical spin giving rise to miniature ‘on-particle’ sensors, 3. Mutual visualization of electronic states in SCO-PseudoS pushing frontiers of nowadays pseudospintronics, and 4. Control over electronic states with nanometer resolution in SuperS-PseudoS giving rise to novel functionalization strategies of graphene successor.","1500000","2017-02-01","2022-01-31"
"TUCAS","Tuneable Catalyst Surfaces for Heterogeneous Catalysis – Electrochemical Switching of Selectivity and Activity","Christoph MAG. RER. NAT. RAMESHAN","TECHNISCHE UNIVERSITAET WIEN","In heterogeneous catalysis surfaces decorated with uniformly dispersed, catalytically highly active particles are a key requirement for excellent performance. One of the main tasks in catalysis research is the continuous improvement or development of catalytically active materials.

An emerging concept in catalyst design, and the aim of this project, is to selectively and reversibly tune and modify the surface chemistry by electrochemical polarisation. Perovskite-type catalysts raise the opportunity to incorporate guest elements as dopants. Upon electrochemical polarisation these dopants emerge from the oxide lattice to form catalytically active clusters or nanoparticles on the surface (by exsolution). In consequence this leads to a strong modification or enhancement of catalytic selectivity and activity. Electrochemical polarisation offers the possibility to adjust the surface chemistry in response to an external signal (here the applied voltage).

Studies in a realistic catalytic reaction environment (in-situ) will enable a direct correlation of surface structure with catalytic activity, selectivity and the electrochemical stimulation. The unique combination of surface science, heterogeneous catalysis and electrochemistry will take this research to a new ground-breaking level. No research group has yet tried to tackle this topic on a fundamental mechanistic level by this multidisciplinary approach.

The proposed project opens unprecedented possibilities for catalyst design and in-situ control due to the versatility of perovskite-type catalyst materials and dopant elements. Nanoparticle exsolution is a highly time- and cost-efficient way of catalyst preparation and it will offer solutions to major problems in heterogeneous catalysis, such as ageing (sintering) or catalyst deactivation (coking). Tuneable catalyst surfaces will facilitate tackling a major concern of the 21st century, the utilisation of CO2 and its conversion to renewable fuel.","1997202","2018-01-01","2022-12-31"
"TUNNEL","Tunneling Spectroscopy in van-der-Waals Device","Hadar Steinberg","THE HEBREW UNIVERSITY OF JERUSALEM","I will expand the experimental reach of tunneling spectroscopy to new materials and device geometries. The technique is ideal for tackling two challenges: (i) Probing Andreev bound states and Majorana states in graphene and topological insulators (TIs) coupled to superconductors, and (ii) realizing momentum-conserving tunneling.
I will utilize a breakthrough in device fabrication to stack layered van-der-Waals materials, such as graphene and hexagonal Boron Nitride (hBN), to form vertical structures. Ultrathin layers of mechanically deposited dielectrics will be used as tunnel-barriers. These can interface any smooth surface, expanding the range of possible device-based tunneling systems.
A tunnel junction has decisive advantages over STM in access to lower temperatures and hence higher energy resolution. Significantly, the effort to probe the energy spectra of graphene and TIs coupled to superconductors is often resolution-limited. I will develop artificial-vortex devices and Josephson devices where induced spectra are expected to reveal the Majorana mode, a quantum state of unusual statistics sought as a platform for fault-tolerant quantum computation.
Using the same technology, I will develop devices where tunneling takes place between extended states. The aim is to realize momentum resolved tunneling for μeV-resolution measurement of dispersions in graphene, other 2D systems, and smooth interfaces. Momentum control will be achieved using density-tuning of the Fermi surfaces or using parallel magnetic field. The high resolution spectra will reveal details of interaction effects, manifest as modifications to the single-electron picture.
Carriers can be injected into a system with full control over their direction and energy – a powerful experimental knob, useful for injecting carriers using one electrode and extracting them in another. Such geometry is sensitive to relaxation effects, and will allow unprecedented resolution studies of out-of-equilibrium systems.","1499875","2015-05-01","2020-04-30"
"TURBOFLOW","Decoding the complexity of turbulence at its origin","Björn Hof","INSTITUTE OF SCIENCE AND TECHNOLOGYAUSTRIA","""Turbulence is the probably most complex and at the same time most relevant example of spatio-temporal disorder in nature. The transport of heat and mass in stars, the formation of planets, as well as flows in the earth atmosphere, oceans or around vehicles are all governed by turbulence. Despite its ubiquity our insights into this complex phenomenon are very limited. In contrast to many studies which are concerned with turbulent flows at high parameter values I will here use a different approach and investigate turbulence when it first arises and where it is the least complex. I will focus on canonical shear flows, comprising pipe, Couette and channel flows. I have recently determined the critical point for transition in pipe flow, which had posed a riddle for more than a century and inhibited further progress towards a fundamental understanding of turbulence close to onset. At first I will clarify if this transition generally applies to all canonical shear flows. Next I will explore links to non-equilibrium phase transitions in other areas of science by determining the critical exponents and the universality class of the onset of shear flow turbulence. I will investigate and identify further bifurcations the turbulent state experiences as it develops from a spatially intermittent to a space filling state. This will for the first time provide a complete picture of the onset of turbulence and establish links to turbulence studies at higher Reynolds numbers. Investigating the mechanisms leading to fully turbulent flow will not only give valuable insights into the nature of fluid turbulence but may also lead to new ways to control it. Finally I will exploit these insights and devise methods that completely relaminarize turbulent flows. Subduing turbulence is of great practical importance since frictional losses in turbulence are much larger than in the laminar state and hence relaminarization leads to substantial energy savings in transport problems.""","1474000","2013-01-01","2017-12-31"
"U-FINE","Ubiquitous optical FIbre NErves","Miguel Gonzalez Herraez","UNIVERSIDAD DE ALCALA","Distributed fibre sensors have become a widely used tool for critical asset monitoring in civil engineering and energy transport. Beyond these specific fields, little or no application has been found for these sensors. The aim of U-FINE is to develop a new class of multi-scale distributed optical fibre sensors that would find use in a wide range of new application domains ranging from biomechanics to smart grids.
The state-of-the-art of conventional distributed fibre sensors is to measure quantities such as temperature or deformation with a spatial resolution of 1 meter over 30 km. This corresponds to 30’000 resolved distinct measurement points with a single interrogation unit and a single optical cable. The realistic ambition of this project is to radically change the interrogation methods available and be able to resolve up to 1’000’000 points. This will be done by either bringing the spatial resolution below 1 cm (still preserving kilometre ranges) and/or by extending the measuring range beyond 200 km (still preserving resolutions of 1-2 meter). Furthermore, we also aim at reducing the acquisition time of these systems in approximately two orders of magnitude, and demonstrating new architectures capable of addressing hundreds of fibre sections in a complex network with a single interrogation unit and passive network devices.
To achieve these performances, merely improving present-day systems is completely insufficient. Completely new interrogation schemes have to be developed. We believe that this project has good solutions for all the above challenges.
To assess the developed technology in realistic conditions, field tests of the developed systems are envisaged towards the end of this project. . In addition to this, with the knowledge acquired through the accomplishment of these basic objectives, we will also pursue the demonstration of the use of distributed fibre sensing systems as high-resolution wearable sensors for ambulatory analysis of human movement.","1477330","2012-12-01","2018-05-31"
"UB12","Ergodic Group Theory","Uri Bader","WEIZMANN INSTITUTE OF SCIENCE LTD","""The aim of the proposed research is gaining a better understanding of locally compact groups and their lattices. Our tools are mainly ergodic theoretical.
We propose a variety of novel ideas that open new horizons for research.
The first meta idea is the adoption of tools from the semi-simple theory in order to apply them for general locally compact groups. In particular, we suggest a construction of a """"Weyl group"""" and an abstract definition of rank for every locally compact group. We are able to construct a """"Coxeter complex"""" and we foresee a construction of a """"building like"""" object.
A second set of ideas concerns the category of measure equivalences, which is a natural generalization of the notion of a lattice in a group. This category is long known to be a measurable counterpart of the better studied category of quasi-isometries, yet it misses a good definition of self measure equivalences of an object, analog to the group of quasi-isometries.
We suggest such a definition, and propose to study it, among a variety of related constructions.
A full implementation of our ideas requires a better understanding of locally compact groups.
Thus, an important aspect of the proposed research is that it leaves plenty of room for the study of specific examples and test cases.""","1150000","2012-09-01","2018-08-31"
"UDynI","Ultrafast Dynamic Imaging of Complex Molecules","Caterina Vozzi","CONSIGLIO NAZIONALE DELLE RICERCHE","High order harmonics are generated when molecules are ionized by an intense femtosecond laser pulse. The freed electron is accelerated in the external driving electric field and, because of the periodic oscillation of this field, is brought back to the parent ion, where it can recombine and give rise to the emission of an XUV photon. This XUV harmonic radiation has been shown to contain information on the electronic structure of the molecule, which can be interpreted as a picture of the molecular orbital. The idea of exploiting high order harmonic generation (HHG) for the tomographic reconstruction of the electronic structure in molecules was first introduced in 2004 for nitrogen molecules. Up to now, despite several attempts, a successful investigation of the molecular structure by HHG was still lacking for molecules more complex than N2. Recently I demonstrated a new experimental and theoretical approach for extending the imaging of molecular orbitals to a triatomic molecule. However, the real breakthrough in molecular imaging, namely the achievement of a time-resolved tomography providing a movie of the wavefunction of molecules while undergoing structural changes, has not yet been attempted.
The goal of this project is to develop time-resolved HHG tomography for real-time imaging of evolving electronic structure in complex molecules undergoing electronic or vibrational excitation.
The key idea of UDynI is to use a mid-IR (1.3-1.8 µm) few-cycle laser source to drive the harmonic generation process. This will allow tackling the limitations that prevented up to now the realization of time-resolved tomography of complex molecules during molecular rearrangement.
I will mainly consider organic molecules, such as hydrocarbons, hydrocarbons with functional groups and aromatic hydrocarbons, which are interesting precursors for the study of biological functions.","1483967","2013-01-01","2017-12-31"
"UfastU","Theory of ultra-fast dynamics in correlated multi-band systems","Martin ECKSTEIN","FRIEDRICH-ALEXANDER-UNIVERSITAET ERLANGEN NUERNBERG","Many of the intriguing properties of complex materials, such as magnetism and superconductivity rely on the cooperative behavior of electrons in systems with several active spin and orbital degrees of freedom. Using femtosecond laser pulses it has become possible to probe and control these systems on microscopic timescales. This does not only provide an entirely new approach to understand the emergence of collective phases, but it may also help to push the current speed limits in information and communication technologies. However, in contrast to the remarkable experimental progress, theory is still unable to provide a microscopic description of the ultrafast dynamics in most materials even on a qualitative level, as long as it remains restricted to single-band models and ad-hoc parameters. To overcome this limitation, we would like develop a versatile computational tool based on the nonequilibrium extension of dynamical mean-field theory (DMFT).
In the previous two decades, the development of equilibrium DMFT into a tool with predictive power has had a transformative effect on our understanding of correlated materials. A successful realization of the proposed research would lay the foundations for a comparable ab-initio understanding of correlated systems out of equilibrium, and provide the numerical techniques to solve multi-orbital quantum impurity models. Already the first applications of a multi-band formalism can lead to seminal insights which are currently out of reach, including an understanding of light-induced superconductivity in materials such as the iron pnictides, or new ways to engineer many-body interactions by external fields and thus drive a system to thermodynamically not accessible phases. The new numerical tools can also be used to unravel dynamical processes in bio-molecules like hemoglobin, whose essential functionality relies on multi-orbital transition metal atoms.","1461500","2017-04-01","2022-03-31"
"UFO","Uncovering the origins of friction","Jean-François Molinari","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Nanotechnology is a new frontier in research and new tools must be developed. As surface to volume ratios become large, engineering at the nanoscale will be dominated by surface science. The study of Contact Mechanics at nanoscales nanotribology- needs to fully account for adhesive forces, third-body interactions and deformation mechanisms at contacting asperities. Understanding these factors as well as the morphological evolution of contact clusters has the potential of explaining the origins of frictional forces and wear. This will guide us in the design of tailored-made lubricants and surface morphologies, which, in turn, will help reduce the high societal cost of wear damage. This ERCstg proposal describes a plan to establish a world-leading group in Contact Mechanics at length scales ranging from the atomic to macroscopic scales relevant to Civil or Mechanical Engineering structural applications. Our approach will have recourse to molecular dynamics coupled with the finite-element method for an accurate description of atomic interactions at the contact surface, and of long-range elastic forces. The project is interdisciplinary as the deepening of our understanding of Contact Mechanics will necessitate Computer Science developments. A central objective of the research will be the release of an open, 3D parallel, finite-element platform dedicated to contact applications. The PI will assemble a team of Engineers and Computer Scientists to ensure a successful and perennial diffusion in the European academic and industrial network. The research will therefore explore the origins of friction, a scientific quest of fundamental importance to many industrial applications, and will also create a stable base for sharing scientific-computing resources.","1773000","2009-09-01","2014-08-31"
"UFOS","Unveiling Planet Formation by Observations and Simulations","Mario FLOCK","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","With each newly detected exoplanet system, the planet formation theory is constantly gaining weight in the astrophysical research. The planets origin is a mystery which can only be solved by understanding the protoplanetary disks evolution. Recent disk observations by the new class of interferometer telescopes are challenging the existing theory of planet formation. They reveal astonishing detailed structures of spirals and rings in the dust emission which have never been seen before. Those structures are often claimed to be caused by embedded planets, which is difficult to explain with current models. This growing discrepancy between observation and theory forces us to realize: a novel disk modeling is essential to move on. Separate gas or dust evolution models have reached their limit and the gap between those has to be closed.
With the UFOS project, I propose an unique and ambitious approach to unite gas and dust evolution models for protoplanetary disks. For the first time, a single global model will mutually link self-consistently: a) the transport of gaseous disk material, b) the radiative transfer, c) magnetic fields and their dissipation and d) the transport and growth of the solid material in form of dust grains.
The development, performing and post-analysis of the models will initiate a new age for the planet formation research. The project results will achieve 1) unprecedented self-consistent precision to answer the question if those novel observed structures are caused by embedded planets or by the gas dynamics itself; 2) to find the locations of dust concentration and growth to unveil the birth places of planets and 3) to close the gap and finally unify self-consistent models of the disk evolution with the new class of observations.
Only such advanced models combined with multi-wavelength observations, can show us the process of planet formation, and so explain the origin of the various of planets and exoplanets in our solar neighborhood and beyond.","1618125","2018-06-01","2023-05-31"
"ULEED","Observing structural dynamics at surfaces with Ultrafast Low-Energy Electron Diffraction","Claus Ropers","GEORG-AUGUST-UNIVERSITAT GOTTINGENSTIFTUNG OFFENTLICHEN RECHTS","This grant application proposes the establishment of Ultrafast Low-Energy Electron Diffraction (ULEED) as a novel and versatile approach to investigate ultrafast structural dynamics at surfaces and in ultrathin films. Two-dimensional systems such as surfaces and molecular monolayers exhibit a multitude of intriguing phases and complex transitions. Studying the ultrafast dynamics of these materials elucidates correlations and microscopic couplings, but the access to structural degrees of freedom with ultrahigh time resolution remains challenging. Low-Energy Electron Diffraction (LEED) is a powerful technique in surface science to determine the atomic-scale structure and symmetry of surfaces. However, time-resolved LEED has proven exceedingly difficult to realize, owing to the problems in realizing suitable low-energy electron pulses of sufficiently short pulse duration and high beam quality.

This project targets both of these present limitations by using laser-triggered nanoscopic electron sources to generate high-brightness beams of low-energy electrons. Specifically, nanotip cathodes driven by nonlinear photoemission will be integrated in compact micro- and nanofabricated electrostatic lens assemblies. This will allow for a drastic reduction of electron beam propagation distances while maintaining a high level of beam control and focusing ability. Using this electron source, we plan to develop a laser-pump/electron-diffraction-probe setup at low electron energies with a temporal resolution of few hundred femtoseconds and less. A number of strategies will be followed to improve the temporal resolution of the setup, including wavelength-tuning of the laser excitation and active spectral compression of the electron pulses using locally enhanced THz fields. We will apply ULEED in the investigation of the structural dynamics within a range surface systems, including molecular monolayers, intrinsic surface reconstructions and adsorbate-induced charge-density waves.","1479750","2015-03-01","2020-02-29"
"ULPPIC","Ultralow power photonic integrated circuits for short range interconnect networks","Dries Van Thourhout","UNIVERSITEIT GENT","It is now generally recognized that current electrical solutions will not suffice to fulfil all requirements for communication on-chip and between chips, which is expected to continue to grow exponentially during the coming years.  Therefore we have to look for alternatives. Optical interconnect is a possibility, which is currently heavily investigated, including in my own on-going research.  However, the requirements in terms of power consumption are very stringent and the current solutions being proposed are still off by an order of magnitude.  Therefore, the objective of this project is to propose, design, fabricate and characterise photonic devices with fundamental lower power consumption through exploiting a large overlap between optical field, active material and electrical drive signals.  For this purpose, we will build a completely new photonics integration platform consisting of self-assembled semiconductor materials as the active core element, embedded within strongly confined photonic cavities defined using the most advanced semiconductor fabrication technologies.  Thereby we are combining rapidly maturing bottom-up techniques such as colloidal nanocrystal synthesis and semiconductor nanowire growth with traditional top-down technologies for realizing completely new types of photonic devices with an order of magnitude improvement in device performance. To reach this objective I will build a multidisciplinary team with experts in photonic device design, wet chemical synthesis, solid state physics, epitaxial nanowire growth and microelectronic fabrication technologies.","1341600","2011-01-01","2015-12-31"
"UltimateMembranes","Energy-efficient membranes for carbon capture by crystal engineering of two-dimensional nanoporous materials","Kumar Varoon AGRAWAL","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The EU integrated strategic energy technology plan, SET-plan, in its 2016 progress report, has called for urgent measures on the carbon capture, however, the high energy-penalty and environmental issues related to the conventional capture process (amine-based scrubbing) has been a major bottleneck. High-performance membranes can reduce the energy penalty for the capture, are environment-friendly (no chemical is used, no waste is generated), can intensify chemical processes, and can be employed for the capture in a decentralized fashion. However, a technological breakthrough is needed to realize such chemically and thermally stable, high-performance membranes. This project seeks to develop the ultimate high-performance membranes for H2/CO2 (pre-combustion capture), CO2/N2 (post-combustion capture), and CO2/CH4 separations (natural gas sweetening). Based on calculations, these membranes will yield a gigantic gas permeance (1 and 0.1 million GPU for the H2 and the CO2 selective membranes, respectively), 1000 and 10-fold higher than that of the state-of-the-art polymeric and nanoporous membranes, respectively, reducing capital expenditure per unit performance and the needed membrane area. For this, we introduce three novel concepts, combining the top-down and the bottom-up crystal engineering approaches to develop size-selective, chemically and thermally stable, nanoporous two-dimensional membranes. First, exfoliated nanoporous 2d nanosheets will be stitched in-plane to synthesize the truly-2d membranes. Second, metal-organic frameworks will be confined across a nanoporous 2d matrix to prepare a composite 2d membrane. Third, atom-thick graphene films with tunable, uniform and size-selective nanopores will be crystallized using a novel thermodynamic equilibrium between the lattice growth and etching. Overall, the innovative concepts developed here will open up several frontiers on the synthesis of high-performance membranes for a wide-range of separation processes.","1875000","2019-06-01","2024-05-31"
"UltimateRB","Direct numerical simulations towards ultimate turbulence","Richard Johannes Antonius Maria STEVENS","UNIVERSITEIT TWENTE","Turbulent thermal convection plays an important role in a wide range of natural and industrial settings, from astrophysical and geophysical flows to process engineering. The paradigmatic representation of thermal convection is Rayleigh-Bénard (RB) flow in which a layer of fluid is heated from below and cooled from above. A major challenge is to determine the scaling relation of the Nusselt number (Nu), i.e. the dimensionless heat transport, with the Rayleigh number (Ra), which is the dimensionless temperature difference between the two plates, expressed as Nu∼Ra^γ. Theory predicts that the scaling exponent γ increases for extremely strong driving when the boundary layers transition from laminar to turbulent. Understanding the transition to this so-called ‘ultimate’ regime is crucial since an extrapolation of results from lab-scale experiments and simulations to astro- and geophysical phenomena becomes meaningless when the transition to this ‘ultimate’ state is not understood. So far, there is no consensus among experimental efforts for obtaining the ‘ultimate’ regime. We propose using direct numerical simulations (DNS) to gain a better understanding of the transition towards the ‘ultimate’ regime. While obtaining ‘ultimate’ thermal convection in simulations has been elusive, new developments make this feasible now. The benefit of simulations is that they allow full access to the flow and temperature fields, while all boundary conditions are set exactly and independently. This allows us to test various physical effects at full dynamic similarity. To trigger the excitation of the ‘ultimate’ regime at lower Ra than in standard small aspect ratio cells, we want to study the effect of roughness, additional shear, and large domains in which a stronger flow can develop than in confined small aspect ratio cells that are traditionally considered. The addition of rotation will be studied to disentangle the complicated effect of rotation on high Ra number thermal convection.","1499375","2019-01-01","2023-12-31"
"ULTIMATESLAM","Instant SLAM: Ultimate Performance in Visual Localisation and Mapping","Andrew John Davison","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","In recent breakthrough work at the boundary of robotics and computer vision, I demonstrated that the core `Simultaneous Localisation and Mapping' (SLAM) approach of probabilistic map-building for mobile robots can be applied to real-time 3D motion estimation from the image stream from a single agile camera. Now I propose to take this line of research to its logical conclusion by investigating a paradigm of `Instant SLAM'. Can monocular visual SLAM be pushed far enough to estimate in real-time the motion of cameras potentially moving and accelerating very rapidly, turned on at arbitrary times and pointed at arbitrary scenes? Is is possible to develop a real-time vision algorithm, running on the standard processors of today or the near future, which can accurately track the motion of a camera attached to flying or bouncing robot, carried by a running person or even just thrown across a room? The ability to track camera motion with so few restrictions on dynamics and prior knowledge would turn simple webcam-type or embedded camera modules into truly flexible, low cost, go-anywhere position sensors with any number of applications in robotics, wearable computing and beyond. Achieving this goal requires both theoretical and practical research on extracting with ultimate efficiency the motion information available in an image sequence, with emphasis on high frame-rate capture, information-theoretic analysis, optimised probabilistic filtering and the relaxation of prior assumptions.","1613659","2008-09-01","2013-08-31"
"ULTRA-FAST","VIDEOGRAPHY OF ULTRAFAST PHENOMENA USING THE FRAME CONCEPT","Victor Elias KRISTENSSON","LUNDS UNIVERSITET","Scientists have always been particularly intrigued by the extremes in nature and made significant efforts to study these; microscopes allow us to observe the smallest objects, while telescopes permit us to explore the largest objects and also those farthest away. The work proposed herein will provide new means and generate insights to phenomena occurring on the shortest timescales in nature.
Past methods to probe ultrafast events – occurring on picosecond timescale or faster – have mostly relied on pump/probe scanning, yet these can only measure the dynamics of such processes if they are repetitive. Understanding all spatiotemporal aspects of ultrafast phenomena, however, requires experimental means to spatially, spectrally and temporally resolve them. Recently the PI invented a “coding” imaging concept called Frequency Recognition Algorithm for Multiple Exposures (FRAME) that can film at up to 5 trillion frames per second. To date, FRAME is the only videography method that can unify a femtosecond temporal resolution with spectroscopic compatibility, making it a powerful tool with high potential for new scientific discoveries. This project aims to (i) develop novel diagnostic tools based on FRAME and (ii) apply FRAME videography to study ultrafast events, whose temporal evolution could not be visualized in the past.
Ultrafast science is a wide field, making the project highly interdisciplinary. For example, within photo-physics, systems will be developed to film plasmas and laser filaments. Diagnostics will be developed to image the lifetime of coherent states as well as fluorescence decays of two fluorophores in parallel, which holds potential within biology, physics and chemistry. A two-color FRAME setup will be developed to temporally track the creation and consumption of two species in a chemical reaction simultaneously. The ensemble of work-packages proposed herein constitutes a significant step forward in the research area of ultrafast imaging and videography.","1998792","2019-04-01","2024-03-31"
"ULTRAFASTEUVPROBE","Ultrafast EUV probe for Molecular Reaction Dynamics","Daniel Strasser","THE HEBREW UNIVERSITY OF JERUSALEM","""This research is aimed at developing and validating a novel approach for time resolved imaging of structural dynamics, using single photon Coulomb explosion imaging (CEI) with ultrafast extreme UV (EUV) pulses to probe laser initiated ultrafast structural rearrangement and fragmentation dynamics. The emerging field of ultrafast EUV pulses attracts increasing amount of scientific attention, predominantly concentrated on understanding aspects of the generation process, as well as on measuring record breaking attosecond pulses at increasingly high photon energies and photon flux. I propose to direct the unique properties of ultrafast EUV pulses towards time resolved studies of molecular reaction dynamics that are inaccessible with conventional ultrafast laser systems. Time resolved single photon CEI will make possible the visualization of complex dynamics in polyatomic systems; specifically, how laser driven electronic excitation couples into nuclear motion in a wide range of molecular systems. In contrast to earlier attempts, in which CEI was driven with intense near-IR pulses that can alter the observed dynamics, the proposed single photon CEI will remove the masking intense field effects and provide a simple and general probe. A comprehensive experimental effort is proposed - to conduct a direct comparison of intense field CEI to the proposed single EUV photon approach. Successful implementation of this research will endow us with a new way to visualize and understand the underlying quantum mechanisms involved in chemical reactions. With this new technology I hope to be able to provide unique insight into molecular fragmentation and rearrangement dynamics during chemical reactions and to resolve long standing basic scientific questions, such as the concerted or sequential nature of double proton transfer in DNA base-pair models. Finally, the """"table top"""" techniques developed in my lab will mature and become applicable to the emerging ultrafast EUV user facilities.""","1499000","2012-11-01","2018-10-31"
"UMWA","Ultimate measurement of the W boson mass 
with ATLAS, at the LHC","Maarten Boonekamp","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","Widely advertised as a discovery machine, the LHC proton collider at CERN will also provide large samples of standard particles, allowing the final, most precise experimental determination of some fundamental parameters of particle physics theory. Together with the expected discoveries, these measurements will allow to elucidate the dynamics of electroweak symmetry breaking. The aim of the present project is to organize and realize a precise measurement of the W resonance parameters, and in particular the W boson mass with a target precision below 0.01%. Most LHC measurements aim at a precision of order 1%, and can be performed on a time scale of about one year, within a small group of collaborators. The measurement of the W boson mass is a special case, as its aimed precision requires perfect understanding of the experimental and physical environment. It therefore requires careful planning. Intermediate measurements of standard processes need to be performed, synchronized, and will play a key role in the final result. The primary tools serving this purpose are the study of the production properties of the J/Psi, W and Z particles in the uncertain LHC environment. These studies will allow to understand the performance of the ATLAS detector and to constrain the dynamics of proton-proton interactions, which represent a major source of uncertainty. Once the above properties are firmly established, the distributions sensitive to the W boson mass can be precisely predicted, and the fundamental parameters can be determined. The coordinator of this project has recognized expertise on electroweak physics, and has led the ATLAS Collaboration's research in this field since 2007. As argued above, the preparation of the measurement of mW requires to build and coordinate a team devoted to the realization of this difficult measurement over the next years. The present grant would provide a unique opportunity to gather all necessary conditions for the success of this project.","1259760","2011-10-01","2016-09-30"
"UncertainENV","The Power of Randomization in Uncertain Environments","Shiri Chechik","TEL AVIV UNIVERSITY","Much of the research on the foundations of graph algorithms is carried out under the assumption that the algorithm has full knowledge of the input data.
In spite of the theoretical appeal and simplicity of this setting, the assumption that the algorithm has full knowledge does not always hold.
Indeed uncertainty and partial knowledge arise in many settings.
One example is where the data is very large, in which case even reading the entire data once is infeasible, and sampling is required.
Another example is  where data changes occur over time (e.g., social networks where information is fluid).
A third example is where processing of the data is distributed over computation nodes, and each node has only local information.

Randomization is a powerful tool in the classic setting of graph algorithms with full knowledge and is often used to simplify the algorithm and to speed-up its running time.
However, physical computers are deterministic machines, and obtaining true randomness can be a hard task to achieve.
Therefore, a central line of research is focused on the derandomization of algorithms that relies on randomness.

The challenge of derandomization also arise in settings where the algorithm has some degree of uncertainty.
In fact, in many cases of uncertainty the challenge and motivation of derandomization is even stronger.
Randomization by itself adds another layer of uncertainty, because different results may be attained in different runs of the algorithm.
In addition, in many cases of uncertainty randomization often comes with additional assumptions on the model itself, and therefore weaken the guarantees of the algorithm.

In this proposal I will investigate the power of randomization in uncertain environments.
I will focus on two fundamental areas of graph algorithms with uncertainty.
The first area relates to dynamic algorithms and the second area concerns distributed graph algorithms.","1500000","2019-10-01","2024-09-30"
"UNCLE","UNCLE: Uranium in Non-Conventional Ligand Environments","Stephen Taylor Liddle","THE UNIVERSITY OF NOTTINGHAM","Metal-metal bonds are fundamental to generating step-changes in our knowledge because the periodic table is composed mainly of metals. The PI has recently made a breakthrough by making the first covalent uranium-gallium bond which exhibits sigma- and pi-donation from gallium to uranium. It is a direct model for the unknown isolobal uranium(IV)-CO &quot; unit, and is very significant to explaining why the widely used N-heterocyclic carbenes are so good at supporting transition metal catalysts and extracting uranium from solutions containing lanthanides such as found in nuclear waste clean up. This result opens the way to non-conventional ligand complexes of uranium and the previous limitation of conventional halide or C-, N-, or O-donor ligands for uranium will be overcome using non-conventional transition metal ligands to establish a new field of uranium-metal bonds. This work will deliver new compounds which will take our understanding of actinide structure, bonding, magnetism and reactivity to a higher platform of understanding, thus bringing an area of the periodic table, which lags behind all others, up to speed and beyond. This project will deliver a whole new field of actinide chemistry, provide unique and hitherto unknown atom efficient reactivity patterns, generate models for the too-hot-to-handle neptunium and plutonium which are present in nuclear waste, and precipitate new ways of thinking about how to solve nuclear waste clean up. This will induce a paradigm shift in uranium chemistry and will be directly included in textbooks of the future. This project will deliver mobile, high calibre, inter-/multi-disciplinary researchers, reversing a strategic skills shortage and retaining them for future employment and benefit to science, industry, and society in Europe.","999996","2010-02-01","2014-09-30"
"UNIC","Ultracold negative ions by laser cooling","Alban Kellerbauer","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Laser cooling is a well-established technique for the creation of ultracold particle ensembles in beams or traps. Over the past 30 years, it has become an indispensable tool in atomic physics and has opened many exciting new research fields. Both in positive atomic ions and in neutral atoms, the valence electron is bound in a Coulomb potential. The resulting infinite series of excited states provides a wide choice of suitable cooling transitions in many ionic and atomic systems. Surprisingly, laser cooling of negative atomic ions has never been achieved. The binding of the valence electron in these systems is based on electron electron correlation effects, which drop off quickly as the excess electron is removed from the neutral core. Consequently, anions are easily neutralized and only a few of them have excited levels. When excited states do occur, they are usually sub-levels of the ground state, meaning that transitions between the ground and excited state are weak and laser cooling would take prohibitively long. However, only a few years ago, a strong transition between the ground state and an opposite-parity excited state was found in the negative osmium ion. With this discovery, the laser cooling of atomic anions has finally come into reach. High-resolution optical spectroscopy on negative osmium has been carried out by the applicant, confirming the existence of a potential laser cooling transition. The aim of the proposed project is the first-ever demonstration of atomic-anion laser cooling. Ultimately, laser-cooled atomic anions could be used to cool any other negative-ion species by confining them simultaneously in a trap. The proposed technique is therefore applicable to a wide range of research fields in which ultracold negative ions are required.","1115970","2011-07-01","2016-06-30"
"UNICON","New Adaptive Computational Methods for Fluid-Structure Interaction using an Unified Continuum Formulation with Applications in Biology, Medicine and Industry","Johan Hoffman","KUNGLIGA TEKNISKA HOEGSKOLAN","For many problems involving a fluid and a structure, decoupling of the two is not possible to accurately model the phenomenon at hand, instead the fluid-structure interaction (FSI) problem has to be solved as a coupled problem. This includes a multitude of important problems in biology, medicine and industry, such as the modeling of insect flight, the blood flow in our heart and arteries, human speech, acoustic noise generation in vehicles and wind induced vibrations in bridges and other structures. Major open challenges of computational FSI include; (i) robustness of the fluid-structure coupling, (ii) efficiency and reliability of the computations in the form of adaptivity and quantitative error estimates, and (iii) in the case of high Reynolds number flow the computation of turbulent flow. In this project we address (i)-(iii) by a novel approach which we refer to as a Unified continuum formulation (UCF), where we formulate the fundamental conservation laws for mass, momentum and energy for the combined FSI domain, which is treated as one single continuum, with the only difference being the constitutive relations for the fluid and the structure. The stability problems connected to FSI are related to the exchange of information (stresses and displacements) over the fluid-structure interface, but with UCF we achieve (i) by the global coupling of the conservation laws where the fluid-structure interface is just an interior surface. We achieve (ii)-(iii) by extending to FSI our technology for adaptive finite element methods for turbulent flow with a posteriori error estimation using duality. We typically discretize the equations using a Lagrangian coordinate system for the structure and Arbitrary Lagrangian-Eulerian (ALE) coordinates for the fluid. Preliminary results for the simulation of blood flow are very promising. The computational algorithms are implemented in the open source software FEniCS (www.fenics.org), of which our group is one of the main developers.","500000","2008-06-01","2013-05-31"
"UniEqTURB","Universal Equilibrium and Beyond - Challenging the Richardson-Kolmogorov Paradigm","Clara VELTE","DANMARKS TEKNISKE UNIVERSITET","Turbulence is at a crossroads: The old, established ideas of Richardson and Kolmogorov have with accumulating evidence come under renewed scrutiny, especially in non-stationary and non-equilibrium flows. Many in the community seek new and more accurate ways to describe turbulence. This is a time of re-evaluation and opportunity!

The assumed statistical equilibrium of the smallest and intermediate scales is identified as the main cause of the potentially erroneous deductions. This problem was not previously noticed because experiments that confirmed the previous theories were all in statistical equilibrium. And those experiments and theories which disagreed were labelled ‘anomalous’, no matter how carefully performed or argued.

The proposed theory-intensive approach will therefore specifically use non-equilibrium and statistically non-stationary flows to:
1. Investigate the underlying mechanisms determining the level of dissipation
2. Quantify the resulting effects on the balance equations of central importance
3. Test the results against the established, as well as competing, theories

I will use stationary and accelerating jets well-suited for studying the non-linear interactions and quantifying departures to the assumed equilibrium and the non-stationary dissipation. The feasibility is demonstrated with preliminary results. The databases which will be established should contribute substantially to settling the long-lived ultimate question of turbulence: what are the true underlying mechanisms that set the level of dissipation.

The results will be ground breaking scientifically and economically. The impact for engineering applications is extensive, since Kolmogorov-based turbulence models are routinely used, and since developing flows constitute the rule rather than the exception in the majority of engineering applications. The potential economic consequences for e.g. transportation, climate predictions and power extraction are impossible to underestimate.","1499036","2019-04-01","2024-03-31"
"UNIGLASS","The Enigmatic Universality of Glass","Andrew Douglas FEFFERMAN","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The explanation for the distinct low temperature behavior of amorphous solids (glasses) is a long-standing open question. Specific puzzles include the nature of the low energy excitations (LEEs) that are responsible for their low temperature thermal and mechanical behavior and the origin of the remarkable universality of their low temperature mechanical dissipation. The phenomenological tunneling model proposes that the LEEs are atomic-scale tunneling two level systems (TLSs) and successfully explains much of the low temperature behavior of glass, but not the universality. Recently, individual TLSs were probed in the amorphous tunnel junction of superconducting qubits, but such dielectric measurements might not access the LEEs responsible for universality. In contrast, I propose to search for individual TLSs using purely mechanical measurements. The glass samples containing the TLSs will be nanomechanical resonators, and the strain coupling between the mechanical mode and the TLS will be used to control the quantum state of the latter. This strain coupling allows coherent state transfer between the mechanical mode and the TLS. Identifying individual TLSs and controlling their quantum state in this manner will demonstrate that the LEEs responsible for the characteristic low temperature properties of glass are indeed TLSs. Furthermore, these measurements will reveal the characteristics of individual TLSs and their interactions with their environment, in contrast to bulk measurements in which, according to the model, the effects of many TLSs are averaged. The results of the proposed study may therefore strongly support the tunneling model. This would require reconsideration of potential explanations for universality which are thought to be inconsistent with the existence of TLSs. Alternatively, if the hypothesized TLSs are absent, then the tunneling model must be replaced by a new interpretation of the low temperature properties of glass.","1929479","2017-09-01","2022-08-31"
"UNIQUE","Non-equilibrium Information and Capacity Envelopes: Towards a Unified Information and Queueing Theory","Markus Fidler","GOTTFRIED WILHELM LEIBNIZ UNIVERSITAET HANNOVER","Dating back sixty years to the seminal works by Shannon, information theory is a cornerstone of communications. Amongst others, it's significance stems from the decoupling of data compression and transmission as accomplished by the celebrated source and channel coding theorems. The success has, however, not been brought forward to communications networks. Yet, particular advances, such as in cross-layer optimization and network coding, show the tremendous potential that may be accessible by a network information theory.

A major challenge for establishing a network information theory is due to the properties of network data traffic that is highly variable (sporadic) and delay-sensitive. In contrast, information theory mostly neglects the dynamics of information and capacity and focuses on averages, respectively, asymptotic limits. Typically, these limits can be achieved with infinitesimally small probability of error assuming, however, arbitrarily long codewords (coding delays). Queueing theory, on the other hand, is employed to analyze network delays using (stochastic) models of a network's traffic arrivals and service. To date a tight link between these models and the information theoretic concepts of entropy and channel capacity is missing.

The goal of this project is to contribute elements of a network information theory that bridge the gap towards communications (queueing) networks. To this end, we use concepts from information theory to explore the dynamics of sources and channels. Our approach envisions envelope functions of information and capacity that have the ability to model the impact of the timescale, and that converge in the limit to the entropy and the channel capacity, respectively. The model will enable queueing theoretical investigations, permitting us to make significant contributions to the field of network information theory, and to provide substantial, new insights and applications from a holistic analysis of communications networks.","1316408","2012-12-01","2017-11-30"
"UNISCAMP","The unity of scattering amplitudes: gauge theory, gravity, strings and number theory","Oliver Schlotterer","UPPSALA UNIVERSITET","Scattering amplitudes are central observables in quantum field theory and provide essential information about the quantum consistency of perturbative gravity. Precise control of the physical and mathematical properties of scattering amplitudes holds the key to long-standing questions on fundamental interactions and the structure of space and time. As a concrete leap in this direction, UNISCAMP addresses predictions in gauge theories, gravity and effective theories through

- the efficient computation and compact representation of scattering amplitudes and,
- decoding their hidden structures & symmetries and their rich web of connections.

String-theory methods will complement conventional approaches to scattering amplitudes, and I will combine the insights from

- the point-particle limit of superstrings & heterotic strings and,
- the recent ambitwistor strings which directly compute field-theory amplitudes.

Both of them naturally incorporate the double-copy relation between gauge-theory & gravity amplitudes and extend the framework to effective field theories describing pions and other low-energy states. It is a primary goal of UNISCAMP to pinpoint the unifying principles connecting a wide range of field and string theories. My expertise in both flavours of string theories will allow to optimally exploit their fruitful synergies and to depart from mainstream approaches. 

Moreover, field- and string-theory amplitudes exhibit an intriguing mathematical structure: Their Feynman- and moduli-space integrals yield special functions such as polylogarithms which became a vibrant common theme of high-energy physics and number theory. As an interdisciplinary goal of UNISCAMP, I will

- investigate the low-energy expansion of multiloop string amplitudes and,
- extract an organizing scheme for iterated integrals on higher-genus Riemann surfaces.

These research objectives should benefit from my experience in collaborations with mathematicians.","1425000","2019-01-01","2023-12-31"
"Uniting PV","Applying silicon solar cell technology to revolutionize the design of thin-film solar cells and enhance their efficiency, cost and stability","Bart Vermang","INTERUNIVERSITAIR MICRO-ELECTRONICA CENTRUM","Thin film (TF) photovoltaics (PV) hold high potential for Building Integrated PV, an important market as European buildings require to be nearly zero-energy by 2020. Currently, Cu(In,Ga)(S,Se)2 (= CIGS(e)) TF solar cells have high efficiency, but also a simple one-dimensional cell design with stability and reliability concerns. Furthermore, its present research has been mainly focused on improving the absorber and buffer layers. 
Scientifically, Uniting PV aims to study the practical boundaries of CIGS(e) TF solar cell efficiency. For that reason, its goal is to revolutionize the design of CIGS(e) solar cells through implementation of advanced three-dimensional silicon (Si) solar cell concepts. This novel design consists of (i) surface passivation layers and (ii) light management methods integrated into ultra-thin (UT) CIGS(e) solar cells: (i) Passivation layers will be studied to reduce charge carrier recombination at CIGS(e) surfaces. The aim is to create new understanding and thus scientific models. (ii) Light management methods will be studied to optimize optical confinement in UT CIGS(e) layers. The aim is to examine the interaction between light management and charge carrier recombination in UT CIGS(e), and to create scientific models. The main reasons to introduce these developments is to reduce charge carrier recombination at the CIGS(e) surfaces and in the CIGS(e) bulk, while maintaining optical confinement. 
Technologically, the project targets to establish a solar cell with: (1) Increased cell efficiency, at least 23.0 % and up to 26.0 %; (2) improved stability and reliability, due to reduced CIGS(e) thickness and passivation layers hindering alkali metal movement; and (3) reduced cost, due to the use of less Ga and In, and industrially viable materials, methods and equipment. Hence, its outcome will be upscalable, valuable for other TF PV materials, and start a new wave of innovation in and collaboration between TF and Si PV research fields.","1986125","2017-03-01","2022-02-28"
"unLiMIt-2D","Unique Light-Matter Interactions with Two-Dimensional Materials","Christian Schneider","JULIUS-MAXIMILIANS-UNIVERSITAT WURZBURG","Controlling light- and matter excitations down to the microscopic scale is one major challenge in modern optics. Applications arising from this field, such as novel coherent- and quantum light sources have the potential to affect our daily life. One particularly appealing material platform in quantum physics consists of  monolayer crystals. The most prominent species, graphene, however remains rather unappealing for photonic applications due to the lack of an electronic bandgap in its pristine form. Monolayers of transition metal dichalcogenides and group III-VI compounds comprise such a direct bandgap, and additionally feature intriguing spinor properties, making them almost ideal candidates to study optics and excitonic effects in two-dimensional systems. 
unLiMIt-2D aims to establish these materials as a new platform in solid-state cavity quantum electrodynamics. The targeted experiments will be based on thin layers embedded in high quality photonic heterostructures providing optical confinement.   
Firstly, I will exploit the combination of ultra-large exciton binding energies, giant absorption and unique spin properties of such materials to form microcavity exciton polaritons. These composite bosons provide the unique possibility to study coherent quantum fluids up to room temperature. Due to the possibility of fabricating such structures by relatively simple means, establishing bosonic condensation effects in atomic monolayers can lead to a paradigm shift in polaritonics. 
Secondly, I will study exciton localization in layered materials, with the perspective to establish a new generation of microcavity-based quantum light sources. Light-matter coupling effects will greatly improve the performance of such sources. I will investigate possibilities of tuning the spectral properties of these localizations via external electric and strain-fields, to gain position control and make use of them as sources of single, indistinguishable photons.","1499932","2016-05-01","2021-04-30"
"UPCON","Ultra-Pure nanowire heterostructures and energy CONversion","Anna Fontcuberta I Morral","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","This proposal is devoted to the synthesis of ultra-pure semiconductor nanowire heterostructures for energy conversion applications in the photovoltaic domain. Nanowires are filamentary crystals with a very high ratio of length to diameter, the latter being in the nanometer range. Nanowires are of significant interest owing to their large surface-to-volume ratio and low-dimensional properties, as well as attractive building blocks of novel devices, including for novel energy conversion applications. The most widely employed nanowire growth method relies on the use of gold, which is known to be an impurity limiting mobility and carrier lifetime in semiconductors. It is generally realized that nanowires with higher purity could enable significant advances in both fundamental studies and technological applications. This proposal combines two complementary and essential aspects of semiconductor nanowires: (i) synthesis in extremely clean conditions and (ii) their application to new concepts of photovoltaic devices. The first part involves the use of Molecular Beam Epitaxy (MBE) system for the synthesis of III-V semiconductor nanowires and heterostructures. Special emphasis will be given in the synthesis of new heterostructure designs, i.e. across the nanowire radius and along the growth axis. The fabrication of ordered arrays of nanowires on large areas and on silicon substrates will also be investigated. In the second part, nanowire based solar cells will be designed, fabricated and characterized. Particular emphasis will be given toward understanding the role of geometry and interfaces in the energy conversion efficiency of the novel nanowire-based solar cells. Here, the high cleanliness and precise heteroepitaxial growth of MBE nanowires will allow us to perform fundamental studies, generating ground-breaking knowledge on the microscopic processes in energy conversion. This project will foster the use of nanotechnology in the energy challenges of the XXI century.","1286000","2010-01-01","2014-12-31"
"UPGAL","Understanding the Physics of Galaxy Formation and Evolution at High Redshift","Emanuele Daddi","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","Understanding the processes regulating galaxy formation is a major open issue in observational cosmology. We now have a fairly detailed census of the diverse high-z galaxy populations, hence time is ripe for fundamental advances in understanding galaxy formation and evolution in the crucial first few billion years. This requires to observationally constrain and clarify the physical processes that operated at those early epochs. Thanks to a new galaxy selection technique that I recently introduced, I have been leading research projects that have now provided major new results on high redshift z~2 galaxies. These include molecular gas first seen in typical high-z galaxies; the major phase of star formation at very high rates; widespread presence of previously unknown Compton-thick AGNs inside massive galaxies; and the existence of evolved galaxy clusters containing X-ray emitting gas already at z~2. Building on the legacy of these discoveries and critical results, I ask for support to fund the establishment of a new research team to lead research aimed at exploring the physics of galaxy formation in the distant Universe. With three postdocs each year for a total of 5 years, we will pave new avenues towards understanding the relation between black holes and galaxies at the time of their major mass growth and assembly. In a full multiwavelength approach, by obtaining and using data from all major observational facilities (both in space and on the ground) we will aim to clarify the physical trigger of downsizing, catch AGN feedback in action and assess its role in galaxy transformations, along with the effects of the environment, gas accretion, star formation and merging in driving galaxy formation.","939600","2009-11-01","2014-10-31"
"UPTEG","Unconventional Principles 
of ThermoElectric Generation","Jean-François Sebastien Denis Robillard","YNCREA HAUTS DE FRANCE","The performance of thermoelectric generation has long since been limited by the fact that it depends on hardly tunable intrinsic materials properties. At the heart of this problem lies a trade-off between sufficient Seebeck coefficient, good electrical properties and suitably low thermal conductivity. The two last being closely related by the ambivalent role of electrons in the conduction of both electrical and thermal currents. Current research focuses on materials composition and structural properties in order to improve this trade-off also known as the figure of merit (zT). Recently, evidences aroused that nanoscale structuration (nanowires, quantum dots, thin-films) can improve zT by means of electron and/or phonon confinement. The aim of this project is to tackle the intrinsic reasons for this low efficiency and bring TE conversion to efficiencies above 10% by exploring two unconventional and complementary approaches:
Phononic Engineering Conversion consists of modulating thermal properties by means of a periodic, precisely designed, arrangement of inclusions on a length scale that compares to phonon means free path. This process is unlocked by state of the art lithography techniques. In its principles, phononic engineering offers an opportunity to tailor the phonon density of states as well as to artificially introduce thermal anisotropy in a semiconductor membrane. Suitable converter architecture is proposed that takes advantage of conductivity reduction and anisotropy to guide and converter heat flow. This approach is fully compatible with standard silicon technologies and is potentially applicable to conformable converters.
The Micro Thermionic Conversion relies on low work function materials and micron scale vacuum gaps to collect a thermally activated current across a virtually zero heat conduction device. This approach, though more risky, envisions devices with equivalent zT around 10 which is far above what can be expected from solid state conversion.","1499507","2013-10-01","2019-07-31"
"UPTIME","Real-TIME probing of Ultrafast Phenomena","Maria Ana RODEIA DE SOUSA BENTES CATALUNA","HERIOT-WATT UNIVERSITY","The perception of the world surrounding us depends on how we illuminate and capture it. For example, our direct observation of high-speed phenomena is limited by the speed of the human visual system. A well known method to visualize fast events is to capture them at very high frame rates with high-speed cameras, and then play the movie in slow-motion, enabling us to understand what happens at short timescales. 

The ultimate goal of this project is to develop the fastest camera on earth, able not only to visualize the changing morphology and position of an object under investigation, but also to simultaneously capture in depth spectroscopic information in both temporal and spectral domains – all of these at unprecedented speeds, several orders of magnitude faster than the current state of the art. 

For the first time, this will enable the simultaneous high-speed visualization and the spatio-spectro-temporal investigation of dynamic phenomena, at timescales never before accessible. This will enable us to gain unique insights into very fast transformations such as chemical reactions, phase transitions, laser ablation and other irreversible phenomena in physics, biology and engineering.

This project will achieve its ambitious objectives through the creative exploitation of novel semiconductor ultrafast lasers and photonic sampling systems, enabling the high-speed illumination and capture of events in the spatial, temporal and spectral domains. 

This project will pave the way for completely new insights into high-speed dynamic events, allowing us to open up entirely new perspectives into the behaviour of the world at such timescales.","1999407","2015-07-01","2020-06-30"
"UrbanWaves","Urban Waves: evaluating structure vulnerability to tsunami and earthquakes","Tiziana Rossetto","UNIVERSITY COLLEGE LONDON","Exposure to coastal floods across the world is forecast to increase to 150 million people and £20 trillion in assets by 2070 (>9% of projected annual global GDP). In addition to cities, potentially vulnerable assets include key infrastructure such as nuclear power plants and ports: the recent Japan earthquake and tsunami demonstrating this. Urban Waves will fill the gap in the engineering design and assessment of buildings in coastal areas subjected to onshore flow from tsunami preceded (or not) by earthquake ground shaking.

In Aim 1 the unique experimental capability developed by the PI to reproduce flows on shorelines from tsunami will be used to provide information for fundamental research into tsunami flows onshore as well as the forces and pressures they exert on model buildings and coastal protection structures. In Aim 2 the experimentally measured force/pressure time-histories will be used to calibrate advanced finite element models of the structures that will then be used to further investigate the influence of bathymetry, topography, tsunami and structure characteristics on the structure forces/pressures. The study findings will be used to propose simplified relationships for tsunami forces/pressures suitable for inclusion in codes of practice (for buildings and coastal defences). In Aim 3, the FE models built will be used to generate fragility functions for buildings that can be used for the assessment of risk to urban areas. The first analytical tsunami fragility functions to be derived, these will also account for the effect of preceding earthquake ground shaking. These will also be compared to data collected after past tsunami events using advanced statistical methods.

Urban Waves capitalises on the PI's recognised expertise in large-scale experiments, structural dynamics, analytical and empirical fragility function derivation and ability to carry out high quality multi-disciplinary research..","1911315","2014-01-01","2018-12-31"
"USECFrontiers","Frontiers of Usable Security – Principles and Methods for Administrator and Developer Usable Security Research","Matthew Smith","RHEINISCHE FRIEDRICH-WILHELMS-UNIVERSITAT BONN","Usability problems are a major cause of many of today’s IT-security incidents. Security systems are often too complicated, time-consuming, and error prone. For more than a decade researchers in the domain of usable security (USEC) have attempted to combat these problems by conducting interdisciplinary research focusing on the root causes of the problems and on the creation of usable security mechanisms. While major improvements have been made, to date USEC research has focused almost entirely on the non-expert end-user. However, many of the most catastrophic security incidents were not caused by end-users, but by developers or administrators. Heartbleed and Shellshock were both caused by single developers yet had global consequences. The recent Sony hack compromised an entire multi-national IT-infrastructure and misappropriated over 100 TB of data, unnoticed. Fundamentally, every software vulnerability and misconfigured system is caused by developers or administrators making mistakes, but very little research has been done into the underlying causalities and possible mitigation strategies. 
I aim to extend the frontiers of usable security by conducting foundational research into USEC methods for developers and administrators. To this end I will research and systemize the hitherto unexamined human factors in a carefully selected set of problems currently faced by developers and administrators, specifically: authentication, secure messaging, systems configuration, intrusion detection, and public key infrastructures. From this pioneering research I will extract and develop principles, methods, and best practices for conducting usability studies and research with these actors and establish a foundation for this emerging research field. In addition to these foundational methodological results, I expect to make fundamental advancements in the above application research domains by including the human factors in these currently purely technical research areas.","1498976","2016-08-01","2021-07-31"
"USED","Ultrafast Spectroscopic Electron Diffraction (USED) of quantum solids and thin films","Fabrizio Carbone","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Ultrafast Spectroscopic Electron Diffraction (USED) of quantum solids and thin films","1464000","2010-11-01","2015-10-31"
"USEMS","Uncovering the Secrets of an Earthquake: Multidisciplinary Study of Physico-Chemical Processes During the Seismic Cycle","Giulio Di Toro","ISTITUTO NAZIONALE DI GEOFISICA E VULCANOLOGIA","Southern Europe and Turkey lie within the highest seismic risk areas in the world. Understanding the physico-chemical processes controlling earthquake generation is essential in seismic hazard assessment. Destructive earthquakes nucleate at depth (10-15 km), therefore monitoring active faults at the Earth’s surface, or interpreting seismic waves, yields only limited information on earthquake mechanics.  We propose to investigate earthquake processes by: 1) installing a new world class high velocity rock friction apparatus to perform experiments under deformation conditions typical of earthquakes; 2) studying fossil seismic sources now exhumed at the Earth's surface; 3) analyzing natural and experimental fault rock materials using a novel multidisciplinary approach involving state of the art techniques in microstructural analysis, mineralogy and petrology; 4) producing new theoretical earthquake models calibrated (and tightly constrained) by field observations, mechanical data from rock-friction experiments and analyses of natural and experimental fault rocks. The integration of such an original and complementary data set shall provide an unprecedented insight into the mechanics of seismic faulting. The installation of the new dedicated rock friction apparatus will allow the European Union to become a key world player competing at the top scientific level in the study of earthquake mechanics. The proposed study has additional implications for understanding other friction-controlled processes important in Earth sciences and hazard mitigation  (e.g., rock landslides). Friction also has broad applications in the industry, including innovative but poorly understood production processes. Our experimental results will help to improve industrial milling techniques and investigate the mechanical-chemical transformations induced during milling. The latter is the basis of a new technique for the production of hydrocarbons and hydrogen from inorganic and organic materials.","1992000","2008-06-01","2013-05-31"
"USNAC","Understanding Type Ia SuperNovae for Accurate Cosmology","Mickael Pierre Manuel RIGAULT","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","With this proposal I aim at removing astrophysical biases in SN cosmology that limit improvements in the derivation of properties of the dark energy (DE). DE is the cause of the recent acceleration of the expansion of the Universe and accounts for ~70% of its total energy. The initial discovery based on distance measurements from Type Ia Supernovae (SNeIa) has since been confirmed with several independent probes. Today, the goal of cosmology is to understand the nature of DE, and SNeIa remain a central probe for this endeavor. However, the nature of SNeIa remains largely unknown, which limits our ability to accurately trace the properties of the Universe. The proposed research program focuses on solving this problem.

The project takes place at the frontier between astronomy, cosmology and data-science, and consists of three main interconnected work-packages: 
1. Understand the influence of the interstellar dust on the SN reddening, based on the unique combination of ground based spectrophotometry and high-resolution HST (space) UltraViolet-photometry; 
2. Map SN progenitor variations across environments through next-generation large-scale transient surveys;
3. Combine the knowledge of the two former packages into a unique SN framework capable of adapting as the Universe evolves across time and distance. This will be used to derive accurate measurement of the Hubble constant and the dark energy equation of state parameters.

The project is made possible by a combination of state-of-the-art SN and galactic datasets from ZTF, SNfactory and an ongoing Hubble Space Telescope program of up to 136 orbits (PI: Rigault). The HST data will enable the first measurement of stellar-age and the interstellar dust at the SN location.","1437188","2018-03-01","2023-02-28"
"USOFT","Ultrasound-based techniques for soft jammed materials","Sébastien Manneville","ECOLE NORMALE SUPERIEURE DE LYON","""Soft materials are involved in most parts of our everyday life. In spite of a tremendous industrial importance, their properties still confront the physicists with challenging questions. Indeed, the constituents of soft materials are often stuck into a disordered structure, as for oil droplets in a concentrated emulsion like mayonnaise and for attractive particles in a colloidal gel. This phenomenon is known as """"jamming"""" and leads to solid-like properties at rest that slowly evolve in time as the system """"ages"""" under spontaneous rearrangements. When a jammed material is submitted to some external stress, a """"yielding"""" transition from solid to liquid behaviour is commonly observed. Besides the fundamental issues raised by such out-of-equilibrium, glassy features, the design of soft materials that can strongly respond to a well-controlled excitation is of great interest for practical applications.

This project aims at developing original tools for both physical investigation and design of soft jammed materials, based on the use of ultrasound. We will first set up ultrafast ultrasonic imaging techniques in the single and multiple scattering regimes in an attempt to overcome some limitations of current optical methods and to elucidate jamming, aging and yielding processes in a wide variety of model and real materials. High-intensity ultrasound will then be used to mechanically stress jammed materials and trigger local rearrangements or even large-scale structuration, leading to new insights into the physics of jamming and to innovations in the field of material design.""","1305378","2010-12-01","2015-11-30"
"UTHOTP","Understanding the Hardness of Theorem Proving","Karl Jakob Nordström","KUNGLIGA TEKNISKA HOEGSKOLAN","This project aims to explore the fundamental question in computer science and mathematics regarding what computational problems can feasibly be solved on a computer. More specifically, we want to study algorithms for proving logic formulas as well as impossibility results for this problem.

Proving formulas in propositional logic is a problem of immense importance both theoretically and practically. On the one hand, this computational task is believed to be intractable in general, and deciding whether this is so is one of the famous million dollar Millennium Problems (the P vs NP problem). On the other hand, today automated theorem provers, or so-called SAT solvers, are routinely used to solve large-scale real-world problem instances with even millions of variables. This contrasts to that there are also known small example formulas with just hundreds of variables that cause even state-of-the-art SAT solvers to stumble.

The main objectives of our project are as follows:

(1) Understand what makes formulas hard or easy in practice by building and studying better theoretical models of the proof systems underlying SAT solvers, and testing the predictions of these models against empirical data.

(2) Gain theoretical insights into other crucial issues in SAT solving such as memory management and parallelization.

(3) Explore the possibility of basing SAT solvers on stronger proof systems than are currently being used.

(4) Clarify the theoretical limitations of such enhanced SAT solvers by studying the corresponding proof systems, which are currently poorly understood.

We see great opportunities for fruitful interplay between the fields of proof complexity and SAT solving in this area, as well as between theoretical results and practical implementations. We believe that resolving the questions posed by this project could potentially have a major impact in theoretical computer science, and in the longer term in more applied areas of computer science and mathematics.","1460000","2012-07-01","2018-06-30"
"VADEMECOM","VAlidation driven DEvelopment of Modern and Efficient COMbustion technologies","Alessandro PARENTE","UNIVERSITE LIBRE DE BRUXELLES","Combustion science will play a major role in the future quest for sustainable, secure and environmentally friendly energy sources. Two thirds of the world energy supply rely on combustion of fossil and alternative fuels, and all scenarios forecast an increasing absolute energy supply through combustion, with an increasing share of renewables. Thus, combustion will remain the major actor in transportation and power generation as well as in manufacturing processes, like steel and glass.

Nevertheless, combustion science will need profound innovation to meet future energy challenges, such as energy efficiency and fuel flexibility, and ensure future generations with affordable and sustainable energy and healthy environment. In this context, MILD combustion represents a very attractive solution for its fuel flexibility and capability to deliver very high combustion efficiency with virtually zero pollutant emissions. Such a combustion regime is the result of a very strong interaction between turbulent mixing and chemical kinetics. The fundamental mechanism of this interaction is not fully understood, thus justifying the need for experimental and numerical investigations. 

The overall objective of the present research proposal is to drive the development of modern and efficient combustion technologies, by means of experimental, theoretical, and numerical simulation approaches. New-generation simulation tools for MILD combustion will be developed, to reduce the dependence on sub-grid models and increase the fidelity of numerical simulations. High-fidelity experimental data will be collected on quasi-industrial systems, to disclose the nature of the interactions between fluid dynamics, chemistry and pollutant formation processes in MILD combustion. Experiment and numerical simulations will be tied together by Validation and Uncertainty Quantification techniques, to allow the ground-breaking application of the developed approaches and promote innovation in the energy sector.","1499110","2017-04-01","2022-03-31"
"VAPORE","Vapor deposition of crystalline porous solids","Rob AMELOOT","KATHOLIEKE UNIVERSITEIT LEUVEN","Metal-organic frameworks (MOFs) are crystalline solids with highly regular pores in the nanometer range. The possibility to create a tailored nano-environment inside the MOF pores makes these materials high-potential candidates for integration with microelectronics, e.g. as sensor coatings, solid electrolytes, etc. However, current solvent-based methods for MOF film deposition, a key enabling step in device integration, are incompatible with microelectronics fabrication because of contamination and corrosion issues.

VAPORE will open up the path to integrate MOFs in microelectronics by developing a solvent-free chemical vapor deposition (CVD) route for MOF films. MOF-CVD will be the first example of vapor-phase deposition of any type of microporous crystalline network solid and marks an important milestone in processing such materials. Development of the MOF-CVD technology platform will start from a proof-of-concept case and will be supported by the following pillars: (1) Insight in the process, (2) expansion of the materials scope and (3) fine-tuning process control. The potential of MOF-CVD coatings will be illustrated in proof-of-concept sensors.

In summary, by growing porous crystalline films from the vapor phase for the first time, VAPORE implements molecular self-assembly as a scalable tool to fabricate highly controlled nanopores. In doing so, the project will enable cross-fertilization between the worlds of nanoscale chemistry and microelectronics, two previously incompatible fields.","1787475","2016-12-01","2021-11-30"
"VARIAMOLS","VAriable ResolutIon Algorithms for macroMOLecular Simulation","Raffaello POTESTIO","UNIVERSITA DEGLI STUDI DI TRENTO","Within the broad spectrum of biological soft matter systems, large proteins and protein assemblies occupy a central role. These molecules are extremely versatile: they can catalyze chemical reactions, transport atoms and molecules across the cellular membrane, bind to foreign bodies to be destroyed, or combine into large molecular machines that perform a variety of different tasks. One of the most prominent problems in the computational study of these macromolecules is that the cost of using accurate atomistic models dramatically increases with system size. Simplified, coarse-grained representations offer an elegant and effective alternative to high-resolution models, and enable the simulation of large systems over extended time scales; the other side of the coin, however, is that the missing chemical detail often represents an insurmountable limitation to the realistic reproduction of the properties of interest. The main goal of the VARIAMOLS project is to develop and apply novel computer-aided methods for the study of large molecular assemblies and their dynamics, thus bridging the existing gap between computational cost and chemical accuracy. Specifically, the research will unfold along two intertwined lines: 1) the development of non-uniform resolution models of the system, which optimize the balance between detail and efficiency; and 2) the study of dynamics-mediated properties of protein assemblies. The working philosophy of VARIAMOLS has two complementary and strictly interconnected aspects: on the one hand, the theoretical and algorithmic advancement of the methods currently employed to represent and simulate biomolecules; on the other hand, the systematic application of the developed methods to real-life case studies of great relevance for medical science and technology, with a particular focus on viruses and antibodies.","1339351","2018-01-01","2022-12-31"
"VASCULARGROWTH","Bioengineering prediction of three-dimensional vascular growth and remodeling in embryonic great-vessel development","Kerem Pekkan","KOC UNIVERSITY","Globally 1 in 100 children are born with significant congenital heart defects (CHD), representing either new genetic mutations or epigenetic insults that alter cardiac morphogenesis in utero. Embryonic CV systems dynamically regulate structure and function over very short time periods throughout morphogenesis and that biomechanical loading conditions within the heart and great-vessels alter morphogenesis and gene expression. This proposal has structured around a common goal of developing a comprehensive and predictive understanding of the biomechanics and regulation of great-vessel development and its plasticity in response to clinically relevant epigenetic changes in loading conditions.  Biomechanical regulation of vascular morphogenesis, including potential aortic arch (AA) reversibility or plasticity after epigenetic events relevant to human CHD are investigated using multimodal experiments in the chick embryo that investigate normal AA growth and remodeling, microsurgical instrumentation that alter ventricular and vascular blood flow loading during critical periods in AA morphogenesis. WP 1 establishes our novel optimization framework, incorporates basic input/output in vivo data sets, and validates. In WP 2 and 3 the numerical models for perturbed biomechanical environment and incorporate new objective functions that have in vivo structural data inputs and predict changes in structure and function. WP 4 incorporates candidate genes and pathways during normal and experimentally altered AA morphogenesis. This proposal develops and validates the first in vivo morphomechanics-integrated three-dimensional mathematical models of AA growth and remodeling that can predict normal growth patterns and abnormal vascular adaptations common in CHD. Multidisciplinary application of bioengineering principles to CHD is likely to provide novel insights and paradigms towards our long-term goal of optimizing CHD interventions, outcomes, and the potential for preventive strategies.","1995140","2013-01-01","2019-07-31"
"VDW-CMAT","Van der Waals Interactions in Complex Materials","Alexandre Tkatchenko","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Van der Waals (vdW) interactions are ubiquitous in nature, playing a major role in defining the structure, stability, and function for a wide variety of molecules and materials. VdW forces make the existence of molecular liquids and solids possible; they largely control protein-protein and drug-protein binding inside our bodies; they give geckos the ability to “defy gravity” attaching to walls and ceilings. An accurate first-principles description of vdW interactions is extremely challenging, since the vdW dispersion energy arises from the correlated motion of electrons and, in principle, requires many-electron quantum mechanics. Rapid increase in computer power and advances in modeling of vdW interactions have allowed to achieve “chemical accuracy” (1 kcal/mol) for binding between small organic molecules. However, the lack of accurate and efficient methods for large and complex systems hinders truly quantitative predictions of properties and functions of technologically relevant materials. We aim to construct and apply a systematic hierarchy of efficient methods for the modeling of vdW interactions with high accuracy and capacity to predict new phenomena in complex materials. Starting from quantum-mechanical first principles (adiabatic-connection fluctuation-dissipation theorem), we unify concepts from quantum chemistry (linear-response coupled-cluster and many-body perturbation theory), density-functional theory (ground-state electron-density response), and statistical mechanics (coupled-fluctuating-dipole model). Our final goal is to enable long time-scale molecular dynamics simulations with predictive power for large and complex systems of thousands of atoms. The project goes well beyond the presently possible applications and once successful will pave the road towards having a suite of first-principles modeling tools for a wide range of materials, such as biomolecules, nanostructures, solids, and organic/inorganic interfaces.","1356999","2011-09-01","2016-08-31"
"VECTORIAL PROBLEMS","Vectorial Elliptic, Parabolic and Variational Problems: Singularities and Regularity","Giuseppe Mingione","UNIVERSITA DEGLI STUDI DI PARMA","The aim of the proposed project is to start a systematic investigation of the fine regularity and singularity properties of solutions to vectorial problems, of elliptic and parabolic type, with special emphasis on the ones admitting a variational structure. A distinguishing feature of vectorial problems is the possibility of having solutions with singularities; in turn the structure of such singularities encodes subtle information about the fundamental nature of the underlying problem. Analytic, geometric and topological phenomena concur in the formation and in the nature of the singularities observed, and the analysis of the singular sets requires a very high interaction of several different methods and techniques coming from different branches of Mathematics. Unraveling the singular sets structure is of paramount importance to clarify the basic nature of the mathematical phenomena considered; specifically, it can allow for their efficient numerical treatment, and for using them in the context of mathematical modeling, towards the applications in Science.","500000","2008-07-01","2013-06-30"
"VERCORS","Verification of Concurrent Data Structures","Marieke Huisman","UNIVERSITEIT TWENTE","Increasing performance demands, application complexity and explicit multi-core parallelism makes concurrency omnipresent in software applications. However, due to the complex interferences between threads in an application, concurrent software is also notoriously hard to get correct. Instead of spending large amounts of money to fix incorrect software, formal techniques are needed to reason about the behaviour of concurrent programs.

In earlier work, we developed a variant of permission-based separation logic that is particularly suited to reason about multithreaded Java programs with dynamic thread creation and termination, and reentrant locks. The VerCors project will extend expressiveness of the logic, to specify and verify concurrent data structures. The verification logic will be parameterised over the locking policy, so that a high-level specification of the behaviour of a data structure can be reused for different implementations. Thus the implementation of a concurrent data structure can be changed, without affecting correctness of the applications using it.

The logic will also be parameterised with concurrency and synchronisation primitives, so that a logic for a different programming language can be defined as an instance of the general logic. It will also be adapted to reason about programs with benign data races, i.e., data races where the same value is written simultaneously by different threads. Also techniques to generate part of the specifications automatically will be developed. Finally, the logic will be adapted to a distributed setting, where data consistency between the different sites has to be maintained.

All results will be integrated in a tool set that generates and proves proof obligations automatically. It will be validated on realistic case studies.","1306500","2011-02-01","2016-01-31"
"VERISYNTH","Automatic Synthesis of Software Verification Tools from Proof Rules","Andrey Rybalchenko","TECHNISCHE UNIVERSITAET MUENCHEN","Software complexity is growing, so is the demand for software verification.  Soon, perhaps within a decade, wide deployment of software verification tools will be indispensable or even mandatory to ensure software reliability in a large number of application domains, including but not restricted to safety and security critical systems.  To adequately respond to the demand we need to eliminate tedious aspects of software verifier development, while providing support for the accomplishment of creative aspects.  We believe that the next generation of software verifiers will be constructed from logical specifications designed by quality/verification engineers with expertise in the application domain.  Give a specification describing a verification method, a corresponding software verifier will be obtained by implementing a frontend that translates software source code into constraints according to the specification and then coupling the frontend with a highly-tuned general-purpose constraint solver, thus eliminating the need for algorithmic implementation efforts from the ground up.  This project proposes the necessary methodology, solving algorithms, and tools for building verifiers of the future.","1476562","2012-12-01","2017-11-30"
"VEWA","Ve-Wa:Vegetation effects on water flow and mixing in high-latitude ecosystems–Capability of headwater catchments to mediate potential climate change","Doerthe Tetzlaff","THE UNIVERSITY COURT OF THE UNIVERSITY OF ABERDEEN","""Our ability to predict consequences of climate change on the physical, chemical and biological characteristics of water resources in high-latitude uplands is a formidable challenge. These regions are highly sensitive to climate induced changes as small differences in temperature determine the status of frozen ground, the state of precipitation, and the magnitude and timing of snow accumulation and melt. Recent findings in mid-latitude regions suggest that there exist “two water worlds” – mobile water expressed in the stream and tightly bound water represented by plant water – which means that a substantial proportion of precipitation that infiltrates the soils becomes isolated from discharge to the streams, indicating that the composition of stream water alone is insufficient to understand routing and transit times of water in catchments. These findings challenge the core assumptions in our perceptual models of how we think biophysical systems work and how we make predictions of water partitioning of how inputs of water are evaporated, stored and reach the streams. High-latitude headwater catchments are characterised by lower evapotranspiration, consequent lower soil moisture deficits and different seasonality than mid-latitude sites. This interdisciplinary proposed project will address novel questions on vegetation-water linkages by using isotopic tracers in different waters as """"fingerprints"""" across different spatial scales along a climate gradient as a precursor to understand future response to change in high-latitude upland catchments. The proposed project will – for the first time - examine the mechanisms of water storage, transmission and release and possible implications of climate change in high-latitude ecosystems along a cross-regional transect. Such geographically extensive comparison has never been conducted in these environments. This allows the consistency of processes and drivers to be assessed across broad spatial scales.""","1500000","2013-10-01","2018-09-30"
"VIAMASS","Visual Recognition Made Super-Scalable","Herve Jegou","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","Hundreds of billions of images are hosted on the World Wide Web. There is a huge interest in mining information in large image collections based on visual content. Direct applications are the automatic organization of visual datasets, visual navigation, object recognition, and the traditional query-by-sample search.

Although recent breakthroughs allow the search in millions of images on a single server with increasing quality, the accuracy of automatic recognition remains low compared to human’s visual analysis. I believe that significant progress is still achievable by a major shift in the paradigm underpinning the image representation: an image should be described with respect to the context provided by the image collection.

The main objective of VIAMASS is to automatically discover visual links within a very large collection of images. These “visual hyper-links” will connect the objects across the images of the collection. This raises a major obstacle with respect to scalability: cross matching all the images is of quadratic complexity when performed with a brute-force approach. To this end, VIAMASS addresses issues at the frontier of the current state of the art in computer vision and signal processing: How to exploit the context provided by the collection to enrich the image representation? How to exploit and magnify recent signal processing and coding techniques to efficiently compare sets of vectors? How to automatically produce geometrical models of objects with little or no supervision? At the end, the ultimate challenge is to invent scalable solutions for the automatic discovery of visual links across images.

My research program impacts the whole processing chain of visual search, from the description level to the mining algorithms that will break the complexity lock.","1498627","2014-04-01","2019-03-31"
"VIBCONTROL","Vibronic control of organic electronic devices","Artem Bakulin","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","Organic electronics (OE) is an expanding research field that exploits the electronic functionalities of organic molecules to make them robust and cost-efficient building blocks for future electronic devices. Due to the ‘soft’ character of organic materials, their electronic properties are defined by vibronic coupling (VC) phenomena which are a result of the interaction between electron and nuclear dynamics of the molecule.
This research program aims to unlock a new direction of experimental studies investigating and exploiting VC in OE devices by using optical control of nuclear motion.
The growing awareness that VC underlies diverse phenomena from physics to biology stimulates a broad interdisciplinary effort to address this issue. However, in the field of OE, the lack of synergy between device and optical studies holds the potential functionality offered by VC effects from being attained. In 2012, I proposed a direct route to control the performance of OE devices by optically switching the vibronic states of the molecules. Though this work came specifically in connection with organic photovoltaics, it provides the starting point for a more fundamental and broad reaching of VC phenomena. The proposed research program will use this opportunity. I will apply state-of-the-art developments in infrared light shaping to create a well-defined coherent superposition of molecular vibrational motions inside devices and study their influence on electron dynamics with device-specific spectroscopic techniques.
This approach combines recent advances in ultrafast spectroscopy and OE to extend our fundamental understanding of molecular charge transport. Our methodology will become a tool for elucidating current pathways in organic nanodevices and offer access to non-equilibrium phenomena down to the level of molecular junctions. This research will lead to the development of new design rules for OE materials serving future advances in molecular electronics, computing and sensing.","1497257","2015-06-01","2020-05-31"
"VIDEOLEARN","Video and 3D Analysis for Visual Learning","Thomas Stefan Brox","ALBERT-LUDWIGS-UNIVERSITAET FREIBURG","""Reliable recognition of thousands of object and action categories is today's key challenge in computer vision. Most contemporary approaches are based on supervised learning algorithms to train object classifiers. While manual annotation has become easier in recent years, it is still not scalable to a large set of categories. Moreover, as it is usually based on human language it does not reflect the visual characteristics of objects, but tries to establish high-level links that should actually be learned after appropriate visual features have been captured.

In this proposal, we aim at reducing the manual labeling effort by making use of the natural organization of visual data as it is provided by a video stream. In the same setting, we also aim at learning a more sophisticated structural representation of objects. Rather than manually specifying parts and attributes of objects that have a counterpart in language, we will seek correlated visual patterns by letting the data speak. Exploiting the natural arrangement of images in video and the inherent 3D scene structure is decisive, since weakly correlated images as obtained from photo collections might not contain rich enough relationship information.

We will also consider the active observer setting, i.e., where the camera can be moving. This allows extracting far more information, but also requires detailed control of the low-level and mid-level computer vision techniques involved, particularly motion estimation and tracking. The importance of these components is often underestimated in contemporary visual learning approaches.

Apart from the impact on the field of computer vision itself, the improved performance in visual recognition that we anticipate in this project has direct consequences for many important applications, particularly automotive systems and robotics, where the use of visual sensory input is more and more considered one of the most important components of future systems.""","1462800","2012-01-01","2016-12-31"
"VirBAcous","Virtual building acoustics: a robust and efficient analysis and optimization framework for noise transmission reduction","Edwin REYNDERS","KATHOLIEKE UNIVERSITEIT LEUVEN","Achieving a sufficient sound insulation of buildings is a complex problem since multiple transmission paths are important, uncertainties can have a large effect, and acoustic performance requirements often conflict with structural and thermal requirements. Furthermore, accurate vibro-acoustic modelling across the entire building acoustics frequency range presently requires a huge computational effort. As a result, the acoustic development of building systems is usually based on general design rules, insufficiently accurate prediction models and many experimental prototype tests. Such development is costly and time consuming, and leads to suboptimal designs. This project therefore aims to develop an efficient yet sufficiently accurate prediction framework for the acoustic design of building systems which takes all uncertainties into account and which opens the way for design optimization. Four fundamental breakthroughs are required. First, a new approach to high-frequency subsystem modelling will overcome the limitations of the current statistical energy analysis paradigm and handle a high degree of geometric and material complexity. Second, a modelling framework for built-up systems will be developed, which incorporates different component model types and which switches between them as the frequency increases. The third development consists of quantifying the combined effect of all uncertain parameters on the overall sound insulation performance in a logically consistent and computationally efficient way. Finally, a robust optimization approach that spans the entire building acoustics frequency range and that accounts for all relevant non-acoustic performance criteria as well will be developed. Each development will be complemented by showcase applications in building acoustics, yet the fundamental nature of the developments make that they will have a profound impact in all disciplines where the study and/or control of mechanical wave propagation are important.","1386875","2017-08-01","2022-07-31"
"VISCUL","Visual Culture for Image Understanding","Vittorio Ferrari","THE UNIVERSITY OF EDINBURGH","The goal of computer vision is to interpret complex visual scenes, by recognizing objects and understanding their spatial arrangement within the scene. Achieving this involves learning
categories from annotated training images. In the current paradigm, each category is learned starting from scratch without any previous knowledge. This is in contrast with how humans learn, who accumulate knowledge about visual concepts which they reuse to help learning new concepts.
The goal of this project is to develop a new paradigm where computers learn visual concepts on top of what they already know, as opposed to learning every concept from scratch. We propose to progressively learn a vast body of visual knowledge, coined Visual Culture, from a variety of available datasets. We will acquire models of the appearance and shape of categories in general, models of specific categories, and models of their spatial organization into scenes. We will start learning from datasets with high degree of supervision and then gradually move to datasets with lower degrees. At each stage we will employ the current body of knowledge to support learning with less supervision. After acquiring Visual Culture from existing datasets, the machine will be ready to learn further with little or no supervision, for example from the Internet. Visual Culture is related to ideas in other fields, but no similar endeavor was undertaken in Computer Vision yet.
This project will make an important step toward mastering the complexity of the visual world, by advancing the state-of-the-art in terms of the number of categories that can be localized, and in
the variability covered by each model.  Moreover, Visual Culture is more than a mere collection of isolated categories, it is  is a web of object, background, and scene models connected by spatial relations and sharing visual properties. This will bring us closer to image understanding, the automatic interpretation of complex novel images.","1481516","2013-01-01","2017-12-31"
"VISION","Video-oriented UWB-based Intelligent Ubiquitous Sensing","Dajana Cassioli","UNIVERSITA DEGLI STUDI DELL'AQUILA","Real-time (RT) sensing with enhanced video capabilities is a powerful tool capturing a significant variety of data, thus providing an excellent substrate to build up an accurate context abstraction. Machine capability of creating a good context abstraction will boost several applications. 3D video allows the recognition of objects shapes, human gestures and facial expressions, improving human-to-machine interaction. Surveillance systems, empowered by automatic recognition of gestures, will suddenly detect a threat. Facial expressions may reveal the status of elderly or disabled people. Remote driving and decision making process of robots for scientific or rescue expeditions will improve. VISION aims to developing an innovative infrastructure providing RT sensing services, with particular emphasis on 3D video, with mobile and context-aware operation. Hence, it will address the numerous challenges raised by the limitations of current technology for wireless sensor networks. VISION will exploit the 60 GHz UWB radios enabling broadband transmissions, miniaturized devices and reduced interference. Due to the inherent high resource consumption of broadband wireless necessary for RT 3D video, VISION sensor network will be optimized at all layers. A comprehensive channel model, based on propagation measurements, will be derived and used to design the UWB signal waveform for reliable broadband transmission. Novel techniques to manage the huge number of nodes required by ubiquitous sensing, and innovative tools to support the development process of intelligent services will be designed. Full cross-layer adaptability to external conditions will assure the system be able to manage the available resources to provide the best achievable quality of service guaranteeing graceful degradation for video, audio and sensing applications. Innovative sensor nodes based on the above concepts will be designed and prototyped for the VISION demonstration.","1173680","2010-04-01","2015-03-31"
"Vision-In-Flight","Neuromechanics of Insect Vision during Aerial Interactions with Applications in Visually Guided Systems","Huai-Ti LIN","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","This project investigates how biological vision operates under the fastest and most challenging motion condition: flight. Specifically, we look beyond gaze stabilization and focus on directed gaze control such as object tracking. Flying insects are ideal model for studying vision in flight due to its relatively simple nervous system and the fixed optics of the compound eyes. Insect vision has elucidated fundamental circuitries of vision via psychophysics, electrophysiology, computational modelling, and connectomics. However, we have limited knowledge on how insects use vision in free flight and what visual signals influence motor control during aerial interactions. This study aims to reveal how flying insects direct their gaze in-flight to extract target information for guidance and to facilitate the execution of complex flight manoeuvres. To achieve this objective, we will advance three emerging techniques: 1) high-precision insect scale motion capture; 2) ultralight wireless neural telemetry; 3) virtual reality for freely flying insects. I was involved in developing the first two methods and they both still require significant development to suit this project. The third budded from a successful ERC project, which enabled virtual reality experiments with freely behaving animals, and also requires additional breakthrough in order to accommodate this project. By advancing these techniques together, we can fully control the visual input of a freely flying insect and simultaneously record relevant visual signals. While modern image sensors and image processing can sometimes surpass biological vision, machine vision systems today still cannot utilize some tactical benefits of directed gaze control. Indeed, learning how to look is one of the best lessons a visually guided system can take from biology. This research informs the control of autonomous systems such as self-driving cars, unmanned aerial taxi, and robotic courier which will revolutionize the upcoming era.","1499968","2018-11-01","2023-10-31"
"VISIRday","VISible to far-IR optical tuning: passive DAYtime cooling by hierarchical structures and hybrid materials","Markus Retsch","UNIVERSITAET BAYREUTH","Efficient daytime cooling without the need for a heat engine is an essential technology to lower our overall energy consumption. Nature offers a chance to off-load heat directly into the cold outer space via the so-called “sky window”: a spectral range from 8 – 13 µm, where our atmosphere is transparent. Concomitantly, solar radiance influx needs to be minimized by scattering and reflection, which would counteract the radiatively removed energy. VISIRday aims to provide ground-breaking new materials and concepts to emit thermal energy directly into this transparent sky window. A radically holistic approach is necessary to understand and design the optical properties of nano- and mesostructured materials over the entire spectral range (300 nm – 20 µm), with the mid-IR sky window being fully emissive, and all other spectral wavelengths being fully reflective. I will therefore combine top-down direct write lithography with intricate bottom-up colloidal self-assembly to device hierarchically structured systems fully addressing these stringent optical properties. A new material class – surface phonon polariton supporting nano- and mesoparticles – with adjustable absorption properties in the mid-IR range, will take a leading role as novel colloidal building block. In combination with polymers and metallic nanostructures my team will demonstrate hybrid structures with finely adjusted and even externally tuneable optical properties. Simulations based on finite element modelling to conceive optimum design rules will complement the experimental work. Inspired by examples from nature, namely white beetles and the Saharan silver ant, I will push the fundamental insights towards novel technologies such as radiative daytime cooling paints and fibres. I am convinced that this project provides the urgently needed materials and concepts to add radiative daytime cooling to the existing mix of green energy technologies.","1487637","2017-03-01","2022-02-28"
"VISLIM","Visual Learning and Inference in Joint Scene Models","Stefan Roth","TECHNISCHE UNIVERSITAT DARMSTADT","""One of the principal difficulties in processing, analyzing, and interpreting digital images is that many attributes of visual scenes relate in complex manners. Despite that, the vast majority of today's top-performing computer vision approaches estimate a particular attribute (e.g., motion, scene segmentation, restored image, object presence, etc.) in isolation; other pertinent attributes are either ignored or crudely pre-computed by ignoring any mutual relation. But since estimating a singular attribute of a visual scene from images is often highly ambiguous, there is substantial potential benefit in estimating several attributes jointly.
The goal of this project is to develop the foundations of modeling, learning and inference in rich, joint representations of visual scenes that naturally encompass several of the pertinent scene attributes. Importantly, this goes beyond combining multiple cues, but rather aims at modeling and inferring multiple scene attributes jointly to take advantage of their interplay and their mutual reinforcement, ultimately working toward a full(er) understanding of visual scenes. While the basic idea of using joint representations of visual scenes has a long history, it has only rarely come to fruition. VISLIM aims to significantly push the current state of the art by developing a more general and versatile toolbox for joint scene modeling that addresses heterogeneous visual representations (discrete and continuous, dense and sparse) as well as a wide range of levels of abstractions (from the pixel level to high-level abstractions). This is expected to lead joint scene models beyond conceptual appeal to practical impact and top-level application performance. No other endeavor in computer vision has attempted to develop a similarly broad foundation for joint scene modeling. In doing so we aim to move closer to image understanding, with significant potential impact in other disciplines of science, technology and humanities.""","1374030","2013-06-01","2018-05-31"
"VISUAL-MS","Visualising Supramolecular Assembly by 
preparative Mass Spectrometry","Giovanni Costantini","THE UNIVERSITY OF WARWICK","""The aim of VISUAL-MS is to investigate the structure and the assembly of individual functional adsorbed molecules with sub-nm resolution. This is an essential and still missing step in the development of a molecular scale foundation of many contemporary research fields. In order to achieve these goals, ultra high resolution scanning probe microscopy techniques are needed. However, these require that complex molecular units are deposited onto well-defined substrates under perfectly controlled conditions and that they are analysed in-situ. This is beyond the current state-of-the-art. In fact, thermal sublimation in ultra high vacuum is the strategy of choice for small, heat-resistant molecules but big functional (bio)molecules are not compatible with this process. VISUAL-MS will address this challenge by adapting a technique developed in mass spectrometry for transferring intact fragile molecules into the gas phase and by combining this with in-situ scanning probe microscopy. This interdisciplinary approach will expand the boundaries of modern surface science by enabling to apply its powerful diagnostic techniques to almost any type of complex functional molecule. It represents an essential step change in analytical capability and will provide groundbreaking new insight into fundamental molecule-substrate interactions. VISUAL-MS will apply this innovative instrumentation to explore the adsorption, mutual interaction and real-time dynamics of peptides on surfaces. The planned experiments will allow to determine which functional groups in a peptide interact with a substrate of given structure and chemistry and to directly visualise, for the first time, the secondary structure of individual peptides with sub-molecular resolution. These breakthrough achievements will open new horizons in the basic understanding of molecule-surface interactions and serve as a reference in the formulation of rationale protocols for the fabrication of (bio)molecular functionalised substrates""","1578211","2013-01-01","2017-12-31"
"VNALG","Von Neumann algebras, group actions and discrete quantum groups","Stefaan Vaes","KATHOLIEKE UNIVERSITEIT LEUVEN","Von Neumann algebras, and more specifically II_1 factors, arise naturally in the study of countable groups and their actions on measure spaces. A central, but extremely hard problem is the classification of these von Neumann algebras in terms of their group/action data. Breakthrough results were recently obtained by Sorin Popa. I presented a combined treatment of these in my Bourbaki lecture notes. In a joint work of Popa and myself, this gave rise to the full classification of certain generalized Bernoulli II_1 factors. In a recent article of mine, it lead for the first time to a family of II_1 factors for which the fusion algebra of finite index bimodules could be entirely computed. Popa's methods open up a wealth of research opportunities. They bring within reach the solution of several long-standing open problems, that constitute the main objectives of the first part of this research proposal: complete descriptions of the finite index subfactor structure of certain II_1 factors, constructions of II_1 factors with a unique group measure space decomposition and computations of orbit equivalence invariants for actions of the free groups. Even approaching these problems would have been completely hopeless just a few years ago. Other constructions of von Neumann algebras arise in the theory of discrete quantum groups. The first rigidity results for quantum group actions on von Neumann algebras constitute the main objective of this second part of the research proposal. Finally, we aim to deal with another connection between quantum groups and operator algebras, through the study of non-commutative random walks and their boundaries. The main originality of this research proposal lies in the interaction between two branches of mathematics: operator algebras and quantum groups. This is clear for the second part of the project and occupies a central place in the first part through subfactor theory.","500000","2008-09-01","2013-08-31"
"VOLATILIS","Origin of volatile elements in the inner Solar System","Evelyn FÜRI","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The objective of project VOLATILIS is to investigate the origin(s) of volatile elements on Earth and other planetary bodies in the inner Solar System. Since primitive and differentiated asteroids, planetary embryos, and the Earth-Moon system represent different stages of planet formation, studies of chondritic meteorites and samples from Vesta, Mars, the Moon, and Earth can provide constraints on the evolution of planetary volatiles from primordial to present-day compositions. However, indigenous volatiles in extraterrestrial samples are often masked by solar and cosmogenic contributions. Only combined analyses of noble gases and other volatiles (N, H) allow the observed volatile signatures to be resolved into constituent components (atmospheric, solar, cosmogenic, indigenous). The Centre de Recherches Pétrographiques et Géochimiques (Nancy, France), the PI’s host institute, is the only laboratory that is equipped with static noble gas mass spectrometers for coupled N-noble analyses of small-sized samples, and with two secondary ionization mass spectrometers for non-destructive volatile element measurements. By coupling these high-precision analytical techniques, we will be able to reliably characterize indigenous planetary volatiles, and to assess the importance of volatile storage during primary accretion or late addition via comets and meteorites. Furthermore, we aim to develop the protocols for N isotope analysis by ion microprobe and by static mass spectrometry in multi-collection mode; these methods will allow us to target micron-sized samples (such as melt inclusions) for N analyses and to improve the analytical precision for coupled N-noble gas studies, respectively. The new data obtained here can be integrated as critical parameters into geochemical and astrophysical models of volatile accretion and fluxes in the inner Solar System, and they are expected to be of great interest to the geo-/cosmochemistry, astrophysics, and astrobiology communities.","1396300","2017-02-01","2022-01-31"
"VORTEX","Exploring electron vortex beams","Johan Verbeeck","UNIVERSITEIT ANTWERPEN","In this project I will exploit new possibilities opened up by the recent succesful demonstration of our ability to create electron vortex beams in a transmission electron microscope. Electron vortex beams carry a helical phase and angular momentum around their propagation axis. They form the counterpart of optical vortex beams that were invented almost 20 years ago and have lead to many exciting new applications in optics.
In preliminary experiments with electron vortices I have demonstrated (Verbeeck et al. Nature, 467,301 (2010)) their usefulness for magnetic state mapping. This property makes them very desirable for solid state physics and materials science since no other tool exists that can map the local magnetisation inside materials with atomic scale resolution. We aim to develop atomic resolution magnetic state mapping and apply it to gain insight in spintronics devices as well as in topological insulators. We will follow two routes to this end, one using the combination of electron vortex beams and electron energy loss spectroscopy (EELS) and another making use of the Aharanov Bohm effect in elastic scattering.
Preliminary experiments proof that both routes are feasible and a wealth of interesting physics is ready to be explored.
We will also explore the potential of electron vortex beams to manipulate nanoparticles and transfer angular momentum from the electron beam to these particles. This would open up the road to assemble and create nanoscale devices and to study the fundamental laws that govern the interaction between vortex beams and particles with different physical properties.

I believe that this highly creative and innovative idea, combined with access to a state of the art transmission electron microscope and a young PI with a proven track record is combined into a project proposal entirely in the spirit of the ERC starting grants.","1458300","2012-01-01","2016-12-31"
"vortex","Taking extrasolar planet imaging to a new level with vector vortex coronagraphy","Olivier Absil","UNIVERSITE DE LIEGE","Vector vortex coronagraphs (VVC) are among the most promising solutions to directly image faint extrasolar planets by dimming the glare of their nearby host star. Manufacturing and efficiently operating such devices is however a challenging enterprise, especially in the thermal infrared regime where warm planets radiate most of their energy. For several years, we have been developing a new class of VVC, called the Annular Groove Phase Mask (AGPM) coronagraph. Etched on a diamond substrate, this coronagraph can be operated at any wavelength, including the thermal infrared, thanks to the excellent transparency properties of diamond. We are now at a stage where the first components have been manufactured and tested. The proposed research program has three main goals. First, we will install and exploit the first generation of AGPM coronagraphs on large telescopes in world-leading observatories. By providing a means to efficiently cancel the starlight in the thermal infrared regime for the first time, our AGPMs will significantly contribute to the discoveries and characterisation of exoplanets beyond a few astronomical units. Second, we aim at developing new AGPM coronagraphs for the next generation of imaging instruments. We will particularly focus our developments on the instruments planned for the future extremely large telescopes, which will bring the direct imaging of exoplanets to a new level. Finally, we will study, develop and test a ground-breaking concept that could improve very significantly the on-sky performance of VVCs in general. This concept is based on the quantum properties of light and in particular on the fact that an optical vortex induces an orbital angular momentum on the input starlight. We propose to use an interferometric device to sort photons based on their orbital angular momentum, so as to separate the planetary light from the residual starlight (including the speckles created by atmospheric turbulence) at the output of the coronagraph.","1499200","2013-09-01","2018-08-31"
"WAAXT","Wave-modulated Arctic Air-sea eXchanges and Turbulence","Peter SUTHERLAND","INSTITUT FRANCAIS DE RECHERCHE POUR L'EXPLOITATION DE LA MER","Wave-modulated Arctic Air-sea eXchanges and Turbulence (WAAXT) is a project designed to improve our understanding of ocean boundary layer processes in a changing Arctic Ocean.  Sea ice extent in the Arctic Ocean has been decreasing since the beginning of the satellite era, meaning that open-water, as opposed to under-ice, oceanographic processes are becoming increasingly important for Arctic dynamics.  One of the most fundamental differences between the open and ice-covered oceans is the presence of surface waves.  Surface waves and wave-driven processes drastically alter air-sea fluxes, upper-ocean turbulence, and the dominant dynamical balance in the upper ocean.  

WAAXT will be based on a series of field experiments to study the small-scale processes associated with this emerging wave climate, with a particular focus on near-surface turbulence.  Three major effects of wave processes will be targeted: 1) Modification and suppression of ice formation by wave motions and the associated elevated near-surface turbulence.  2) Physical breakup of sea ice by wave motions, and the associated contributions to the modification of air-sea fluxes, upper-ocean structure, and melt rates.  3) Interactions between wave-driven turbulence, especially breaking and Langmuir circulations, with the unique salinity-based stratification in the Arctic basin.  A key aspect of these processes is their horizontal variability, which will be captured using a multi-platform approach.  Experimental work will begin in a natural laboratory in the Saint Lawrence Estuary and move to the Arctic as scientific and technical capacity is developed.

The long-term goal for WAAXT is to produce the data and parameterizations needed to understand climate-scale feedbacks associated with the emerging wave climate in the Arctic basin.","2000000","2019-01-01","2023-12-31"
"WALLXBIRGEOM","Wall-crossing and Birational Geometry","Arend Bayer","THE UNIVERSITY OF EDINBURGH","We will use modern techniques in algebraic geometry, originating from string theory and mirror symmetry, to study fundamental problems of classical flavour.  More concretely, we apply wall-crossing in the derived category to the birational geometry of moduli spaces.

Bridgeland stability is a notion of stability for complexes in the derived category.  Wall-crossing describes how moduli spaces of stable complexes change under deformation of the stability condition, often via a birational surgery occurring in its minimal model program (MMP). This relates wall-crossing to the most basic question of algebraic geometry, the classification of algebraic varieties.

Our previous results additionally provide a very direct connection between Bridgeland stability conditions and positivity of divisors, the main tool of modern birational geometry.  This makes the above link significantly more effective, precise and useful.  We will exploit this in the following long-term projects:

1. Prove a Bogomolov-Gieseker type inequality for threefolds that we conjectured previously. This would provide a solution in dimension three to well-known open problems of seemingly completely different nature: the existence of Bridgeland stability conditions, Fujita's conjecture on very ampleness of adjoint line bundles, and projective normality of toric varieties.

2. Study the birational geometry of moduli space of sheaves via wall-crossing, adding more geometric meaning to their MMP.

3. Prove that the MMP for local Calabi-Yau threefolds is completely induced by deformation of Bridgeland stability conditions. The motivation is a derived version of the Kawamata-Morrison cone conjecture, classical questions on Chern classes of stable bundles, and mirror symmetry.

4. Answer major open questions on the  birational geometry of the moduli space of genus zero curves (for example, the F-conjecture) using exceptional collections in the derived category and wall-crossing.","1282912","2013-12-01","2018-11-30"
"WAPITI","Water-mass transformation and Pathways In The Weddell Sea: uncovering the dynamics of a global climate chokepoint from In-situ measurements","Jean-Baptiste Bruno Sallée","SORBONNE UNIVERSITE","Deep water formed around the Antarctic continent drives the world ocean circulation. 50-70% of this deep water is formed within only about 10% of the Antarctic circumpolar band: the Weddell Sea. Subtle changes in the circulation of the Weddell Sea can lead to major changes in floating ice-shelves, with critical implications for global sea-level, the production of deep water and the global ocean overturning circulation. Despite these critical climate implications, the Antarctic shelf circulation remains poorly understood. 

I propose an ambitious project at the crossroads of experimental and numerical oceanography. By drawing on the strengths of each discipline I will explore the regional water-mass pathways in the Weddell Sea: an unchartered cornerstone for understanding the polar ocean circulation and its links to global climate. A key issue facing climate scientists will be addressed: “What sets the tridimensional water-mass structure and pathways in the Weddell Sea and modulates the flow of deep waters between the Antarctica ice-shelves and the global ocean circulation?”

To address this question I propose to investigate several key aspects of the Weddell Sea system: the dynamical forcing of the Weddell gyre and its response to atmospheric variability; the forcing and the circulation on the continental shelf and its interaction with the gyre; and the time-scale and mixing associated with bottom water sinking along the continental shelf. WAPITI approaches these objectives through a series of innovations, including (i) an ambitious field experiment to investigate the shelf circulation and processes, (ii) a powerful conceptual framework applied for the first time to a realistic eddy-resolving model of the Weddell gyre, and (iii) a novel instrument that will be developed to directly observe the sinking of deep water into the abyssal ocean for the first time. Collectively, the project will contribute a new insight into global climate feedbacks.","1998125","2015-05-01","2021-04-30"
"WARMCOASTS","Sea level and extreme waves in the Last Interglacial","Alessio ROVERE","UNIVERSITAET BREMEN","Past interglacials are periods of the earth’s history when climate was warmer than the pre-industrial, and are often considered as process-analogs for a future warmer climate. During the Last Interglacial (LIG, ~128-116 ka), polar temperatures were few degrees higher than pre-industrial, ice sheets were smaller and sea level was higher than today. Studies also suggest that waves in the North Atlantic might have been more intense in the LIG than today. Understanding sea level changes and extreme wave intensity during the LIG is key to assess the future of the world’s ice sheets and coastlines under warmer climatic conditions. For this reason, the LIG is the most studied among past interglacials, but recent research highlighted that the LIG is far from a ‘solved problem’, especially for which concerns sea level and coastal dynamics. There are in fact three relevant research gaps. 
First, widely accepted estimates suggest that LIG global mean sea level was 5-10 m higher than today, but recent studies proved that previously unrecognized processes concur to make current LIG sea level estimates very uncertain. Second, it is unclear if LIG sea level was characterized by rapid oscillations that caused sea level to rise abruptly at rates higher than at present (up to 10 mm per year in the LIG, compare with 3 mm per year today). A third research gap is related to the highly controversial notion that the LIG was characterized by ‘superstorms’, producing waves more intense than those observed today. 
In this project, we want to employ a multidisciplinary combination of methods to study Last Interglacial peak sea level, sea level variations and extreme waves. WARMCOASTS will develop both new datasets and merge methods from geology, earth modeling, surface processes modeling and hydrodynamic modeling to advance the current state-of-the-art. The results of this project will be functional to better understand coastal processes under slightly warmer climate conditions.","1499965","2019-04-01","2024-03-31"
"WASCOSYS","Wavefunctions for strongly correlated systems","Norbert Schuch","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Strongly correlated quantum systems, which are at the heart of many open problems in condensed matter,
quantum chemistry, or high-energy physics, are challenging to understand due to their intricate entanglement
structure. Quantum information theory provides the right framework to characterize highly entangled
states and has given rise to the class of Tensor Network States, which capture the entanglement structure of
strongly correlated systems by building the global wavefunction from local tensors and provide an efficient
description of their low-energy states.

In this project, we will develop a framework for the systematic study of strongly correlated systems using
exact wavefunctions based on Tensor Network States. It will give us the tools to construct controlled families
of states by encoding the relevant structure of the system directly into the wavefunction, rather than a
Hamiltonian, and to study their behavior. Since the tensor describing the wavefunction also gives rise to an
associated Hamiltonian, this establishes a framework for building solvable models with the tensor as the
new central object.

The novelty of our approach lies in the fact that quantum information gives us the tools to systematically
construct wavefunctions for general strongly correlated systems, while at the same time, encoding the
structure of the problem directly into the wavefunction results in small families of states with a direct
physical interpretation of the parameters, unlike for fully variational approaches.

We will apply our framework to study the physics of a range of strongly correlated models, in particular
frustrated fermionic and spin systems, in order to understand the possible physics they can exhibit. This
will enhance our understanding of the physics of strongly correlated systems, and, together with numerical
results, experimental findings, and quantum simulations, ultimately lead to new applications and materials
based on strongly correlated matter.","1338500","2015-03-01","2020-02-29"
"WASSR","Water anomalies in the stretched and supercooled regions","Frederic Caupin","UNIVERSITE LYON 1 CLAUDE BERNARD","Water is the most abundant liquid on Earth s surface. It is essential for life. Surprisingly, despite extensive studies, many of its properties remain to be explained. The aim of this project is to contribute to the understanding of water, by studying its anomalies in the metastable liquid state. A liquid should transform into vapour at its boiling point, or into a solid at its freezing point. However, it can be observed beyond these limits. It is then in a metastable state, separated from the stable phase by an energy barrier due to surface tension. The short lifetime of the liquid under such extreme conditions renders measurements particularly challenging. Nevertheless, we find them worth to undertake, because they will bring valuable information on its structure. Our proposal is twofold: in the negative pressure range, where the liquid is metastable with respect to the vapour, we will stretch water to a high degree by several methods, and measure the extension of its equation of state. This part is based on our knowledge of different techniques to obtain large negative pressures; we will use optical methods to access the physical properties. in the supercooled range, where the liquid is metastable with respect to the solid, we will develop new viscometers to measure the viscosity of supercooled liquid water under pressure. Our aim is not only to provide the missing viscosity data for supercooled water up to 300 MPa, but also to check its relation with translational and rotational diffusion, which can reveal a change in the liquid structure.","1335400","2009-10-01","2015-09-30"
"Waterscales","Mathematical and computational foundations for modeling cerebral fluid flow.","Marie Elisabeth ROGNES","SIMULA RESEARCH LABORATORY AS","Your brain has its own waterscape: whether you are reading or sleeping, fluid flows through the brain tissue and clears waste in the process. These physiological processes are crucial for the well-being of the brain. In spite of their importance we understand them but little. Mathematics and numerics could play a crucial role in gaining new insight. Indeed, medical doctors express an urgent need for multiscale modeling of water transport through the brain, to overcome limitations in traditional techniques. Surprisingly little attention has been paid to the numerics of the brain's waterscape however, and fundamental knowledge is missing.

In response, the Waterscales ambition is to establish the mathematical and computational foundations for predictively modeling fluid flow and solute transport through the brain across scales -- from the cellular to the organ level. The project aims to bridge multiscale fluid mechanics and cellular electrophysiology to pioneer new families of mathematical models that couple macroscale, mesoscale and microscale flow with glial cell dynamics. For these models, we will design numerical discretizations that preserve key properties and that allow for whole organ simulations. To evaluate predictability, we will develop a new computational platform for model adaptivity and calibration. The project is multidisciplinary combining mathematics, mechanics, scientific computing, and physiology.

If successful, this project enables the first in silico studies of the brain's waterscape across scales. The new models would open up a new research field within computational neuroscience with ample opportunities for further mathematical and more applied study. The processes at hand are associated with neurodegenerative diseases e.g. dementia and with brain swelling caused by e.g. stroke. The Waterscales project will provide the field with a sorely needed, new avenue of investigation to understand these conditions, with tremendous long-term impact.","1500000","2017-04-01","2022-03-31"
"WAVELENGTH STANDARDS","Development of new wavelength standards for the search
for habitable planets","Ansgar Reiners","GEORG-AUGUST-UNIVERSITAT GOTTINGENSTIFTUNG OFFENTLICHEN RECHTS","""The search for extrasolar planets is moving into the domain of Earth-like, habitable planets with more precise measurement techniques and a focus on low-mass, very cool stars. New spectrographs are being developed with the goal to find Earth-like planets in the habitable zones of stars other than the Sun. Extensive photometric programs, both ground- and space-based are running and being planned to discover transiting planets particularly useful for detailed investigation of exo-planetary atmospheres. For the characterization of exo-planets, and to confirm the planetary status of transit candidates, high-precision radial velocity measurements (m/s) are required, in particular at near-infrared (NIR) wavelengths to discover Earth-like planets around low-mass stars.  The largest obstacle for NIR programs is currently the lack of reliable wavelength standards. Potential calibration sources include absorption cells, gas emission lamps, and Fabry-Perot (FP) etalons. In most cases, detailed NIR characterization is still lacking. A very promising source is the Laser frequency comb (LFC), but the LFC signal paradoxically needs to be degenerated to be useful for astronomical spectrographs because spectral line density is usually too high. It is currently not clear whether the LFC can become a viable calibration scheme, in particular for programs carried out at 4m-class telescopes.  I propose to systematically investigate wavelength calibration sources with a focus on NIR wavelengths and to develop wavelength calibration sources for NIR wavelengths. The development is of fundamental nature providing new strategies for the upcoming generation of NIR high-precision spectrographs. I suggest a new mechanism in which a FP should be coupled to a double-laser PDH mechanism providing reliable wavelength reference. Calibration lamps filled with UNe or CN are promising alternative sources of calibration lines and should be studied in detail.""","1437200","2012-01-01","2016-12-31"
"WBT","Finding order to harness chaos: A new approach to understanding and controlling high Reynolds-number wall-bounded turbulence","Bharathram Ganapathisubramani","UNIVERSITY OF SOUTHAMPTON","The enormous impact and significance of high Reynolds-number wall-bounded turbulence in various applications ranging from transportation and energy generation systems to meteorology and oceanography cannot be understated. However, almost all existing ideas in modelling and controlling wall-bounded turbulence are based on our limited understanding of low Reynolds-number flows. In higher Reynolds-numbers, we simply assume the existence of mutual independence of the large-scales located farther away from the wall from the small-scales near the wall and vice-versa. However, this notion of independence is incorrect. In fact, multiscale interactions between large- and small-scales play a significant role in various turbulent transport processes in practical situations. Consequently, our predictive models and control schemes that cannot account for or take advantage of these interactions have very limited success. Therefore, the central question posed in this research project is: What is the physics of scale interactions at higher Reynolds-numbers and how do we take advantage of it?

The aim is to explore the essence of scale interactions and develop fundamental understanding by performing novel experiments in high Reynolds-number boundary layers. New control methodologies based upon the existence of interactions between large- and small-scales will be devised and applied to reduce skin-friction drag. Additionally, unconventional, yet highly innovative experiments will be devised to ``simulate'' essential aspects of high Reynolds-number scale interactions in a controlled laboratory environment. State-of-the-art laser diagnostics techniques including tomographic PIV and multiple-plane PIV will be performed together with other methods such as hot-wire/laser anemometry to study the physics of scale interactions in these flows. The ultimate goal is to develop new initiatives aimed at predicting and controlling wall-bounded flows in order to meet current and future challenges.","1486500","2012-01-01","2017-12-31"
"WD3D","Evolution of white dwarfs with 3D model atmospheres","Pier-Emmanuel Tremblay","THE UNIVERSITY OF WARWICK","The vast majority of stars will become white dwarfs at the end of the stellar life cycle. These remnants are precise cosmic clocks owing to their well constrained cooling rates. They provide one of the most sensitive tests of when baryonic structure formation began in the Universe. These compact matter laboratories also unravel the mass-loss in the post-main-sequence evolution and establish critical constraints for galactic evolution models. I will design a robust theoretical framework to shed new light on the interior structure of white dwarfs, associate them with their progenitor stars, and enhance their potential as probes of fundamental astrophysical relations. I have recently computed the first 3D simulations of pure-hydrogen white dwarf atmospheres including full radiation-hydrodynamics. These improved calculations demonstrate that the widely used 1D model atmospheres are unable to correctly solve the thermodynamic stratification of convective layers, and therefore lead to incorrect masses and cooling ages. My ambitious goal is to expand the 3D simulations to stellar remnants of all atmospheric compositions and connect these surface calculations to interior structure models. The project is timely since my improved theoretical tools will be essential to analyse the forthcoming Gaia sample, where the number of known white dwarfs is expected to increase by a factor of ten. I will use my theoretical framework with Gaia data, supplemented by other surveys and dedicated follow-up observations, to extract an unprecedented wealth of information from white dwarfs. I will set the standards for the star formation history and initial mass function in the Milky Way, as well as constrain the fundamental mass-radius relation for white dwarfs. I will also study evolved planetary systems that are currently being accreted in the convection zone of their white dwarf hosts, providing direct and unique insight into the bulk composition of exo-terrestrial material.","1454650","2016-06-01","2021-05-31"
"Weakinteract","Weak interactions in self-organizations studied by NMR spectroscopy in the supramolecular solid-state","Antoine Loquet","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Self-assembly is a fundamental process by which individual subunits organize into ordered supramolecular entities, usually through weak interactions. A longstanding goal is to engineer synthetic self-organized structures, often inspired by protein assemblies found in the context of living cells, to design materials of high potentiality, e.g. drug delivery, scaffolding or electronic applications. There is a tremendous interest in physical chemistry to understand the role of weak interactions at the supramolecular interfaces. However, self-organizations usually form soft material, lacking crystalline order and at the same time exhibiting poor solubility. As a consequence, standard techniques for structural investigation such as X-ray crystallography or solution NMR usually fail or deliver only partial information, preventing an atomic-level understanding and therefore the design of new architectures. 
The Weakinteract project aims at developing NMR spectroscopy in the relevant supramolecular solid-state for those non-crystalline and insoluble self-organizations. Weakinteract will exploit strategic isotope labeling, state-of-the-art solid-state NMR methods and integration of hybrid approaches to elucidate the assembly mechanisms, revealing the weak interactions at the supramolecular interfaces. The project comprises three different aspects of growing complexity: (1) Elaboration of a proof-of-concept for atomic resolution structure determination of self-assembled nanotubes in hydrogel form. (2) Determination of the structural basis for bacterial filaments (3) Investigation of the phenomenon of heterogeneous supramolecular templating, in the context of amyloid fold initiation. One major aim of Weakinteract is to provide a robust approach dedicated to chemists, biophysicists and structural biologists in order to tackle weak interactions in the relevant assembled state, ultimately delivering atomic level structures and an understanding of the assembly process.","1472425","2015-09-01","2020-08-31"
"WEEG","""""""Chips on the go"""": towards truly wearable EEG systems""","Esther Olivia Rodriguez-Villegas","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","Epilepsy affects about 50 million people. Routine electroencephalography (EEG) tests are not long enough to properly diagnose these patients. Long time in-patient monitoring is possible but costly for health organizations and removes the patient from his natural environment. Ambulatory EEG is an alternative but existing ambulatory systems are still too bulky and unaesthetic to wear while carrying out normal life. A truly wearable EEG system (WEEG) would: a) have the capability of monitoring for long periods of time; b) not present redundant data sections to the doctor hence reducing the interpretation time; c) be aesthetically discreet for the user; d) be comfortable to wear. Sleep disorders affect approximately 6% of the population. WEEG is still a necessary technology for the diagnosis and continuous monitoring of sleep disorders without the sufferer having to stay in hospital overnight. WEEG systems could also be used for early detection of drowsiness and prevention of road accidents. This project aims to tackle some of the most important technological challenges standing in the way of future electroencephalography (EEG) systems. Specifically, this research will focus on the microelectronic related issues of: a) reducing power to enable long term monitoring; b) reducing the size of EEG systems; c) reducing the amount of specialist time required to interpret the signals. These are key stepping stones for achievement of a truly wearable ambulatory EEG system (WEEG). Simultaneously this will advance knowledge of the practical limitations of different circuit design techniques which will now be used with specifications that largely differ from any other application for which they were used before.","1775713","2010-01-01","2015-12-31"
"WeThaw","Mineral Weathering in Thawing Permafrost: Causes and Consequences","Sophie OPFERGELT","UNIVERSITE CATHOLIQUE DE LOUVAIN","Enhanced thawing of the permafrost in response to warming of the Earth’s high latitude regions exposes previously frozen soil organic carbon (SOC) to microbial decomposition, liberating carbon to the atmosphere and creating a dangerous positive feedback on climate warming. Thawing the permafrost may also unlock a cascade of mineral weathering reactions. These will be accompanied by mineral nutrient release and generation of reactive surfaces which will influence plant growth, microbial SOC degradation and SOC stabilisation. Arguably, weathering is an important but hitherto neglected component for correctly assessing and predicting the permafrost carbon feedback. The goal of WeThaw is to provide the first comprehensive assessment of the mineral weathering response in permafrost regions subject to thawing. By addressing this crucial knowledge gap, WeThaw will significantly augment our capacity to develop models that can accurately predict the permafrost carbon feedback.

Specifically, I will provide the first estimate of the permafrost’s mineral element reservoir which is susceptible to rapidly respond to enhanced thawing, and I will assess the impact of thawing on the soil nutrient storage capacity. To determine the impact of increased mineral weathering on mineral nutrient availability in terrestrial and aquatic ecosystems in permafrost regions, the abiotic and biotic sources and processes controlling their uptake and release will be unraveled by combining novel geochemical techniques, involving the non-traditional silicon, magnesium and lithium stable isotopes, with soil mineral and physico-chemical characterisations. I posit that this groundbreaking approach has the potential to deliver unprecedented insights into mineral weathering dynamics in warming permafrost regions. This frontier research which crosses disciplinary boundaries is a mandatory step for being able to robustly explain the role of mineral weathering in modulating the permafrost carbon feedback.","1999985","2017-09-01","2022-08-31"
"WFNQMC","Development of a Novel Computational Toolbox for Stochastic Electronic Structure in Chemistry and Condensed Matter","George Henry BOOTH","KING'S COLLEGE LONDON","Atomistic computational modelling is increasingly an integral component of almost all areas of chemistry, physics and materials science, especially as its predictive ability improves. However, not all systems are equal when it comes to the predictive capabilities of current methods, and many systems are too large or many-body effects too complex to be routinely tractable. The result is that the promise of ‘materials by design’, deduction of reaction pathways or routine simulation on a par with experimental accuracy has not in general come to fruition. This ambitious proposal aims to address this, with development of a suite of novel approaches to stochastically sample the wavefunction. Recent work by the PI has already made huge strides in this direction, with an emerging approach in quantum chemical space having remarkable success for accurate solutions of small problems. Here, we first propose a number of novel developments to order to extend this approach from a tool for small systems to a widespread disruptive technology, with application to a variety of challenging problems. This involves development of the scope, scaling, accuracy and capabilities of this sampling, including admitting time-dependent, spectral and relativistic extensions. Next, we aim to take the successful philosophy of this sampling and exploit its powerful approach in order to reformulate a number of well-established electronic structure tools. Allowing stochasticity has the potential to yield low-scaling formulations of these methods, able to naturally exploit inherent sparsity, and have a revolutionary impact on their use and success. This proposal is highly interdisciplinary, spanning a range of applications and techniques in quantum chemistry, condensed matter and materials science, brought together under the banner of exploiting inherent sparsity by establishing a new paradigm of stochastic tools for correlated electrons, with the opportunity for tremendous impact in a number of fields.","1500000","2018-02-01","2023-01-31"
"WHIPLASH","WHat next? an Integrated PLanetary Atmosphere Simulator: from Habitable worlds to Hot jupiters","jeremy Jean Maurice Henri Leconte","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Thousands of exoplanets have now been found. In the next decade, the grand challenge is to characterize their atmospheres. This is the only way to unravel the origin of the wild, unexpected diversity we have uncovered. For this task, there are several planned missions—JWST being our next best opportunity. However, to be ready for the analysis and interpretation of such high-precision observations, we need new-generation tools fit to address the multiple challenges they will raise. Indeed, until now, most atmospheric characterization observations—e.g. transit/eclipse spectroscopy—are analyzed with spherically symmetric, steady state 1D models that cannot accurately represent the very anisotropic atmospheres of most transiting exoplanets. This issue is worsened by the ubiquity of clouds, whose inhomogeneous spatial distribution—patchiness—prevents any satisfactory treatment in 1D.
In this project, we will develop a new framework to constrain the physics and composition of exo-atmospheres that will allow us to overcome these difficulties when analyzing and interpreting observations. This will be done by exploiting a new 3D planetary atmosphere simulator that integrates a global climate model and a 3D Monte Carlo radiative transfer code to generate observables. Using such an innovative approach, this ERC project will thus answer the following fundamental questions: 
- What are the necessary conditions to sustain liquid water on terrestrial exoplanets? How can we infer observationally whether an atmosphere meeting these requirements is actually present?
- Can clouds explain the puzzling features of observed hot, gaseous exoplanets? What can these observations tell us on the dynamical and microphysical properties of clouds inside these atmospheres?  
If we want theory to keep pace with the quality of future data, such a project is the necessary counterpart to the huge ongoing observational effort made by the community.","1480421","2016-09-01","2021-08-31"
"WIMPS KAIROS","The Moment of Truth for WIMP Dark Matter","Gianfranco Bertone","UNIVERSITEIT VAN AMSTERDAM","Identifying Dark Matter is a top priority in Particle Physics and Cosmology. Among Dark Matter candidates, WIMPs (weakly interacting massive particles) play a special role, since they naturally arise from well motivated extensions of the standard model of particle physics. As I have argued in a recent \textsl{Nature} paper, with the advent of the Large Hadron Collider at CERN, and of a new generation of astroparticle experiments, the moment of truth has come for WIMPs, for we will either discover them in the next 5 to 10 years, or we will inevitably witness the decline of the WIMP paradigm.
My collaborators and I have actually been preparing for this crucial moment for Dark Matter searches by setting up sophisticated statistical tools, strong connections with experimental collaborations, and an extensive expertise in theoretical models. We are now ready to perform the most complete analysis of Dark Matter data (from direct, indirect and accelerator searches, including all astrophysical uncertainties) in the framework of the most promising BSM (Beyond the Standard Model) theories, including Supersymmetry and Universal Extra Dimensions.
Backed from a well established network of international collaborators, this project aims at becoming part of the theoretical backbone of astroparticle activities in Europe, and to exploit the data that will become available from the LHC at CERN, as well as from several infrastructures included in the ESFRI and ASPERA roadmaps, such as underground Dark Matter detectors and Neutrinos and Cherenkov Telescopes. No matter what the experimental results are, the impact on our understanding of the Universe will be dramatic, for we will either severely constrain possible extensions of the Standard Model, and push them to unnatural territory, or we will finally obtain incontrovertible for Dark Matter, therefore opening a new era in Particle Physics and Cosmology.","1248120","2011-11-01","2016-10-31"
"WINDMIL","Smart Monitoring, Inspection and Life-Cycle Assessment of Wind Turbines","Eleni Chatzi","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","The excessive energy consumption that Europe is faced with, calls for sustainable resource management and policy-making. Amongst renewable sources of the global energy pool, wind energy holds the lead. Nonetheless, wind turbine (WT) facilities are conjoined with a number of shortcomings relating to their short life-span and the lack of efficient management schemes. With a number of WTs currently reaching their design span, stakeholders and policy makers are convinced of the necessity for reliable life-cycle assessment methodologies. However, existing tools have not yet caught up with the maturity of the WT technology, leaving visual inspection and offline non-destructive evaluation methods as the norm.

This proposal aims to establish a smart framework for the monitoring, inspection and life-cycle assessment of WTs, able to guide WT operators in the management of these assets from cradle-to-grave. Our project is founded on a minimal intervention principle, coupling easily deployed and affordable sensor technology with state-of-the-art numerical modeling and data processing tools. An integrated approach is proposed comprising: (i) a new monitoring paradigm for WTs relying on fusion of structural response information, (ii) simulation of influential, yet little explored, factors affecting structural response, such as structure-foundation-soil interaction and fatigue (ii) a stochastic framework for detecting anomalies in both a short- (damage) and long-term (deterioration) scale.

Our end goal is to deliver a “protection-suit” for WTs comprising a hardware (sensor) solution and a modular readily implementable software package, titled ETH-WINDMIL. The suggested kit aims to completely redefine the status quo in current Supervisory Control And Data Acquisition systems. This pursuit is well founded on background work of the PI within the area of structural monitoring, with a focus in translating the value of information into quantifiable terms and engineering practice.","1486224","2016-05-01","2021-04-30"
"WIQOJO","Wideband Quantum Optics with Josephson Junctions","Max Hofheinz","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","Circuit quantum optics (quantum optics with microwave photons in electronic circuits) has allowed to solve several hard problems in traditional quantum optics and to explore new physics. However, so far only one of two regimes of circuit quantum optics has been explored, the circuit quantum electrodynamics regime, where photons reside in electrical resonators.

In this project we want to develop the other regime of circuit quantum optics, the wideband regime, where photons are wave packets propagating along transmission lines. To do so we will build devices based on dynamical Coulomb blockade in Josephson junctions, a phenomenon relating tunneling of Cooper pairs to the emission and absorption of photons. This effect is well understood, but only DC current has been studied so far. We want to employ the photonic aspect of dynamical Coulomb blockade: Engineering the impedance seen by the junction and applying appropriate voltages allows to select specific single- or multi-photon processes that we want to use to build single photon sources, detectors and amplifiers and many other devices. Together they will fully enable wideband circuit quantum optics.

The successful project will also extend the frequency range accessible to circuit quantum optics: Current quantum circuits can be operated only in a limited range around 5 GHz due to engineering constraints. Our approach lifts these constraints and the proposed devices should function in the range from a few GHz up to 1 THz. This extended frequency window will enable the development of hybrid quantum systems coupling quantum circuits to single dopants, molecules, quantum dots or other mesoscopic devices. The output of our project will also be helpful for other domains where radiation in the GHz to THz has to be measured at the single photon level, for example astronomy.","1640587","2012-01-01","2016-12-31"
"WIREDETECT","High resolution X-ray detectors based on nanowire arrays","Jesper WALLENTIN","LUNDS UNIVERSITET","In this project I will develop ultra-high resolution X-ray detectors based on semiconductor nanowires, whose spatial resolution will be radically better than the current state of the art. In X-ray detectors the primary X-ray absorption induces a cascade of secondary electrons and photons which are measured at the front or back of the detector, but during the long transport to the point of detection these can spread orthogonally to the optical axis. This limits the resolution in present bulk detectors.
My novel concept is to create a nanostructured detector based on an array of semiconductor nanowires, which will confine and physically prevent spreading of the secondary electrons and photons. In a nanowire array, the pixel size is the diameter of the nanowire, which can be as low as 10 nm, while the nanowires can be as long as the X-ray absorption length. The very high aspect ratio of nanowires allows detectors with simultaneously very high spatial resolution and sensitivity. I will investigate both direct detectors and scintillators, in which the secondary electrons and photons are detected, respectively. 
The objective is to create detectors based on arrays of 10 nm-diameter nanowires. Time- and temperature resolved measurements will be used to improve understanding of the X-ray physics in these nanodevices, with strong quantum confinement of electrons and phonons and high surface to volume ratio. I will test the detectors within an imaging project targeting the neural connectome, and compare the nanowire detectors with commercial ones. This novel detector concept could revolutionize high-resolution imaging of samples on the nanoscale, maintaining the unique ability of X-rays to study samples in realistic conditions: DNA within live cells, the strained channel in single operational transistors or individual nanoparticles in a charging battery. High resolution detectors could also be employed in X-ray spectroscopy and diffraction.","1498971","2019-02-01","2024-01-31"
"WoCaFi","Unlocking the Entire Wood Matrix for the Next Generation of Carbon Fibers","Michael Hummel","AALTO KORKEAKOULUSAATIO SR","WoCaFi envisions a game-changing approach for the production of bio-based carbon fibers in which the drawbacks of traditional cellulose and lignin fibers are entirely bypassed by a new type of hybrid precursor fibers containing simultaneously all wood biopolymers cellulose, hemicellulose and lignin.
These unique fully wood-based multi-component filaments are accessible via a novel ionic liquid-based dry-jet wet spinning technique. The process provides the possibility to orientate lignin and hemicellulose embedded in a cellulose matrix. The special morphology of the resulting composite filaments is envisioned to increase the mechanical properties of thereof derived carbon fibers significantly, targeting 2000 MPa tensile strength and 200 GPa tensile modulus. These bio-based, low cost carbon fibers will reduce the dependency on non-renewable petroleum-based feedstocks and are highly suitable for lightweight applications in the automotive, sports and leisure sectors.
Most distinctively, our technique also enables us to spin wood almost in its native form. Thus, the pretreatment steps and intensity can be reduced drastically and pronounced synergistic effects between the bio-polymers are created. This will lead to higher carbon yields and a significantly enhanced graphitization. In very recent initial trials on a continuous single tow carbonization line we found indicators that the oxidation step, typically accounting for almost 50% of the carbonization heating energy costs, can be reduced or omitted completely depending on the lignin content of the precursor fiber.
This – in combination with activated wood as low cost raw material – would be the absolute game changer in developing low-cost, bio-based carbon fibers.

In this project the PI, who has developed the spinning technique and a strong background in organic chemistry and spinning physics, will lead a group of 2 PhD students and 1 Postdoc. The Postdoc will complement the team with enhanced spectroscopic knowledge.","1481008","2017-01-01","2021-12-31"
"WU TANG","Selective Conversion of Water and CO2 Using Interfacial Electrochemical Engineering","Wilson SMITH","TECHNISCHE UNIVERSITEIT DELFT","The recycling of CO2 will play an important role in mitigating the energy and environmental problems that our future societies will no doubt face.  Electrochemistry is a powerful technology that can make use of renewable electricity from solar and wind to power the transformation of CO2 and water to valuable chemicals and fuels.  However, the electrochemical conversion of CO2 is not ready for large-scale deployment due to the poor activity, selectivity, and stability of the current catalysts used. The only way to be able to achieve better understanding of this complicated system is through careful characterization of the catalyst/electrolyte interface during electrochemical measurements, as well as the development of new theoretical models that include the effects of the electrolyte.
 In this proposal, I will develop an integrated approach to study the effects of the catalyst and electrolyte compositions on the formation of desired chemical products during electrochemical CO2 reduction.
 To ensure a robust model of the catalyst/electrolyte interface can be established, I will focus on manipulating the catalyst and electrolyte compositions in parallel, while observing the formation of reaction intermediates as a function of applied potential. The proposal will focus on Cu-based electrodes, as Cu has uniquely shown the ability to form hydrocarbon products.  To understand how the product formation changes, operando techniques will be used to monitor the reaction intermediates during electrochemical cycling, to reveal new insights to the reaction pathway for a given product.  A theoretical model will be developed in parallel that focuses on understanding the nature of the electrochemical activity of ions used in this reaction.  Finally, the transport and reactivity of these ions will be evaluated in use with a bipolar membrane, which can effectively separate the electrochemical environments of the CO2 reduction reaction and corresponding water oxidation reaction.","2000000","2017-11-01","2022-10-31"
"X-CITED!","Electronic transitions and bistability:  states, switches, transitions and dynamics studied with high-resolution X-ray spectroscopy","György Albert Vankó","MAGYAR TUDOMANYOS AKADEMIA WIGNER FIZIKAI KUTATOKOZPONT","We propose to study transition metal compounds of uncommon transport properties and excitation characteristics applying emerging high-resolution X-ray spectroscopy. The objective is to determine the microscopic origin of the unconventional behaviour of systems with strong electron correlation through systematic investigations, as well as to reveal bistability conditions and excitation characteristics of switchable molecular systems. The main techniques involved are synchrotron radiation (SR)-based spectroscopies, which can explore the fine details of the electronic structure. Besides using existing end stations of SR facilities, we plan to build a portable spectrometer that can be advantageously used both in a laboratory (e.g., with a radioactive source) and at specially dedicated beamlines of SR facilities, in order to benefit from their specializations in extreme conditions and advanced sample environments, in particular unconventional experiments. This spectrometer should also be able to work in a time-resolved mode so that it could address the dynamics of electronic excitations on the attosecond to nanosecond time scale. The suggested work is expected to push high-resolution X-ray spectroscopies toward maturity, which should open up new horizons in electronic structure and dynamics studies of condensed matter research.","1125960","2010-12-01","2015-11-30"
"X-RAY-BIOIMAGING","X-ray phase-contrast imaging for biomedical applications","Franz Pfeiffer","TECHNISCHE UNIVERSITAET MUENCHEN","In conventional x-ray imaging, contrast is obtained through the differences in the absorption cross-section of the constituents of the object. The technique yields excellent results where highly absorbing structures such as bones are embedded in a matrix of relatively weakly absorbing material, for example the surrounding tissue of the human body. However, in cases where different forms of tissue with similar absorption cross-sections are under investigation (for example, in mammography or neurology), the x-ray absorption contrast is relatively poor. Consequently, differentiating pathologic from non-pathologic tissue in an absorption radiograph obtained with a current hospital-based x-ray system remains practically impossible for certain tissue compositions. The goal of this research project is to overcome these limitations by developing and applying the potential of x-ray phase-contrast imaging for pre-clinical, biomedical x-ray imaging applications. The anticipated results of this project shall provide the scientific basis for future routine exploitation of biomedical x-ray phase contrast imaging through academic research and biomedical imaging device manufacturers. While I envision that the method will ultimately be applicable and beneficiary for several x-ray medical diagnostics applications (i.e., including computer tomography on humans), this project will focus on the first successful implementation of x-ray phase-contrast bioimaging for pre-clinical, small-animal applications.","1994000","2010-01-01","2014-12-31"
"XBCBCAT","From Supramolecular Chemistry to Organocatalysis: Fundamental Studies on the Use of Little-Explored Non-Covalent Interactions in Organic Synthesis","Stefan Matthias Huber","RUHR-UNIVERSITAET BOCHUM","Hydrogen-bonds have found widespread use in various fields of chemistry, including supramolecular chemistry, organic chemistry, and more lately organocatalysis. Although a multitude of structurally different hydrogen-bond donors has been developed, their mode of action is in all cases necessarily based on the same interacting atom, hydrogen. 

In this proposal, we aim to develop first applications for two, previously very little explored non-covalent interactions that are based on electrophilic halogen or chalcogen substituents (“halogen-bonds” and “chalcogen bonds”). 

The first objective is to open the way for the use of chiral multidentate halogen-bond donors (i.e., halogen-based Lewis acids) for enantiodiscrimination. After the synthesis of suitable candidate compounds, we will apply them in the following research areas: a) the resolution of racemic mixtures by co-crystallization with chiral halogen-bond donors, and b) the use of these Lewis acids in enantioselective organocatalysis. 

Within the second objective, we will strive to establish first-of-its-kind applications of chalcogen-based Lewis acids in organic synthesis and organocatalysis. In contrast to halogen-bonds, chalcogen-bonds feature two substituents on the interacting atom as well as two electrophilic axes. In the first phase of this aim, we will synthesize neutral and cationic, mono- and bidentate candidate compounds and determine their association constants with a variety of Lewis bases. Based on this date, we subsequently seek to use these novel Lewis acids as activators or catalysts in organic transformations. 

We anticipate that the realization of these applications, all of which are unprecedented, will be a crucial first step towards establishing further non-covalent interactions as useful tools in chiral recognition and chemical synthesis. In the long-term, we foresee these little-explored interactions becoming powerful complements to the ubiquitous hydrogen-bonds.","1497916","2015-05-01","2020-04-30"
"XCHEM","Photoinduced Chemistry: Development and Application of Computational Methods for New Understanding","Martin James Paterson","HERIOT-WATT UNIVERSITY","The interaction of light and matter gives rise to a multitude of important and fascinating phenomena.
Computational studies of excited states are vital to further our basic understanding of these processes, and
design and optimise new processes for particular applications. However, the computational chemistry of
excited states gives rise to many challenging features, including differential static and dynamic correlation
effects, which can often be difficult to separate. Furthermore, regions of non-adiabatic coupling between
various potential energy surfaces are ubiquitous in photochemistry. Such regions where the Born-
Oppenheimer approximation breaks down are among the most difficult to treat.
The computational chemist must use a wide variety of methods to study photochemistry. However,
one important ‘tool’ in the computational arsenal is currently missing for general photochemical problems:
namely the ability to undertake systematically converging computations over all of the relevant regions of
the various (multi-state) potential energy surfaces. The Monte-Carlo Configuration Interaction (MC-CI)
method is ideal for this purpose, and has many desirable features, including automatic inclusion of strong
static correlation effects, and a balanced treatment of all states. Development of MC-CI methods, including
gradients and non-adiabatic couplings is proposed. This will give rise to the unprecedented ability to
benchmark a large variety of photochemical problems, across the entire potential energy surfaces, with
systematic accuracy. The method will be further extended by coupling within molecular mechanics in a
quantum mechanics / molecular mechanics (QM/MM) framework to study general excited state / open-shell
problems in complex environments.
The work will lead onto the applications research which spans the length scales of chemistry from
small molecules to large supramolecular systems. The above MC-CI method and other state-of-the-art
techniques will be applied to photochemical problems of enormous scientific interest. These include high
accuracy studies of inorganic photochemistry where the computational demands can be greatest, but also
where high-level electronic structure and dynamics simulation offers exceptional possibility to understand
complex molecular photochemistry. A practical area of photochemical research with a huge potential is
photodynamic therapy. Here light is used to destroy cancer tissue via the creation of the highly reactive
singlet molecular oxygen species. A deeper understanding of the many processes involved in this is required.
These include, single- vs multi-photon absorption, sensitizer internal conversion and intersystem crossing,
energy transfer processes with molecular oxygen, solvent effects, and aggregation effects. Detailed and
systematic studies of these fundamental aspects are proposed. The final applied area of study follows
naturally from this and is the supramolecular photochemistry of host-guest molecular sensors. Here advances
are required to allow a detailed understanding. These include the use of molecular dynamics simulation in
conjunction with QM/MM and statistical sampling.","1319728","2010-08-01","2015-07-31"
"XD-STRING","The Structure of the Extra Dimensions of String Theory","Alessandro Tomasiello","UNIVERSITA' DEGLI STUDI DI MILANO-BICOCCA","String theory predicts the existence of several spatial dimensions in addition to the three of our everyday experience. The space spanned by these dimensions might be small enough to have escaped detection so far.
The aim of this project is to characterize which spaces are allowed by the dynamics of the theory, and what physics they give rise to. A few years ago, I discovered a reformulation of supergravity in terms of differential forms, based on the so-called generalized complex geometry. This method was originally limited to string theory vacuum solutions, and over the years it has permitted to find many of them, often with applications to AdS/CFT. Recently, I was able to extend it  to deal with any kind of spacetime dependence; this will allow to probe the choice of extra dimensions more extensively, for example by studying black hole solutions. It will help single out interesting geometries for the extra dimensions, even before one sets out to understand the effective four-dimensional Lagrangian that would result from compactifying string theory on it.
Moreover, I plan to extend the method even further, to deal with controlled supersymmetry breaking. That would open the possibility of producing systematically vacuum solutions which have a positive cosmological constant. The vacua obtained in this way would be fully classical, and under better control than current models.","679200","2012-09-01","2017-08-31"
"XFLOW","Ultrafast X-Ray Tomography of Turbulent Bubble Flows","Markus Schubert","HELMHOLTZ-ZENTRUM DRESDEN-ROSSENDORF EV","Multiphase reactors are omnipresent in chemical engineering and dominate today's manufacturing of chemical products such that they are present in most of our daily products. That implies a huge economic and ecologic impact of the reactor performance. The basic idea of a multiphase reactor is to contact chemical precursors and catalysts in a sufficient time for the reaction to proceed, but reactor performance is crucially affected by the complex reactor hydrodynamics. A proper optimization would imply that multiphase flows are adequately understood.
Gas bubbled into a pool of liquid is the simplest example of a multiphase reactor. Bubble columns or distillation columns, however, house millions of bubbles emerging in swarms with interactions such as coalescence and breakage events that determine the whole process behaviour. The understanding of such disperse gas-liquid flows is still fragmentary and requires a ground-breaking update.
The aim of the project is to apply the worldwide fastest tomographic imaging method to study such turbulent gas-liquid dispersed flows in column reactors such as bubble columns and tray columns. The project intends to provide unique insights into the bubble swarm behaviour at operating conditions that have been hidden so far from the engineer's eyes.
The project is foreseen to enhance the fundamental understanding of hydrodynamic parameters, evolving flow patterns and coherent structures as well as coalescence and breakage mechanisms, regardless of if the systems are pressurized, filled with particle packings, operated with organic liquid, slurries or with internals.
The interdisciplinary team shall re-establish the process intensification route for multiphase reactors by a new understanding of small-scale phenomena, their mathematical description and extrapolation towards the reactor scale and therewith providing a tool for reactor optimization.","1172640","2013-01-01","2016-12-31"
"XSHAPE","Expressive Shape: Intuitive Creative and Optimization of 3D Geometry","Marc Alexa","TECHNISCHE UNIVERSITAT BERLIN","We propose radically new concepts for creating digital and real shapes with the help of computers, considering characteristics of human perception, cognition, and established workflows in art and design. Traditionally, real objects were created and optimized based directly on their visual impression. With the introduction of CAD/CAM, this immediate feedback has been lost, replaced by an engineering pipeline that capitalizes on mathematical representations and accurate machining. We believe to have identified the fundamental problems in this process, and propose research that leads to tools that support creation of shapes by humans and for humans.
The research is concerned with data structures and algorithms that support the optimization of virtual and real shapes so that they possess and clearly convey desired features. This will lead to user interfaces for shape design based on features that humans understand and already use for communication. It will also lead to techniques that optimize the geometry of shapes so that the desired features stand out in likely viewing and illumination conditions. We will further extend the optimization to include the illumination, opening up an entirely new way to create the visual world around us.
While the research is primarily concerned with geometry, it relies on results in perception, cognitive science, mathematics, and other disciplines, and by means of cross-pollination might lead to fruitful insights across the boundaries of computer science. The resulting tools will help making digital shapes a commodity, with effects on markets, industry, and society similar to what we have experienced for digital music or images.","1348000","2010-11-01","2015-12-31"
"XSTREAM","X-ray-waveforms at the Space-Time Resolution Extreme for Atomic-scale Movies","Tenio POPMINTCHEV","TECHNISCHE UNIVERSITAET WIEN","Nonlinear optics revolutionized the ability to create directed, coherent beams particularly in spectral regions where lasers based on conventional population inversion are not practical. New breakthroughs in extreme nonlinear optics promise a similar revolution in the X-ray regime. In a dramatic and unanticipated breakthrough, an international team lead by the PI demonstrated that the high harmonic generation process (HHG) driven by mid-IR lasers can be used to generate keV photons, implementing a >5000 order nonlinear process, while still maintaining the full phase matching that is necessary for good conversion efficiency. This work represents the most extreme, fully coherent upconversion for electromagnetic waves in the 50 year history of nonlinear optics. Moreover, the limits of HHG are still not understood, either theoretically or experimentally. It may be possible to generate coherent hard X-rays using a tabletop-scale apparatus.

In another surprising breakthrough, the PI showed that UV-driven HHG in multiply ionized plasma can be also highly efficient, representing a 2nd route towards the X-ray region. Remarkably, this regime provides X-rays with contrasting spectral and temporal properties. Furthermore, by shaping the polarization of a bi-color mid-IR driving laser the PI, the JILA team in collaboration with Technion, demonstrated robust phase matching of circularly polarized soft X-rays. 

In the proposed work, the fundamental atomic, phase matching plus group velocity matching limits of HHG in the multi-keV X-ray regime will be explored using the 3 most promising, complimentary approaches: 1) mid-IR driven HHG, 2) UV driven HHG, and 3) all-optical quasi phase matching. The knowledge gained as a result of this effort will identify the best path forward for generating bright coherent X-ray beams on a tabletop, at photon energies of 1-10 keV and greater with unprecedented attosecond-to-zeptosecond pulse durations, and arbitrary polarization state.","1513335","2017-08-01","2022-07-31"
"YlideLigands","Tailoring Ylidic Compounds as Ligands for Organometallic Chemistry","Viktoria Daeschlein-Gessner","RUHR-UNIVERSITAET BOCHUM","Lewis bases are a fundamental class of compounds that are of utmost importance in almost any chemical transformation. According to the HSAB concept, they determine important properties such as the stability or solubility of compounds or the selectivity of reactions. Yet, Lewis bases are used far beyond simple acid-base pairs. In coordination chemistry they act as efficient σ-donor ligands, which crucially affect the electronics of the metal and thus its reactivity. Additionally, bulky Lewis bases as part of Frustrated Lewis Pairs are applicable in bond activation reactions and also in catalysis. Typical Lewis bases are neutral compounds with a free pair of electrons, such as amines or phosphines. In contrast, carbon-centred Lewis bases such as carbenes have long been underestimated due to their usually high reactivity and sensitivity. Yet, the last decades have revealed a revolution in this context. Carbenes in particular have proven to be powerful reagents not only as ligands, but also in organocatalysis and bond activation chemistry. Bisylides and their dianionic congeners (methandiides) with formally two electron pairs at carbon are further classes of carbon bases that have started to find applications, but which are still profoundly underdeveloped.  

This project takes aim at the development and application of novel ylidic, carbon-centred Lewis bases. By means of a smart molecular design, systems with unusual electronic properties and donor capacities will be prepared and their reactivity towards main group element compounds and transition metal complexes will be explored. Employing experimental and computational methods a fundamental understanding of the electronic structure and its influencing factors will be provided. This will allow a manipulation and tailoring of the properties and reactivities and thus open applications such as in bond activation reactions or their use as electronically flexible ligands in catalytically active metal complexes.","1500000","2016-07-01","2021-06-30"
"ZoomDeep","Zooming in on the core-mantle boundary","Sanne COTTAAR","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The core-mantle boundary (CMB) is the interface between the liquid iron core and the silicate solid mantle, and is the most significant internal boundary of the Earth. The core and the mantle interact across the boundary through transfer of heat and material, and various coupling mechanisms. While the nature and variability of these interactions remains uncertain, they strongly affect the convection in the mantle, responsible for plate tectonics and intra-plate volcanism, as well as the much more vigorous convection in the core, responsible for the geodynamo. Constraining the interactions at the CMB is crucial to understanding physical processes in the deep Earth and the thermal, compositional, and dynamical evolution of the Earth. 
The CMB interactions are strongly controlled by heterogeneous structures on or near the boundary. On the mantle side, seismological imaging has observed slow velocity layering and patches, but their physical significance remains uncertain, and it is unclear whether they represent global or local features. Turning to the core, suggestions of a stable light-element-enriched layer have been made. The estimated thickness of such a layer varies from 40 to 450 km, and the origin of the inferred light elements is heavily debated. 
In ZoomDeep, I propose innovative seismic techniques to image the structure near the CMB with unprecedented resolution.  One technique, dubbed 'the Frequency Fan', will be newly developed, while another technique has recently been successfully applied at the Earth's surface and will be adapted to the CMB. ZoomDeep will lead to the first high-resolution maps of the structures near the CMB and will specifically focus on the roots of mantle upwellings beneath volcanic hotspots. The implications of these maps on fundamental questions impacting core and mantle dynamics will be assessed in multi-disciplinary approaches. The results of this work will transform our understanding of the dynamics and evolution of the Earth.","1407784","2019-01-01","2023-12-31"
