"project_acronym","project","main_researcher","institution","details_project","budget_project","date_start_project","date_end_project"
"0MSPIN","Spintronics based on relativistic phenomena in systems with zero magnetic moment","Tomáš Jungwirth","FYZIKALNI USTAV AV CR V.V.I","The 0MSPIN project consists of an extensive integrated theoretical, experimental and device development programme of research opening a radical new approach to spintronics. Spintronics has the potential to supersede existing storage and memory applications, and to provide alternatives to current CMOS technology. Ferromagnetic matels used in all current spintronics applications may make it impractical to realise the full potential of spintronics. Metals are unsuitable for transistor and information processing applications, for opto-electronics, or for high-density integration. The 0MSPIN project aims to remove the major road-block holding back the development of spintronics in a radical way: removing the ferromagnetic component from key active parts or from the whole of the spintronic devices. This approach is based on exploiting the combination of exchange and spin-orbit coupling phenomena and material systems with zero macroscopic moment. The goal of the 0MSPIN is to provide a new paradigm by which spintronics can enter the realms of conventional semiconductors in both fundamental condensed matter research and in information technologies. In the central part of the proposal, the research towards this goal is embedded within a materials science project whose aim is to introduce into physics and microelectronics an entirely new class of semiconductors. 0MSPIN seeks to exploit three classes of material systems: (1) Antiferromagnetic bi-metallic 3d-5d alloys (e.g. Mn2Au). (2) Antiferromagnetic I-II-V semiconductors (e.g. LiMnAs). (3) Non-magnetic spin-orbit coupled semiconductors with injected spin-polarized currents (e.g. 2D III-V structures). Proof of concept devices operating at high temperatures will be fabricated to show-case new functionalities offered by zero-moment systems for sensing and memory applications, information processing, and opto-electronics technologies.","1938000","2011-06-01","2016-05-31"
"14Constraint","Radiocarbon constraints for models of C cycling in terrestrial ecosystems: from process understanding to global benchmarking","Susan Trumbore","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","The overall goal of 14Constraint is to enhance the availability and use of radiocarbon data as constraints for process-based understanding of the age distribution of carbon in and respired by soils and ecosystems.  Carbon enters ecosystems by a single process, photosynthesis.  It returns by a range of processes that depend on plant allocation and turnover, the efficiency and rate of litter decomposition and the mechanisms stabilizing C in soils. Thus the age distribution of respired CO2 and the age of C residing in plants, litter and soils are diagnostic properties of ecosystems that provide key constraints for testing carbon cycle models.  Radiocarbon, especially the transit of ‘bomb’ 14C created in the 1960s, is a powerful tool for tracing C exchange on decadal to centennial timescales. 14Constraint will assemble a global database of existing radiocarbon data (WP1) and demonstrate how they can constrain and test ecosystem carbon cycle models.  WP2 will fill data gaps and add new data from sites in key biomes that have ancillary data sufficient to construct belowground C and 14C budgets.  These detailed investigations will focus on the role of time lags caused in necromass and fine roots, as well as the dynamics of deep soil C. Spatial extrapolation beyond the WP2 sites will require sampling along global gradients designed to explore the relative roles of mineralogy, vegetation and climate on the age of C in and respired from soil (WP3).  Products of this 14Constraint will include the first publicly available global synthesis of terrestrial 14C data, and will add over 5000 new measurements.  This project is urgently needed before atmospheric 14C levels decline to below 1950 levels as expected in the next decade.","2283747","2016-12-01","2021-11-30"
"1stProposal","An alternative development of analytic number theory and applications","ANDREW Granville","UNIVERSITY COLLEGE LONDON","The traditional (Riemann) approach to analytic number theory uses the zeros of zeta functions. This requires the associated multiplicative function, say f(n), to have special enough properties that the associated Dirichlet series may be analytically continued.  In this proposal we continue to develop an approach which requires less of the multiplicative function, linking the original question with the mean value of f.  Such techniques have been around for a long time but have generally been regarded as “ad hoc”. In this project we aim to show that one can develop a coherent approach to the whole subject, not only reproving all of the old results, but also many new ones that appear  inaccessible to traditional methods.

Our first goal is to complete a monograph yielding a reworking of all the classical theory using these new methods and then to push forward in new directions.  The most important is to extend these techniques to GL(n) L-functions, which we hope will now be feasible having found the correct framework in which to proceed.  Since we rarely know how to analytically continue such L-functions this could be of great benefit to the subject. 

We are developing the large sieve so that it can be used for individual moduli, and will determine a strong form of that. Also a new method to give asymptotics for mean values, when they are not too small.

We wish to incorporate techniques of analytic number theory into our theory, for example recent advances on mean values of Dirichlet polynomials.  Also the recent breakthroughs on the sieve suggest strong links  that need further exploration.

Additive combinatorics yields important results in many areas. There are strong analogies between its results, and those for multiplicative functions, especially in large value spectrum theory, and its applications. We hope to develop these further.
                   Much of this is joint work with K Soundararajan of Stanford University.","2011742","2015-08-01","2020-07-31"
"2DHIBSA","Nanoscopic and Hierachical Materials via Living Crystallization-Driven Self-Assembly","Ian MANNERS","UNIVERSITY OF BRISTOL","A key synthetic challenge of widespread interest in chemical science involves the creation of well-defined 2D functional materials that exist on a length-scale of nanometers to microns. In this ambitious 5 year proposal we aim to tackle this issue by exploiting the unique opportunities made possible by recent developments with the living crystallization-driven self-assembly (CDSA) platform. Using this solution processing approach, amphiphilic block copolymers (BCPs) with crystallizable blocks, related amphiphiles, and polymers with charged end groups will be used to predictably construct monodisperse samples of tailored, functional soft matter-based 2D nanostructures with controlled shape, size, and spatially-defined chemistries. Many of the resulting nanostructures will also offer unprecedented opportunities as precursors to materials with hierarchical structures through further solution-based “bottom-up” assembly methods. In addition to fundamental studies, the proposed work also aims to make important impact in the cutting-edge fields of liquid crystals, interface stabilization, catalysis, supramolecular polymers, and hierarchical materials.","2499597","2018-05-01","2023-04-30"
"2DNanoSpec","Nanoscale Vibrational Spectroscopy of Sensitive 2D Molecular Materials","Renato ZENOBI","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","I propose to investigate the nanometer scale organization of delicate 2-dimensional molecular materials using nanoscale vibrational spectroscopy. 2D structures are of great scientific and technological importance, for example as novel materials (graphene, MoS2, WS2, etc.), and in the form of biological membranes and synthetic 2D-polymers. Powerful methods for their analysis and imaging with molecular selectivity and sufficient spatial resolution, however, are lacking. Tip-enhanced Raman spectroscopy (TERS) allows label-free spectroscopic identification of molecular species, with ≈10 nm spatial resolution, and with single molecule sensitivity for strong Raman scatterers. So far, however, TERS is not being carried out in liquids, which is the natural environment for membranes, and its application to poor Raman scatterers such as components of 2D polymers, lipids, or other membrane compounds (proteins, sugars) is difficult. TERS has the potential to overcome the restrictions of other optical/spectroscopic methods to study 2D materials, namely (i) insufficient spatial resolution of diffraction-limited optical methods; (ii) the need for labelling for all methods relying on fluorescence; and (iii) the inability of some methods to work in liquids. I propose to address a number of scientific questions associated with the spatial organization, and the occurrence of defects in sensitive 2D molecular materials. The success of these studies will also rely critically on technical innovations of TERS that notably address the problem of energy dissipation. This will for the first time allow its application to study of complex, delicate 2D molecular systems without photochemical damage.","2311696","2017-09-01","2022-08-31"
"2G-CSAFE","Combustion of Sustainable Alternative Fuels for Engines used in aeronautics and automotives","Philippe Dagaut","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","This project aims at promoting sustainable combustion technologies for transport via validation of advanced combustion kinetic models obtained using sophisticated new laboratory experiments, engines, and theoretical computations, breaking through the current frontier of knowledge. It will focus on the unexplored kinetics of ignition and combustion of 2nd generation (2G) biofuels and blends with conventional fuels, which should provide energy safety and sustainability to Europe. The motivation is that no accurate kinetic models are available for the ignition, oxidation and combustion of 2G-biofuels, and improved ignition control is needed for new compression ignition engines. Crucial information is missing: data from well characterised experiments on combustion-generated pollutants and data on key-intermediates for fuels ignition in new engines.
To provide that knowledge new well-instrumented complementary experiments and kinetic modelling will be used. Measurements of key-intermediates, stables species, and pollutants will be performed. New ignition control strategies will be designed, opening new technological horizons. Kinetic modelling will be used for rationalising the results. Due to the complexity of 2G-biofuels and their unusual composition, innovative surrogates will be designed. Kinetic models for surrogate fuels will be generalised for extension to other compounds. The experimental results, together with ab-initio and detailed modelling, will serve to characterise the kinetics of ignition, combustion, and pollutants formation of fuels including 2G biofuels, and provide relevant data and models.
This research is risky because this is (i) the 1st effort to measure radicals by reactor/CRDS coupling, (ii) the 1st effort to use a μ-channel reactor to build ignition databases for conventional and bio-fuels, (iii) the 1st effort to design and use controlled generation and injection of reactive species to control ignition/combustion in compression ignition engines","2498450","2011-12-01","2016-11-30"
"3-TOP","Exploring the physics of 3-dimensional topological insulators","Laurens Wigbolt Molenkamp","JULIUS-MAXIMILIANS-UNIVERSITAT WURZBURG","Topological insulators constitute a novel class of materials where the topological details of the bulk band structure induce a robust surface state on the edges of the material. While transport data for 2-dimensional topological insulators have recently become available, experiments on their 3-dimensional counterparts are mainly limited to photoelectron spectroscopy. At the same time, a plethora of interesting novel physical phenomena have been predicted to occur in such systems.
In this proposal, we sketch an approach to tackle the transport and magnetic properties of the surface states in these materials. This starts with high quality layer growth, using molecular beam epitaxy, of bulk layers of HgTe, Bi2Se3 and Bi2Te3, which are the prime candidates to show the novel physics expected in this field. The existence of the relevant surface states will be assessed spectroscopically, but from there on research will focus on fabricating and characterizing nanostructures designed to elucidate the transport and magnetic properties of the topological surfaces using electrical, optical and scanning probe techniques. Apart from a general characterization of the Dirac band structure of the surface states, research will focus on the predicted magnetic monopole-like response of the system to an electrical test charge. In addition, much effort will be devoted to contacting the surface state with superconducting and magnetic top layers, with the final aim of demonstrating Majorana fermion behavior. As a final benefit, growth of thin high quality thin Bi2Se3 or Bi2Te3 layers could allow for a demonstration of the (2-dimensional) quantum spin Hall effect at room temperature - offering a road map to dissipation-less transport for the semiconductor industry.","2419590","2011-04-01","2016-03-31"
"3D-E","3D Engineered Environments for Regenerative Medicine","Ruth Elizabeth Cameron","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","""This proposal develops a unified, underpinning technology to create novel, complex and biomimetic 3D environments for the control of tissue growth.  As director of Cambridge Centre for Medical Materials, I have recently been approached by medical colleagues to help to solve important problems in the separate therapeutic areas of breast cancer, cardiac disease and blood disorders.  In each case, the solution lies in complex 3D engineered environments for cell culture.  These colleagues make it clear that existing 3D scaffolds fail to provide the required complex orientational and spatial anisotropy, and are limited in their ability to impart appropriate biochemical and mechanical cues.

I have a strong track record in this area.  A particular success has been the use of a freeze drying technology to make collagen based porous implants for the cartilage-bone interface in the knee, which has now been commercialised.  The novelty of this proposal lies in the broadening of the established scientific base of this technology to enable biomacromolecular structures with:
(A) controlled and complex pore orientation to mimic many normal multi-oriented tissue structures
(B) compositional and positional control to match varying local biochemical environments,
(C) the attachment of novel peptides designed to control cell behaviour, and
(D) mechanical control at both a local and macroscopic level to provide mechanical cues for cells.
These will be complemented by the development of
(E) robust characterisation methodologies for the structures created.
These advances will then be employed in each of the medical areas above.

This approach is highly interdisciplinary.  Existing working relationships with experts in each medical field will guarantee expertise and licensed facilities in the required biological disciplines. Funds for this proposal would therefore establish a rich hub of mutually beneficial research and opportunities for cross-disciplinary sharing of expertise.""","2486267","2013-04-01","2018-03-31"
"3DIMAGE","3D Imaging Across Lengthscales: From Atoms to Grains","Paul Anthony Midgley","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","""Understanding structure-property relationships across lengthscales is key to the design of functional and structural materials and devices. Moreover, the complexity of modern devices extends to three dimensions and as such 3D characterization is required across those lengthscales to provide a complete understanding and enable improvement in the material’s physical and chemical behaviour. 3D imaging and analysis from the atomic scale through to granular microstructure is proposed through the development of electron tomography using (S)TEM, and ‘dual beam’ SEM-FIB, techniques offering complementary approaches to 3D imaging across lengthscales stretching over 5 orders of magnitude.

We propose to extend tomography to include novel methods to determine atom positions in 3D with approaches incorporating new reconstruction algorithms, image processing  and complementary nano-diffraction techniques. At the nanoscale, true 3D nano-metrology of morphology and composition is a key objective of the project, minimizing reconstruction and visualization artefacts. Mapping strain and optical properties in 3D are ambitious and exciting challenges that will yield new information at the nanoscale. Using the SEM-FIB, 3D ‘mesoscale’ structures will be revealed: morphology, crystallography and composition can be mapped simultaneously, with ~5nm resolution and over volumes too large to tackle by (S)TEM and too small for most x-ray techniques. In parallel, we will apply 3D imaging to a wide variety of key materials including heterogeneous catalysts, aerospace alloys, biomaterials, photovoltaic materials, and novel semiconductors.

We will collaborate with many departments in Cambridge and institutes worldwide. The personnel on the proposal will cover all aspects of the tomography proposed using high-end TEMs, including an aberration-corrected Titan, and a Helios dual beam. Importantly, a postdoc is dedicated to developing new algorithms for reconstruction, image and spectral processing.""","2337330","2012-01-01","2017-12-31"
"3DNANOMECH","Three-dimensional molecular resolution mapping of soft matter-liquid interfaces","Ricardo Garcia","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","Optical, electron and probe microscopes are enabling tools for discoveries and knowledge generation in nanoscale sicence and technology. High resolution –nanoscale or molecular-, noninvasive and label-free imaging of three-dimensional soft matter-liquid interfaces has not been achieved by any microscopy method.
Force microscopy (AFM) is considered the second most relevant advance in materials science since 1960. Despite its impressive range of applications, the technique has some key limitations. Force microscopy has not three dimensional  depth. What lies above or in the subsurface is not readily characterized.

3DNanoMech proposes to design, build and operate a high speed force-based method for the three-dimensional characterization soft matter-liquid interfaces (3D AFM).  The microscope will combine a detection method based on force perturbations, adaptive algorithms,  high speed piezo actuators and quantitative-oriented multifrequency approaches. The development of the microscope cannot be separated from its applications:  imaging the error-free DNA repair and to understand the relationship existing between the nanomechanical properties and the malignancy of cancer cells. Those problems encompass the different spatial –molecular-nano-mesoscopic- and time –milli to seconds- scales of the instrument.

In short, 3DNanoMech aims to image, map and measure with picoNewton, millisecond and angstrom resolution soft matter surfaces and interfaces in liquid. The long-term vision of 3DNanoMech is to replace models or computer animations  of bimolecular-liquid interfaces by real time, molecular resolution maps of properties and processes.","2499928","2014-02-01","2019-01-31"
"3SPIN","Three Dimensional Spintronics","Russell Paul Cowburn","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Spintronics, in which both the spin and the charge of the electron are used, is one of the most exciting new disciplines to emerge from nanoscience. The 3SPIN project seeks to open a new research front within spintronics: namely 3-dimensional spintronics, in which magnetic nanostructures are formed into a 3-dimensional interacting network of unrivalled density and hence technological benefit. 3SPIN will explore early-stage science that could underpin 3-dimensional metallic spintronics. The thesis of the project is: that by careful control of the constituent nanostructure properties, a 3-dimensional medium can be created in which a large number of topological solitons can exist. Although hardly studied at all to date, these solitons should be stable at room temperature, extremely compact and easy to manipulate and propagate. This makes them potentially ideal candidates to form the basis of a new spintronics in which the soliton is the basic transport vector instead of electrical current. ¬3.5M of funding is requested to form a new team of 5 researchers who, over a period of 60 months, will perform computer simulations and experimental studies of solitons in 3-dimensional networks of magnetic nanostructures and develop a laboratory demonstrator 3-dimensional memory device using solitons to represent and store data. A high performance electron beam lithography system (cost 1M¬) will be purchased to allow state-of-the-art magnetic nanostructures to be fabricated with perfect control over their magnetic properties, thus allowing the ideal conditions for solitons to be created and controllably manipulated. Outputs from the project will be a complete understanding of the properties of these new objects and a road map charting the next steps for research in the field.","2799996","2010-03-01","2016-02-29"
"4-TOPS","Four experiments in Topological Superconductivity.","Laurens Molenkamp","JULIUS-MAXIMILIANS-UNIVERSITAT WURZBURG","Topological materials have developed rapidly in recent years, with my previous ERC-AG project 3-TOP playing a major role in this development. While so far no bulk topological superconductor has been unambiguously demonstrated, their properties can be studied in a very flexible manner by inducing superconductivity through the proximity effect into the surface or edge states of a topological insulator. In 4-TOPS we will explore the possibilities of this approach in full, and conduct a thorough study of induced superconductivity in both two and three dimensional HgTe based topological insulators. The 4 avenues we will follow are:

-SQUID based devices to investigate full phase dependent spectroscopy of the gapless Andreev bound state by studying their Josephson radiation and current-phase relationships.
-Experiments aimed at providing unambiguous proof of localized Majorana states in TI junctions by studying tunnelling transport into such states.  
-Attempts to induce superconductivity in Quantum Hall states with the aim of creating a chiral topological superconductor. These chiral superconductors host Majorana fermions at their edges, which, at least in the case of a single QH edge mode, follow non-Abelian statistics and are therefore promising for explorations in topological quantum computing.
-Studies of induced superconductivity in Weyl semimetals, a completely unexplored state of matter.

Taken together, these four sets of experiments will greatly enhance our understanding of topological superconductivity, which is not only a subject of great academic interest as it constitutes the study of new phases of matter, but also has potential application in the field of quantum information processing.","2497567","2017-06-01","2022-05-31"
"4D IMAGING","Towards 4D Imaging of Fundamental Processes on the Atomic and Sub-Atomic Scale","Ferenc Krausz","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","State-of-the-art microscopy and diffraction imaging provides insight into the atomic and sub-atomic structure of matter. They permit determination of the positions of atoms in a crystal lattice or in a molecule as well as the distribution of electrons inside atoms. State-of-the-art time-resolved spectroscopy with femtosecond and attosecond resolution provides access to dynamic changes in the atomic and electronic structure of matter. Our proposal aims at combining these two frontier techniques of XXI century science to make a long-standing dream of scientist come true: the direct observation of atoms and electrons in their natural state: in motion. Shifts in the atoms positions by tens to hundreds of picometers can make chemical bonds break apart or newly form, changing the structure and/or chemical composition of matter. Electronic motion on similar scales may result in the emission of light, or the initiation of processes that lead to a change in physical or chemical properties, or biological function. These motions happen within femtoseconds and attoseconds, respectively. To make them observable, we need a 4-dimensional (4D) imaging technique capable of recording freeze-frame snapshots of microscopic systems with picometer spatial resolution and femtosecond to attosecond exposure time. The motion can then be visualized by slow-motion replay of the freeze-frame shots. The goal of this project is to develop a 4D imaging technique that will ultimately offer picometer resolution is space and attosecond resolution in time.","2500000","2010-03-01","2015-02-28"
"4D-EEG","4D-EEG: A new tool to investigate the spatial and temporal activity patterns in the brain","Franciscus C.T. Van Der Helm","TECHNISCHE UNIVERSITEIT DELFT","Our first goal is to develop a new tool to determine brain activity with a high temporal (< 1 msec) and spatial (about 2 mm) resolution with the focus on motor control. High density EEG (up to 256 electrodes) will be used for EEG source localization. Advanced force-controlled robot manipulators will be used to impose continuous force perturbations to the joints. Advanced closed-loop system identification algorithms will identify the dynamic EEG response of multiple brain areas to the perturbation, leading to a functional interpretation of EEG. The propagation of the signal in time and 3D space through the cortex can be monitored: 4D-EEG. Preliminary experiments with EEG localization have shown that the continuous force perturbations resulted in a better signal-to-noise ratio and coherence than the current method using transient perturbations..
4D-EEG will be a direct measure of the neural activity in the brain with an excellent temporal response and easy to use in combination with motor control tasks. The new 4D-EEG method is expected to provide a breakthrough in comparison to functional MRI (fMRI) when elucidating the meaning of cortical map plasticity in motor learning.
Our second goal is to generate and validate new hypotheses about the longitudinal relationship between motor learning and cortical map plasticity by clinically using 4D-EEG in an intensive, repeated measurement design in patients suffering from a stroke. The application of 4D-EEG combined with haptic robots will allow us to discover how dynamics in cortical map plasticity are related with upper limb recovery after stroke in terms of neural repair and using behavioral compensation strategies while performing a meaningful motor tasks.. The non-invasive 4D-EEG technique combined with haptic robots will open the window about what and how patients (re)learn when showing motor recovery after stroke in order to allow us to develop more effective patient-tailored therapies in neuro-rehabilitation.","3477202","2012-06-01","2017-05-31"
"4DBIOSERS","Four-Dimensional Monitoring of Tumour Growth by Surface Enhanced Raman Scattering","Luis LIZ-MARZAN","ASOCIACION CENTRO DE INVESTIGACION COOPERATIVA EN BIOMATERIALES- CIC biomaGUNE","Optical bioimaging is limited by visible light penetration depth and stability of fluorescent dyes over extended periods of time. Surface enhanced Raman scattering (SERS) offers the possibility to overcome these drawbacks, through SERS-encoded nanoparticle tags, which can be excited with near-IR light (within the biological transparency window), providing high intensity, stable, multiplexed signals. SERS can also be used to monitor relevant bioanalytes within cells and tissues, during the development of diseases, such as tumours. In 4DBIOSERS we shall combine both capabilities of SERS, to go well beyond the current state of the art, by building three-dimensional scaffolds that support tissue (tumour) growth within a controlled environment, so that not only the fate of each (SERS-labelled) cell within the tumour can be monitored in real time (thus adding a fourth dimension to SERS bioimaging), but also recording the release of tumour metabolites and other indicators of cellular activity. Although 4DBIOSERS can be applied to a variety of diseases, we shall focus on cancer, melanoma and breast cancer in particular, as these are readily accessible by optical methods. We aim at acquiring a better understanding of tumour growth and dynamics, while avoiding animal experimentation. 3D printing will be used to generate hybrid scaffolds where tumour and healthy cells will be co-incubated to simulate a more realistic environment, thus going well beyond the potential of 2D cell cultures. Each cell type will be encoded with ultra-bright SERS tags, so that real-time monitoring can be achieved by confocal SERS microscopy. Tumour development will be correlated with simultaneous detection of various cancer biomarkers, during standard conditions and upon addition of selected drugs. The scope of 4DBIOSERS is multidisciplinary, as it involves the design of high-end nanocomposites, development of 3D cell culture models and optimization of emerging SERS tomography methods.","2410771","2018-10-01","2023-09-30"
"4PI-SKY","4 pi sky: Extreme Astrophysics with Revolutionary Radio Telescopes","Robert Philip Fender","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Extreme astrophysical events   such as relativistic flows, cataclysmic explosions and black hole accretion   are one of the key areas for astrophysics in the 21st century. The extremes of physics experienced in these environments are beyond anything achievable in any laboratory on Earth, and provide a unique glimpse at the laws of physics operating in extraordinary regimes. All of these events are associated with transient radio emission, a tracer both of the acceleration of particles to relativistic energies, and coherent emitting regions with huge effective temperatures. By studying radio bursts from these phenomena we can pinpoint the sources of explosive events, understand the budget of kinetic feedback by explosive events in the ambient medium, and probe the physical state of the universe back to the epoch of reionisation, less than a billion years after the big bang. In seeking to push back the frontiers of extreme astrophysics, I will use a trio of revolutionary new radio telescopes, LOFAR, ASKAP and MeerKAT, pathfinders for the Square Kilometre Array, and all facilities in which I have a major role in the search for transients. I will build an infrastructure which transforms their combined operations for the discovery, classification and reporting of transient astrophysical events, over the whole sky, making them much more than the sum of their parts. This will include development of environments for the coordinated handling of extreme astrophysical events, in real time, via automated systems, as well as novel techniques for the detection of these events in a sea of noise. I will furthermore augment this program by buying in as a major partner to a rapid-response robotic optical telescope, and by cementing my relationship with an orbiting X-ray facility. This multiwavelength dimension will secure the astrophysical interpretation of our observational results and help to revolutionise high-energy astrophysics via a strong scientific exploitation program.","2999847","2011-07-01","2017-06-30"
"A-DATADRIVE-B","Advanced Data-Driven Black-box modelling","Johan Adelia K Suykens","KATHOLIEKE UNIVERSITEIT LEUVEN","Making accurate predictions is a crucial factor in many systems (such as in modelling energy consumption, power load forecasting, traffic networks, process industry, environmental modelling, biomedicine, brain-machine interfaces) for cost savings, efficiency, health, safety and organizational purposes.  In this proposal we aim at realizing a new generation of more advanced black-box modelling techniques for estimating predictive models from measured data. We will study different optimization modelling frameworks in order to obtain improved black-box modelling approaches. This will be done by specifying models through constrained optimization problems by studying different candidate core models (parametric models, support vector machines and kernel methods) together with additional sets of constraints and regularization mechanisms. Different candidate mathematical frameworks will be considered with models that possess primal and (Lagrange) dual model representations, functional analysis in reproducing kernel Hilbert spaces, operator splitting and optimization in Banach spaces. Several aspects that are relevant to black-box models will be studied including incorporation of prior knowledge, structured dynamical systems, tensorial data representations, interpretability and sparsity, and general purpose optimization algorithms. The methods should be suitable for handling larger data sets and high dimensional input spaces. The final goal is also to realize a next generation software tool (including symbolic generation of models and handling different supervised and unsupervised learning tasks, static and dynamic systems) that can be generically applied to data from different application areas. The proposal A-DATADRIVE-B aims at getting end-users connected to the more advanced methods through a user-friendly data-driven black-box modelling tool. The methods and tool will be tested in connection to several real-life applications.","2485800","2012-04-01","2017-03-31"
"A2C2","Atmospheric flow Analogues and Climate Change","Pascal Yiou","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","""The A2C2 project treats two major challenges in climate and atmospheric research: the time dependence of the climate attractor to external forcings (solar, volcanic eruptions and anthropogenic), and the attribution of extreme climate events occurring in the northern extra-tropics. The main difficulties are the limited climate information, the computer cost of model simulations, and mathematical assumptions that are hardly verified and often overlooked in the literature.
A2C2 proposes a practical framework to overcome those three difficulties, linking the theory of dynamical systems and statistics. We will generalize the methodology of flow analogues to multiple databases in order to obtain probabilistic descriptions of analogue decompositions.
The project is divided into three workpackages (WP). WP1 embeds the analogue method in the theory of dynamical systems in order to provide a metric of an attractor deformation in time. The important methodological step is to detect trends or persisting outliers in the dates and scores of analogues when the system yields time-varying forcings. This is done from idealized models and full size climate models in which the forcings (anthropogenic and natural) are known.
A2C2 creates an open source toolkit to compute flow analogues from a wide array of databases (WP2). WP3 treats the two scientific challenges with the analogue method and multiple model ensembles, hence allowing uncertainty estimates under realistic mathematical hypotheses. The flow analogue methodology allows a systematic and quasi real-time analysis of extreme events, which is currently out of the reach of conventional climate modeling approaches.
The major breakthrough of A2C2 is to bridge the gap between operational needs (the immediate analysis of climate events) and the understanding long-term climate changes. A2C2 opens new research horizons for the exploitation of ensembles of simulations and reliable estimates of uncertainty.""","1491457","2014-03-01","2019-02-28"
"A2F2","Beyond Biopolymers: Protein-Sized Aromatic Amide Functional Foldamers","Ivan Huc","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","Nature has evolved ultimate chemical functions based on controlling and altering conformation of its molecular machinery. Prominent examples include enzyme catalysis and information storage/duplication in nucleic acids. These achievements are based on large and complex yet remarkably defined structures obtained through folding of polymeric chains and a subtle interplay of non-covalent forces. Nature uses a limited set of building blocks – e.g. twenty amino-acids and four nucleobases – with specific abilities to impart well-defined folds. In the last decade, chemists have discovered foldamers: non-natural oligomers and polymers also prone to adopt folded structures. The emergence of foldamers has far reaching implications. A new major long term prospect is open to chemistry: the de novo synthesis of artificial objects resembling biopolymers in terms of their size, complexity, and efficiency at achieving defined functions, yet having chemical structures beyond the reach of biopolymers amenable to new properties and functions. The PI of this project has shown internationally recognized leadership in the development of a class of foldamers, aromatic oligoamides, whose features arguably make them the most suitable candidates to systematically explore what folded structures beyond biopolymers give access to. This project aims at developing methods to allow the routine fabrication of 20-40 units long aromatic oligoamide foldamers (6-15 kDa) designed to fold into artificial molecular containers having engineerable cavities and surfaces for molecular recognition of organic substrates, in particular large peptides and saccharides, polymers, and proteins. The methodology rests on modelling based design, multistep organic synthesis of heterocyclic monomers and their assembly into long sequences, structural elucidation using, among other techniques, x-ray crystallography, and the physico-chemical characterization of molecular recognition events.","2496216","2013-06-01","2018-05-31"
"AAMOT","Arithmetic of automorphic motives","Michael Harris","INSTITUT DES HAUTES ETUDES SCIENTIFIQUES","The primary purpose of this project is to build on recent spectacular progress in the Langlands program to study the arithmetic properties of automorphic motives constructed in the cohomology of Shimura varieties. Because automorphic methods are available to study the L-functions of these motives, which include elliptic curves and certain families of Calabi-Yau varieties over totally real fields (possibly after base change), they represent the most accessible class of varieties for which one can hope to verify fundamental conjectures on special values of L-functions, including Deligne's conjecture and the Main Conjecture of Iwasawa theory. Immediate goals include the proof of irreducibility of automorphic Galois representations; the establishment of period relations for automorphic and potentially automorphic realizations of motives in the cohomology of distinct Shimura varieties; the construction of p-adic L-functions for these and related motives, notably adjoint and tensor product L-functions in p-adic families; and the geometrization of the p-adic and mod p Langlands program. All four goals, as well as the others mentioned in the body of the proposal, are interconnected; the final goal provides a bridge to related work in geometric representation theory, algebraic geometry, and mathematical physics.","1491348","2012-06-01","2018-05-31"
"AARTFAAC","Amsterdam-ASTRON Radio Transient Facility And Analysis Centre: Probing the Extremes of Astrophysics","Ralph Antoine Marie Joseph Wijers","UNIVERSITEIT VAN AMSTERDAM","Some of the most extreme tests of physical law come from its manifestations in the behaviour of black holes and neutron stars, and as such these objects should be used as fundamental physics labs. Due to advances in both theoretical work and observational techniques, I have a major opportunity now to significantly push this agenda forward and get better answers to questions like: How are black holes born? How can energy be extracted from black holes? What is the origin of magnetic fields and cosmic rays in jets and shocks? Is their primary energy stream hadronic or magnetic? I propose to do this by exploiting the advent of wide-field radio astronomy: extreme objects are very rare and usually transient, so not only must one survey large areas of sky, but also must one do this often. I propose to form and shape a group that will use the LOFAR wide-field radio telescope to hunt for these extreme transients and systematically collect enough well-documented examples of the behaviour of each type of transient. Furthermore, I propose to expand LOFAR with a true 24/7 all-sky monitor to catch and study even the rarest of events. Next, I will use my experience in gamma-ray burst followup to conduct a vigorous multi-wavelength programme of study of these objects, to constrain their physics from as many angles as possible. This will eventually include results from multi-messenger astrophysics, in which we use neutrinos, gravity waves, and other non-electromagnetic messengers as extra diagnostics of the physics of these sources. Finally, I will build on my experience in modelling accretion phenomena and relativistic explosions to develop a theoretical framework for these phenomena and constrain the resulting models with the rich data sets we obtain.","3499128","2010-10-01","2016-09-30"
"ABACUS","Ab-initio adiabatic-connection curves for density-functional analysis and construction","Trygve Ulf Helgaker","UNIVERSITETET I OSLO","Quantum chemistry provides two approaches to molecular electronic-structure calculations: the systematically refinable but expensive many-body wave-function methods and the inexpensive but not systematically refinable Kohn Sham method of density-functional theory (DFT). The accuracy of Kohn Sham calculations is determined by the quality of the exchange correlation functional, from which the effects of exchange and correlation among the electrons are extracted using the density rather than the wave function. However, the exact exchange correlation functional is unknown—instead, many approximate forms have been developed, by fitting to experimental data or by satisfying exact relations. Here, a new approach to density-functional analysis and construction is proposed: the Lieb variation principle, usually regarded as conceptually important but impracticable. By invoking the Lieb principle, it becomes possible to approach the development of approximate functionals in a novel manner, being directly guided by the behaviour of exact functional, accurately calculated for a wide variety of chemical systems. In particular, this principle will be used to calculate ab-initio adiabatic connection curves, studying the exchange correlation functional for a fixed density as the electronic interactions are turned on from zero to one. Pilot calculations have indicated the feasibility of this approach in simple cases—here, a comprehensive set of adiabatic-connection curves will be generated and utilized for calibration, construction, and analysis of density functionals, the objective being to produce improved functionals for Kohn Sham calculations by modelling or fitting such curves. The ABACUS approach will be particularly important in cases where little experimental information is available—for example, for understanding and modelling the behaviour of the exchange correlation functional in electromagnetic fields.","2017932","2011-03-01","2016-02-29"
"ACB","The Analytic Conformal Bootstrap","Luis Fernando ALDAY","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The aim of the present proposal is to establish a research team developing and exploiting innovative techniques to study conformal field theories (CFT) analytically. Our approach does not rely on a Lagrangian description but on symmetries and consistency conditions. As such it applies to any CFT, offering a unified framework to study generic CFTs analytically. The initial implementation of this program has already led to striking new results and insights for both Lagrangian and non-Lagrangian CFTs.

The overarching aims of my team will be: To develop an analytic bootstrap program for CFTs in general dimensions; to complement these techniques with more traditional methods and develop a systematic machinery to obtain analytic results for generic CFTs; and to use these results to gain new insights into the mathematical structure of the space of quantum field theories. 

The proposal will bring together researchers from different areas. The objectives in brief are:

1) Develop an alternative to Feynman diagram computations for Lagrangian CFTs.
2) Develop a machinery to compute loops for QFT on AdS, with and without gravity.
3) Develop an analytic approach to non-perturbative N=4 SYM and other CFTs. 
4) Determine the space of all CFTs.
5) Gain new insights into the mathematical structure of the space of quantum field theories. 

The outputs of this proposal will include a new way of doing perturbative computations based on symmetries; a constructive derivation of the AdS/CFT duality; new analytic techniques to attack strongly coupled systems and invaluable new lessons about the space of CFTs and QFTs.
Success in this research will lead to a completely new, unified way to view and solve CFTs, with a huge impact on several branches of physics and mathematics.","2171483","2018-12-01","2023-11-30"
"ACCELERATES","Acceleration in Extreme Shocks: from the microphysics to laboratory and astrophysics scenarios","Luis Miguel De Oliveira E Silva","INSTITUTO SUPERIOR TECNICO","What is the origin of cosmic rays, what are the dominant acceleration mechanisms in relativistic shocks, how do cosmic rays self-consistently influence the shock dynamics, how are relativistic collisionless shocks formed are longstanding scientific questions, closely tied to extreme plasma physics processes, and where a close interplay between the micro-instabilities and the global dynamics is critical.
Relativistic shocks are closely connected with the propagation of intense streams of particles pervasive in many astrophysical scenarios. The possibility of exciting shocks in the laboratory will also be available very soon with multi-PW lasers or intense relativistic particle beams.
Computational modeling is now established as a prominent research tool, by enabling the fully kinetic modeling of these systems for the first time. With the fast paced developments in high performance computing, the time is ripe for a focused research programme on simulation-based studies of relativistic shocks. This proposal therefore focuses on using self-consistent ab initio massively parallel simulations to study the physics of relativistic shocks, bridging the gap between the multidimensional microphysics of shock onset, formation, and propagation and the global system dynamics. Particular focus will be given to the shock acceleration mechanisms and the radiation signatures of the various physical processes, with the goal of solving some of the central questions in plasma/relativistic phenomena in astrophysics and in the laboratory, and opening new avenues between theoretical/massive computational studies, laboratory experiments and astrophysical observations.","1588800","2011-06-01","2016-07-31"
"ACCI","Atmospheric Chemistry-Climate Interactions","John Adrian Pyle","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Global change involves a large number of complex interactions between various earth system processes. In the atmosphere, one component of the earth system, there are crucial feedbacks between physical, chemical and biological processes. Thus many of the drivers of climate change depend on chemical processes in the atmosphere including, in addition to ozone and water vapour, methane, nitrous oxide, the halocarbons as well as a range of inorganic and organic aerosols. The link between chemistry and climate is two-way and changes in climate can influence atmospheric chemistry processes in a variety of ways.
Previous studies have looked at these interactions in isolation but the time is now right for more comprehensive studies. The crucial contribution that will be made here is in improving our understanding of the processes within this complex system. Process understanding has been the hallmark of my previous work.  The earth system scope here will be ambitiously wide but with a similar drive to understand fundamental processes.
The ambitious programme of research is built around four interrelated questions using new state-of-the-art modelling tools: How will the composition of the stratosphere change in the future, given changes in the concentrations of ozone depleting substances and greenhouse gases? How will these changes in the stratosphere affect tropospheric composition and climate? How will the composition of the troposphere change in the future, given changes in the emissions of ozone precursors and greenhouse gases? How will these changes in the troposphere affect the troposphere-stratosphere climate system?
ACCI will break new ground in bringing all of these questions into a single modelling and diagnostic framework, enabling interrelated questions to be answered which should radically improve our overall projections for global change.","2496926","2011-05-01","2017-04-30"
"ACCLIMATE","Elucidating the Causes and Effects of Atlantic Circulation Changes through Model-Data Integration","Claire Waelbroeck","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Rapid changes in ocean circulation and climate have been observed in marine sediment and ice cores, notably over the last 60 thousand years (ky), highlighting the non-linear character of the climate system and underlining the possibility of rapid climate shifts in response to anthropogenic greenhouse gas forcing.
To date, these rapid changes in climate and ocean circulation are still not fully explained. Two main obstacles prevent going beyond the current state of knowledge:
- Paleoclimatic proxy data are by essence only indirect indicators of the climatic variables, and thus can not be directly compared with model outputs;
- A 4-D (latitude, longitude, water depth, time) reconstruction of Atlantic water masses over the past 40 ky is lacking: previous studies have generated isolated records with disparate timescales which do not allow the causes of circulation changes to be identified.
Overcoming these two major limitations will lead to major breakthroughs in climate research. Concretely, I will create the first database of Atlantic deep-sea records over the last 40 ky, and extract full climatic information from these records through an innovative model-data integration scheme using an isotopic proxy forward modeling approach. The novelty and exceptional potential of this scheme is twofold: (i) it avoids hypotheses on proxy interpretation and hence suppresses or strongly reduces the errors of interpretation of paleoclimatic records; (ii) it produces states of the climate system that best explain the observations over the last 40 ky, while being consistent with the model physics.
Expected results include:
• The elucidation of the mechanisms explaining rapid changes in ocean circulation and climate over the last 40 ky,
• Improved climate model physics and parameterizations,
• The first projections of future climate changes obtained with a model able to reproduce the highly non linear behavior of the climate system observed over the last 40 ky.","3000000","2014-02-01","2019-01-31"
"ACCOPT","ACelerated COnvex OPTimization","Yurii NESTEROV","UNIVERSITE CATHOLIQUE DE LOUVAIN","The amazing rate of progress in the computer technologies and telecommunications presents many new challenges for Optimization Theory. New problems are usually very big in size, very special in structure and possibly have a distributed data support. This makes them unsolvable by the standard optimization methods. In these situations, old theoretical models, based on the hidden Black-Box information, cannot work. New theoretical and algorithmic solutions are urgently needed. In this project we will concentrate on development of fast optimization methods for problems of big and very big size. All the new methods will be endowed with provable efficiency guarantees for large classes of optimization problems, arising in practical applications. Our main tool is the acceleration technique developed for the standard Black-Box methods as applied to smooth convex functions. However, we will have to adapt it to deal with different situations. 
The first line of development will be based on the smoothing technique as applied to a non-smooth functions. We propose to substantially extend this approach to generate approximate solutions in relative scale. The second line of research will be related to applying acceleration techniques to the second-order methods minimizing functions with sparse Hessians. Finally, we aim to develop fast gradient methods for huge-scale problems. The size of these problems is so big that even the usual vector operations are extremely expensive. Thus, we propose to develop new methods with sublinear iteration costs. In our approach, the main source for achieving improvements will be the proper use of problem structure.
Our overall aim is to be able to solve in a routine way many important problems, which currently look unsolvable. Moreover, the theoretical development of Convex Optimization will reach the state, when there is no gap between theory and practice: the theoretically most efficient methods will definitely outperform any homebred heuristics.","2090038","2018-09-01","2023-08-31"
"ACCRETE","Accretion and Early Differentiation of the Earth and Terrestrial Planets","David Crowhurst Rubie","UNIVERSITAET BAYREUTH","Formation of the Earth and the other terrestrial planets of our Solar System (Mercury, Venus and Mars) commenced 4.568 billion years ago and occurred on a time scale of about 100 million years. These planets grew by the process of accretion, which involved numerous collisions with smaller (Moon- to Mars-size) bodies. Impacts with such bodies released sufficient energy to cause large-scale melting and the formation of deep “magma oceans”. Such magma oceans enabled liquid metal to separate from liquid silicate, sink and accumulate to form the metallic cores of the planets. Thus core formation in terrestrial planets was a multistage process, intimately related to the major impacts during accretion, that determined the chemistry of planetary mantles. However, until now, accretion, as modelled by astrophysicists, and core formation, as modelled by geochemists, have been treated as completely independent processes. The fundamental and crucial aim of this ambitious interdisciplinary proposal is to integrate astrophysical models of planetary accretion with geochemical models of planetary differentiation together with cosmochemical constraints obtained from meteorites. The research will involve integrating new models of planetary accretion with core formation models based on the partitioning of a large number of elements between liquid metal and liquid silicate that we will determine experimentally at pressures up to about 100 gigapascals (equivalent to 2400 km deep in the Earth). By comparing our results with the known physical and chemical characteristics of the terrestrial planets, we will obtain a comprehensive understanding of how these planets formed, grew and evolved, both physically and chemically, with time. The integration of chemistry and planetary differentiation with accretion models is a new ground-breaking concept that will lead, through synergies and feedback, to major new advances in the Earth and planetary sciences.","1826200","2012-05-01","2018-04-30"
"ACRCC","Understanding the atmospheric circulation response to climate change","Theodore Shepherd","THE UNIVERSITY OF READING","Computer models based on known physical laws are our primary tool for predicting climate change. Yet the state-of-the-art models exhibit a disturbingly wide range of predictions of future climate change, especially when examined at the regional scale, which has not decreased as the models have become more comprehensive. The reasons for this are not understood. This represents a basic challenge to our fundamental understanding of climate.

The divergence of model projections is presumably related to systematic model errors in the large-scale fluxes of heat, moisture and momentum that control regional aspects of climate. That these errors stubbornly persist in spite of increases in the spatial resolution of the models suggests that they are associated with errors in the representation of unresolved processes, whose effects must be parameterised.

Most attention in climate science has hitherto focused on the thermodynamic aspects of climate. Dynamical aspects, which involve the atmospheric circulation, have received much less attention. However regional climate, including persistent climate regimes and extremes, is strongly controlled by atmospheric circulation patterns, which exhibit chaotic variability and whose representation in climate models depends sensitively on parameterised processes. Moreover the dynamical aspects of model projections are much less robust than the thermodynamic ones. There are good reasons to believe that model bias, the divergence of model projections, and chaotic variability are somehow related, although the relationships are not well understood. This calls for studying them together.

My proposed research will focus on this problem, addressing these three aspects of the atmospheric circulation response to climate change in parallel: (i) diagnosing the sources of model error; (ii) elucidating the relationship between model error and the spread in model projections; (iii) understanding the physical mechanisms of atmospheric variability.","2489151","2014-03-01","2020-02-29"
"ACROSS","3D Reconstruction and Modeling across Different Levels of Abstraction","Leif Kobbelt","RHEINISCH-WESTFAELISCHE TECHNISCHE HOCHSCHULE AACHEN","""Digital 3D models are gaining more and more importance in diverse application fields ranging from computer graphics, multimedia and simulation sciences to engineering, architecture, and medicine. Powerful technologies to digitize the 3D shape of real objects and scenes are becoming available even to consumers.  However, the raw geometric data emerging from, e.g., 3D scanning or multi-view stereo often lacks a consistent structure and meta-information which are necessary for the effective deployment of such models in sophisticated down-stream applications like animation, simulation, or CAD/CAM that go beyond mere visualization. Our goal is to develop new fundamental algorithms which transform raw geometric input data into augmented 3D models that are equipped with structural meta information such as feature aligned meshes, patch segmentations, local and global geometric constraints, statistical shape variation data, or even procedural descriptions. Our methodological approach is inspired by the human perceptual system that integrates bottom-up (data-driven) and top-down (model-driven) mechanisms in its hierarchical processing. Similarly we combine algorithms operating on different levels of abstraction into reconstruction and modeling networks. Instead of developing an individual solution for each specific application scenario, we create an eco-system of algorithms for automatic processing and interactive design of highly complex 3D models. A key concept is the information flow across all levels of abstraction in a bottom-up as well as top-down fashion. We not only aim at optimizing geometric representations but in fact at bridging the gap between reconstruction and recognition of geometric objects. The results from this project will make it possible to bring 3D models of real world objects into many highly relevant applications in science, industry, and entertainment, greatly reducing the excessive manual effort that is still necessary today.""","2482000","2014-03-01","2019-02-28"
"Actanthrope","Computational Foundations of Anthropomorphic Action","Jean Paul Laumond","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Actanthrope intends to promote a neuro-robotics perspective to explore original models of anthropomorphic action. The project targets contributions to humanoid robot autonomy (for rescue and service robotics), to advanced human body simulation (for applications in ergonomics), and to a new theory of embodied intelligence (by promoting a motion-based semiotics of the human action).

Actions take place in the physical space while they originate in the –robot or human– sensory-motor space. Geometry is the core abstraction that makes the link between these spaces. Considering that the structure of actions inherits from that of the body, the underlying intuition is that actions can be segmented within discrete sub-spaces lying in the entire continuous posture space. Such sub-spaces are viewed as symbols bridging deliberative reasoning and reactive control. Actanthrope argues that geometric approaches to motion segmentation and generation as promising and innovative routes to explore embodied intelligence:
- Motion segmentation: what are the sub-manifolds that define the structure of a given action?
- Motion generation: among all the solution paths within a given sub-manifold, what is the underlying law that makes the selection?
In Robotics these questions are related to the competition between abstract symbol manipulation and physical signal processing. In Computational Neuroscience the questions refer to the quest of motion invariants. The ambition of the project is to promote a dual perspective: exploring the computational foundations of human action to make better robots, while simultaneously doing better robotics to better understand human action.

A unique “Anthropomorphic Action Factory” supports the methodology. It aims at attracting to a single lab, researchers with complementary know-how and solid mathematical background. All of them will benefit from unique equipments, while being stimulated by four challenges dealing with locomotion and manipulation actions.","2500000","2014-01-01","2018-12-31"
"ADDECCO","Adaptive Schemes for Deterministic and Stochastic Flow Problems","Remi Abgrall","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","The numerical simulation of complex compressible flow problem is still a challenge nowaday even for simple models. In our opinion, the most important hard points that need currently to be tackled and solved is  how to obtain stable, scalable, very  accurate, easy to code and to maintain schemes on complex geometries. The method should easily handle  mesh refinement, even near the boundary where the most interesting engineering quantities have to be evaluated. Unsteady  uncertainties  in the model, for example in  the geometry or  the boundary conditions should represented  efficiently.This proposal goal is to  design, develop and evaluate solutions to each of the above problems. Our work program  will lead to significant breakthroughs for flow simulations. More specifically, we propose to work on 3 connected problems: 1-A class of very high order numerical schemes   able to easily deal with the geometry of boundaries and still can solve steep problems. The geometry is generally defined by CAD tools. The output  is used to generate a mesh which  is then used by the scheme. Hence, any mesh refinement process is disconnected from the CAD, a situation  that prevents the spread of mesh adaptation techniques in industry! 2-A class of very high order numerical schemes  which  can utilize possibly solution dependant basis functions in order to lower the number of degrees of freedom, for example to compute accurately  boundary layers with low resolutions.  3-A general non intrusive technique for handling uncertainties in order to deal with irregular probability density functions (pdf)  and also  to handle pdf that may evolve in time, for example thanks to an optimisation loop. The curse of dimensionality will be dealt thanks  Harten's multiresolution method combined with sparse grid methods. Currently, and up to our knowledge,  no scheme  has each of these properties. This research program will have an impact on numerical schemes and industrial applications.","1432769","2008-12-01","2013-11-30"
"ADEQUATE","Advanced optoelectronic Devices with Enhanced QUAntum efficiency at THz frEquencies","Carlo Sirtori","UNIVERSITE PARIS DIDEROT - PARIS 7","The aim of this project is the realisation of efficient mid-infrared and THz optoelectronic emitters. This work is motivated by the fact that the spontaneous emission in this frequency range is characterized by an extremely long lifetime when compared to non-radiative processes, giving rise to devices with very low quantum efficiency. To this end we want to develop hybrid light-matter systems, already well known in quantum optics, within optoelectronics devices, that will be driven by electrical injection. With this project we want to extend the field of optoelectronics by introducing some of the concepts of quantum optic, particularly the light-matter strong coupling, into semiconductor devices. More precisely this project aims at the implementation of novel optoelectronic emitters operating in the strong coupling regime between an intersubband excitation of a two-dimensional electron gas and a microcavity photonic mode. The quasiparticles issued from this coupling are called intersubband polaritons. The major difficulties and challenges of this project, do not lay in the observation of these quantum effects, but in their exploitation for a specific function, in particular an efficient electrical to optical conversion. To obtain efficient quantum emitters in the THz frequency range we will follow two different approaches: - In the first case we will try to exploit the additional characteristic time of the system introduced by the light-matter interaction in the strong (or ultra-strong) coupling regime. - The second approach will exploit the fact that, under certain conditions, intersubband polaritons have a bosonic character; as a consequence they can undergo stimulated scattering, giving rise to polaritons lasers as it has been shown for excitonic polaritons.","1761000","2010-05-01","2015-04-30"
"ADMIRE","Atomic-scale Design of Majorana states and their Innovative Real-space Exploration","Roland WIESENDANGER","UNIVERSITAET HAMBURG","Fault-tolerant topological quantum computation has become one of the most exciting research directions in modern condensed matter physics. As a key operation the braiding of non-Abelian anyons has been proposed theoretically. Such exotic quasiparticles can be realized as zero-energy Majorana bound states at the ends of  one-dimensional magnetic nanowires in proximity to s-wave superconductors in the presence of high spin-orbit coupling. In contrast to previous attempts to realize such systems experimentally, based on the growth of semiconducting nanowires or the self-assembly of ferromagnetic nanowires on s-wave superconductors, we propose to design Majorana bound states in artificially constructed single-atom chains with non-collinear spin-textures on elemental superconducting substrates using scanning tunnelling microscope (STM)-based atom manipulation techniques. We would like to study at the atomic level the formation of Shiba bands as a result of hybridization of individual Shiba impurity states as well as the emergence of zero-energy Majorana bound states as a function of chain structure, length, and composition. Moreover, we will construct model-type platforms, such as T-junctions, rings, and more complex network structures with atomic-scale precision as a basis for demonstrating the manipulation and braiding of Majorana bound states. We will make use of sophisticated experimental techniques, such as spin-resolved scanning tunnelling spectroscopy (STS) at micro-eV energy resolution, scanning Josephson tunnelling spectroscopy, and multi-probe STS under well-defined ultra-high vacuum conditions, in order to directly probe the nature of the magnetic state of the atomic wires, the spin-polarization of the emergent Majorana states, as well as the spatial nature of the superconducting order parameter in real space. Finally, we will try to directly probe the quantum exchange statistics of non-Abelian anyons in these atomically precise fabricated model-type systems.","2499750","2019-01-01","2023-12-31"
"AdOMiS","Adaptive Optical Microscopy Systems: Unifying theory, practice and applications","Martin BOOTH","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Recent technological advances in optical microscopy have vastly broadened the possibilities for applications in the biomedical sciences. Fluorescence microscopy is the central tool for investigation of molecular structures and dynamics that take place in the cellular and tissue environment. Coupled with progress in labeling methods, these microscopes permit observation of biological structures and processes with unprecedented sensitivity and resolution. This work has been enabled by the engineering development of diverse optical systems that provide different capabilities for the imaging toolkit. All such methods rely upon high fidelity optics to provide optimal resolution and efficiency, but they all suffer from aberrations caused by refractive index variations within the specimen. It is widely accepted that in many applications this fundamental problem prevents optimum operation and limits capability. Adaptive optics (AO) has been introduced to overcome these limitations by correcting aberrations and a range of demonstrations has shown clearly its potential. Indeed, it shows great promise to improve virtually all types of research or commercial microscopes, but significant challenges must still be met before AO can be widely implemented in routine imaging. Current advances are being made through development of bespoke AO solutions to individual imaging tasks. However, the diversity of microscopy methods means that individual solutions are often not translatable to other systems. This proposal is directed towards the creation of theoretical and practical frameworks that tie together AO concepts and provide a suite of scientific tools with broad application. This will be achieved through a systems approach that encompasses theoretical modelling, optical engineering and the requirements of biological applications. Additional outputs will include practical designs, operating protocols and software algorithms that will support next generation AO microscope systems.","3234789","2016-09-01","2021-08-31"
"ADOR","Assembly-disassembly-organisation-reassembly of microporous materials","Russell MORRIS","THE UNIVERSITY COURT OF THE UNIVERSITY OF ST ANDREWS","Microporous materials are an important class of solid; the two main members of this family are zeolites and metal-organic frameworks (MOFs). Zeolites are industrial solids whose applications range from catalysis, through ion exchange and adsorption technologies to medicine. MOFs are some of the most exciting new materials to have been developed over the last two decades, and they are just beginning to be applied commercially. 

Over recent years the applicant’s group has developed new synthetic strategies to prepare microporous materials, called the Assembly-Disassembly-Organisation-Reassembly (ADOR) process. In significant preliminary work the ADOR process has shown to be an extremely important new synthetic methodology that differs fundamentally from traditional solvothermal methods. 

In this project I will look to overturn the conventional thinking in materials science by developing methodologies that can target both zeolites and MOF materials that are difficult to prepare using traditional methods – the so-called ‘unfeasible’ materials. The importance of such a new methodology is that it will open up routes to materials that have different properties (both chemical and topological) to those we currently have. Since zeolites and MOFs have so many actual and potential uses, the preparation of materials with different properties has a high chance of leading to new technologies in the medium/long term. To complete the major objective I will look to complete four closely linked activities covering the development of design strategies for zeolites and MOFs (activities 1 & 2), mechanistic studies to understand the process at the molecular level using in situ characterisation techniques (activity 3) and an exploration of potential applied science for the prepared materials (activity 4).","2489220","2018-10-01","2023-09-30"
"ADORA","Asymptotic approach to spatial and dynamical organizations","Benoit PERTHAME","SORBONNE UNIVERSITE","The understanding of spatial, social and dynamical organization of large numbers of agents is presently a fundamental issue in modern science. ADORA focuses on problems motivated by biology because, more than anywhere else, access to precise and many data has opened the route to novel and complex biomathematical models. The problems we address are written in terms of  nonlinear partial differential equations. The flux-limited Keller-Segel system, the integrate-and-fire Fokker-Planck equation, kinetic equations with internal state, nonlocal parabolic equations and constrained Hamilton-Jacobi equations are among examples of  the equations under investigation. 

The role of mathematics is not only to understand the analytical structure of these new problems, but it is also to explain the qualitative behavior of solutions and to quantify their properties. The challenge arises here because these  goals should be achieved  through a hierarchy of scales. Indeed, the problems under consideration share the common feature that the large scale behavior cannot be understood precisely without access to a hierarchy of finer scales, down to the individual behavior and sometimes its molecular determinants. 

Major difficulties arise because the numerous scales present in these equations have to be discovered and  singularities appear in the asymptotic process which yields deep compactness obstructions. Our vision is that the complexity inherent to models of biology can be enlightened by mathematical analysis and a classification of the possible asymptotic regimes. 

However an enormous effort is needed to uncover the equations intimate mathematical structures, and bring them at the level of conceptual understanding they deserve being given the applications motivating these questions which range from medical science or neuroscience to cell biology.","2192500","2017-09-01","2022-08-31"
"AdS-CFT-solvable","Origins of integrability in AdS/CFT correspondence","Vladimir Kazakov","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Fundamental interactions in nature are well described by quantum gauge fields in 4 space-time dimensions (4d). When the strength of gauge interaction is weak the Feynman perturbation techniques are very efficient for the description of most of the experimentally observable consequences of the Standard model and for the study of high energy processes in QCD.
But in the intermediate and strong coupling regime, such as the relatively small energies in QCD, the perturbation theory fails leaving us with no reliable analytic methods (except the Monte-Carlo simulation). The project aims at working out new analytic and computational methods for strongly coupled gauge theories in 4d. We will employ for that two important discoveries: 1) the gauge-string duality (AdS/CFT correspondence) relating certain strongly coupled gauge Conformal Field
Theories to the weakly coupled string theories on Anty-deSitter space; 2) the solvability, or integrability of maximally supersymmetric (N=4) 4d super Yang-Mills (SYM) theory in multicolor limit. Integrability made possible pioneering exact numerical and analytic results in the N=4 multicolor SYM at any coupling, effectively summing up all 4d Feynman diagrams. Recently, we conjectured a system of functional equations - the AdS/CFT Y-system – for the exact spectrum of anomalous dimensions of all local operators in N=4 SYM. The conjecture has passed all available checks. My project is aimed at the understanding of origins of this, still mysterious integrability. Deriving the AdS/CFT Y-system from the first principles on both sides of gauge-string duality should provide a long-awaited proof of the AdS/CFT correspondence itself. I plan to use the Y-system to study the systematic weak and strong coupling expansions and the so called BFKL limit, as well as for calculation of multi-point correlation functions of N=4 SYM. We hope on new insights into the strong coupling dynamics of less supersymmetric gauge theories and of QCD.","1456140","2013-11-01","2018-10-31"
"ADSNeSP","Active and Driven Systems: Nonequilibrium Statistical Physics","Michael Elmhirst CATES","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Active Matter systems, such as self-propelled colloids, violate time-reversal symmetry by producing entropy locally, typically converting fuel into mechanical motion at the particle scale. Other driven systems instead produce entropy because of global forcing by external fields, or boundary conditions that impose macroscopic fluxes (such as the momentum flux across a fluid sheared between moving parallel walls). 

Nonequilibrium statistical physics (NeSP) is the basic toolbox for both classes of system. In recent years, much progress in NeSP has stemmed from bottom-up work on driven systems. This has provided a number of exactly solved benchmark models, and extended approximation techniques to address driven non-ergodic systems, such as sheared glasses. Meanwhile, work on fluctuation theorems and stochastic thermodynamics have created profound, model-independent insights into dynamics far from equilibrium. 

More recently, the field of Active Matter has moved forward rapidly, leaving in its wake a series of generic and profound NeSP questions that now need answers:  When is time-reversal symmetry, broken at the microscale, restored by coarse-graining? If it is restored, is an effective thermodynamic description is possible? How different is an active system's behaviour from a globally forced one? 

ADSNeSP aims to distil from recent Active Matter research such fundamental questions; answer them first in the context of specific models and second in more general terms; and then, using the tools and insights gained, shed new light on longstanding problems in the wider class of driven systems.

I believe these new tools and insights will be substantial, because local activity takes systems far from equilibrium in a conceptually distinct direction from most types of global driving. By focusing on general principles and on simple models of activity, I seek to create a new vantage point that can inform, and potentially transform, wider areas of statistical physics.","2043630","2017-10-01","2022-09-30"
"AEDNA","Amorphous and Evolutionary DNA Nanotechnology","Friedrich SIMMEL","TECHNISCHE UNIVERSITAET MUENCHEN","Amorphous and evolutionary DNA nanotechnology (AEDNA) explores novel conceptual directions and applications for DNA nanotechnology, which are based on intelligent, DNA-programmed soft hybrid materials, and the utilization of evolutionary principles for the optimization of nucleic acid nanocomponents.
Amorphous DNA nanotechnology first aims at the creation of cell-sized, DNA-programmed microgels – DNA cells – with sensor, computation, communication, and actuator functions. Interacting DNA cells will be arranged into chemical cell consortia and artificial tissues using microfluidics, micromanipulation and 3D bioprinting techniques. Spatially distributed chemical circuits will then be utilized to establish collective behaviors such as quorum sensing, pattern formation, and self-differentiation within these consortia and tissues. The approach will be further scaled up to produce multicomponent DNA gel compositions that become active and differentiate upon mixing. 
In evolutionary nanotechnology, techniques derived from directed molecular evolution experiments will be applied to optimize the arrangement of functional nucleic acids on DNA and RNA nanoscaffolds. Compartmentalization and microfluidics will be utilized to screen for nucleic acid nanostructures capable of superstructure formation, and also for the development of ligand-sensitive components for molecular programming. An evolutionary approach will then be applied to amorphous DNA cells, resulting in DNA cell populations which contain individuals with different molecular identities. 
The proposal will pave the way for the creation of macroscopic materials with DNA-programmed intelligence, resulting in novel applications for DNA nanotechnology and molecular programming in diverse fields such as environmental and biological sensing, biocatalysis, smart adaptive materials, and soft robotics.","2157698","2016-06-01","2021-05-31"
"AEROCAT","Non-ordered nanoparticle superstructures – aerogels as efficient (electro-)catalysts","Alexander Eychmüller","TECHNISCHE UNIVERSITAET DRESDEN","""AEROCAT aims at the elucidation of the potential of nanoparticle derived aerogels in catalytic applications. The materials will be produced from a variety of nanoparticles available in colloidal solutions, amongst which are metals and metal oxides. The evolving aerogels are extremely light, highly porous solids and have been demonstrated to exhibit in many cases the important properties of the nanosized objects they consist of instead of simply those of the respective bulk solids. The resulting aerogel materials will be characterized with respect to their morphology and composition and their resulting (electro-)catalytic properties examined in the light of the inherent electronic nature of the nanosized constituents. Using the knowledge gained within the project the aerogel materials will be further re-processed in order to exploit their full potential relevant to catalysis and electrocatalysis.
From the vast variety of possible applications of nanoparticle-based hydro- and aerogels like thermoelectrics, LEDs, pollutant clearance, sensorics and others we choose our strictly focused approach
(i) due to the paramount importance of catalysis for the Chemical Industry,
(ii) because we have successfully studied the Ethanol electrooxidation on a Pd-nanoparticle aerogel,
(iii) we have patented on the oxygen reduction reaction in fuel cells with bimetallic aerogels,
(iv) and we gained first and extremely promising results on the semi-hydrogenation of Acetylene on a mixed Pd/ZnO-nanoparticle aerogel.
With this we are on the forefront of a research field which impact might not be overestimated. We should quickly explore its potentials and transfer on a short track the knowledge gained into pre-industrial testing.""","2194000","2014-02-01","2019-01-31"
"AFMIDMOA","""Applying Fundamental Mathematics in Discrete Mathematics, Optimization, and Algorithmics""","Alexander Schrijver","UNIVERSITEIT VAN AMSTERDAM","""This proposal aims at strengthening the connections between more fundamentally oriented areas of mathematics like algebra, geometry, analysis, and topology, and the more applied oriented and more recently emerging disciplines of discrete mathematics, optimization, and algorithmics.

The overall goal of the project is to obtain, with methods from fundamental mathematics, new effective tools to unravel the complexity of structures like graphs, networks, codes, knots, polynomials, and tensors, and to get a grip on such complex structures by new efficient characterizations, sharper bounds, and faster algorithms.

In the last few years, there have been several new developments where methods from representation theory, invariant theory, algebraic geometry, measure theory, functional analysis, and topology found new applications in discrete mathematics and optimization, both theoretically and algorithmically. Among the typical application areas are networks, coding, routing, timetabling, statistical and quantum physics, and computer science.
The project focuses in particular on:

A. Understanding partition functions with invariant theory and algebraic geometry
B. Graph limits, regularity, Hilbert spaces, and low rank approximation of polynomials
C. Reducing complexity in optimization by exploiting symmetry with representation theory
D. Reducing complexity in discrete optimization by homotopy and cohomology

These research modules are interconnected by themes like symmetry, regularity, and complexity, and by common methods from algebra, analysis, geometry, and topology.""","2001598","2014-01-01","2018-12-31"
"AFRICA-GHG","AFRICA-GHG: The role of African tropical forests on the Greenhouse Gases balance of the atmosphere","Riccardo Valentini","FONDAZIONE CENTRO EURO-MEDITERRANEOSUI CAMBIAMENTI CLIMATICI","The role of the African continent in the global carbon cycle, and therefore in climate change, is increasingly recognised. Despite the increasingly acknowledged importance of Africa in the global carbon cycle and its high vulnerability to climate change there is still a lack of studies on the carbon cycle in representative African ecosystems (in particular tropical forests), and on the effects of climate on ecosystem-atmosphere exchange. In the present proposal we want to focus on these spoecifc objectives : 1. Understand the role of African tropical rainforest on the GHG balance of the atmosphere and revise their role on the global methane and N2O emissions. 2. Determine the carbon source/sink strength of African tropical rainforest in the pre-industrial versus the XXth century by temporal reconstruction of biomass growth with biogeochemical markers 3. Understand and quantify carbon and GHG fluxes variability across African tropical forests (west east equatorial belt) 4.Analyse the impact of forest degradation and deforestation on carbon and other GHG emissions","2406950","2010-04-01","2014-12-31"
"AFTERTHEGOLDRUSH","Addressing global sustainability challenges by changing perceptions in catalyst design","Graham John Hutchings","CARDIFF UNIVERSITY","One of the greatest challenges facing society is the sustainability of resources.  At present, a step change in the sustainable use of resources is needed and catalysis lies at the heart of the solution by providing new routes to carbon dioxide mitigation, energy security and water conservation. It is clear that new high efficiency game-changing catalysts are required to meet the challenge. This proposal will focus on excellence in catalyst design by learning from recent step change advances in gold catalysis by challenging perceptions. Intense interest in gold catalysts over the past two decades has accelerated our understanding of gold particle-size effects, gold-support and gold-metal interactions, the interchange between atomic and ionic gold species, and the role of the gold-support interface in creating and maintaining catalytic activity.  The field has also driven the development of cutting-edge techniques, particularly in microscopy and transient kinetics, providing detailed structural characterisation on the nano-scale and probing the short-range and often short-lived interactions.  By comparison, our understanding of other metal catalysts has remained relatively static.

The proposed programme will engender a step change in the design of supported-metal catalysts, by exploiting the learning and the techniques emerging from gold catalysis.  The research will be set out in two themes. In Theme 1 two established key grand challenges will be attacked; namely, energy vectors and greenhouse gas control.  Theme 2 will address two new and emerging grand challenges in catalysis namely the effective low temperature activation of primary carbon hydrogen bonds and CO2 utilisation where instead of treating CO2 as a thermodynamic endpoint, the aim will be to re-use it as a feedstock for bulk chemical and fuel production. The legacy of the research will be the development of a new catalyst design approach that will provide a tool box for future catalyst development.","2279785","2012-04-01","2017-03-31"
"AGNOSTIC","Actively Enhanced Cognition based Framework for Design of Complex Systems","Björn Ottersten","UNIVERSITE DU LUXEMBOURG","Parameterized mathematical models have been central to the understanding and design of communication, networking, and radar systems. However, they often lack the ability to model intricate interactions innate in complex systems. On the other hand, data-driven approaches do not need explicit mathematical models for data generation and have a wider applicability at the cost of flexibility. These approaches need labelled data, representing all the facets of the system interaction with the environment. With the aforementioned systems becoming increasingly complex with intricate interactions and operating in dynamic environments, the number of system configurations can be rather large leading to paucity of labelled data. Thus there are emerging networks of systems of critical importance whose cognition is not effectively covered by traditional approaches. AGNOSTIC uses the process of exploration through system probing and exploitation of observed data in an iterative manner drawing upon traditional model-based approaches and data-driven discriminative learning to enhance functionality, performance, and robustness through the notion of active cognition. AGNOSTIC clearly departs from a passive assimilation of data and aims to formalize the exploitation/exploration framework in dynamic environments. The development of this framework in three applications areas is central to AGNOSTIC. The project aims to provide active cognition in radar to learn the environment and other active systems to ensure situational awareness and coexistence; to apply active probing in radio access networks to infer network behaviour towards spectrum sharing and self-configuration; and to learn and adapt to user demand for content distribution in caching networks, drastically improving network efficiency. Although these cognitive systems interact with the environment in very different ways, sufficient abstraction allows cross-fertilization of insights and approaches motivating their joint treatment.","2499595","2017-10-01","2022-09-30"
"AIRSEA","Air-Sea Exchanges driven by Light","Christian George","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The scientific motivation of this project is the significant presence of organic compounds at the surface of the ocean. They form the link between ocean biogeochemistry through the physico-chemical processes near the water-air interface with primary and secondary aerosol formation and evolution in the air aloft and finally to the climate impact of marine boundary layer aerosols. However, their photochemistry and photosensitizer properties have only been suggested and discussed but never fully addressed because they were beyond reach. This project suggests going significantly beyond this matter of fact by a combination of innovative tools and the development of new ideas.
This project is therefore devoted to new laboratory investigations of processes occurring at the air sea interface to predict emission, formation and evolution of halogenated radicals and aerosols from this vast interface between oceans and atmosphere. It progresses from fundamental laboratory measurements, marine science, surface chemistry, photochemistry … and is therefore interdisciplinary in nature.
It will lead to the development of innovative techniques for characterising chemical processing at the air sea interface (e.g., a multiphase atmospheric simulation chamber, a time-resolved fluorescence technique for characterising chemical processing at the air-sea interface). It will allow the assessment of new emerging ideas such as a quantitative description of the importance of photosensitized reactions in the visible at the air/sea interface as a major source of halogenated radicals and aerosols in the marine environment.
This new understanding will impact on our ability to describe atmospheric chemistry in the marine environment which has strong impact on the urban air quality of coastal regions (which by the way represent highly populated regions ) but also on climate change by providing new input for global climate models.","2366276","2012-04-01","2017-03-31"
"ALEM","ADDITIONAL LOSSES IN ELECTRICAL MACHINES","Matti Antero Arkkio","AALTO KORKEAKOULUSAATIO SR","""Electrical motors consume about 40 % of the electrical energy produced in the European Union. About 90 % of this energy is converted to mechanical work. However, 0.5-2.5 % of it goes to so called additional load losses whose exact origins are unknown. Our ambitious aim is to reveal the origins of these losses, build up numerical tools for modeling them and optimize electrical motors to minimize the losses.

As the hypothesis of the research, we assume that the additional losses mainly result from the deterioration of the core materials during the manufacturing process of the machine. By calorimetric measurements, we have found that the core losses of electrical machines may be twice as large as comprehensive loss models predict. The electrical steel sheets are punched, welded together and shrink fit to the frame. This causes residual strains in the core sheets deteriorating their magnetic characteristics. The cutting burrs make galvanic contacts between the sheets and form paths for inter-lamination currents. Another potential source of additional losses are the circulating currents between the parallel strands of random-wound armature windings. The stochastic nature of these potential sources of additional losses puts more challenge on the research.

We shall develop a physical loss model that couples the mechanical strains and electromagnetic losses in electrical steel sheets and apply the new model for comprehensive loss analysis of electrical machines. The stochastic variables related to the core losses and circulating-current losses will be discretized together with the temporal and spatial discretization of the electromechanical field variables. The numerical stochastic loss model will be used to search for such machine constructions that are insensitive to the manufacturing defects. We shall validate the new numerical loss models by electromechanical and calorimetric measurements.""","2489949","2014-03-01","2019-02-28"
"ALEXANDRIA","""Foundations for Temporal Retrieval, Exploration and Analytics in Web Archives""","Wolfgang Nejdl","GOTTFRIED WILHELM LEIBNIZ UNIVERSITAET HANNOVER","""Significant parts of our cultural heritage are produced on the Web, yet only insufficient opportunities exist for accessing and exploring the past of the Web. The ALEXANDRIA project aims to develop models, tools and techniques necessary to archive and index relevant parts of the Web, and to retrieve and explore this information in a meaningful way. While the easy accessibility to the current Web is a good baseline, optimal access to Web archives requires new models and algorithms for retrieval, exploration, and analytics which go far beyond what is needed to access the current state of the Web. This includes taking into account the unique temporal dimension of Web archives, structured semantic information already available on the Web, as well as social media and network information.

Within ALEXANDRIA, we will significantly advance semantic and time-based indexing for Web archives using human-compiled knowledge available on the Web, to efficiently index, retrieve and explore information about entities and events from the past. In doing so, we will focus on the concurrent evolution of this knowledge and the Web content to be indexed, and take into account diversity and incompleteness of this knowledge. We will further investigate mixed crowd- and machine-based Web analytics to support long- running and collaborative retrieval and analysis processes on Web archives. Usage of implicit human feedback will be essential to provide better indexing through insights during the analysis process and to better focus harvesting of content.

The ALEXANDRIA Testbed will provide an important context for research, exploration and evaluation of the concepts, methods and algorithms developed in this project, and will provide both relevant collections and algorithms that enable further research on and practical application of our research results to existing archives like the Internet Archive, the Internet Memory Foundation and Web archives maintained by European national libraries.""","2493600","2014-03-01","2019-02-28"
"ALEXANDRIA","Large-Scale Formal Proof for the Working Mathematician","Lawrence PAULSON","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Mathematical proofs have always been prone to error. Today, proofs can be hundreds of pages long and combine results from many specialisms, making them almost impossible to check. One solution is to deploy modern verification technology. Interactive theorem provers have demonstrated their potential as vehicles for formalising mathematics through achievements such as the verification of the Kepler Conjecture. Proofs done using such tools reach a high standard of correctness.

However, existing theorem provers are unsuitable for mathematics. Their formal proofs are unreadable. They struggle to do simple tasks, such as evaluating limits. They lack much basic mathematics, and the material they do have is difficult to locate and apply.

ALEXANDRIA will create a proof development environment attractive to working mathematicians, utilising the best technology available across computer science. Its focus will be the management and use of large-scale mathematical knowledge, both theorems and algorithms. The project will employ mathematicians to investigate the formalisation of mathematics in practice. Our already substantial formalised libraries will serve as the starting point. They will be extended and annotated to support sophisticated searches. Techniques will be borrowed from machine learning, information retrieval and natural language processing.  Algorithms will be treated similarly: ALEXANDRIA will help users find and invoke the proof methods and algorithms appropriate for the task.

ALEXANDRIA will provide (1) comprehensive formal mathematical libraries; (2) search within libraries, and the mining of libraries for proof patterns; (3)  automated support for the construction of large formal proofs; (4) sound and practical computer algebra tools. 

ALEXANDRIA will be based on legible structured proofs. Formal proofs should be not mere code, but a machine-checkable form of communication between mathematicians.","2430140","2017-09-01","2022-08-31"
"ALGAME","Algorithms, Games, Mechanisms, and the Price of Anarchy","Elias Koutsoupias","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The objective of this proposal is to bring together a local team of young researchers who will work closely with international collaborators to advance the state of the art of Algorithmic Game Theory and open new venues of research at the interface of Computer Science, Game Theory, and Economics. The proposal consists mainly of three intertwined research strands: algorithmic mechanism design, price of anarchy, and online algorithms.

Specifically, we will attempt to resolve some outstanding open problems in algorithmic mechanism design: characterizing the incentive compatible mechanisms for important domains, such as the domain of combinatorial auctions, and resolving the approximation ratio of mechanisms for scheduling unrelated machines. More generally, we will study centralized and distributed algorithms whose inputs are controlled by selfish agents that are interested in the outcome of the computation. We will investigate new notions of mechanisms with strong truthfulness and limited susceptibility to externalities that can facilitate modular design of mechanisms of complex domains.

We will expand the current research on the price of anarchy to time-dependent games where the players can select not only how to act but also when to act. We also plan to resolve outstanding questions on the price of stability and to build a robust approach to these questions, similar to smooth analysis. For repeated games, we will investigate convergence of simple strategies (e.g., fictitious play), online fairness, and strategic considerations (e.g., metagames). More generally, our aim is to find a productive formulation of playing unknown games by drawing on the fields of online algorithms and machine learning.","2461000","2013-04-01","2019-03-31"
"AlgoRNN","Recurrent Neural Networks and Related Machines That Learn Algorithms","Juergen Schmidhuber","UNIVERSITA DELLA SVIZZERA ITALIANA","Recurrent neural networks (RNNs) are general parallel-sequential computers. Some learn their programs or weights. Our supervised Long Short-Term Memory (LSTM) RNNs were the first to win pattern recognition contests, and recently enabled best known results in speech and handwriting recognition, machine translation, etc. They are now available to billions of users through the world's most valuable public companies including Google and Apple. Nevertheless, in lots of real-world tasks RNNs do not yet live up to their full potential. Although universal in theory, in practice they fail to learn important types of algorithms. This ERC project will go far beyond today's best RNNs through novel RNN-like systems that address some of the biggest open RNN problems and hottest RNN research topics: (1) How can RNNs learn to control (through internal spotlights of attention) separate large short-memory structures such as sub-networks with fast weights, to improve performance on many natural short-term memory-intensive tasks which are currently hard to learn by RNNs, such as answering detailed questions on recently observed videos? (2) How can such RNN-like systems metalearn entire learning algorithms that outperform the original learning algorithms? (3) How to achieve efficient transfer learning from one RNN-learned set of problem-solving programs to new RNN programs solving new tasks? In other words, how can one RNN-like system actively learn to exploit algorithmic information contained in the programs running on another? We will test our systems existing benchmarks, and create new, more challenging multi-task benchmarks. This will be supported by a rather cheap, GPU-based mini-brain for implementing large RNNs.","2500000","2017-10-01","2022-09-30"
"ALGSTRONGCRYPTO","Algebraic Methods for Stronger Crypto","Ronald John Fitzgerald CRAMER","STICHTING NEDERLANDSE WETENSCHAPPELIJK ONDERZOEK INSTITUTEN","Our field is cryptology. Our overarching objective is to advance  significantly the frontiers in
design and analysis of high-security cryptography for the future generation.
 Particularly, we wish to enhance the efficiency, functionality, and, last-but-not-least, fundamental understanding of cryptographic security against very powerful adversaries.
Our approach here is to develop completely novel methods by
 deepening, strengthening and broadening the
 algebraic foundations of the field.
Concretely, our lens builds on
the arithmetic codex. This is a  general, abstract cryptographic primitive whose basic theory we recently developed and whose asymptotic part, which  relies on algebraic geometry, enjoys crucial applications in surprising foundational results on constant communication-rate two-party cryptography. A codex is a linear (error correcting) code that, when endowing its ambient vector space just with  coordinate-wise multiplication, can be viewed as simulating, up to some  degree, richer arithmetical structures such as finite fields  (or  products thereof), or generally, finite-dimensional algebras over  finite fields. Besides this degree, coordinate-localities for which simulation holds and for which it does not at all are also captured.
Our method is based on novel perspectives on codices  which significantly
 widen their scope and strengthen their utility.  Particularly, we bring
 symmetries, computational- and complexity theoretic aspects, and connections with algebraic number theory, -geometry, and -combinatorics into play in novel ways. Our applications range from public-key cryptography to secure multi-party computation.
Our proposal is subdivided into 3 interconnected modules:


(1) Algebraic- and Number Theoretical Cryptanalysis
(2) Construction of Algebraic Crypto Primitives
(3) Advanced Theory of Arithmetic Codices","2447439","2017-10-01","2022-09-30"
"ALKAGE","Algebraic and Kähler geometry","Jean-Pierre, Raymond, Philippe Demailly","UNIVERSITE GRENOBLE ALPES","The purpose of this project is to study basic questions in algebraic and Kähler geometry. It is well known that the structure of projective or Kähler manifolds is governed by positivity or negativity properties of the curvature tensor. However, many fundamental problems are still wide open. Since the mid 1980's, I have developed a large number of key concepts and results that have led to important progress in transcendental algebraic geometry. Let me mention the discovery of holomorphic Morse inequalities, systematic applications of L² estimates with singular hermitian metrics, and a much improved understanding of Monge-Ampère equations and of singularities of plurisuharmonic functions.  My first goal will be to investigate the Green-Griffiths-Lang conjecture asserting that an entire curve drawn in a variety of general type is algebraically degenerate. The subject is intimately related to important questions concerning Diophantine equations, especially higher dimensional generalizations of Faltings' theorem - the so-called Vojta program. One can rely here on a breakthrough I made in 2010, showing that all such entire curves must satisfy algebraic differential equations. A second closely related area of research of this project is the analysis of the structure of projective or compact Kähler manifolds. It can be seen as a generalization of the classification theory of surfaces by Kodaira, and of the more recent results for dimension 3 (Kawamata, Kollár, Mori, Shokurov, ...) to other dimensions. My plan is to combine powerful recent results obtained on the duality of positive cohomology cones with an analysis of the instability of the tangent bundle, i.e. of the Harder-Narasimhan filtration. On these ground-breaking questions, I intend  to go much further and to enhance my national and international collaborations. These subjects already attract many young researchers and postdocs throughout the world, and the grant could be used to create even stronger interactions.","1809345","2015-09-01","2020-08-31"
"ALLEGRO","Active large-scale learning for visual recognition","Cordelia Schmid","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","A massive and ever growing amount of digital image and video content
is available today, on sites such as
Flickr and YouTube, in audiovisual archives such as those of BBC and
INA, and in personal collections. In most cases, it comes with
additional information, such as text, audio or other metadata, that forms a
rather sparse and noisy, yet rich and diverse source of annotation,
ideally suited to emerging weakly supervised and active machine
learning technology. The ALLEGRO project will take visual recognition
to the next level by using this largely untapped source of data to
automatically learn visual models.  The main research objective of
our project is the development of new algorithms and computer software
capable of autonomously exploring evolving data collections, selecting
the relevant information, and determining the visual models most
appropriate for different object, scene, and activity categories. An
emphasis will be put on learning visual models from video, a
particularly rich source of information, and on the representation of
human activities, one of today's most challenging problems in computer
vision. Although this project addresses fundamental research
issues, it is expected to result in significant advances in
high-impact applications that range from visual mining of the Web and
automated annotation and organization of family photo and video albums
to large-scale information retrieval in television archives.","2493322","2013-04-01","2019-03-31"
"ALMA","Attosecond Control of Light and Matter","Anne L'huillier","LUNDS UNIVERSITET","Attosecond light pulses are generated when an intense laser interacts with a gas target. These pulses are not only short, enabling the study of electronic processes at their natural time scale, but also coherent. The vision of this proposal is to extend temporal coherent control concepts to a completely new regime of time and energy, combining (i) ultrashort pulses (ii) broadband excitation (iii) high photon energy, allowing scientists to reach not only valence but also inner shells in atoms and molecules, and, when needed, (iv) high spatial resolution. We want to explore how elementary electronic processes in atoms, molecules and more complex systems can be controlled by using well designed sequences of attosecond pulses. The research project proposed is organized into four parts: 1. Attosecond control of light leading to controlled sequences of attosecond pulses We will develop techniques to generate sequences of attosecond pulses with a variable number of pulses and controlled carrier-envelope-phase variation between consecutive pulses. 2. Attosecond control of electronic processes in atoms and molecules We will investigate the dynamics and coherence of phenomena induced by attosecond excitation of electron wave packets in various systems and we will explore how they can be controlled by a controlled sequence of ultrashort pulses. 3. Intense attosecond sources to reach the nonlinear regime We will optimize attosecond light sources in a systematic way, including amplification of the radiation by injecting a free electron laser. This will open up the possibility to develop nonlinear measurement and control schemes. 4. Attosecond control in more complex systems, including high spatial resolution We will develop ultrafast microscopy techniques, in order to obtain meaningful temporal information in surface and solid state physics. Two directions will be explored, digital in line microscopic holography and photoemission electron microscopy.","2250000","2008-12-01","2013-11-30"
"AlmaCrypt","Algorithmic and Mathematical Cryptology","Antoine Joux","SORBONNE UNIVERSITE","Cryptology is a foundation of information security in the digital world. Today's internet is protected by a form of cryptography based on complexity theoretic hardness assumptions.  Ideally, they should be strong to ensure security and versatile to offer a wide range of functionalities and allow efficient implementations.  However, these assumptions are largely untested and internet security could be built on sand.
The main ambition of Almacrypt is to remedy this issue by challenging the assumptions through an advanced algorithmic analysis.
In particular, this proposal questions the two pillars of public-key encryption: factoring and discrete logarithms.  Recently, the PI contributed to show that in some cases, the discrete logarithm problem is considerably weaker than previously assumed.  A main objective is to ponder the security of other cases of the discrete logarithm problem, including elliptic curves, and of factoring. We will study the generalization of the recent techniques and search for new algorithmic options with comparable or better efficiency.
We will also study hardness assumptions based on codes and subset-sum, two candidates for post-quantum cryptography. We will consider the applicability of recent algorithmic and mathematical techniques to the resolution of the corresponding putative hard problems, refine the analysis of the algorithms and design new algorithm tools.
Cryptology is not limited to the above assumptions: other hard problems have been proposed to aim at post-quantum security and/or to offer extra functionalities. Should the security of these other assumptions become critical, they would be added to Almacrypt's scope. They could also serve to demonstrate other applications of our algorithmic progress.
In addition to its scientific goal, Almacrypt also aims at seeding a strengthened research community dedicated to algorithmic and mathematical cryptology.
--","2403125","2016-01-01","2021-12-31"
"ALPAM","Atomic-Level Physics of Advanced Materials","Börje Johansson","KUNGLIGA TEKNISKA HOEGSKOLAN","Most of the technological materials have been developed by very expensive and cumbersome trial and error methods. On the other hand, computer based theoretical design of advanced materials is an area where rapid and extensive developments are taking place. Within my group new theoretical tools have now been established which are extremely well suited to the study of complex materials. In this approach basic quantum mechanical theories are used to describe fundamental properties of alloys and compounds. The utilization of such calculations to investigate possible optimizations of certain key properties represents a major departure from the traditional design philosophy. The purpose of my project is to build up a new competence in the field of computer-aided simulations of advanced materials. The main goal will be to achieve a deep understanding of the behaviour of complex metallic systems under equilibrium and non-equilibrium conditions at the atomic level by studying their electronic, magnetic and atomic structure using the most modern and advanced computational methods. This will enable us to establish a set of materials parameters and composition-structure-property relations that are needed for materials optimization. 

The research will be focused on fundamental technological properties related to defects in advanced metallic alloys (high-performance steels, superalloys, and refractory, energy related and geochemical materials) and alloy phases (solid solutions, intermetallic compounds), which will be studied by means of parameter free atomistic simulations combined with continuum modelling. As a first example, we will study the Fe-Cr system, which is of great interest to industry as well as in connection to nuclear waste. The Fe-Cr-Ni system will form another large group of materials under the aegis of this project. Special emphasis will also be placed on those Fe-alloys which exist under extreme conditions and are possible candidates for the Earth core.","2000000","2009-03-01","2014-02-28"
"ALPHA","Alpha Shape Theory Extended","Herbert Edelsbrunner","INSTITUTE OF SCIENCE AND TECHNOLOGYAUSTRIA","Alpha shapes were invented in the early 80s of last century, and their implementation in three dimensions in the early 90s was at the forefront of the exact arithmetic paradigm that enabled fast and correct geometric software. In the late 90s, alpha shapes motivated the development of the wrap algorithm for surface reconstruction, and of persistent homology, which was the starting point of rapidly expanding interest in topological algorithms aimed at data analysis questions. 

We now see alpha shapes, wrap complexes, and persistent homology as three aspects of a larger theory, which we propose to fully develop. This viewpoint was a long time coming and finds its clear expression within a generalized
version of discrete Morse theory. This unified framework offers new opportunities, including
(I) the adaptive reconstruction of shapes driven by the cavity structure;

(II) the stochastic analysis of all aspects of the theory;
(III) the computation of persistence of dense data, both in scale and in depth;

(IV) the study of long-range order in periodic and near-periodic point configurations.
These capabilities will significantly deepen as well as widen the theory and enable new applications in the  sciences. To gain focus, we concentrate on low-dimensional applications in structural molecular biology and particle systems.","1678432","2018-07-01","2023-06-30"
"ALPROS","Artificial Life-like Processive Systems","Roeland Johannes Maria Nolte","STICHTING KATHOLIEKE UNIVERSITEIT","Toroidal processive enzymes (e.g. enzymes/proteins that are able to thread onto biopolymers and to perform stepwise reactions along the polymer chain) are among the most fascinating tools involved in the clockwork machinery of life. Processive catalysis is ubiquitous in Nature, viz. DNA polymerases, endo- and exo-nucleases and; it plays a crucial role in numerous events of the cell’s life, including most of the replication, transmission, and expression and repair processes of the genetic information. In the case of DNA polymerases the protein catalyst encircles the DNA and whilst moving along it, make copies of high fidelity. Although numerous works have been reported in relation with the synthesis of natural enzymes' analogues, very few efforts have been paid in comparison to mimic these processive properties. It is the goal of this proposal to rectify this oversight and unravel the essential components of Nature’s polymer catalysts. The individual projects are designed to specifically target the essential aspects of processive catalysis, i.e. rate of motion, rate of catalysis, and transfer of information. One project is aimed at extending the research into a processive catalytic system that is more suitable for industrial application. Two projects involve more farsighted studies and are designed to push the research way beyond the current boundaries into the area of Turing machines and bio-rotaxane catalysts which can modify DNA in a non-natural process. The vision of this proposal is to open up the field of ‘processive catalysis’ and invigorate the next generation of chemists to develop information transfer and toroidal processive catalysts. The construction of synthetic analogues of processive enzymes could open a gate toward a large range of applications, ranging from intelligent tailoring of polymers to information storage and processing.","1603699","2012-02-01","2017-01-31"
"AMDROMA","Algorithmic and Mechanism Design Research in Online MArkets","Stefano LEONARDI","UNIVERSITA DEGLI STUDI DI ROMA LA SAPIENZA","Online markets currently form an important  share of the global economy.  The Internet hosts classical markets (real-estate, stocks, e-commerce) as well allowing new markets with previously unknown features (web-based advertisement, viral marketing,  digital goods, crowdsourcing, sharing economy).  Algorithms play a central role in many decision processes involved in online markets.  For example,  algorithms  run electronic auctions, trade stocks,  adjusts prices dynamically, and harvest big data to provide economic information.  Thus, it is of paramount importance to understand the algorithmic and mechanism design foundations of online markets.

The algorithmic research issues that we consider involve algorithmic mechanism design, online and approximation algorithms, modelling uncertainty in online market design, and large-scale data analysisonline and approximation algorithms, large-scale optimization and data mining. The aim of this research project is to combine these fields to consider research questions that are central for today's Internet economy.  We plan to apply these techniques so as to solve fundamental algorithmic problems motivated by web-basedInternet advertisement,  Internet market designsharing economy, and crowdsourcingonline labour marketplaces.  While my planned research is focussedcentered on foundational work with rigorous design and analysis of in algorithms and mechanismsic design and analysis, it will also include as an important component empirical validation on large-scale real-life datasets.","1780150","2018-07-01","2023-06-30"
"AMETIST","Advanced III-V Materials and Processes Enabling Ultrahigh-efficiency ( 50%) Photovoltaics","Mircea Dorel GUINA","TAMPEREEN KORKEAKOULUSAATIO SR","Compound semiconductor solar cells are providing the highest photovoltaic conversion efficiency, yet their performance lacks far behind the theoretical potential. This is a position we will challenge by engineering advanced III-V optoelectronics materials and heterostructures for better utilization of the solar spectrum, enabling efficiencies approaching practical limits. The work is strongly motivated by the global need for renewable energy sources. To this end, AMETIST framework is based on three vectors of excellence in: i) material science and epitaxial processes, ii) advanced solar cells exploiting nanophotonics concepts, and iii) new device fabrication technologies. 

Novel heterostructures (e.g. GaInNAsSb, GaNAsBi), providing absorption in a broad spectral range from 0.7 eV to 1.4 eV, will be synthesized and monolithically integrated in tandem cells with up to 8-junctions. Nanophotonic methods for light-trapping, spectral and spatial control of solar radiation will be developed to further enhance the absorption. To ensure a high long-term impact, the project will validate the use of state-of-the-art molecular-beam-epitaxy processes for fabrication of economically viable ultra-high efficiency solar cells. The ultimate efficiency target is to reach a level of 55%. This would enable to generate renewable/ecological/sustainable energy at a levelized production cost below ~7 ¢/kWh, comparable or cheaper than fossil fuels. The work will also bring a new breath of developments for more efficient space photovoltaic systems. 

AMETIST will leverage the leading position of the applicant in topical technology areas relevant for the project (i.e. epitaxy of III-N/Bi-V alloys and key achievements concerning GaInNAsSb-based tandem solar cells). Thus it renders a unique opportunity to capitalize on the group expertize and position Europe at the forefront in the global competition for demonstrating more efficient and economically viable photovoltaic technologies.","2492719","2017-01-01","2021-12-31"
"AMIMOS","Agile MIMO Systems for Communications, Biomedicine, and Defense","Bjorn Ottersten","KUNGLIGA TEKNISKA HOEGSKOLAN","This proposal targets the emerging frontier research field of multiple-input multiple-output (MIMO) systems along with several innovative and somewhat unconventional applications of such systems. The use of arrays of transmitters and receivers will have a profound impact on future medical imaging/therapy systems, radar systems, and radio communication networks. Multiple transmitters provide a tremendous versatility and allow waveforms to be adapted temporally and spatially to environmental conditions. This is useful for individually tailored illumination of human tissue in biomedical imaging or ultrasound therapy. In radar systems, multiple transmit beams can be formed simultaneously via separate waveform designs allowing accurate target classification. In a wireless communication system, multiple communication signals can be directed to one or more users at the same time on the same frequency carrier. In addition, multiple receivers can be used in the above applications to provide increased detection performance, interference rejection, and improved estimation accuracy. The joint modelling, analysis, and design of these multidimensional transmit and receive schemes form the core of this research proposal. Ultimately, our research aims at developing the fundamental tools that will allow the design of wireless communication systems with an order-of-magnitude higher capacity at a lower cost than today; of ultrasound therapy systems maximizing delivered power while reducing treatment duration and unwanted illumination; and of distributed aperture multi-beam radars allowing more effective target location, identification, and classification. Europe has several successful industries that are active in biomedical imaging/therapy, radar systems, and wireless communications. The future success of these sectors critically depends on the ability to innovate and integrate new technology.","1872720","2009-01-01","2013-12-31"
"AMPLify","Allocation Made PracticaL","Toby Walsh","TECHNISCHE UNIVERSITAT BERLIN","Allocation Made PracticaL

The AMPLify project will lay the foundations of a new field, computational behavioural game theory that brings a computational perspective, computational implementation, and behavioural insights to game theory. These foundations will be laid by tackling a pressing problem facing society today: the efficient and fair allocation of resources and costs. Research in allocation has previously considered simple, abstract models like cake cutting. We propose to develop richer models that capture important new features like asynchronicity which occur in many markets being developed in our highly connected and online world. The mechanisms currently used to allocate resources and costs are limited to these simple, abstract models and also do not take into account  how people actually behave in practice. We will therefore design new mechanisms for these richer allocation problems that exploit insights gained from behavioural game theory like loss aversion. We will also tackle the complexity of these rich models and mechanisms with computational tools. Finally, we will use computation to increase both the efficiency and fairness of allocations. As a result, we will be able to do more with fewer resources and greater fairness. Our initial case studies in resource and cost allocation demonstrate that we can  improve efficiency greatly, offering one company alone savings of up to 10% (which is worth tens of millions of dollars every year). We predict even greater impact with the more sophisticated mechanisms to be developed during the course of this project.","2499681","2016-06-01","2021-05-31"
"AMPLIPORE","Understanding negative gas adsorption in highly porous networks for the design of pressure amplifying materials","Stefan Kaskel","TECHNISCHE UNIVERSITAET DRESDEN","Negative gas adsorption (NGA) is a new, counterintuitive and paradoxical phenomenon, for the first time
reported by my group in 2016: Normal solid materials with significant outer or inner surface area always
take up gas when the pressure in the surrounding reservoir is increased (adsorption). NGA networks instead
react at a certain point in the opposite direction: They release gas upon external pressure increase, leading to
an overall pressure amplification in a closed system. Comparable phenomena have never been reported
before. What is so exciting about NGA? We have a unique material in hand, that counteracts to an external
force by force amplification.
So far NGA has solely been observed in one of our new coordination polymers, featuring a colossal selfcompression
associated with a mesopore-to-micropore transformation. Gas pressure amplifying materials
could lead to important innovations in gas releasing rescue systems, pneumatic control systems (production,
transportation), micropumps, microfluidic devices, pneumatic actuators, and artificial lungs. A fundamental
understanding of the physical mechanisms, structures, and thermodynamic boundary conditions is an
essential prerequisite for any industrial application of this counterintuitive phenomenon.
Combining strong synthetic methodologies with advanced analytical techniques, AMPLIPORE will elucidate
the characteristic molecular and mesoscopic materials signatures as well as thermodynamic boundary
conditions of NGA phenomena. We will elaborate a generic NGA-materials concept to tailor the pressure
amplification and explore temperature and pressure ranges at which NGA can be applied. Developing tailormade
instrumentation for kinetic investigations of NGA will give fundamental insights into the intrinsic and
macroscopic dynamics of crystal-to-crystal transformations for applications in micropneumatic systems.","2363125","2017-09-01","2022-08-31"
"AMSTAT","Problems at the Applied Mathematics-Statistics Interface","Andrew Stuart","THE UNIVERSITY OF WARWICK","Applied mathematics is concerned with developing models with predictive capability, and with probing those models to obtain qualitative and quantitative insight into the phenomena being modelled. Statistics is data-driven and is aimed at the development of methodologies to optimize the information derived from data. The increasing complexity of phenomena that scientists and engineers wish to model, together with our increased ability to gather, store and interrogate data, mean that the subjects of applied mathematics and statistics are increasingly required to work in conjunction. This research proposal is concerned with a research program at the interface between these two disciplines, aimed at problems in differential equations where profusion of data and the sophisticated model combine to produce the mathematical problem of obtaining information from a probability measure on function space. Applications are far-reaching and include the atmospheric sciences, geophysics, chemistry, econometrics and signal processing.  The objectives of the research are: (i) to create the systematic foundations for a range of problems at the applied mathematics and statistics interface which share the common mathematical structure underpinning the range of applications described above; (ii)  to exploit this common mathematical structure to design effecient algorithms to sample probability measures on function space; (iii) to apply these algorithms to attack a range of significant problems arising in molecular dynamics and in the atmospheric sciences.","1693501","2008-12-01","2014-11-30"
"analysisdirac","The analysis of the Dirac operator: the hypoelliptic Laplacian and its applications","Jean-Michel Philippe Marie-José Bismut","UNIVERSITE PARIS-SUD","This proposal is devoted to the applications of a new hypoelliptic Dirac operator,
whose analytic properties have been studied by Lebeau and myself. Its construction connects classical Hodge theory with the geodesic flow, and more generally any geometrically defined Hodge Laplacian with a dynamical system on the cotangent bundle. The proper description of this object can be given in analytic, index theoretic and probabilistic terms, which explains both its potential many applications, and also its complexity.","1112400","2012-02-01","2017-01-31"
"ANAMULTISCALE","Analysis of Multiscale Systems Driven by Functionals","Alexander Mielke","FORSCHUNGSVERBUND BERLIN EV","Many complex phenomena in the sciences are described by nonlinear partial differential equations, the solutions of which exhibit oscillations and concentration effects on multiple temporal or spatial scales. Our aim is to use methods from applied analysis to contribute to the understanding of the interplay of effects on different scales. The central question is to determine those quantities on the microscale which are needed to for the correct description of the macroscopic evolution.
We aim to develop a mathematical framework for analyzing and modeling coupled systems with multiple scales.  This will include Hamiltonian dynamics as well as different types of dissipation like gradient flows or rate-independent dynamics.  The choice of models will be guided by specific applications in material modeling (e.g., thermoplasticity, pattern formation, porous media) and optoelectronics (pulse interaction, Maxwell-Bloch systems, semiconductors, quantum mechanics).  The research will address mathematically fundamental issues like existence and stability of solutions but will mainly be devoted to the modeling of multiscale phenomena in evolution systems. We will focus on systems with geometric structures, where the dynamics is driven by functionals. Thus, we can go much beyond the classical theory of homogenization and singular perturbations. The novel features of our approach are
- the combination of different dynamical effects in one framework,
- the use of geometric and metric structures for coupled partial differential equations,
- the exploitation of Gamma-convergence for evolution systems driven by functionals.","1390000","2011-04-01","2017-03-31"
"ANGEOM","Geometric analysis in the Euclidean space","Xavier Tolsa Domenech","UNIVERSITAT AUTONOMA DE BARCELONA","""We propose to study different questions in the area of the so called geometric analysis. Most of the topics we are interested in deal with the connection between the behavior of singular integrals and the geometry of sets and measures. The study of this connection has been shown to be extremely helpful in the solution of certain long standing problems in the last years, such as the solution of the Painlev\'e problem or the obtaining of the optimal distortion bounds for quasiconformal mappings by Astala.
More specifically, we would like to study the relationship between the L^2 boundedness of singular integrals associated with Riesz and other related kernels, and rectifiability and other geometric notions. The so called David-Semmes problem is probably the main open problem in this area. Up to now, the techniques used to deal with this problem come from multiscale analysis and involve ideas from Littlewood-Paley theory and quantitative techniques of rectifiability. We propose to apply new ideas that combine variational arguments with other techniques which have connections with mass transportation. Further, we think that it is worth to explore in more detail the connection among mass transportation, singular integrals, and uniform rectifiability.
We are also interested in the field of quasiconformal mappings. We plan to study a problem regarding the quasiconformal distortion of quasicircles. This problem consists in proving that the bounds obtained recently by S. Smirnov on the dimension of K-quasicircles are optimal. We want to apply techniques from quantitative geometric measure theory to deal with this question.
Another question  that we intend to explore lies in the interplay of harmonic analysis, geometric measure theory and partial differential equations. This concerns an old problem on the unique continuation of harmonic functions at the boundary open C^1  or Lipschitz domain. All the results known by now deal with smoother Dini domains.""","1105930","2013-05-01","2018-04-30"
"ANTEGEFI","Analytic Techniques for Geometric and Functional Inequalities","Nicola Fusco","UNIVERSITA DEGLI STUDI DI NAPOLI FEDERICO II","Isoperimetric and Sobolev inequalities are the best known examples of geometric-functional inequalities. In recent years the PI and collaborators have obtained new and sharp quantitative versions of these and other important related inequalities. These results have been obtained by the combined use of classical symmetrization methods, new tools coming from mass transportation theory, deep geometric measure tools and ad hoc symmetrizations. The objective of this project is to further develop thes techniques in order to get: sharp quantitative versions of Faber-Krahn inequality, Gaussian isoperimetric inequality, Brunn-Minkowski inequality, Poincaré and Sobolev logarithm inequalities; sharp decay rates for the quantitative Sobolev inequalities and Polya-Szegö inequality.","600000","2009-01-01","2013-12-31"
"AOC","Adversary-Oriented Computing","Rachid Guerraoui","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","""Recent technological evolutions, including the cloud, the multicore, the social and the mobiles ones, are turning computing ubiquitously distributed. Yet, building high-assurance distributed programs is notoriously challenging. One of the main reasons is that these systems usually seek to achieve several goals at the same time. In short, they need to be efficient, responding effectively in various average-case conditions, as well as reliable, behaving correctly in severe, worst-case conditions. As a consequence, they typically intermingle different strategies: each to cope with some specific condition, e.g., with or without node failures, message losses, time-outs, contention, cache misses,
over-sizing, malicious attacks, etc. The resulting programs end up hard to design, prove, verify, implement, test and debug. Not surprisingly, there are anecdotal evidences of the fragility of the most celebrated distributed systems.

The goal of this project is to contribute to building high-assurance distributed programs by introducing a new dimension for separating and isolating their concerns, as well as a new scheme for composing and reusing them in a modular manner. In short, the project will explore the inherent power and limitations of a novel paradigm, Adversary-Oriented Computing (AOC). Sub-programs, each implementing a specific strategy to cope with a given adversary, modelling a specific working condition, are designed, proved, verified, implemented, tested and debugged independently. They are then composed, possibly dynamically, as black-boxes within the same global program. The AOC project is ambitious and it seeks to fundamentally revisit the way distributed algorithms are designed and distributed systems are implemented. The gain expected in comparison with today's approaches is substantial, and I believe it will be proportional to the degree of difficulty of the distributed problem at hand.""","2147012","2014-06-01","2019-05-31"
"APEG","Algorithmic Performance Guarantees: Foundations and Applications","Susanne ALBERS","TECHNISCHE UNIVERSITAET MUENCHEN","Optimization problems are ubiquitous in computer science. Almost every problem involves the optimization of some objective function. However a major part of these problems cannot be solved to optimality. Therefore, algorithms that achieve provably good performance guarantees are of immense importance. Considerable progress has already been made, but great challenges remain: Some fundamental problems are not well understood. Moreover, for central problems arising in new applications, no solutions are known at all. 

The goal of APEG is to significantly advance the state of the art on algorithmic performance guarantees. Specifically, the project has two missions: First, it will develop new algorithmic techniques, breaking new ground in the areas of online algorithms, approximations algorithms and algorithmic game theory. Second, it will apply these techniques to solve fundamental problems that are central in these algorithmic disciplines. APEG will attack long-standing open problems, some of which have been unresolved for several decades. Furthermore, it will formulate and investigate new algorithmic problems that arise in modern applications. The research agenda encompasses a broad spectrum of classical and timely topics including (a) resource allocation in computer systems, (b) data structuring, (c) graph problems, with relations to Internet advertising, (d) complex networks and (e) massively parallel systems. In addition to basic optimization objectives, the project will also study the new performance metric of energy minimization in computer systems. 

Overall, APEG pursues cutting-edge algorithms research, focusing on both foundational problems and applications. Any progress promises to be a breakthrough or significant contribution.","2404250","2016-10-01","2021-09-30"
"APHOTOREACTOR","Entirely Self-organized: Arrayed Single-Particle-in-a-Cavity Reactors for Highly Efficient and Selective Catalytic/Photocatalytic Energy Conversion and Solar Light Reaction Engineering","Patrik Schmuki","FRIEDRICH-ALEXANDER-UNIVERSITAET ERLANGEN NUERNBERG","The proposal is built on the core idea to use an ensemble of multiple level self-organization processes to create a next generation photocatalytic platform that provides unprecedented property and reactivity control. As a main output, the project will yield a novel highly precise combined catalyst/photocatalyst assembly to: 1) provide a massive step ahead in photocatalytic applications such as direct solar hydrogen generation, pollution degradation (incl. CO2 decomposition), N2 fixation, or photocatalytic organic synthesis. It will drastically enhance efficiency and selectivity of photocatalytic reactions, and enable a high number of organic synthetic reactions to be carried out economically (and ecologically) via combined catalytic/photocatalytic pathways. Even more, it will establish an entirely new generation of “100% depoisoning”, anti-aggregation catalysts with substantially enhanced catalyst life-time. For this, a series of self-assembly processes on the mesoscale will be used to create highly uniform arrays of single-catalyst-particle-in-a-single-TiO2-cavity; target is a 100% reliable placement of a single <10 nm particle in a 10 nm cavity. Thus catalytic features of, for example Pt nanoparticles, can ideally interact with the photocatalytic properties of a TiO2 cavity. The cavity will be optimized for optical and electronic properties by doping and band-gap engineering; the geometry will be tuned to the range of a few nm.. This nanoscopic design yields to a radical change in the controllability of length and time-scales (reactant, charge carrier and ionic transport in the substrate) in combined photocatalytic/catalytic reactions. It is of key importance that all nanoscale assembly principles used in this work are scalable and allow to create square meters of nanoscopically ordered catalyst surfaces. We target to demonstrate the feasibility of the implementation of the nanoscale principles in a prototype macroscopic reactor.","2427000","2014-03-01","2019-02-28"
"APPROXNP","Approximation of NP-hard optimization problems","Johan Håstad","KUNGLIGA TEKNISKA HOEGSKOLAN","The proposed project aims to create a center of excellence that aims at understanding the approximability of NP-hard optimization problems.  In particular, for central problems like vertex cover, coloring of graphs, and various constraint satisfaction problems we want to study upper and lower bounds on how well they can be approximated in polynomial time.  Many existing strong results are based on what is known as the Unique Games Conjecture (UGC) and a significant part of the project will be devoted to studying this conjecture.  We expect that a major step needed to be taken in this process is to further develop the understanding of Boolean functions on the Boolean hypercube.  We anticipate that the tools needed for this will come in the form of harmonic analysis which in its turn will rely on the corresponding results  in the analysis of functions over the domain of real numbers.","2376000","2009-01-01","2014-12-31"
"APRA","Active Polymers for Renewable Functional Actuators","Eugene TERENTJEV","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The idea of mechanical actuator based on intrinsic material properties of liquid-crystalline elastomers (rather than complex engineering of interacting components) has been understood for 20+ years. The remarkable characteristics of LCE actuation (fully reversible action; large-amplitude, with a stroke of 5%-300%; stress-strain-speed response almost exactly matching the human muscle) make it highly attractive in biomedical engineering, robotics, smart textiles, and other fields. Yet, there is a profound difficulty (bottleneck), which remains the reason why this concept has not found its way into any practical devices & applications. LCE actuation requires alignment (monodomain structure) of the local anisotropy in the permanently crosslinked polymer network - which has been impossible to achieve in any useful large-scale configuration except the flat film, due to the unavoidable restrictions of two competing processes: orientational alignment and network crosslinking. 
Recently, we made a breakthrough, developing LCE vitrimers (polymer networks covalently crosslinked by a bond-exchange reaction). Vitrimers are much more stable than other transient elastomer networks, allow easy thermal re-moulding (making the material fully renewable), and permit molding of complex shapes with intricate local alignment (which are impossible in traditional elastomers). This project will bridge from the concept to technology, tuning the material design for robust nematic LCE vitrimers, imparting photo-actuation capacity with a controlled wavelength, and finally utilising them in practical-engineering actuator applications where the reversible mechanical action is stimulated by light, solvent exposure, or more traditionally - heat. These applications include (but not limited to): continuous spinning light-driven motor, tactile dynamic Braille display, capillary pump and toggle flow switch for microfuidics, active textile fibre, and heliotracking filament that always points at the Sun.","2012136","2018-10-01","2023-09-30"
"AQUAMS","Analysis of quantum many-body systems","Robert Seiringer","INSTITUTE OF SCIENCE AND TECHNOLOGYAUSTRIA","The main focus of this project is the mathematical analysis of many-body quantum systems, in particular, interacting quantum gases at low temperature. The recent experimental advances in studying ultra-cold atomic gases have led to renewed interest in these systems. They display a rich variety of quantum phenomena, including, e.g., Bose–Einstein condensation and superfluidity, which makes them interesting both from a physical and a mathematical point of view.
The goal of this project is the development of new mathematical tools for dealing with complex problems in many-body quantum systems. New mathematical methods lead to different points of view and thus increase our understanding of physical systems. From the point of view of mathematical physics, there has been significant progress in the last few years in understanding the interesting phenomena occurring in quantum gases, and the goal of this project is to investigate some of the key issues that remain unsolved. Due to the complex nature of the problems, new mathematical ideas
and methods will have to be developed for this purpose. One of the main question addressed in this proposal is the validity of the Bogoliubov approximation for the excitation spectrum of many-body quantum systems. While its accuracy has been
successfully shown for the ground state energy of various models, its predictions concerning the excitation spectrum have so far only been verified in the Hartree limit, an extreme form of a mean-field limit where the interaction among the particles is very weak and ranges over the whole system. The central part of this project is concerned with the extension of these results to the case of short-range interactions. Apart from being mathematically much more challenging, the short-range case is the
one most relevant for the description of actual physical systems. Hence progress along these lines can be expected to yield valuable insight into the complex behavior of these many-body quantum systems.","1497755","2016-10-01","2021-09-30"
"ARIPHYHIMO","Arithmetic and physics of Higgs moduli spaces","Tamas Hausel","INSTITUTE OF SCIENCE AND TECHNOLOGYAUSTRIA","The proposal studies problems concerning the geometry and topology of moduli spaces of Higgs bundles on a Riemann surface motivated by parallel considerations in number theory and mathematical physics. In this way the proposal bridges various duality theories in string theory with the Langlands program in number theory.

The heart of the proposal is a circle of precise conjectures relating to the topology of the moduli space of Higgs bundles. The formulation and motivations of the conjectures make direct contact with the Langlands program in number theory, various duality conjectures in string theory, algebraic combinatorics, knot theory and low dimensional topology and representation theory of  quivers, finite groups and algebras of Lie type and Cherednik algebras.","1304945","2013-04-01","2018-08-31"
"ARITHQUANTUMCHAOS","Arithmetic and Quantum Chaos","Zeev Rudnick","TEL AVIV UNIVERSITY","Quantum Chaos is an emerging discipline which is crossing over from Physics into Pure Mathematics. The recent crossover is driven in part by a connection with Number Theory. This project explores several aspects of this interrelationship and is composed of a number of sub-projects. The sub-projects deal with: statistics of energy levels and wave functions of pseudo-integrable systems, a hitherto unexplored subject in the mathematical community which is not well understood in the physics community; with statistics of zeros of zeta functions over function fields, a purely number theoretic topic which is linked to the subproject on Quantum Chaos through the mysterious connections to Random Matrix Theory and an analogy between energy levels and zeta zeros; and with spatial statistics in arithmetic.","1714000","2013-02-01","2019-01-31"
"ARMOS","Advanced multifunctional Reactors for green Mobility and Solar fuels","Athanasios Konstandopoulos","ETHNIKO KENTRO EREVNAS KAI TECHNOLOGIKIS ANAPTYXIS","Green Mobility requires an integrated approach to the chain fuel/engine/emissions. The present project aims at ground breaking advances in the area of Green Mobility by (a) enabling the production of affordable, carbon-neutral, clean, solar fuels using exclusively renewable/recyclable raw materials, namely solar energy, water and captured Carbon Dioxide from combustion power plants (b) developing a highly compact, multifunctional reactor, able to eliminate gaseous and particulate emissions from the exhaust of engines operated on such clean fuels.

The overall research approach will be based on material science, engineering and simulation technology developed by the PI over the past 20 years in the area of Diesel Emission Control Reactors, which will be further extended and cross-fertilized in the area of Solar Thermochemical Reactors, an emerging discipline of high importance for sustainable development, where the PI’s research group has already made significant contributions, and received the 2006 European Commission’s Descartes Prize for the development of the first ever solar reactor, holding the potential to produce on a large scale, pure renewable Hydrogen from the thermochemical splitting of water, also known as the HYDROSOL technology.","1750000","2011-02-01","2017-01-31"
"ARPEMA","Anionic redox processes:  A transformational approach for advanced energy materials","Jean-Marie Tarascon","COLLEGE DE FRANCE","Redox chemistry provides the fundamental basis for numerous energy-related electrochemical devices, among which Li-ion batteries (LIB) have become the premier energy storage technology for portable electronics and vehicle electrification. Throughout its history, LIB technology has relied on cationic redox reactions as the sole source of energy storage capacity.  This is no longer true. In 2013 we demonstrated that Li-driven reversible formation of (O2)n peroxo-groups in new layered oxides led to extraordinary increases in energy storage capacity. This finding, which is receiving worldwide attention, represents a transformational approach for creating advanced energy materials for not only energy storage, but also water splitting applications as both involve peroxo species. However, as is often the case with new discoveries, the fundamental science at work needs to be rationalized and understood. Specifically, what are the mechanisms for ion and electron transport in these Li-driven anionic redox reactions?  
To address these seminal questions and to widen the spectrum of materials (transition metal and anion) showing anionic redox chemistry, we propose a comprehensive research program that combines experimental and computational methods. The experimental methods include structural and electrochemical analyses (both ex-situ and in-situ), and computational modeling will be based on first-principles DFT for identifying the fundamental processes that enable anionic redox activity. The knowledge gained from these studies, in combination with our expertise in inorganic synthesis, will enable us to design a new generation of Li-ion battery materials that exhibit substantial increases (20 -30%) in energy storage capacity, with additional impacts on the development of Na-ion batteries and the design of water splitting catalysts, with the feasibility to surpass current water splitting efficiencies via novel (O2)n-based electrocatalysts.","2249196","2015-10-01","2020-09-30"
"ARS","Autonomous Robotic Surgery","Paolo FIORINI","UNIVERSITA DEGLI STUDI DI VERONA","The goal of the ARS project is the derivation of a unified framework for the autonomous execution of robotic tasks in challenging environments in which accurate performance and safety are of paramount importance. We have chosen surgery as the research scenario because of its importance, its intrinsic challenges, and the presence of three factors that make this project feasible and timely. In fact, we have recently concluded the I-SUR project demonstrating the feasibility of autonomous surgical actions, we have access to the first big data made available to researchers of clinical robotic surgeries, and we will be able to demonstrate the project results on the high performance surgical robot “da Vinci Research Kit”. The impact of autonomous robots on the workforce is a current subject of discussion, but surgical autonomy will be welcome by the medical personnel, e.g. to carry out simple intervention steps, react faster to unexpected events, or monitor the insurgence of fatigue. The framework for autonomous robotic surgery will include five main research objectives. The first will address the analysis of robotic surgery data set to extract action and knowledge models of the intervention. The second objective will focus on planning, which will consist of instantiating the intervention models to a patient specific anatomy. The third objective will address the design of the hybrid controllers for the discrete and continuous parts of the intervention. The fourth research objective will focus on real time reasoning to assess the intervention state and the overall surgical situation. Finally, the last research objective will address the verification, validation and benchmark of the autonomous surgical robotic capabilities. The research results to be achieved by ARS will contribute to paving the way towards enhancing autonomy and operational capabilities of service robots, with the ambitious goal of bridging the gap between robotic and human task execution capability.","2750000","2017-10-01","2022-09-30"
"ARTHUS","Advances in Research on Theories of the Dark Universe - Inhomogeneity Effects in Relativistic Cosmology","Thomas BUCHERT","UNIVERSITE LYON 1 CLAUDE BERNARD","The project ARTHUS aims at determining the physical origin of Dark Energy: in addition to the energy sources of the standard model of cosmology, effective terms arise through spatially averaging inhomogeneous cosmological models in General Relativity. It has been demonstrated that these additional terms can play the role of Dark Energy on large scales (but they can also mimic Dark Matter on scales of mass accumulations). The underlying rationale is that fluctuations in the Universe generically couple to spatially averaged intrinsic properties of space, such as its averaged scalar curvature, thus changing the global evolution of the effective (spatially averaged) cosmological model. At present, we understand these so- called backreaction effects only qualitatively. The project ARTHUS is directed towards a conclusive quantitative evaluation of these effects by developing generic and non-perturbative relativistic models of structure formation, by statistically measuring the key-variables of the models in observations and in simulation data, and by reinterpreting observational results in light of the new models. It is to be emphasized that there is no doubt about the existence of backreaction effects; the question is whether they are even capable of getting rid of the dark sources (as some models discussed in the literature suggest), or whether their impact is substantially smaller. The project thus addresses an essential issue of current cosmological research: to find pertinent answers concerning the quantitative impact of inhomogeneity effects, a necessary, worldwide recognized step toward high-precision cosmology. If the project objectives are attained, the results will have a far-reaching impact on theoretical and observational cosmology, on the interpretation of astronomical experiments such as Planck and Euclid, as well as on a wide spectrum of particle physics theories and experiments.","2091000","2017-09-01","2022-08-31"
"ARTIMATTER","""Lego-Style Materials, Structures and Devices Assembled on Demand from Isolated Atomic Planes""","Andre Geim","THE UNIVERSITY OF MANCHESTER","""Following the advent of graphene with its wide range of unique properties, several other one-atom-thick crystals have been isolated and their preliminary studies have been undertaken. They range from semiconducting monolayers of MoS2 and NbSe2, which similar to graphene exhibit the electric field effect and relatively high electronic quality, to wide-gap insulators such as boron-nitride monolayers that can serve as atomically-thin tunnel barriers.
This library of two-dimensional crystals opens a possibility to construct various 3D structures with on-demand properties, which do not exist in nature but can be assembled in Lego style by stacking individual atomic planes on top of each other in a desired sequence. This project is to explore this new avenue.
We will design, fabricate and study multilayer materials ranging from basic heterostructures that consist of a few alternating layers of graphene and boron nitride and already exhibit a rich spectrum of new phenomena, as recently demonstrated by the applicant’s group, to complex artificial materials containing many layers of different 2D crystals and mimicking, for example, layered superconductors. In a similar manner, various electronic, optoelectronic, micromechanical and other devices will be developed and investigated. The applicant’s aim is to search for new materials with unique properties, novel devices with better characteristics and new physics that is likely to emerge along the way.
The proposed research offers many exciting opportunities and can lead to the development of a large unexplored field with impact exceeding even that of graphene research. This presents a unique, once-in-decade, opportunity to make a very significant breakthrough in condensed matter physics and materials science.""","2200000","2013-05-01","2018-04-30"
"ARTISYM","Artificial endosymbiosis","Jan Van hest","TECHNISCHE UNIVERSITEIT EINDHOVEN","Living organisms have acquired new functionalities by uptake and integration of species to create symbiotic life-forms. This process of endosymbiosis has intrigued scientists over the years, albeit mostly from an evolution biology perspective. With the advance of chemical and synthetic biology, our ability to create molecular-life-like systems has increased tremendously, which enables us to build cell and organelle-like structures. However, these advances have not been taken to a level to study comprehensively if endosymbiosis can be applied to non-living systems or to integrate living with non-living matter. The aim of the research described in the ARTISYM proposal is to establish the field of artificial endosymbiosis. Two lines of research will be followed. First, we will incorporate artificial organelles in living cells to design hybrid cells with acquired functionality. This investigation is scientifically of great interest, as it will show us how to introduce novel compartmentalized pathways into living organisms. It also serves an important societal goal, as with these compartments dysfunctional cellular processes can be corrected. We will follow both a transient and a permanent approach. With the transient route biodegradable nanoreactors are introduced to supply living cells temporarily with novel function. Functionality is permanently introduced using genetic engineering to express protein-based nanoreactors in living cells, or via organelle transplantation of healthy mitochondria in diseased living cells. Secondly I aim to create artificial cells with the ability to perform endosymbiosis; the uptake and presence of artificial organelles in synthetic vesicles allows them to dynamically respond to their environment. Responses that are envisaged are shape changes, motility, and growth and division. Furthermore, the incorporation of natural organelles in liposomes provides biocatalytic cascades with the necessary cofactors to function in an artificial cell","2500000","2017-01-01","2021-12-31"
"ARYLATOR","New Catalytic Reactions and Exchange Pathways: Delivering Versatile and Reliable Arylation","Guy Charles Lloyd-Jones","THE UNIVERSITY OF EDINBURGH","This proposal details the mechanism-based discovery of ground-breaking new catalyst systems for a broad range of arylation processes that will be of immediate and long-lasting utility to the pharmaceutical, agrochemical, and materials chemistry industries. These industries have become highly dependent on coupling technologies employing homogeneous late transition metal catalysis and this reliance will grow further,  particularly if the substrate scope can be broadened, the economics, in terms of reagents and catalyst, made more favourable, the reliability at scale-up improved, and the generation of side-products, of particular importance for optical and electronic properties of materials, minimized or eliminated.

This proposal addresses these issues by conducting a detailed and comprehensive mechanistic investigation of direct arylation, so that a substantial expansion of the reaction scope can be achieved. At present, the regioselectivity can be very high, however catalyst turnover rates are moderate, and the arene is required to be in a fairly narrow window of activity. Specific aspects to be addressed in terms of mechanistic study are: catalyst speciation and pathways for deactivation; pathways for homocoupling; influence of anions and dummy ligands; protodemetalloidation pathways. Areas proposed for mechanism-informed development are: expansion of metalloid tolerance; expansion of arene scope; use of traceless activators and directors, new couplings via ligand exchange, the evolution of simpler / cheaper and more selective / active catalysts; expansion to oxidative double arylations (Ar-H + Ar’-H) with control, and without resort to super-stoichiometric bias.

The long-term legacy of these studies will be detailed insight for current and emerging systems, as well as readily extrapolated information for the design of new, more efficient catalyst systems in academia, and their scaleable application in industry","2114223","2014-02-01","2019-01-31"
"ASAP","Adaptive Security and Privacy","Bashar Nuseibeh","THE OPEN UNIVERSITY","With the prevalence of mobile computing devices and the increasing availability of pervasive services, ubiquitous computing (Ubicomp) is a reality for many people. This reality is generating opportunities for people to interact socially in new and richer ways, and to work more effectively in a variety of new environments. More generally, Ubicomp infrastructures – controlled by software – will determine users’ access to critical services.

With these opportunities come higher risks of misuse by malicious agents. Therefore, the role and design of software for managing use and protecting against misuse is critical, and the engineering of software that is both functionally effective while safe guarding user assets from harm is a key challenge. Indeed the very nature of Ubicomp means that software must adapt to the changing needs of users and their environment, and, more critically, to the different threats to users’ security and privacy.

ASAP proposes to radically re-conceptualise software engineering for Ubicomp in ways that are cognisant of the changing functional needs of users, of the changing threats to user assets, and of the changing relationships between them. We propose to deliver adaptive software capabilities for supporting users in managing their privacy requirements, and adaptive software capabilities to deliver secure software that underpin those requirements. A key novelty of our approach is its holistic treatment of security and human behaviour. To achieve this, it draws upon contributions from requirements engineering, security & privacy engineering, and human-computer interaction. Our aim is to contribute to software engineering that empowers and protects Ubicomp users. Underpinning our approach will be the development of representations of security and privacy problem structures that capture user requirements, the context in which those requirements arise, and the adaptive software that aims to meet those requirements.","2499041","2012-10-01","2018-09-30"
"ASC3","Asymmetric Cluster Catalysis & Chemistry","Ulrich Kaspar Heiz","TECHNISCHE UNIVERSITAET MUENCHEN","The objective of the present scientific proposal is the implementation of a novel approach in selective and asymmetric heterogeneous catalysis. We aim to exploit the structure and chirality of small, supported metal and bimetal clusters for triggering selective and enantioselective reactions. Our Ansatz is beyond doubt of fundamental nature. Although chemistry and in particular catalysis evolved on a largely empirical basis in the past, we strongly believe the complexity of the challenges at hand to make this a less ideal approach. In consequence, developing selective and asymmetric cluster catalysis will be based on a detailed molecular understanding and will not only require intense methodological developments for the synthesis and characterization of asymmetric catalysts and the detection of chiral and isomeric product molecules but also make use of innovative basic science in the fields of surface chemistry, cluster science, spectroscopy and kinetics. As complex as the involved challenges are, we aim at mastering the following ground-breaking steps: (a) development of cutting-edge spectroscopic methodologies for the isomer and enantiomer sensitive in situ detection of product molecules. (b) preparation and characterization of isomer- and enantioselective heterogeneous catalysts based on chiral metal clusters or molecule-cluster-complexes. (c) investigations of the selectivity and enantioselectivity of cluster based heterogeneous catalysts and formulation of concepts for understanding the observed selective and asymmetric chemistry.
Besides the importance of the science carried out within this proposal, the proposed experimental methodology will also open up opportunities in other fields of chemistry like catalysis, analytical chemistry, spectroscopy, surface science, and nanomaterials.","2301600","2010-04-01","2015-03-31"
"ASCIR","Active Suspensions with Controlled Interaction Rules","Clemens Bechinger","UNIVERSITAT KONSTANZ","Self-propelling, i.e., active colloidal particles constitute a novel class of non-equilibrium systems which exhibit structural and dynamical features similar to those in assemblies of bacteria or other motile organisms. Due to their reduced complexity, they provide an intriguing chance to understand the formation of dynamical structures in non-equilibrium systems in unprecedented detail. A central question in this rapidly growing field is, how interaction-rules determine the creation of e.g. swarms or complex networks. In addition to ordinary inter particle and hydrodynamic forces, interaction-rules can be much more complex. For example, they can regulate the particle motility depending on their relative orientation, their local density or otherwise distinct particle configurations.
Here, we propose an experimental approach which aims towards controlling the amplitude and direction of the particle’s motility in dense active suspensions on a single particle level. Particle-propulsion is achieved by a light-activated diffusiophoretic mechanism, where the particle motility is controlled by an incident light field. By means of an acoustic-optical modulator and a feed-back loop, we create dynamical and spatially-resolved light fields which depend on the current configuration of active particles and user-defined interaction rules. Because these rules are imposed externally and not by internal forces, this permits the experimental realization of a wide range of rules (linear, non-linear, and even non-reciprocal) in dense, two-dimensional active systems. We expect, that the experimental realization of user-defined interaction-rules largely extends our understanding how active matter can organize in dynamical structures. In addition, the perspective of enhanced control of active particles, as suggested within this proposal, will be of considerable importance for their use as autonomous micro robots which will deliver payloads in liquid environments.","2036750","2016-10-01","2021-09-30"
"ASD","Atomistic Spin-Dynamics; Methodology and Applications","Olof Ragnar Eriksson","UPPSALA UNIVERSITET","Our aim is to provide a theoretical framework for studies of dynamical aspects of magnetic materials and magnetisation reversal, which has potential for applications for magnetic data storage and magnetic memory devices. The project focuses on developing and using an atomistic spin dynamics simulation method. Our goal is to identify novel materials and device geometries with improved performance. The scientific questions which will be addressed concern the understanding of the fundamental temporal limit of magnetisation switching and reversal, and the mechanisms which govern this limit. The methodological developments concern the ability to, from first principles theory, calculate the interatomic exchange parameters of materials in general, in particular for correlated electron materials, via the use of dynamical mean-field theory. The theoretical development also involves an atomistic spin dynamics simulation method, which once it has been established, will be released as a public software package. The proposed theoretical research will be intimately connected to world-leading experimental efforts, especially in Europe where a leading activity in experimental studies of magnetisation dynamics has been established. The ambition with this project is to become world-leading in the theory of simulating spin-dynamics phenomena, and to promote education and training of young researchers. To achieve our goals we will build up an open and lively environment, where the advances in the theoretical knowledge of spin-dynamics phenomena will be used to address important questions in information technology. In this environment the next generation research leaders will be fostered and trained, thus ensuring that the society of tomorrow is equipped with the scientific competence to tackle the challenges of our future.","2130000","2010-01-01","2014-12-31"
"ASES","""Advancing computational chemistry with new accurate, robust and scalable electronic structure methods""","Hans-Joachim Werner","UNIVERSITAET STUTTGART","""The objective of this proposal is to tackle two of the greatest challenges in quantum chemistry: (i) extending the applicability of highly accurate wave function methods to large molecular systems, and (ii) developing accurate and robust multi-reference methods that can be used for studying important but very difficult problems in transition metal chemistry, catalysis, and photochemistry. Solutions to these problems have now come within reach due to three advances we recently reported: first, the steep scaling of the computational cost with molecular size can be reduced to linear by exploiting the short-range character of electron correlation (local correlation methods). Second, the accuracy, efficiency, and robustness of these local correlation methods can be strongly improved by new  tensor decomposition approaches and the inclusion of terms depending explicitly on the inter-electronic distances (F12 methods). Third, the development of highly complex electronic structure theories can be greatly facilitated and accelerated by new automated tensor network evaluation techniques. We are certain that by combining and generalizing these advances the long-standing problems (i) and (ii) can be solved. We will focus especially on highly scalable algorithms in order to use massively parallel computer systems efficiently. For linear-scaling methods this means that the size of the molecules that can be treated in a fixed time will grow linearly with the number of available processors. We will furthermore explore new multi-reference ansätze and implement analytical energy gradients and response properties for local methods. Hybrid and embedding methods to account for solvent and environment effects will also be investigated. It is our priority to make our new methods as easy to use, robust, and widely applicable as possible. We believe that they will open entirely new horizons for innumerable applications in chemistry, physics, biology, and materials science.""","2454000","2013-02-01","2018-01-31"
"ASTERISK","ASTERoseismic Investigations with SONG and Kepler","Jørgen Christensen-Dalsgaard","AARHUS UNIVERSITET","The project aims at a breakthrough in our understanding of stellar evolution, by combining advanced observations of stellar oscillations with state-of-the-art modelling of stars. This will largely be based on very extensive and precise data on stellar oscillations from the NASA Kepler mission launched in March 2009, but additional high-quality data will also be included. In particular, my group is developing the global SONG network for observations of stellar oscillations. These observational efforts will be supplemented by sophisticated modelling of stellar evolution, and by the development of asteroseismic tools to use the observations to probe stellar interiors. This will lead to a far more reliable determination of stellar ages, and hence ages of other astrophysical objects; it will compare the properties of the Sun with other stars and hence provide an understanding of the life history of the Sun; it will investigate the physical processes that control stellar properties, both at the level of the thermodynamical properties of stellar plasmas and the hydrodynamical instabilities that play a central role in stellar evolution; and it will characterize central stars in extra-solar planetary systems, determining the size and age of the star and hence constrain the evolution of the planetary systems. The Kepler data will be analysed in a large international collaboration coordinated by our group. The SONG network, which will become partially operational during the present project, will yield even detailed information about the conditions in the interior of stars, allowing tests of subtle but central aspects of the physics of stellar interiors. The projects involve the organization of a central data archive for asteroseismic data, at the Royal Library, Copenhagen.","2498149","2011-04-01","2016-03-31"
"ASTEX","Attosecond Science by Transmission and Emission of X-rays","Jonathan Philip Marangos","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","""This is a programme of advanced research with potential for high scientific impact and applications to areas of great strategic importance such as renewable energy and biomolecular technology. The aim is to develop and apply a combination of cutting-edge tools to observe and understand dynamics in molecules and condensed phase matter with attosecond temporal and nanometre spatial resolutions. The programme, will exploit two new types of measurements that my group have already begun to develop: high harmonic generation (HHG) spectroscopy and attosecond absorption pump-probe spectroscopy, and will apply them to the measurement of attosecond electron dynamics in large molecules and the condensed phase. These methods rely upon the emission and transmission of soft X-ray attosecond fields that make accessible measurement not only of larger molecules in the gas phase but also thin (micron to nanometre) samples in the condensed phase. This is a research project that will open new frontiers both experimentally and theoretically. The challenge of this research is high and will be met by a concerted programme that is well matched to my teams experimental and theoretical expertise in attosecond physics, ultrafast intense-field science, soft X-ray techniques and advanced techniques for creating gaseous and condensed phase samples.""","2344390","2012-04-01","2017-03-31"
"ASTONISH","Atomic-scale STudies Of the Nature of and conditions for Inducing Superconductivity at High-temperatures","Roland Martin Wiesendanger","UNIVERSITAET HAMBURG","""One of the greatest challenges these days in condensed matter physics is the fundamental understanding of the mechanisms leading to high-Tc superconductivity and ultimately, as a result of that, the discovery of a material system exhibiting a superconducting state with a transition temperature Tc above room temperature. While several different classes of high-Tc materials have been discovered in the past decades, including the well-known CuO-based superconductors (cuprates) or the more recently discovered class of Fe-based superconductors (pnictides), the mechanisms behind high-Tc superconductivity remain controversial. Up to date, no theory exists which would allow for a rational design of a superconducting material with a transition temperature above room temperature. On the other hand, experiments on rather complex material systems often suffer from material imperfections or from a lack of tunability of materials’ properties within a wide range. Our experimental studies within this project therefore will focus on model-type systems which can be prepared and thoroughly characterized with atomic level precision. The growth of the model-type samples will be controlled vertically one atomic layer at a time and laterally by making use of single-atom manipulation techniques. Atomic-scale characterization at low energy-scales will be performed by low-temperature spin-resolved elastic and inelastic scanning tunnelling microscopy (STM) and spectroscopy (STS) as well as by non-contact atomic force microscopy and spectroscopy based techniques. Transport experiments will be conducted by a four-probe STM setup under well-defined ultra-high vacuum conditions. By having access to the electronic and spin, as well as to the vibrational degrees of freedom down to the atomic level, we hope to be able to identify the nature of and the conditions for inducing superconductivity at high temperatures, which could ultimately lead a knowledge-based design of high-Tc superconductors.""","2170696","2014-01-01","2018-12-31"
"ASTRODYN","Astrophysical Dynamos","Axel Brandenburg","KUNGLIGA TEKNISKA HOEGSKOLAN","Magnetic fields in stars, planets, accretion discs, and galaxies are believed to be the result of a dynamo process converting kinetic energy into magnetic energy. This work focuses on the solar dynamo, but dynamos in other astrophysical systems will also be addressed. In particular, direct high-resolution three-dimensional simulations are used to understand particular aspects of the solar dynamo and ultimately to simulate the solar dynamo as a whole. Phenomenological approaches will be avoided in favor of obtaining rigorous results. A major problem is catastrophic quenching, i.e. the decline of dynamo effects in inverse proportion to the magnetic Reynolds number, which is huge. Tremendous advances have been made in the last few years since the cause of catastrophic quenching in dynamos has been understood in terms of magnetic helicity evolution. The numerical tools are now in place to allow for magnetic helicity fluxes via coronal mass ejections, thus alleviating catastrophic quenching. This work employs simulations in spherical shells, augmented by Cartesian simulations in special cases. The roles of the near-surface shear layer, the tachocline, as well as pumping in the bulk of the convection zone are to be clarified. The Pencil Code will be used for most applications. The code is third order in time and sixth order in space and is used for solving the hydromagnetic equations. It is a public domain code developed by roughly 20 scientists world wide and maintained under an a central versioning system at Nordita. Automatic nightly tests of currently 30 applications ensure the integrity of the code. It is used for a wide range of applications and may include the effects of radiation, self-gravity, dust, chemistry, variable ionization, cosmic rays, in addition to those of magnetohydrodynamics. The code with its infrastructure offers a good opportunity for individuals within a broad group of people to develop new tools that may automatically be useful to others.","2220000","2009-02-01","2014-01-31"
"ASTROGEOBIOSPHERE","An astronomical perspective on Earth's geological record and evolution of life","Birger Schmitz","LUNDS UNIVERSITET","""This project will develop the use of relict, extraterrestrial minerals in Archean to Cenozoic slowly formed sediments as tracers of events in the solar system and cosmos, and to decipher the possible relation between such events and evolution of life and environmental change on Earth. There has been consensus that it would not be possible to reconstruct variations in the flux of different types of meteorites to Earth through the ages. Meteorite falls are rare and meteorites weather and decay rapidly on the Earth surface. However, the last years we have developed the first realistic approach to circumvent these problems. Almost all meteorite types contain a small fraction of spinel minerals that survives weathering and can be recovered from large samples of condensed sediments of any age. Inside the spinels we can locate by synchrotron-light X-ray tomography 1-30 micron sized inclusions of most of the other minerals that made up the original meteorite.  With cutting-edge frontier microanalyses such as Ne-21 (solar wind, galactic rays), oxygen isotopes (meteorite group and type) and cosmic ray tracks (supernova densities) we will be able to unravel from the geological record fundamental new information about the solar system at specific times through the past 3.8 Gyr. Variations in flux and types of meteorites may reflect solar-system and galaxy gravity disturbances as well as the sequence of disruptions of the parent bodies for meteorite types known and not yet known. Cosmic-ray tracks in spinels may identify the galactic year (230 Myr) in the geological record. For the first time it will be possible to systematically relate major global biotic and tectonic events, changes in sea-level, climate and asteroid and comet impacts to what happened in the larger astronomical realm. In essence, the project is a robust approach to establish a pioneer """"astrostratigraphy"""" for Earth's geological record, complementing existing bio-, chemo-, and magnetostratigraphies.""","1950000","2012-04-01","2017-03-31"
"ATHENE","Designing new technical wastewater treatment solutions targeted for organic micropollutant biodegradation, by understanding enzymatic pathways and assessing detoxification","Thomas Ternes","Bundesanstalt fuer Gewaesserkunde","The identification of degradation pathways relevant for organic micropollutants in biological wastewater treatment processes is currently a major gap, preventing a profound evaluation of the capability of biological wastewater treatment. By elucidating the responsible enzymatic reactions of mixed microbial populations this project will cover this gap and thereby allow finding technical solutions that harness the true potential of biological processes for an enhanced biodegradation and detoxification. Due to the multi-disciplinary approach Athene will have impacts on the fields of biological wastewater treatment, analytical and environmental chemistry, environmental microbiology, water and (eco)toxicity. The multi-disciplinary approach of the project requires the involvement of a co-investigator experienced in process engineering and microbiology in wastewater treatment. Athene will go far beyond state-of-the-art in the following fields: a) efficiency in chemical analysis and structure identification of transformation products at environmental relevant concentrations; b) identification of enzymatic pathways relevant for micropollutant degradation in biological wastewater treatment; c) designing innovative technical solutions to maximize biodegradation; d) map and model relevant enzymatic pathways for environmental concentrations. Furthermore, designing biological wastewater treatment processes by understanding enzymatic pathways relevant for organic micropollutants removal represents a paradigm shift for municipal wastewater treatment. In the context of the actual scientific discussion about the relevance of trace organics in the aquatic environment and in drinking water, this topic is deemed as highly innovative: for its potential of proposing new technical options as well as for the gain in understanding compound persistency. Finally enzymatic reactions as well as the treatment schemes will be assessed for there capability to reduce toxiciological effects.","3473400","2011-04-01","2017-03-31"
"ATLAS","Bioengineered autonomous cell-biomaterials devices for generating humanised micro-tissues for regenerative medicine","João Felipe Colardelle da Luz Mano","UNIVERSIDADE DE AVEIRO","New generations of devices for tissue engineering (TE) should rationalize better the physical and biochemical cues operating in tandem during native regeneration, in particular at the scale/organizational-level of the stem cell niche. The understanding and the deconstruction of these factors (e.g. multiple cell types exchanging both paracrine and direct signals, structural and chemical arrangement of the extra-cellular matrix, mechanical signals…) should be then incorporated into the design of truly biomimetic biomaterials. ATLAS proposes rather unique toolboxes combining smart biomaterials and cells for the ground-breaking advances of engineering fully time-self-regulated complex 2D and 3D devices, able to adjust the cascade of processes leading to faster high-quality new tissue formation with minimum pre-processing of cells. Versatile biomaterials based on marine-origin macromolecules will be used, namely in the supramolecular assembly of instructive multilayers as nanostratified building-blocks for engineer such structures. The backbone of these biopolymers will be equipped with a variety of (bio)chemical elements permitting: post-processing chemistry and micro-patterning, specific/non-specific cell attachment, and cell-controlled degradation. Aiming at being applied in bone TE, ATLAS will integrate cells from different units of tissue physiology, namely bone and hematopoietic basic elements and consider the interactions between the immune and skeletal systems. These ingredients will permit to architect innovative films with high-level dialogue control with cells, but in particular sophisticated quasi-closed 3D capsules able to compartmentalise such components in a “globe-like” organization, providing local and long-range order for in vitro microtissue development and function. Such hybrid devices could be used in more generalised front-edge applications, including as disease models for drug discovery or test new therapies in vitro.","2498988","2015-12-01","2020-11-30"
"ATM-GTP","Atmospheric Gas-to-Particle conversion","Markku KULMALA","HELSINGIN YLIOPISTO","Atmospheric Gas-to-Particle conversion (ATM-GTP) is a 5-year project focusing on one of the most critical atmospheric processes relevant to global climate and air quality: the first steps of atmospheric aerosol particle formation and growth. The project will concentrate on the currently lacking environmentally-specific knowledge about the interacting, non-linear, physical and chemical atmospheric processes associated with nano-scale gas-to-particle conversion (GTP). The main scientific objective of ATM-GTP is to create a deep understanding on atmospheric GTP taking place at the sub-5 nm size range, particularly in heavily-polluted Chinese mega cities like Beijing and in pristine environments like Siberia and Nordic high-latitude regions. We also aim to find out how nano-GTM is associated with air quality-climate interactions and feedbacks. We are interested in quantifying the effect of nano-GTP on the COBACC (Continental Biosphere-Aerosol-Cloud-Climate) feedback loop that is important in Arctic and boreal regions. Our approach enables to point out the effective reduction mechanisms of the secondary air pollution by a factor of 5-10 and to make reliable estimates of the global and regional aerosol loads, including anthropogenic and biogenic contributions to these loads. We can estimate the future role of Northern Hemispheric biosphere in reducing the global radiative forcing via the quantified feedbacks. The project is carried out by the world-leading scientist in atmospheric aerosol science, being also one of the founders of terrestrial ecosystem meteorology, together with his research team. The project uses novel infrastructures including SMEAR (Stations Measuring Ecosystem Atmospheric Relations) stations, related modelling platforms and regional data from Russia and China. The work will be carried out in synergy with several national, Nordic and EU research-innovation projects: Finnish Center of Excellence-ATM, Nordic CoE-CRAICC and EU-FP7-BACCHUS.","2500000","2017-06-01","2022-05-31"
"ATMNUCLE","Atmospheric nucleation: from molecular to global scale","Markku Tapio Kulmala","HELSINGIN YLIOPISTO","Atmospheric aerosol particles and trace gases affect the quality of our life in many ways (e.g. health effects, changes in climate and hydrological cycle). Trace gases and atmospheric aerosols are tightly connected via physical, chemical, meteorological and biological processes occurring in the atmosphere and at the atmosphere-biosphere interface. One important phenomenon is atmospheric aerosol formation, which involves the production of nanometer-size particles by nucleation and their growth to detectable sizes.  The main scientific objectives of this project are 1) to quantify the mechanisms responsible for atmospheric new particle formation and 2) to find out how important this process is for the behaviour of the global aerosol system and, ultimately, for the whole climate system.  Our scientific plan is designed as a research chain that aims to advance our understanding of climate and air quality through a series of connected activities. We start from molecular simulations and laboratory measurements to understand nucleation and aerosol thermodynamic processes. We measure nanoparticles and atmospheric clusters at 15-20 sites all around the world using state of the art instrumentation and study feedbacks and interactions between climate and biosphere. With these atmospheric boundary layer studies we form a link to regional-scale processes and further to global-scale phenomena. In order to be able to simulate global climate and air quality, the most recent progress on this chain of processes must be compiled, integrated and implemented in Climate Change and Air Quality numerical models via novel parameterizations.","2000000","2009-01-01","2013-12-31"
"ATMOPACS","Atmospheric Organic Particulate Matter, Air Quality and Climate Change Studies","Spyridon Pandis","FOUNDATION FOR RESEARCH AND TECHNOLOGY HELLAS","Despite its importance for human health and climate change organic aerosol (OA) remains one of the least understood aspects of atmospheric chemistry. We propose to develop an innovative new framework for the description of OA in chemical transport and climate models that will be able to overcome the challenges posed by the chemical complexity of OA while capturing its essential features.

The objectives of ATMOPACS are: (i) The development of a new unified framework for the description of OA based on its two most important parameters: volatility and oxygen content. (ii) The development of measurement techniques for the volatility distribution and oxygen content distribution of OA. This will allow the experimental characterization of OA in this new “coordinate system”. (iii) The study of the major OA processes (partitioning, chemical aging, hygroscopicity, CCN formation, nucleation) in this new framework combining lab and field measurements. (iv) The development and evaluation of the next generation of regional and global CTMs using the above framework. (v) The quantification of the importance of the various sources and formation pathways of OA in Europe and the world, of the sensitivity of OA to emission control strategies, and its role in the direct and indirect effects of aerosols on climate.

The proposed work involves a combination of laboratory measurements, field measurements including novel “atmospheric perturbation experiments”, OA model development, and modelling in urban, regional, and global scales. Therefore, it will span the system scales starting from the nanoscale to the global. The modelling tools that will be developed will be made available to all other research groups.","2496000","2011-01-01","2015-12-31"
"ATOMAG","From Attosecond Magnetism towards Ultrafast Spin Photonics","Jean-Yves Bigot","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","We propose to investigate a new frontier in Physics: the study of Magnetic systems using attosecond laser pulses. The main disciplines concerned are: Ultrafast laser sciences, Magnetism and Spin-Photonics, Relativistic Quantum Electrodynamics. Three issues of modern magnetism are addressed. 1. How fast can one modify and control the magnetization of a magnetic system ? 2. What is the role and essence of the coherent interaction between light and spins ? 3. How far spin-photonics can bring us to the real world of data acquisition and storage ? - We want first to provide solid ground experiments, unravelling the mechanisms involved in the demagnetization induced by laser pulses in a variety of magnetic materials (ferromagnetic nanostructures, aggregates and molecular magnets). We will explore the ultrafast magnetization dynamics of magnets using an attosecond laser source. - Second we want to explore how the photon field interacts with the spins. We will investigate the dynamical regime when the potential of the atoms is dressed by the Coulomb potential induced by the laser field. A strong support from the relativistic Quantum Electro-Dynamics is necessary towards that goal. - Third, even though our general approach is fundamental, we want to provide a benchmark of what is realistically possible in ultrafast spin-photonics, breaking the conventional thought that spin photonics is hard to implement at the application level. We will realize ultimate devices combining magneto-optical microscopy with the conventional magnetic recording. This new field will raise the interest of a number of competitive laboratories at the international level. Due to the overlapping disciplines the project also carries a large amount of educational impact both fundamental and applied.","2492561","2010-05-01","2015-04-30"
"AtomicGaugeSimulator","Classical and Atomic Quantum Simulation of Gauge Theories in Particle and Condensed Matter Physics","Uwe-Jens Richard Christian Wiese","UNIVERSITAET BERN","Gauge theories play a central role in particle and condensed matter physics. Heavy-ion collisions explore the strong dynamics of quarks and gluons, which also governs the deep interior of neutron stars, while strongly correlated electrons determine the physics of high-temperature superconductors and spin liquids. Numerical simulations of such systems are often hindered by sign problems. In quantum link models - an alternative formulation of gauge theories developed by the applicant - gauge fields emerge from discrete quantum variables. In the past year, in close collaboration with atomic physicists, we have established quantum link models as a framework for the atomic quantum simulation of dynamical gauge fields. Abelian gauge theories can be realized with Bose-Fermi mixtures of ultracold atoms in an optical lattice, while non-Abelian gauge fields arise from fermionic constituents embodied by alkaline-earth atoms. Quantum simulators, which do not suffer from the sign problem, shall be constructed to address non-trivial dynamics, including quantum phase transitions in spin liquids, the real-time dynamics of confining strings as well as of chiral symmetry restoration at finite temperature and baryon density, baryon superfluidity, or color-flavor locking. New classical simulation algorithms shall be developed in order to solve severe sign problems, to investigate confining gauge theories, and to validate the proposed quantum simulators. Starting from U(1) and SU(2) gauge theories, an atomic physics tool box shall be developed for quantum simulation of gauge theories of increasing complexity, ultimately aiming at 4-d Quantum Chromodynamics (QCD). This project is based on innovative ideas from particle, condensed matter, and computational physics, and requires an interdisciplinary team of researchers. It has the potential to drastically increase the power of simulations and to address very challenging problems that cannot be solved with classical simulation methods.","1975242","2014-02-01","2019-01-31"
"ATTO","A new concept for ultra-high capacity wireless networks","Piet DEMEESTER","UNIVERSITEIT GENT","The project will address the following key question: 

How can we provide fibre-like connectivity to moving objects (robots, humans) with the following characteristics: very high dedicated bitrate of 100 Gb/s per object, very low latency of <10 μs, very high reliability of 99.999%, very high density of more than one object per m2 and this at low power consumption? 

Achieving this would be groundbreaking and it requires a completely new and high-risk approach: applying close proximity wireless communications using low interference ultra-small cells (called “ATTO-cells”) integrated in floors and connected to antennas on the (parallel) floor-facing surface of ground moving objects. This makes it possible to obtain very high densities with very good channel conditions. The technological challenges involved are groundbreaking in mobile networking (overall architecture, handover with extremely low latencies), wireless subsystems (60 GHz substrate integrated waveguide-based distributed antenna systems connected to RF transceivers integrated in floors, low crosstalk between ATTO-cells) and optical interconnect subsystems (simple non-blocking optical coherent remote selection of ATTO-cells, transparent low power 100 Gb/s coherent optical / RF transceiver interconnection using analogue equalization and symbol interleaving to support 4x4 MIMO). By providing this unique communication infrastructure in high density settings, the ATTO concept will not only support the highly demanding future 5G services (UHD streaming, cloud computing and storage, augmented and virtual reality, a range of IoT services, etc.), but also even more demanding services, that are challenging our imagination such as mobile robot swarms or brain computer interfaces with PFlops computing capabilities.
This new concept for ultra-high capacity wireless networks will open up many more opportunities in reconfigurable robot factories, intelligent hospitals, flexible offices, dense public spaces, etc.","2496250","2017-01-01","2021-12-31"
"Attoclock","Clocking fundamental attosecond electron dynamics","Ursula Keller","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","The attoclock is a powerful, new, and unconventional tool to study fundamental attosecond dynamics on an atomic scale. We established its potential by using the first attoclock to measure the tunneling delay time in laser-induced ionization of helium and argon atoms, with surprising results. Building on these first proof-of-principle measurements, I propose to amplify and expand this tool concept to explore the following key questions:  How fast can light liberate electrons from a single atom, a single molecule, or a solid-state system? Related are more questions: How fast can an electron tunnel through a potential barrier? How fast is a multi-photon absorption process? How fast is single-photon photoemission? Many of these questions will undoubtedly spark more questions – revealing deeper and more detailed insights on the dynamics of some of the most fundamental and relevant optoelectronic processes.
There are still many unknown and unexplored areas here. Theory has failed to offer definitive answers. Simulations based on the exact time-dependent Schrödinger equation have not been possible in most cases. Therefore one uses approximations and simpler models to capture the essential physics. Such semi-classical models potentially will help to understand attosecond energy and charge transport in larger molecular systems.  Indeed the attoclock provides a unique tool to explore different semi-classical models.
For example, the question of whether electron tunneling through an energetically forbidden region takes a finite time or is instantaneous has been subject to ongoing debate for the last sixty years. The tunnelling process, charge transfer, and energy transport all play key roles in electronics, energy conversion, chemical and biological reactions, and fundamental processes important for improved information, health, and energy technologies. We believe the attoclock can help refine and resolve key models for many of these important underlying attosecond processes.","2319796","2013-03-01","2018-02-28"
"Auger-Horizon","A large-scale radio detector for the Pierre Auger cosmic-ray Observatory – precision measurements of ultra-high-energy cosmic rays","Jörg HÖRANDEL","STICHTING KATHOLIEKE UNIVERSITEIT","Cosmic Rays (ionized atomic nuclei) are the only matter from beyond our solar system or even from extragalactic space, that we can directly investigate. Up to energies of 10^17 eV they most likely originate in our Galaxy. The highest-energy cosmic rays (>10^18 eV) cannot be magnetically bound any more to the Galaxy and are most likely of extragalactic origin.
The pure existence of these particles raises the question about their origin – how and where are they accelerated? How do they propagate through the universe and interact? How can we directly probe extragalactic matter and how can we locate its origin? 
A key to understand the origin of cosmic rays is to measure the particle species (atomic mass). A precise mass measurement will allow discriminating astrophysical models and will clarify the reason for the observed suppression of the cosmic-ray flux at the highest energies, namely the maximum energy of the accelerators or the energy losses during propagation.
I address these questions by employing a new technique to precisely measure the cosmic-ray mass composition, which my group pioneered, the radio detection of air showers (induced by high-energy cosmic rays in the atmosphere) on very large scales, detecting horizontal air showers with zenith angles from 60° to 90°. 
The new set-up will be the world-largest radio array, operated together with the well-established Auger surface and fluorescence detectors, forming a unique set-up to measure the properties of cosmic rays with unprecedented precision for energies above 10^17.5 eV. The radio technique is a cost-effective and robust method to measure the cosmic-ray energy and mass, complementary to established techniques. The energy scale of the radio measurements is established from first principles. The proposed detectors will also enhance the detection capabilities for high-energy neutrinos and the search for new physics through precision measurements of the electromagnetic and muonic shower components.","3499249","2018-10-01","2023-09-30"
"AUTOHEPARIN","Automated Synthesis of Heparin and Chondroitin Libraries for the Preparation of Diverse Carbohydrate Arrays","Peter Seeberger","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","While heparin, a glacosaminoglycan (GAG) has served as an anticoagulant for more than 60 years, the structure-activity relationship of heparin and chondroitin sulfate for specific interactions with proteins are still poorly understood. It has become evident that defined lengths and sequences or patterns are responsible for binding to a particular protein and modulating its biological activity.  Determination of the structure-activity relationships of heparins and chondroitins creates an opportunity to modulate processes underlying viral entry, angiogenesis, kidney diseases and diseases of the central nervous system. The isolation of pure GAGs is extremely tedious and chemical synthesis is often the only means to access defined oligosaccharides. Currently available synthetic methods for the preparation of heparins and chondroitins are time consuming and lack generality. Therefore, it is still impossible to create large collections of GAG oligosaccharides for systematic studies of GAG-protein interactions.  The overall goal of the project is the development of all aspects of automated GAG synthesis, the procurement of a large collection of heparin and chondroitin oligosaccharides of 2-10 sugars in length with a linker for ready attachment to microarray surfaces and other tools. These molecular tools will be employed to study the interaction of GAGs with growth factors, chemokines and other proteins. The specific aims include: 1) Synthesis of uronic acid and galactosamine building blocks; 2) Development of a new linker for automated GAG solid phase synthesis; 3) Construction of a new automated oligosaccharide synthesizer; 4) Development of methods for the automated assembly of heparin and chondroitin sulfate oligosaccharides; 5) Synthesis of a collection of defined heparin and chondroitin sulfate oligosaccharides; 6) Construction of synthetic GAG microarrays and SPR; 7) Preparation of GAG dendrimers and quantum dots.","2500000","2009-01-01","2014-12-31"
"AXION","Axions: From Heaven to Earth","Frank Wilczek","STOCKHOLMS UNIVERSITET","Axions are hypothetical particles whose existence would solve two major problems: the strong P, T problem (a major blemish on the standard model); and the dark matter problem. It is a most important goal to either observe or rule out the existence of a cosmic axion background. It appears that decisive observations may be possible, but only after orchestrating insight from specialities ranging from quantum field theory and astrophysical modeling to ultra-low noise quantum measurement theory. Detailed predictions for the magnitude and structure of the cosmic axion background depend on cosmological and astrophysical modeling, which can be constrained by theoretical insight and numerical simulation. In parallel, we must optimize strategies for extracting accessible signals from that very weakly interacting source.

While the existence of axions as fundamental particles remains hypothetical, the equations governing how axions interact with electromagnetic fields also govern (with different parameters) how certain materials interact with electromagnetic fields. Thus those materials embody “emergent” axions. The equations have remarkable properties, which one can test in these materials, and possibly put to practical use.

Closely related to axions, mathematically, are anyons. Anyons are particle-like excitations that elude the familiar classification into bosons and fermions. Theoretical and numerical studies indicate that they are common emergent features of highly entangled states of matter in two dimensions. Recent work suggests the existence of states of matter, both natural and engineered, in which anyon dynamics is both important and experimentally accessible. Since the equations for anyons and axions are remarkably similar, and both have common, deep roots in symmetry and topology, it will be fruitful to consider them together.","2324391","2017-09-01","2022-08-31"
"B-PhosphoChem","Exploration of the 2D-Chemistry of Black Phosphorous","Andreas Hirsch","FRIEDRICH-ALEXANDER-UNIVERSITAET ERLANGEN NUERNBERG","We propose the development of the chemistry of black phosphorus (BP). B-PhosphoChem will constitute a new text book chapter in the realm of synthetic chemistry located at the interface of inorganic-, organic-, and materials chemistry as well as solid state physics. B-PhosphoChem will provide the basis for exciting and so far elusive applications such as ion batteries and stable high performance devices. Thin sheets of BP represent a new class of 2D materials and have recently raised tremendous interest in the scientific community. Outstanding physical properties such as high charge carrier mobility, combined with transparency and the persistence of a band gap have been discovered. However, the chemistry of BP remains still unexplored. B-PhosphoChem will close this gap and will a) provide the opportunity to modulate and fine tune the physical properties, b) allow for considerably improving the processability and increasing the solubility, c) establish concepts for the desired chemical stabilization, d) give access to the combination of BP properties with those of other compound classes, e) reveal the fundamental chemical properties and reactivity principles, and f) provide methods for establishing practical applications. Five work packages will be addressed: 1) Production of Thin Layer BP, 2) Supramolecular Chemistry of BP, 3) Intercalation Compounds of BP, 4) Covalent Chemistry of BP, and 5) BP-Based Materials and Devices. The work packages will be supported by systematic calculations. For our group, whose core competence is synthetic organic and supramolecular chemistry, the orientation towards inorganic phosphorus chemistry constitutes a major step into a completely new direction. However, we are convinced to be the most predestinated research group in the world successfully facing this challenge because of our leadership and well documented interdisciplinary experience in synthesizing and characterizing 0D-, 1D-, and 2D nanostructures.","2491250","2017-08-01","2022-07-31"
"BACKUP","Unveiling the relationship between brain connectivity and function by integrated photonics","Lorenzo PAVESI","UNIVERSITA DEGLI STUDI DI TRENTO","I will address the fundamental question of which is the role of neuron activity and plasticity in information elaboration and storage in the brain. I, together with an interdisciplinary team, will develop a hybrid neuro-morphic computing platform. Integrated photonic circuits will be interfaced to both electronic circuits and neuronal circuits (in vitro experiments) to emulate brain functions and develop schemes able to supplement (backup) neuronal functions. The photonic network is based on massive reconfigurable matrices of nonlinear nodes formed by microring resonators, which enter in regime of self-pulsing and chaos by positive optical feedback. These networks resemble human brain. I will push this analogy further by interfacing the photonic network with neurons making hybrid network. By using optogenetics, I will control the synaptic strengthen-ing and the neuron activity. Deep learning algorithms will model the biological network functionality, initial-ly within a separate artificial network and, then, in an integrated hybrid artificial-biological network.
My project aims at:
1. Developing a photonic integrated reservoir-computing network (RCN);
2. Developing dynamic memories in photonic integrated circuits using RCN;
3. Developing hybrid interfaces between a neuronal network and a photonic integrated circuit;
4. Developing a hybrid electronic, photonic and biological network that computes jointly;
5. Addressing neuronal network activity by photonic RCN to simulate in vitro memory storage and retrieval;
6. Elaborating the signal from RCN and neuronal circuits in order to cope with plastic changes in pathologi-cal brain conditions such as amnesia and epilepsy.
The long-term vision is that hybrid neuromorphic photonic networks will (a) clarify the way brain thinks, (b) compute beyond von Neumann, and (c) control and supplement specific neuronal functions.","2499825","2018-11-01","2023-10-31"
"BAYES-KNOWLEDGE","Effective Bayesian Modelling with Knowledge before Data","Norman Fenton","QUEEN MARY UNIVERSITY OF LONDON","This project aims to improve evidence-based decision-making. What makes it radical is that it plans to do this in situations (common for critical risk assessment problems) where there is little or even no data, and hence where traditional statistics cannot be used.  To address this problem Bayesian analysis, which enables domain experts to supplement observed data with subjective probabilities, is normally used. As real-world problems typically involve multiple uncertain variables, Bayesian analysis is extended using a technique called Bayesian networks (BNs). But, despite many great benefits, BNs have been under-exploited, especially in areas where they offer the greatest potential for improvements (law, medicine and systems engineering).  This is mainly because of widespread resistance to relying on subjective knowledge. To address this problem much current research assumes sufficient data are available to make the expert’s input minimal or even redundant; with such data it may be possible to ‘learn’ the underlying BN model. But this approach offers nothing when there is limited or no data. Even when ‘big’ data are available the resulting models may be superficially objective but fundamentally flawed as they fail to capture the underlying causal structure that only expert knowledge can provide.

Our solution is to develop a method to systemize the way expert driven causal BN models can be built and used effectively either in the absence of data or as a means of determining what future data is really required. The method involves a new way of framing problems and extensions to BN theory, notation and tools. Working with relevant domain experts, along with cognitive psychologists, our methods will be developed and tested experimentally on real-world critical decision-problems in medicine, law, forensics, and transport. As the work complements current data-driven approaches, it will lead to improved BN modelling both when there is extensive data as well as none.","1572562","2014-04-01","2018-03-31"
"BCCI","Bidirectional cortical communication interface","Wolfgang Rosenstiel","EBERHARD KARLS UNIVERSITAET TUEBINGEN","This project aims at establishing bidirectional communication via the cortical areas of the brain. In recent years there have been extensive research efforts for establishing an efferent pathway from the brain by means of cortical recordings to allow patients suffering from amyotrophic lateral sclerosis (ALS), stroke or high spinal cord lesions to interact with their environment (Birbaumer and Cohen, 2007; Wolpaw et al., 2002). As an extension this project will investigate the possibility of an afferent pathway to the brain by means of cortical stimulation, since it is ex-pected that stimulation might help to increase the information transfer rate for the efferent path-way. To achieve this there are two possible stimulation paradigms to be investigated. The first is based on the identification of optimal brain states for communication and the active maintenance of these states by stimulation. Inspired by classical conditioning, the second stimulation paradigm seeks to support and accelerate the rehabilitation process in stroke patients, as well as the learning process needed for the efferent communication pathway in ALS patients. By development of visual cortical prostheses (Schmidt et al., 1996) it became apparent that there are several fundamental problems related to cortical stimulation, which need to be solved before it is possible to evoke well-defined neural responses by stimulation - a prerequisite of the stimulation paradigms mentioned above. To overcome these problems it is envisaged to adapt stimulus parameters based on the current background brain activity by a feedback system in real time. Leveraging prior knowledge from microstimulation studies the feasibility of this approach will be evaluated by simultaneous stimulation and recording from ECoG grids and accompanied by the development of suitable algorithms.","1169400","2009-02-01","2012-10-31"
"BEAM-ME-UP","From Radio-Frequency to Giga-Bit
Optical- and Quantum-Wireless","Lajos Hanzo","UNIVERSITY OF SOUTHAMPTON","The majority of the globe's population carries a mobile phone, but with the increasing proliferation of smart phones and tablet-computers the tele-traffic is predicted to grow 1000-fold over the next decade, especially, when aiming for creating the impression of ubiquitous and flawless 'tele-presence' based on crisp, three-dimensional (3D) video with its sense of joy and wonder. For tele-presence to become a reality requires a further quantum-leap from the popular 3G/4G smart phones and tablet-computers.  This project will create the link-level enabling techniques of this transformational quantum leap to immersive Giga-bit 3D video communications, relying on Optical Wireless (OW) hotspots and their ad hoc networking.

As a result, the Beam-Me-Up project will contribute to job- and wealth-creation in numeorus ways, as exemplified by the often-quoted economic benefits of 3G/4G phones on businesses.  From an environmental perspective, flawless tele-presence has the potential of eliminating millions of flights/trips and hence will considerably reduce CO2 emissions, whilst reducing the related business-costs as well as saving precious time for the work-force.  However, the transfiguration of the voice-only phone into today's intelligent smart phone was facilitated by a 1000-fold transmission-rate increase, which would result in a proportionally increased power consumption, CO2 emissions and in a soaring energy-bill.  Tele-presence based on crisp Avatar-style 3D video has even higher bitrates and energy consumption. These radically new high-rate 3D tele-presence services can no longer be accommodated in the severely congested Radio Frequency (RF) band.

Hence the project will create a suite of new OW system components, operating in the visible-light domain and will conceive low-power, low-complexity OW solutions to enable immersive Giga-bit 3D wireless video communications over heterogeneous networks.","2470416","2013-03-01","2018-02-28"
"BEAMING","Detecting massive-planet/brown-dwarf/low-mass-stellar companions with the beaming effect","Moshe Zvi Mazeh","TEL AVIV UNIVERSITY","""I propose to lead an international observational effort to characterize the population of massive planets, brown dwarf and stellar secondaries orbiting their parent stars with short periods, up to 10-30 days. The effort will utilize the superb, accurate, continuous lightcurves of more than hundred thousand stars obtained recently by two space missions – CoRoT and Kepler. I propose to use these lightcurves to detect non-transiting low-mass companions with a new algorithm, BEER, which I developed recently together with Simchon Faigler. BEER searches for the beaming effect, which causes the stellar intensity to increase if the star is moving towards the observer. The combination of the beaming effect with other modulations induced by a low-mass companion produces periodic modulation with a specific signature, which is used to detect small non-transiting companions. The accuracy of the space mission lightcurves is enough to detect massive planets with short periods. The proposed project is equivalent to a radial-velocity survey of tens of thousands of stars, instead of the presently active surveys which observe only hundreds of stars.
We will use an assortment of telescopes to perform radial velocity follow-up observations in order to confirm the existence of the detected companions, and to derive their masses and orbital eccentricities. We will discover many tens, if not hundreds, of new massive planets and brown dwarfs with short periods, and many thousands of new binaries. The findings will enable us to map the mass, period, and eccentricity distributions of planets and stellar companions, determine the upper mass of planets, understand the nature of the brown-dwarf desert, and put strong constrains on the theory of planet and binary formation and evolution.""","1737600","2012-01-01","2016-12-31"
"BI-DSC","Building Integrated Dye Sensitized Solar Cells","Adélio Miguel Magalhaes Mendes","UNIVERSIDADE DO PORTO","In the last decade, solar and photovoltaic (PV) technologies have emerged as a potentially major technology for power generation in the world. So far the PV field has been dominated by silicon devices, even though this technology is still expensive.Dye-sensitized solar cells (DSC) are an important type of thin-film photovoltaics due to their potential for low-cost fabrication and versatile applications, and because their aesthetic appearance, semi-transparency and different color possibilities.This advantageous characteristic makes DSC the first choice for building integrated photovoltaics.Despite their great potential, DSCs for building applications are still not available at commercial level. However, to bring DSCs to a marketable product several developments are still needed and the present project targets to give relevant answers to three key limitations: encapsulation, glass substrate enhanced electrical conductivity and more efficient and low-cost raw-materials. Recently, the proponent successfully addressed the hermetic devices sealing by developing a laser-assisted glass sealing procedure.Thus, BI-DSC proposal envisages the development of DSC modules 30x30cm2, containing four individual cells, and their incorporation in a 1m2 double glass sheet arrangement for BIPV with an energy efficiency of at least 9% and a lifetime of 20 years. Additionally, aiming at enhanced efficiency of the final device and decreased total costs of DSCs manufacturing, new materials will be also pursued. The following inner-components were identified as critical: carbon-based counter-electrode; carbon quantum-dots and hierarchically TiO2 photoelectrode. It is then clear that this project is divided into two research though parallel directions: a fundamental research line, contributing to the development of the new generation DSC technology; while a more applied research line targets the development of a DSC functional module that can be used to pave the way for its industrialization.","1989300","2013-03-01","2018-08-31"
"BIC","Cavitation across scales: following Bubbles from Inception to Collapse","Carlo Massimo Casciola","UNIVERSITA DEGLI STUDI DI ROMA LA SAPIENZA","Cavitation is the formation of vapor cavities inside a liquid due to low pressure. Cavitation is an ubiquitous and destructive phenomenon common to most engineering applications that deal with flowing water. At the same time, the extreme conditions realized in cavitation are increasingly exploited in medicine, chemistry, and biology. What makes cavitation unpredictable is its multiscale nature: nucleation of vapor bubbles heavily depends on micro- and nanoscale details; mesoscale phenomena, as bubble collapse, determine relevant macroscopic effects, e.g., cavitation damage. In addition, macroscopic flow conditions, such as turbulence, have a major impact on it.

The objective of the BIC project is to develop the lacking multiscale description of cavitation, by proposing new integrated numerical methods capable to perform quantitative predictions. The detailed and physically sound understanding of the multifaceted phenomena involved in cavitation (nucleation, bubble growth, transport, and collapse in turbulent flows) fostered by BIC project will result in new methods for designing fluid machinery, but also therapies in ultrasound medicine and chemical reactors. The BIC project builds upon the exceptionally broad experience of the PI and of his research group in numerical simulations of flows at different scales that include advanced atomistic simulations of nanoscale wetting phenomena, mesoscale models for multiphase flows, and particle-laden turbulent flows. The envisaged numerical methodologies (free-energy atomistic simulations, phase-field models, and Direct Numerical Simulation of bubble-laden flows) will be supported by targeted experimental activities, designed to validate models and characterize realistic conditions.","2491200","2014-02-01","2019-01-31"
"BIMPC","Biologically-Inspired Massively-Parallel Computation","Stephen Byram Furber","THE UNIVERSITY OF MANCHESTER","""We aim to establish a world-leading research capability in Europe for advancing novel models of asynchronous computation based upon principles inspired by brain function. This work will accelerate progress towards an understanding of how the potential of brain-inspired many-core architectures may be harnessed. The results will include new brain-inspired models of asynchronous computation and new brain- inspired approaches to fault-tolerance and reliability in complex computer systems.

Many-core processors are now established as the way forward for computing from embedded systems to supercomputers. An emerging problem with leading-edge silicon technology is a reduction in the yield and reliability of modern processors due to high variability in the manufacture of the components and interconnect as transistor geometries shrink towards atomic scales. We are faced with the longstanding problem of how to make use of a potentially large array of parallel processors, but with the new constraint that the individual elements are the system are inherently unreliable.

The human brain remains as one of the great frontiers of science – how does this organ upon which we all depend so critically actually do its job? A great deal is known about the underlying technology – the neuron – and we can observe large-scale brain activity through techniques such as magnetic resonance imaging, but this knowledge barely starts to tell us how the brain works. Something is happening at the intermediate levels of processing that we have yet to begin to understand, but the essence of the brain's massively-parallel information processing capabilities and robustness to component failure lies in these intermediate levels.

These two issues draws us towards two high-level research questions:

• Can our growing understanding of brain function point the way to more efficient parallel, fault-tolerant computing?
• Can massively parallel computing resources accelerate our understanding of brain function""","2399761","2013-03-01","2018-02-28"
"BIO-H-BORROW","Biocatalytic Amine Synthesis via Hydrogen Borrowing","Nicholas TURNER","THE UNIVERSITY OF MANCHESTER","Amine containing compounds are ubiquitous in everyday life and find applications ranging from polymers to pharmaceuticals. The vast majority of amines are synthetic and manufactured on large scale which creates waste as well as requiring high temperatures and pressures. The increasing availability of biocatalysts, together with an understanding of how they can be used in organic synthesis (biocatalytic retrosynthesis), has stimulated chemists to consider new ways of making target molecules. In this context, the iterative construction of C-N bonds via biocatalytic hydrogen borrowing represents a powerful and unexplored way to synthesise a wide range of target amine molecules in an efficient manner. Hydrogen borrowing involves telescoping redox neutral reactions together using only catalytic amounts of hydrogen.
In this project we will engineer the three key target biocatalysts (reductive aminase, amine dehydrogenase, alcohol dehydrogenase) required for biocatalytic hydrogen borrowing such that they possess the required regio-, chemo- and stereo-selectivity for practical application. Recently discovered reductive aminases (RedAms) and amine dehydrogenases (AmDHs) will be engineered for enantioselective coupling of alcohols (1o, 2o) with ammonia/amines (1o, 2o, 3o) under redox neutral conditions. Alcohol dehydrogenases will be engineered for low enantioselectivity. Hydrogen borrowing requires mutually compatible cofactors shared by two enzymes and in some cases will require redesign of cofactor specificity. Thereafter we shall develop conditions for the combined use of these biocatalysts under hydrogen borrowing conditions (catalytic NADH, NADPH), to enable the conversion of simple and sustainable feedstocks (alcohols) into amines using ammonia as the nitrogen source.
The main deliverables of BIO-H-BORROW will be a set of novel engineered biocatalysts together with redox neutral cascades for the synthesis of amine products from inexpensive and renewable precursors.","2337548","2017-06-01","2022-05-31"
"BioBlood","Development of a Bio-Inspired Blood Factory for Personalised Healthcare","Athanasios Mantalaris","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","Personalized medicine is a medical model that proposes the customization of healthcare, with decisions and practices being tailored to the individual patient by use of patient-specific information and/or application of patient-specific cell-based therapies. BioBlood aims to deliver personalised healthcare through a “step change” in the clinical field of haemato-oncology. BioBlood represents an engineered bio-inspired integrated experimental/modelling platform for normal and abnormal haematopoiesis that receives disease & patient input (patient primary cells & patient/disease-specific data) and will produce cellular (red blood cell product) and drug (optimal drug treatment) therapies as its output. Blood supply to meet demand is the primary challenge for Blood Banks and requires significant resources to avoid shortages and ensure safety. An alternative, practical and cost-effective solution to conventional donated blood is essential to reduce patient morbidity and mortality, stabilise and guarantee the donor supply, limit multiple donor exposures, reduce risk of infection of known or as yet unidentified pathogens, and ensure a robust and safe turn-around for blood supply management. BioBlood aims to meet this challenge by developing a novel in vitro platform for the mass production of RBCs for clinical use.  More than £32b/year is spent to develop and bring new drugs to market, which takes 14 years. Most patients diagnosed with leukaemias are unable to tolerate treatment and would benefit from novel agents. There is a need to optimise current treatment schedules for cancers such as AML to limit toxicities and improve clinical trial pathways for new drugs to enable personalised healthcare. BioBlood’s   in vitro & in silico platform would be a powerful tool to tailor treatments in a patient- and leukaemia-specific chemotherapy schedule by considering the level of toxicity to the specific individual and treatment efficiency for the specific leukaemia a priori.","2498903","2014-01-01","2018-12-31"
"BIOCARB","Carbonate Biomineralization in the Marine Environment: Paleo-climate proxies and the origin of vital effects","Anders Meibom","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","This interdisciplinary proposal has the objective to greatly enhance our understanding of fundamental biomineralization processes involved in the formation of calcium carbonates by marine organisms, such as corals, foraminifera and bivalves, in order to better understand vital effects. This is essential to the application of these carbonates as proxies for global (paleo-) environmental change. The core of the proposal is an experimental capability that I have pioneered during 2008: Dynamic stable isotopic labeling during formation of carbonate skeletons, tests, and shells, combined with NanoSIMS imaging. The NanoSIMS ion microprobe is a state-of-the-art analytical technology that allows precise elemental and isotopic imaging with a spatial resolution of ~100 nanometers. NanoSIMS imaging of the isotopic label(s) in the resulting biocarbonates and in associated cell-structures will be used to uncover cellular-level transport processes, timescales of formation of different biocarbonate components, as well as trace-elemental and isotopic fractionations. This will uncover the origin of vital effects. With this proposal, I establish a new scientific frontier and guarantee European leadership. The technical and scientific developments resulting from this work are broadly applicable and will radically change scientific ideas about marine carbonate biomineralization and compositional vital effects.","2182000","2010-07-01","2015-06-30"
"BIOCOMPLEX","Physical Aspects of the Evolution of Biological Complexity","Raymond Ethan Goldstein","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","One of the most fundamental issues in evolutionary biology is the nature of transitions from single cell organisms to multicellular ones, with accompanying cellular differentiation and specialization. Not surprisingly for microscopic life in fluid environments, many of the relevant physical considerations involve diffusion, mixing, and sensing, for the efficient exchange of nutrients and metabolites with the environment is one of the most basic features of life. This proposal describes a combination of experimental and theoretical research aimed at some of the key mysteries surrounding transport and sensing by and in complex, multicellular organisms, and the implications of those findings for the explanation of driving forces behind transitions to multicellularity. There are two main components of the research. The first involves studies of single and multicellular algae which serves as model systems for allometric scaling laws in evolution. Of particular importance are the synchronization dynamics of the eukaryotic flagella that provide motility, enhance nutrient transport, and allow phototaxis in these organisms. The second thrust involves investigation of the ubiquitous phenomenon of cytoplasmic streaming in aquatic and terrestrial plants. Despite decades of research, there is no clear consensus on the metabolic role of this persistent circulation of the fluid contents of cell. Building on recent theoretical developmnts we will study its implications for internal transport and mixing, homeostasis, and development in large cells. In each case, state-of-the art experimental methods from physics, fluid dynamics, and cell biology will be used in combination with advanced theoretical methods for the study of the stochastic nonlinear PDEs that form the natural description of these systems.","2500000","2010-01-01","2015-12-31"
"BioELCell","Bioproducts Engineered from Lignocelluloses: from plants and upcycling to next generation materials","Orlando Rojas Gaona","AALTO KORKEAKOULUSAATIO SR","BioELCell will deliver ground-breaking approaches to create next material generation based on renewable resources, mainly cellulose and lignin micro- and nano-particles (MNC, MNL). Our action will disassemble and re-engineer these plant-based polymers into functional materials that will respond to the demands of the bioeconomy of the future, critically important to Europe and the world. My ambitious, high gain research plan is underpinned in the use of multiphase systems with ultra-low interfacial tension to facilitate nanocellulose liberation and atomization of lignin solution streams into spherical particles. 

BioELCell will design novel routes to control MNC and MNL reassembly in new 1-D, 2-D and 3-D structures. The systematic methodologies that I propose will address the main challenges for lignocellulose processing and deployment, considering the important effects of interactions with water. This BioELCell action presents a transformative approach by integrating complementary disciplines that will lead to a far-reaching understanding of lignocellulosic biopolymers and solve key challenges in their use, paving the way to functional product development. Results of this project permeates directly or indirectly in the grand challenges for engineering, namely, water use, carbon sequestration, nitrogen cycle, food and advanced materials. Indeed, after addressing the key fundamental elements of the research lines, BioELCell vindicates such effects based on rational use of plant-based materials as a sustainable resource, making possible the generation of new functions and advanced materials.

BioELCell goes far beyond what is known today about cellulose and lignin micro and nano-particles, some of the most promising materials of our century, which are emerging as key elements for the success of a sustainable society.","2486182","2018-08-01","2023-07-31"
"BIOGEOS","Bio-mediated Geo-material Strengthening for engineering applications","Lyesse LALOUI","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Given the increasing scarcity of suitable land for development, soil strengthening technologies have emerged in the past decade and go hand-in-hand with the implementation of the majority of foundation solutions. The goal is to alter the soil structure and its mechanical properties for ultimately securing the integrity of structures. The BIOGEOS project puts the focus on bio-mediated soil improvement, which falls within the broader framework of multi-physical processes in geo-mechanics. The goal of the project is to engineer a novel, natural material under controlled processes, for ultimately providing solutions to real problems in the geo-engineering and geo-energy fields by advancing knowledge around complex multi-physical phenomena in porous media. The bio-cemented geo-material, which is produced by carefully integrating the metabolic activity of native soil bacteria, is produced through the bio-mineralization of calcite bonds, which act as natural cementation for endowing the subsurface with real cohesion and increased resistance. A principal characteristic of the project is its multi-scale approach through advanced experimentation to identify the main physical mechanisms involved in the formation of the bio-mineralized bonds and their behaviour under mechanical loading. The development of such a bio-mediated technology will lead to innovative applications in a series of engineering problems such as the restoration of weak foundations, seismic retrofitting, erosion protection, and the enhancement of heat transfer in thermo-active geo-structures. The project foresees to adopt multiple loading conditions for its laboratory characterization and ultimately pass to the large experimental scale. BIOGEOS further aims to provide new knowledge around the way we perceive materials in relation with their micro-structure by implementing state-of-the-art inspection of the material’s structure in 3D space and subsequent prediction of their behaviour through numerical tools.","2497115","2018-11-01","2023-10-31"
"BIOINCMED","Bioinorganic Chemistry for the Design of New Medicines","Peter John Sadler","THE UNIVERSITY OF WARWICK","Bioinorganic chemistry is a rapidly expanding area of research, but the potential for the therapeutic application of metal complexes is highly underdeveloped. The basic principles required to guide the development of metal-containing therapeutic agents are lacking, despite the unique therapeutic opportunities which they offer. It is the goal of the proposed research to establish basic principles of medicinal coordination chemistry of metals that will allow the rational screening of future metallopharmaceuticals. We propose to utilize the power of inorganic chemistry to provide new knowledge of and new approaches for intervention in biological systems. This will be based on improved understanding of reactions of metal complexes under physiological conditions, on improving the specificity of their interactions, and gaining control over the potential toxicity of synthetic metal complexes. The research programme is highly interdisciplinary involving chemistry, physics, biology and pharmacology, with potential for the discovery of truly novel medicines, especially for the treatment of diseases and conditions which are currently intractable, such as cancer. The challenging and ambitious goals of the present work involve transition metal complexes with novel chemical and biochemical mechanisms of action. They will contain novel features which allow them (i) to be selectively activated by light in cells, or (ii) to be activated by a structural transition, or (ii) exhibit catalytic activity in cells. This ground-breaking research potentially has a very high impact and is based on recent discoveries in the applicant s laboratory. A feature of the programme is the use of state-of-the-art-and-beyond methodology to advance knowledge of medicinal metal coordination chemistry.","1565397","2010-07-01","2015-12-31"
"BIOMATE","Soft Biomade Materials: Modular Protein Polymers and their nano-assemblies","Martinus Abraham Cohen Stuart","WAGENINGEN UNIVERSITY","From a polymer chemistry perspective, the way in which nature produces its plethora of different proteins is a miracle of precision: the synthesis of each single molecule is directed by the sequence information chemically coded in DNA. The present state of recombinant DNA technology should in principle allow us to make genes that code for entirely new, very sophisticated amino acid polymers, which are chosen and designed by man to serve as new polymer materials. It has been shown that it is indeed possible to make use of the protein biosynthetic machinery and produce such de novo protein polymers, but it is not clear what their potentials are in terms of new materials with desired functionalities.
I propose to develop a new class of protein polymers, chosen such that they form nanostructured materials by triggered folding and multimolecular assembly. The plan is based on three innovative ideas: (i) each new protein polymer will be constructed from a limited set of selected amino acid sequences, called modules (hence the term modular protein polymers) (ii) new, high-yield fermentation strategies will be developed so that polymers will become available in significant quantities for evaluation and application; (iii)  the design of modular protein polymers is carried out as a cyclic process in which sequence selection, construction of artificial genes, optimisation of fermentation for high yield, studying polymer folding and assembly, and modelling of the nanostructure by molecular simulation are all logically connected, allowing efficient selection of target sequences.
This project is a cross-road. It brings together biotechnology and polymer science, creating a unique set of biomaterials for medical and pharmaceutical use, that can be easily extended into a manifold of biofunctional materials. Moreover, it will provide us with fresh tools and valuable insights to tackle the subtle relations between protein sequence and folding.","2497044","2011-05-01","2016-04-30"
"BioMet","Selective Functionalization of Saturated Hydrocarbons","Ilan MAREK","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","Despite that C–H functionalization represents a paradigm shift from the standard logic of organic synthesis, the selective activation of non-functionalized alkanes has puzzled chemists for centuries and is always referred to one of the remaining major challenges in chemical sciences. Alkanes are inert compounds representing the major constituents of natural gas and petroleum. Converting these cheap and widely available hydrocarbon feedstocks into added-value intermediates would tremendously affect the field of chemistry. For long saturated hydrocarbons, one must distinguish between non-equivalent but chemically very similar alkane substrate C−H bonds, and for functionalization at the terminus position, one must favor activation of the stronger, primary C−H bonds at the expense of weaker and numerous secondary C-H bonds. The goal of this work is to develop a general principle in organic synthesis for the preparation of a wide variety of more complex molecular architectures from saturated hydrocarbons. In our approach, the alkane will first be transformed into an alkene that will subsequently be engaged in a metal-catalyzed hydrometalation/migration sequence. The first step of the sequence, ideally represented by the removal of two hydrogen atoms, will be performed by the use of a mutated strain of Rhodococcus. The position and geometry of the formed double bond has no effect on the second step of the reaction as the metal-catalyzed hydrometalation/migration will isomerize the double bond along the carbon skeleton to selectively produce the primary organometallic species. Trapping the resulting organometallic derivatives with a large variety of electrophiles will provide the desired functionalized alkane. This work will lead to the invention of new, selective and efficient processes for the utilization of simple hydrocarbons and valorize the synthetic potential of raw hydrocarbon feedstock for the environmentally benign production of new compounds and new materials.","2499375","2018-11-01","2023-10-31"
"BIOMOL. SIMULATION","Development of multi-scale molecular models, force fields and computer software for biomolecular simulation","Willem Frederik Van Gunsteren","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","During the past decades the PI has helped shape the research field of computer simulation of biomolecular systems at the atomic level. He has carried out one of the first molecular dynamics (MD) simulations of proteins, and has since then contributed many different methodological improvements and developed one of the major atomic-level force fields for simulations of proteins, carbohydrates, nucleotides and lipids. Methodology and force field have been implemented in a set of programs called GROMOS (GROningen MOlecular Simulation package), which is currently used in hundreds of academic and industrial research groups from over 50 countries on all continents. It is proposed to develop a next generation of molecular models, force fields, multi-scaling simulation methodology and software for biomolecular simulations which is at least an order of magnitude more accurate in terms of energetics, and which is 1000 times more efficient through the use of coarse-grained molecular models than the currently available software and models.","1320000","2008-11-01","2014-09-30"
"BIOTENSORS","Biomedical Data Fusion using Tensor based Blind Source Separation","Sabine Jeanne A Van Huffel","KATHOLIEKE UNIVERSITEIT LEUVEN",""": the quest for a general functional tensor framework for blind source separation

Our overall objective is the development of a general functional framework for solving tensor based blind source separation (BSS) problems in biomedical data fusion, using tensor decompositions (TDs) as basic core. We claim that TDs will allow the extraction of fairly complicated sources of biomedical activity from fairly complicated sets of uni- and multimodal data. The power of the new techniques will be demonstrated for three well-chosen representative biomedical applications for which extensive expertise and fully validated datasets are available in the PI’s team, namely:
•	Metabolite quantification and brain tumour tissue typing using Magnetic Resonance  Spectroscopic Imaging,
•	Functional monitoring including seizure detection and polysomnography,
•	Cognitive brain functioning and seizure zone localization using simultaneous Electroencephalography-functional MR Imaging integration.

Solving these challenging problems requires that algorithmic progress is made in several directions:
•	Algorithms need to be based on multilinear extensions of numerical linear algebra.
•	New grounds for separation, such as representability in a given function class, need to be explored.
•	Prior knowledge needs to be exploited via appropriate health relevant constraints.
•	Biomedical data fusion requires the combination of  TDs, coupled via relevant constraints.
•	Algorithms for TD updating are important for continuous long-term patient monitoring.
The algorithms are eventually integrated in an easy-to-use open source software platform that is general enough for use in other BSS applications.

Having been involved in biomedical signal processing over a period of 20 years, the PI has a good overview of the field and the opportunities. By working directly at the forefront in close collaboration with the clinical scientists who actually use our software, we can have a huge impact.""","2500000","2014-04-01","2019-03-31"
"BLOWDISOL","""BLOW UP, DISPERSION AND SOLITONS""","Franck Merle","UNIVERSITE DE CERGY-PONTOISE","""Many physical models involve nonlinear dispersive problems, like wave
or laser propagation, plasmas, ferromagnetism, etc. So far, the mathematical under-
standing of these equations is rather poor. In particular, we know little about the
detailed qualitative behavior of their solutions. Our point is that an apparent com-
plexity hides universal properties of these models; investigating and uncovering such
properties has started only recently. More than the equations themselves, these univer-
sal properties are essential for physical modelisation.
By considering several standard models such as the nonlinear Schrodinger, nonlinear
wave, generalized KdV equations and related geometric problems, the goal of this pro-
posal is to describe the generic global behavior of the solutions and the proﬁles which
emerge either for large time or by concentration due to strong nonlinear eﬀects, if pos-
sible through a few relevant solutions (sometimes explicit solutions, like solitons). In
order to do this, we have to elaborate diﬀerent mathematical tools depending on the
context and the speciﬁcity of the problems. Particular emphasis will be placed on
- large time asymptotics for global solutions, decomposition of generic solutions into
sums of decoupled solitons in non integrable situations,
- description of critical phenomenon for blow up in the Hamiltonian situation, stable
or generic behavior for blow up on critical dynamics, various relevant regularisations of
the problem,
- global existence for defocusing supercritical problems and blow up dynamics in the
focusing cases.
We believe that the PI and his team have the ability to tackle these problems at present.
The proposal will open whole ﬁelds of investigation in Partial Diﬀerential Equations in
the future, clarify and simplify our knowledge on the dynamical behavior of solutions
of these problems and provide Physicists some new insight on these models.""","2079798","2012-04-01","2017-03-31"
"BORYLENEFUN","The versatile metal-boron multiple bond: application of borylenes to metathesis, catalysis, and macromolecules","Holger Christoph Braunschweig","JULIUS-MAXIMILIANS-UNIVERSITAT WURZBURG","Borylated molecules and polymers are of great interest due to their broad application in organic synthesis and materials science. The functionalisation of organic substrates with boryl groups R2B  is based on classical synthetic methods e.g. hydro- and diboration of C-C multiple bonds. Likewise, borylenes B-R should be versatile reagents for corresponding functionalisations, however, the chemistry of such species remained unexplored due to their high instability.
Pioneering work in our laboratories has proven that complexes of the type [LxM=B-R] not only stabilise elusive borylenes B-R in the coordination sphere of various transition metals but, more importantly, serve as unprecedented sources for these species under ambient conditions in condensed phase. Thus, the major objective of the current proposal is to establish novel reactivity patterns based on B-R fragments for the functionalisation of organometallic and organic substrates. Particular attention will be paid to the synthesis of novel molecular and polymeric species with significant potential as materials. Given the pronounced importance of boron containing species in organic synthesis, catalysis and materials science, the proposed project is expected to have a significant impact on these areas of applied molecular science. In addition, a wide range of fundamental aspects will be covered, targeting e.g. novel conjugated cyclic systems or molecules with unprecedented boron-element combinations.
The following subjects will be pursued:
1)Cationic and anionic dimetalloborylenes as complementary building blocks in synthesis
2)Application of borylene metathesis in stoichiometric and catalytic transformations
3)Borylene transfer for organometallic synthesis and borylene based pi-conjugated materials","2496762","2011-05-01","2016-04-30"
"BOTMED","Microrobotics and Nanomedicine","Bradley James Nelson","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","The introduction of minimally invasive surgery in the 1980’s created a paradigm shift in surgical procedures. Health care is now in a position to make a more dramatic leap by integrating newly developed wireless microrobotic technologies with nanomedicine to perform precisely targeted, localized endoluminal techniques. Devices capable of entering the human body through natural orifices or small incisions to deliver drugs, perform diagnostic procedures, and excise and repair tissue will be used. These new procedures will result in less trauma to the patient and faster recovery times, and will enable new therapies that have not yet been conceived. In order to realize this, many new technologies must be developed and synergistically integrated, and medical therapies for which the technology will prove successful must be aggressively pursued.

This proposed project will result in the realization of animal trials in which wireless microrobotic devices will be used to investigate a variety of extremely delicate ophthalmic therapies. The therapies to be pursued include the delivery of tissue plasminogen activator (t-PA) to blocked retinal veins, the peeling of epiretinal membranes from the retina, and the development of diagnostic procedures based on mapping oxygen concentration at the vitreous-retina interface. With successful animal trials, a path to human trials and commercialization will follow. Clearly, many systems in the body have the potential to benefit from the endoluminal technologies that this project considers, including the digestive system, the circulatory system, the urinary system, the central nervous system, the respiratory system, the female reproductive system and even the fetus. Microrobotic retinal therapies will greatly illuminate the potential that the integration of microrobotics and nanomedicine holds for society, and greatly accelerate this trend in Europe.","2498044","2011-04-01","2016-03-31"
"BPT","BEYOND PLATE TECTONICS","Trond Helge Torsvik","UNIVERSITETET I OSLO","Plate tectonics characterises the complex and dynamic evolution of the outer shell of the Earth in terms of rigid plates. These tectonic plates overlie and interact with the Earth's mantle, which is slowly convecting owing to energy released by the decay of radioactive nuclides in the Earth's interior. Even though links between mantle convection and plate tectonics are becoming more evident, notably through subsurface tomographic images, advances in mineral physics and improved absolute plate motion reference frames, there is still no generally accepted mechanism that consistently explains plate tectonics and mantle convection in one framework. We will integrate plate tectonics into mantle dynamics and develop a theory that explains plate motions quantitatively and dynamically. This requires consistent and detailed reconstructions of plate motions through time (Objective 1).

A new model of plate kinematics will be linked to the mantle with the aid of a new global reference frame based on moving hotspots and on palaeomagnetic data. The global reference frame will be corrected for true polar wander in order to develop a global plate motion reference frame with respect to the mantle back to Pangea (ca. 320 million years) and possibly Gondwana assembly (ca. 550 million years). The resulting plate reconstructions will constitute the input to subduction models that are meant to test the consistency between the reference frame and subduction histories. The final outcome will be a novel global subduction reference frame, to be used to unravel links between the surface and deep Earth (Objective 2).","2499010","2011-05-01","2016-04-30"
"BrainBIT","All-optical brain-to-brain behaviour and information transfer","Francesco PAVONE","UNIVERSITA DEGLI STUDI DI FIRENZE","Exchange of information between different brains usually takes place through the interaction between bodies and the external environment. The ultimate goal of this project is to establish a novel paradigm of brain-to-brain communication based on direct full-optical recording and controlled stimulation of neuronal activity in different subjects. To pursue this challenging objective, we propose to develop optical technologies well beyond the state of the art for simultaneous neuronal “reading” and “writing” across large volumes and with high spatial and temporal resolution, targeted to the transfer of advantageous behaviour in physiological and pathological conditions. 
We will perform whole-brain high-resolution imaging in zebrafish larvae to disentangle the activity patterns related to different tasks. We will then use these patterns as stimulation templates in other larvae to investigate spatio-temporal subject-invariant signatures of specific behavioural states. This ‘pump and probe’ strategy will allow gaining deep insights into the complex relationship between neuronal activity and subject behaviour.
To move towards clinics-oriented studies on brain stimulation therapies, we will complement whole-brain experiments in zebrafish with large area functional imaging and optostimulation in mammals. We will investigate all-optical brain-to-brain information transfer to boost an advantageous behaviour, i.e. motor recovery, in a mouse model of stroke. Mice showing more effective responses to rehabilitation will provide neuronal activity templates to be elicited in other animals, in order to increase rehabilitation efficiency.
We strongly believe that the implementation of new technologies for all-optical transfer of behaviour between different subjects will offer unprecedented views of neuronal activity in healthy and injured brain, paving the way to more effective brain stimulation therapies.","2370250","2016-12-01","2021-11-30"
"BREAD","Breaking the curse of dimensionality: numerical challenges in high dimensional analysis and simulation","Albert Cohen","UNIVERSITE PIERRE ET MARIE CURIE - PARIS 6","""This project is concerned with problems that involve a very large number of variables, and whose efficient numerical treatment is challenged by the so-called curse of dimensionality, meaning that computational complexity increases exponentially in the variable dimension.

The PI intend to establish in his host institution a scientific leadership on the mathematical understanding and numerical treatment of these problems, and to contribute to the development of this area of research through international collaborations, organization of workshops and research schools, and training of postdocs and PhD students.

High dimensional problems are ubiquitous in an increasing number of areas of scientific computing, among which statistical or active learning theory, parametric and stochastic partial differential equations, parameter optimization in numerical codes. There is a high demand from the industrial world of efficient numerical methods for treating such problems.
The practical success of various numerical algorithms, that have been developed in recent years in these application areas, is often limited to moderate dimensional setting.
In addition, these developments tend to be, as a rule, rather problem specific and not always founded on a solid mathematical analysis.

The central scientific objectives of this project are therefore: (i) to identify fundamental mathematical principles behind overcoming the curse of dimensionality, (ii) to understand how these principles enter in relevant instances of the above applications, and (iii) based on the these principles beyond particular problem classes, to develop broadly applicable numerical strategies that benefit from such mechanisms.

The performances of these strategies should be provably independent of the variable dimension, and in that sense break the curse of dimensionality. They will be tested on both synthetic benchmark tests and real world problems coming from the afore-mentioned applications.""","1848000","2014-01-01","2018-12-31"
"BRIDGES","Bridging Non-Equilibrium Problems: From the Fourier Law to Gene Expression","Jean-Pierre Eckmann","UNIVERSITE DE GENEVE","My goal is to study several important open mathematical problems in non-equilibrium (NEQ) systems and to build a bridge between these problems and NEQ aspects of soft sciences, in particular biological questions. Traffic on this bridge is going to be two-way, the mathematics carrying a long history as a language of science towards the soft sciences, and the soft sciences fruitfully asking new questions and building new paradigms for mathematical research.
Out-of-equilibrium systems pose several fascinating problems: The Fourier law which says that resistance of a wire is proportional to its length is still presenting hard problems for research, and even the existence and the convergence to a NEQ steady state are continuously posing new puzzles, as do questions of smoothness and correlations of such states. These will be addressed with stochastic differential equations, and with particlescatterer systems, both canonical and grand-canonical. The latter are extensions of the well-known Lorentz gas and the study of hyperbolic billiards.
Another field where NEQ plays an important role is the study of glassy systems. They were studied with molecular dynamics (MD) but I have used a topological variant, which mimics astonishingly well what happens in MD simulations. The aim is to extend this to 3 dimensions, where new problems appear.
Finally, I will apply the NEQ studies to biological systems: How a system copes with the varying environment,adapting in this way to a novel type of NEQ. I will study networks of communication among neurons,which are like random graphs with the additional property of being embedded, and the arrangement of genes on chromosomes in such a way as to optimize the adaptation to the different cell types which must be produced using the same genetic information.
I will answer such questions with students and collaborators, who will specialize in the subprojects but will interact with my help across the common bridge.","2135385","2012-04-01","2017-07-31"
"BROWSE","Beam-steered Reconfigurable Optical-Wireless System for Energy-efficient communication","Antonius Marcellus Jozef Koonen","TECHNISCHE UNIVERSITEIT EINDHOVEN","The exploding need for wireless communication capacity is getting beyond the capabilities of traditional radio techniques. The available radio bandwidth gets exhausted, wireless devices start interfering with each other in this overcrowded radio spectrum, and high-capacity radio is power-hungry. Optics can offer a breakthrough, by means of the huge bandwidth of its spectrum, together with intelligent networking.
Our ambition is to make a giant step forward in wireless communications, by a revolutionary combination of novel free-space optical beam diversity techniques, an intelligently routed optical fibre platform, and flexible radio communication techniques. This hybrid technology will increase the available wireless bandwidth by several orders of magnitude, while operating very energy-efficiently.
We will investigate the use of narrowly confined optical pencil beams aided by optical beam tracking for the downstream part of the communication channel, and radio technology for upstream. The optical beams allow extremely high data rates (10-100 Gbit/s) as their carrier frequency is orders of magnitude larger than that of radio waves, and can serve many users without interference due to their spatial confinement. Moreover, they reduce the power consumption by their excellent directivity. In the (less demanding) upstream path, we will explore low-power highly-integrated radio technology for offering capacities of 3-30 Gbit/s. We combine these with intelligent optical routing techniques in the fibre backbone network, and with user localisation and tracking capabilities using advanced upstream radio techniques, in order to deliver ultra-broadband services to every user, tailored for his device. We will explore an autonomic network management and control system to orchestrate the heterogeneous resources and evolve these as the user’s needs, context, device capabilities and energy requirements change.","2430353","2012-09-01","2017-12-31"
"BSMOXFORD","Physics Beyond the Standard Model at the LHC and with Atom Interferometers","Savas Dimopoulos","EUROPEAN ORGANIZATION FOR NUCLEAR RESEARCH","Elementary particle physics is entering a spectacular new era in which experiments at the Large Hadron Collider (LHC) at CERN will soon start probing some of the deepest questions in physics, such as: Why is gravity so weak? Do elementary particles have substructure? What is the origin of mass? Are there new dimensions? Can we produce black holes in the lab? Could there be other universes with different physical laws?  While the LHC pushes the energy frontier, the unprecedented precision of Atom Interferometry, has pointed me to a new tool for fundamental physics. These experiments based on the quantum interference of atoms can test General Relativity on the surface of the Earth, detect gravity waves, and test short-distance gravity, charge quantization, and quantum mechanics with unprecedented precision in the next decade.  This ERC Advanced grant proposal is aimed at setting up a world-leading European center for development of  a deeper theory of  fundamental physics. The next 10 years is the optimal time for such studies to benefit from the wealth of new data that will  emerge from the LHC, astrophysical observations and atom interferometry. This is a once-in-a-generation opportunity for making ground-breaking progress, and will open up many new research horizons.","2200000","2009-05-01","2014-04-30"
"ByoPiC","The Baryon Picture of the Cosmos","nabila AGHANIM","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The cosmological paradigm of structure formation is both extremely successful and plagued by many enigmas. Not only the nature of the main matter component, dark matter, shaping the structure skeleton in the form of a cosmic web, is mysterious; but also half of the ordinary matter (i.e. baryons) at late times of the cosmic history, remains unobserved, or hidden! ByoPiC focuses on this key and currently unresolved issue in astrophysics and cosmology: Where and how are half of the baryons hidden at late times? ByoPiC will answer that central question by detecting, mapping, and assessing the physical properties of hot ionised baryons at large cosmic scales and at late times. This will give a completely new picture of the cosmic web, added to its standard tracers, i.e. galaxies made of cold and dense baryons. To this end, ByoPiC will perform the first statistically consistent, joint analysis of complementary multiwavelength data: Planck observations tracing hot, ionised baryons via the Sunyaev-Zeldovich effect, optimally combined with optical and near infrared galaxy surveys as tracers of cold baryons. This joint analysis will rely on innovative statistical tools to recover all the (cross)information contained in these data in order to detect most of the hidden baryons in cosmic web elements such as (super)clusters and filaments. These newly detected elements will then be assembled to reconstruct the cosmic web as traced by both hot ionised baryons and galaxies. Thanks to that, ByoPiC will perform the most complete and detailed assessment of the census and contribution of hot ionised baryons to the total baryon budget, and identify the main physical processes driving their evolution in the cosmic web. Catalogues of new (super)clusters and filaments, and innovative tools, will be key deliverable products, allowing for an optimal preparation of future surveys.","2488350","2017-01-01","2021-12-31"
"C-SENSE","Exploiting low dimensional models in sensing, computation and signal processing","Michael DAVIES","THE UNIVERSITY OF EDINBURGH","The aim of this project is to develop the next generation of compressive and computational sensing and processing techniques. 
The ability to identify and exploit good signal representations is pivotal in many signal and data processing tasks. During the last decade sparse representations have provided stunning performance gains for applications such as: imaging coding, computer vision, super-resolution microscopy and most recently in MRI, achieving many-fold acceleration through compressed sensing (CS).
However in most real world sensing it is generally not possible to fully adopt the random sampling strategies advocated by CS. Systems are often nonlinear, measurements have limited dynamic range, noise is rarely Gaussian and reconstruction is not always the final goal. Furthermore, iterative reconstruction techniques are often not adopted in commercial imaging systems as they typically incur at least an order of magnitude more computation than traditional techniques. Thus there is a real need for a new framework for generalized computationally accelerated sensing and processing techniques. 
The research proposed here will build on the PIs recent work in this area and will develop and analyse a much richer class of hierarchical low dimensional signal models, accommodating everything from physical laws to data-driven models such as deep neural networks. It will provide quantitative guidance for system design and address sensing tasks beyond reconstruction including detection, classification and statistical estimation. It will also exploit low dimensional structure to reduce computational cost as well as estimation accuracy, challenging the notion that exploiting prior information must come at a computational cost.
This research will result in a new generation of data-driven, physics-aware and task-orientated sensing systems in application domains such as advanced radar, CT and MR imaging and emerging sensing modalities such as multispectral time-of-flight cameras.","2212048","2016-09-01","2021-08-31"
"C8","Consistent computation of the chemistry-cloud continuum and climate change in Cyprus","Johannes Lelieveld","THE CYPRUS RESEARCH AND EDUCATIONAL FOUNDATION","We have developed a new numerical method to consistently compute atmospheric trace gas and aerosol chemistry and cloud processes. The method is computationally efficient so that it can be used in climate models. For the first time cloud droplet formation on multi-component particles can be represented based on first principles rather than parameterisations. This allows for a direct coupling in models between aerosol chemical composition and the continuum between hazes and clouds as a function of ambient relative humidity.  We will apply the method in a new nested global-limited area model system to study atmospheric chemistry   climate interactions and anthropogenic influences. We will focus on the Mediterranean region because it is a hot spot in climate change exposed to drying and air pollution. The limited area model will also be applied as cloud-resolving model to study aerosol influences on precipitation and storm development. By simulating realistic meteorological conditions at high spatial resolution our method can be straightforwardly tested against observations.  Central questions are: - How does the simulated haze-cloud continuum compare with remote sensing measurements and what is the consequence of abandoning the traditional and artificial distinction between aerosols and clouds?  - How are cloud and precipitation formation influenced by atmospheric chemical composition changes? - To what extent do haze and cloud formation in polluted air exert forcings of synoptic meteorological conditions and climate?  - Can aerosol pollution in the Mediterranean region exacerbate the predicted   and observed   drying in a changing climate? The model system is user-friendly and will facilitate air quality and climate studies by regional scientists. The project will be part of the Energy, Environment and Water Centre of the newly founded Cyprus Institute, provide input to climate impact assessments and contribute to a regional outreach programme.","2196000","2009-01-01","2014-12-31"
"CADENCE","Catalytic Dual-Function Devices Against Cancer","Jesus Santamaria","UNIVERSIDAD DE ZARAGOZA","Despite intense research efforts in almost every branch of the natural sciences, cancer continues to be one of the leading causes of death worldwide. It is thus remarkable that little or no therapeutic use has been made of a whole discipline, heterogeneous catalysis, which is noted for its specificity and for enabling chemical reactions in otherwise passive environments. At least in part, this could be attributed to practical difficulties: the selective delivery of a catalyst to a tumour and the remote activation of its catalytic function only after it has reached its target are highly challenging objectives. Only recently, the necessary tools to overcome these problems seem within reach.
CADENCE aims for a breakthrough in cancer therapy by developing a new therapeutic concept. The central hypothesis is that a growing tumour can be treated as a special type of reactor in which reaction conditions can be tailored to achieve two objectives: i) molecules essential to tumour growth are locally depleted and ii) toxic, short-lived products are generated in situ.
To implement this novel approach we will make use of core concepts of reactor engineering (kinetics, heat and mass transfer, catalyst design), as well as of ideas borrowed from other areas, mainly those of bio-orthogonal chemistry and controlled drug delivery. We will explore two different strategies (classical EPR effect and stem cells as Trojan Horses) to deliver optimized catalysts to the tumour. Once the catalysts have reached the tumour they will be remotely activated using near-infrared (NIR) light, that affords the highest penetration into body tissues. 
This is an ambitious project, addressing all the key steps from catalyst design to in vivo studies. Given the novel perspective provided by CADENCE, even partial success in any of the approaches to be tested would have a significant impact on the therapeutic toolbox available to treat cancer.","2483136","2017-09-01","2022-08-31"
"CALCULUS","Commonsense and Anticipation enriched Learning of Continuous representations sUpporting Language UnderStanding","Marie-Francine MOENS","KATHOLIEKE UNIVERSITEIT LEUVEN","Natural language understanding (NLU) by the machine is of large scientific, economic and social value. Humans perform the NLU task in an efficient way by relying on their capability to imagine or anticipate situations. They engage commonsense and world knowledge that is often acquired through perceptual experiences to make explicit what is left implicit in language. Inspired by these characteristics CALCULUS will design, implement and evaluate innovative paradigms supporting NLU, where it will combine old but powerful ideas for language understanding from the early days of artificial intelligence with new approaches from machine learning. The project focuses on the effective learning of anticipatory, continuous, non-symbolic representations of event frames and narrative structures of events that are trained on language and visual data. The grammatical structure of language is grounded in the geometric structure of visual data while embodying aspects of commonsense and world knowledge. The reusable representations are evaluated in a selection of NLU tasks requiring efficient real-time retrieval of the representations and parsing of the targeted written texts. Finally, we will evaluate the inference potential of the anticipatory representations in situations not seen in the training data and when inferring spatial and temporal information in metric real world spaces that is not mentioned in the processed language. The machine learning methods focus on learning latent variable models relying on Bayesian probabilistic models and neural networks and focus on settings with limited training data that are manually annotated. The best models will be integrated in a demonstrator that translates the language of stories to events happening in a 3-D virtual world. The PI has interdisciplinary expertise in natural language processing, joint processing of language and visual data, information retrieval and machine learning needed for the successful realization of the project.","2227500","2018-09-01","2023-08-31"
"CAP","Computers Arguing with People","Sarit Kraus","BAR ILAN UNIVERSITY","An important form of negotiation is argumentation. This is the ability to argue and to persuade the other party to accept a desired agreement, to acquire or give information, to coordinate goals and actions, and to find and verify evidence. This is a key capability in negotiating with humans.
While automated negotiations between software agents can often exchange offers and counteroffers, humans require persuasion. This challenges the design of agents arguing with people, with the objective that the outcome of the negotiation will meet the preferences of the arguer agent.
CAP’s objective is to enable automated agents to argue and persuade humans.
To achieve this, we intend to develop the following key components:
1) The extension of current game theory models of persuasion and bargaining to more realistic settings, 2) Algorithms and heuristics for generation and evaluation of arguments during negotiation with people, 3) Algorithms and heuristics for managing inconsistent views of the negotiation environment, and decision procedures for revelation, signalling, and requesting information, 4) The revision and update of the agent’s mental state and incorporation of social context, 5) Identifying strategies for expressing emotions in negotiations, 6) Technology for general opponent modelling from sparse and noisy data.
To demonstrate the developed methods, we will implement two training systems  for people to improve their interviewing capabilities, and for training negotiators in inter-culture negotiations.
CAP will revolutionise the state of the art of automated systems negotiating with people. It will also create breakthroughs in the research of multi-agent systems in general, and will change paradigms by providing new directions for the way computers interact with people.","2334057","2011-07-01","2016-06-30"
"CAPABLE","Composite integrated photonic platform by femtosecond laser micromachining","Roberto OSELLAME","CONSIGLIO NAZIONALE DELLE RICERCHE","The quantum technology revolution promises a transformational impact on the society and economics worldwide. It will enable breakthrough advancements in such diverse fields as secure communications, computing, metrology, and imaging.  Quantum photonics, which recently received an incredible boost by the use of integrated optical circuits, is an excellent technological platform to enable such revolution, as it already plays a relevant role in many of the above applications. However, some major technical roadblocks needs to be overcome. Currently, the various components required for a complete quantum photonic system are produced on very different materials by dedicated fabrication technologies, as no single material is able to fulfil all the requirements for single-photon generation, manipulation, storage and detection. This project proposes a new hybrid approach for integrated quantum photonic systems based on femtosecond laser microfabrication (FLM), enabling the innovative miniaturization of various components on different materials, but with a single tool and with very favourable integration capabilities. 
This project will mainly focus on two major breakthroughs: the first one will be increasing the complexity achievable in the photonic platform and demonstrating unprecedented quantum computation capability; the second one will be the integration in the platform of multiple single-photon quantum memories and their interconnection.
Achievement of these goals will only be possible by taking full advantage of the unique features of FLM, from the possibility to machine very different materials, to the 3D capabilities in waveguide writing and selective material removal.
The successful demonstration and functional validation of this hybrid, integrated photonic platform will represent a significant leap for photonic microsystems in quantum computing and quantum communications.","2381875","2017-10-01","2022-09-30"
"CAPaCITy","Designing Conjugated Polymers for Photocatalysis and Ion Transport","Jenny NELSON","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","Solar energy conversion will play an essential role in the future supply of clean energy. Secure access to energy sources will require energy conversion technologies that are low impact, distributed and accessible both technically and financially. Molecular electronic materials embody these possibilities, offering facile synthesis, low energy production and the versatility to allow performance to be maximized for specific applications. Moreover, they bring appealing similarities with nature’s intrinsically low impact energy conversion materials. Whilst molecular semiconductors have been studied in detail for solar-to-electric energy conversion they have seldom been studied for solar-to-chemical conversion or for charge storage. However, they bring exciting potential advantages in terms of their light harvesting properties, the range of microstructures possible and the ability to tune their electrical properties. Polymer materials applied to solar chemical generation could open up an innovative route to artificial fuels, with the option to control light harvesting and charge separation through structural control. Polymer materials applied to mixed (electronic / ionic) conduction provide a route to lower cost electrochemical storage, as well as to biocompatible devices and sensors. Stimulated by recent experimental breakthroughs in the application of polymers as photocatalysts and ion transport media I will exploit my expertise in multi-scale modelling and functional characterization of molecular electronic materials and devices to develop a design framework for energy conversion and storage in conjugated polymer materials. This proposal aims to disentangle the parameters that govern the performance of conjugated polymer based photocatalysts and ion transport media to discover the underlying functional mechanisms. The tools generated will serve to enable the design and development of high performance materials for energy conversion devices.","2351550","2017-10-01","2022-09-30"
"CAPRI","Chemical and photochemical dynamics of reactions in solution","Andrew John Orr-Ewing","UNIVERSITY OF BRISTOL","Ultrafast laser methods will be employed to examine the dynamics of chemical and photochemical reactions in liquid solutions.  By contrasting the solution phase dynamics with those observed for isolated collisions in the gas phase, the fundamental role of solvent on chemical pathways will be explored at a molecular level.  The experimental studies will be complemented by computational simulations that explicitly include treatment of the effects of solvent on reaction energy pathways and reactant and product motions.

The research addresses a major challenge in Chemistry to understand the role of solvent on the mechanisms of chemical reactions.  Questions that will be examined include how the solvent modifies reaction barriers and other regions of the reaction potential energy surface (PESs), alters the couplings between PESs, most importantly at conical intersections between electronic states, influences and constrains the dynamical stereochemistry of passage through transition states, and dissipates excess product energy.

The experimental strategy will be to obtain absorption spectra of transient species with lifetimes of ~100 fs – 1000 ps using broad bandwidth light sources in the infrared, visible and ultraviolet regions.  Time-evolutions of such spectra reveal the formation and decay of short-lived species that might be highly reactive radicals or internally (vibrationally and electronically) excited molecules.  The transient species decay by reaction or energy loss to the solvent.  Statistical mechanical theories of reactions in solution treat such processes using linear response theory, but the experimental data will challenge this paradigm by seeking evidence for breakdown of the linear response interaction of solvent and solute on short timescales because of microscopic chemical dynamics that perturb the solvent structure.  The work will build on our pioneering experiments at the Rutherford Appleton Laboratory that prove the feasilbility of the methods.","2666684","2012-02-01","2017-01-31"
"CARBONANOBRIDGE","Neuron Networking with Nano Bridges via the Synthesis and Integration of Functionalized Carbon Nanotubes","Maurizio Prato","UNIVERSITA DEGLI STUDI DI TRIESTE","We propose the development of novel nanodevices, such as nanoscale bridges and nanovectors, based on functionalized carbon nanotubes (CNT) for manipulating neurons and neuronal network activity in vitro. The main aim is to put forward innovative solutions that have the potential to circumvent the problems currently faced by spinal cord lesions or by neurodegenerative diseases. The unifying theme is to use recent advances in chemistry and nanotechnology to gain insight into the functioning of hybrid neuronal/CNT networks, relevant for the development of novel implantable devices to control neuronal signaling and improve synapse formation in a controlled fashion. The proposal s core strategy is to exploit the expertise of the PI in the chemical control of CNT properties to develop devices reaching various degrees of functional integration with the physiological electrical activity of cells and their networks, and to understand how such global dynamics are orchestrated when integrated by different substrates. An unconventional strategy will be represented by the electrical characterization of micro and nano patterned substrates by AFM and conductive tip AFM, both before and after neurons have grown on the substrates. We will also use the capability of AFM to identify critical positions in the neuronal network, while delivering time-dependent chemical stimulations. We will apply nanotechnology to contemporary neuroscience in the perspective of novel neuro-implantable devices and drug nanovectors, engineered to treat neurological and neurodegenerative lesions. The scientific strategy at the core of the proposal is the convergence between nanotechnology, chemistry and neurobiology. Such convergence, beyond helping understand the functioning and malfunctioning of the brain, can stimulate further research in this area and may ultimately lead to a new generation of nanomedicine applications in neurology and to new opportunities for the health care industry.","2500000","2009-02-01","2014-01-31"
"CARENET","Content-Aware Wireless Networks: Fundamental Limits, Algorithms, and Architectures","Giuseppe CAIRE","TECHNISCHE UNIVERSITAT BERLIN","Wireless communication networks are the essential connectivity tissue of the modern digital age. Wireless data traffic is predicted to increase by almost three orders of magnitude in the next five years. It is unlikely that such increase can be tackled by an incremental “more-of-the-same” approach. This proposal stems from the observation that the killer application for wireless networks is on-demand access to Internet content. CARENET advocates a novel content-aware approach to wireless networks design that can provably solve the scalability problem of current systems, thus supporting the paradigmatic shift “from Gigabits per second for a few to Terabytes per month for all”.  CARENET’s vision is to serve an arbitrarily large number of users with bounded transmission resources (bandwidth, number of transmit antennas, and power). The fundamental question is: how can such a per-user throughput scalability be achieved in the presence of on-demand requests, for which users do not access simultaneously the same content? CARENET builds on a novel information theoretic formulation of content-aware networks and on several recent results in information theory, network coding, channel coding, and protocol design, stimulated by the PI’s recent work. Key elements of the proposed content-aware architectures are new caching strategies, where content is stored across the wireless network nodes. These strategies are supported by the ever-growing on-board memory of wireless devices and by the new features of the forthcoming 5G-like technology. Our thesis is that scalability is possible through the novel content-aware design, while it is information-theoretically impossible otherwise. Our overarching goal envisions the delivery of one Terabyte per month to each user at an affordable cost and good Quality of Experience, rather than the traditional Gigabit per second peak rates targeted by conventional technology development.","2497500","2018-10-01","2023-09-30"
"CartiLube","Lubricating Cartilage: exploring the relation between lubrication and gene-regulation to alleviate osteoarthritis","Jacob KLEIN","WEIZMANN INSTITUTE OF SCIENCE LTD","Can we exploit insights from the remarkably lubricated surfaces of articular cartilage, to create lubricants that may alleviate osteoarthritis (OA), the most widespread joint disease, affecting millions? These, succinctly, are the challenges of the present proposal. They are driven by our recent finding that lubrication of destabilised joints leads to changes in gene-regulation of the cartilage-embedded chondrocytes to protect against development of the disease. OA alleviation is known to arise through orthopedically suppressing shear-stresses on the cartilage, and a central premise of this project is that, by reducing friction at the articulating cartilage through suitable lubrication, we may achieve the same beneficial effect on the disease. The objectives of this project are to better understand the origins of cartilage boundary lubrication through examination of friction-reduction by its main molecular components, and exploit that understanding to create lubricants that, on intra-articular injection, will lubricate cartilage sufficiently well to achieve alleviation of OA via gene regulation. The project will examine, via both nanotribometric and macroscopic measurements, how the main molecular species implicated in cartilage lubrication, lipids, hyaluronan and lubricin, and their combinations, act together to form optimally lubricating boundary layers on model surfaces as well as on excised cartilage. Based on this, we shall develop suitable materials to lubricate cartilage in joints, using mouse models. Lubricants will further be optimized with respect to their retention in the joint and cartilage targeting, both in model studies and in vivo. The effect of the lubricants in regulating gene expression, in reducing pain and cartilage degradation, and in promoting stem-cell adhesion to the cartilage will be studied in a mouse model in which OA has been induced. Our results will have implications for treatment of a common, debilitating disease.","2499944","2017-09-01","2022-08-31"
"CATCHIT","Coherently Advanced Tissue and Cell Holographic Imaging and Trapping","Monika Ritsch-Marte","MEDIZINISCHE UNIVERSITAT INNSBRUCK","We envisage a new generation of dynamic holographic laser tweezers and stretching tools with unprecedented spatial control of gradient and scattering light forces, to unravel functional mysteries of cell biology and genetics: Based on our recently developed, highly successful and widely recognized amplitude and phase shaping techniques with cascaded spatial light modulators (SLM), we will create new holographic optical manipulators consisting of a line-shaped trap with balanced net scattering forces and controllable local phase-gradients. Combining these line stretchers with spiral phase contrast imaging or nonlinear optical microscopy will allow quantitative study of functional shape changes. The novel tool is hugely more versatile than standard optical tweezers, since direction and magnitude of the scattering force can be designed to precisely follow the structure. In combination with conventional multi-spot traps the line stretcher acts as a sensitive and adaptable local force sensor. In collaboration with local experts we want to tackle hot topics in Genetics, e.g. search for force profile signatures in regions with Copy Number Variations. Possibly the approach may shed light on basic physical characteristics such as, for example, chromosomal fragility in Fra(X) syndrome, the most common monogenic cause of mental retardation. The new design intrinsically offers enhanced microscopic resolution, as SLM-synthesized apertures and waveforms can enlarge the number of spatial frequencies forming the image. Ultimately, nonlinear holography can be implemented, sending phase shaped wavefronts to target samples. This can, e.g., be used to push the sensitivity of nonlinear chemical imaging, or for controlled photo-activation of targeted regions in neurons.","1987428","2010-05-01","2015-04-30"
"CATGOLD","ADVANCING GOLD CATALYSIS","Antonio María Echavarren Pablos","FUNDACIO PRIVADA INSTITUT CATALA D'INVESTIGACIO QUIMICA","We plan to chase new goals by exploring the limits of gold chemistry and organic synthesis. A major goal is to promote copper to the level of gold as the catalyst of choice for the activation of alkynes under homogeneous conditions. Another major goal is to develop enantioselective reactions based on a new chiral catalyst design to overcome the inherent limitations of the linear coordination of d10 M(I) coinage metals. We whish to contribute to bridge the gap between homogeneous and heterogeneous gold catalysis discovering new reactions for C-C bond formation via cross-coupling and C-H activation. We will apply new methods based on Au catalysis to fill the gap that exists between chemical synthesis and physical methods such as graphite exfoliation or laser ablation for the synthesis of nanographenes and other large acenes.","2499060","2013-03-01","2018-02-28"
"Cathedral","Post-Snowden Circuits and Design Methods for Security","Ingrid VERBAUWHEDE","KATHOLIEKE UNIVERSITEIT LEUVEN",": Comprehensive set of circuits and design methods to create next generation electronic circuits with strong built-in trust and security. 
       Electronics are integrating/invading into the human environment at an amazing speed, called the Internet-of-Things and next the Internet-of-Everything.  This creates huge security problems. Distributed (e.g. body) sensors, pick up often very private data, which is sent digitally into the cloud, over wireless and wired links. Protection of this data relies on high-quality cryptographic algorithms and protocols. The nodes need to be cheap and lightweight, making them very vulnerable to eavesdropping and abuse. Moreover, post-Snowden, society realizes that the attack capabilities of intelligence agencies, and probably following soon of organized crime and other hackers, are orders of magnitude stronger than imagined. Thus there is a strong demand to re-establish trust in ICT systems. 
       In this proposal we focus on the root of trust: the digital hardware. The overall objective is to provide fundamental enabling technologies for secure trustworthy digital circuits which can be applied in a wide range of applications. To master complexity, digital hardware design is traditionally split into different abstraction layers. We revisit these abstraction layers from a security viewpoint: we look at process variations to the benefit of security, standard cell compatible digital design flow with security as design objective, hardware IP blocks for next generation cryptographic algorithms and protocols (e.g. authenticated encryption schemes, post-quantum public key schemes), integration into embedded HW/SW platforms, and methods to provide trust evidence to higher levels of abstraction. To strengthen the security we investigate the links between the layers. Finally an embedded application is selected as design driver, the security evaluation of which will be fed back to the individual layers.","2369250","2016-09-01","2021-08-31"
"CausalStats","Statistics, Prediction and Causality for Large-Scale Data","Peter Lukas Bühlmann","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Understanding cause-effect relationships between variables is of great interest in many fields of science. However, causal inference from data is much more ambitious and difficult than inferring (undirected) measures of association such as correlations, partial correlations or multivariate regression coefficients, mainly because of fundamental identifiability
problems. A main objective of the proposal is to exploit advantages from large-scale heterogeneous data for causal inference where heterogeneity arises from different experimental conditions or different unknown sub-populations. A key idea is to consider invariance or stability across different experimental conditions of certain conditional probability distributions: the invariants correspond on the one hand to (properly defined) causal variables which are of main interest in causality; andon the other hand, they correspond to the features for constructing powerful predictions for new scenarios which are unobserved in the data (new probability distributions). This opens novel perspectives: causal inference
can be phrased as a prediction problem of a certain kind, and vice versa, new prediction methods which work well across different scenarios (unobserved in the data) should be based on or regularized towards causal variables. Fundamental identifiability limits will become weaker with increased degree of heterogeneity, as we expect in large-scale data. The topic is essentially unexplored, yet it opens new avenues for causal inference, structural equation and graphical modeling, and robust prediction based on large-scale complex data. We will develop mathematical theory, statistical methodology and efficient algorithms; and we will also work and collaborate on major application problems such as inferring causal effects (i.e., total intervention effects) from gene knock-out or RNA interference perturbation experiments, genome-wide association studies and novel prediction tasks in economics.","2184375","2018-10-01","2023-09-30"
"CC-TOP","Cryosphere-Carbon on Top of the Earth (CC-Top):Decreasing Uncertainties of Thawing Permafrost and Collapsing Methane Hydrates in the Arctic","Örjan GUSTAFSSON","STOCKHOLMS UNIVERSITET","The enormous quantities of frozen carbon in the Arctic, held in shallow soils and sediments, act as “capacitors” of the global carbon system. Thawing permafrost (PF) and collapsing methane hydrates are top candidates to cause a net transfer of carbon from land/ocean to the atmosphere this century, yet uncertainties abound. 

Our program targets the East Siberian Arctic Ocean (ESAO), the World’s largest shelf sea, as it holds 80% of coastal PF, 80% of subsea PF and 75% of shallow hydrates. Our initial findings (e.g., Science, 2010; Nature, 2012; PNAS; 2013; Nature Geoscience, 2013, 2014) are challenging earlier notions by showing complexities in terrestrial PF-Carbon remobilization and extensive venting of methane from subsea PF/hydrates. The objective of the CC-Top Program is to transform descriptive and data-lean pictures into quantitative understanding of the CC system, to pin down the present and predict future releases from these “Sleeping Giants” of the global carbon system.

The CC-Top program combines unique Arctic field capacities with powerful molecular-isotopic characterization of PF-carbon/methane to break through on:

The “awakening” of terrestrial PF-C pools: CC-Top will employ great pan-arctic rivers as natural integrators and by probing the δ13C/Δ14C and molecular fingerprints, apportion release fluxes of different PF-C pools.

The ESAO subsea cryosphere/methane: CC-Top will use recent spatially-extensive observations, deep sediment cores and gap-filling expeditions to (i) estimate distribution of subsea PF and hydrates; (ii) establish thermal state (thawing rate) of subsea PF-C; (iii) apportion sources of releasing methane btw subsea-PF, shallow hydrates vs seepage from the deep petroleum megapool using source-diagnostic triple-isotope fingerprinting.

Arctic Ocean slope hydrates: CC-Top will investigate sites (discovered by us 2008-2014) of collapsed hydrates venting methane, to characterize geospatial distribution and causes of destabilization.","2499756","2016-11-01","2021-10-31"
"CCICO","Coupled and Competing Instabilities in Complex Oxides","Nicola Ann Spaldin","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","""The CCICO project will build a comprehensive understanding of how proximity to previously unexplored combinations of instabilities, as well as previously unidentified types of ordering, manifest in novel behaviors, and will develop design guidelines for practical realization of new materials with such behaviors.  Taking transition-metal oxides as our model systems, we will develop and apply first-principles electronic structure theory methods to explore an extensive array of new combinations of orderings, with a focus on interactions between the electronic -- Jahn-Teller, orbital and charge -- and structural -- rotations, ferroelectric and other distortions -- degrees of freedom.  Our goal is to spawn a new field of study based on a novel combination of orderings in the same way that the field of multiferroics was jump-started ten years ago by our work understanding the coexistence of ferroelectricity and magnetism.  Conversely, we will apply the computational tools developed in our history of studying multiferroics, particularly descriptions of proximity to structural and magnetic phase transitions, to characterizing observed behaviors such as exotic superconductivity in existing materials.  In the process we will search for and characterize elusive or poorly characterized forms of order in solids, with a focus on ferrotoroidicity and emergent local dipoles.  A final application is to create designer materials for solid-state experiments relevant to high-energy physics and cosmology.  Promising compounds that are amenable to bulk synthesis will be made in our new oxide single-crystal growth laboratory; materials that require thin-film routes will be pursued in collaboration with colleagues.""","2000000","2012-03-01","2017-02-28"
"CellMechanoControl","The physical basis of cellular mechanochemical 
control circuits","Christoph Friedrich Schmidt","GEORG-AUGUST-UNIVERSITAT GOTTINGENSTIFTUNG OFFENTLICHEN RECHTS","Biological cells possess a chemical “sense of smell” and a physical “sense of touch”. Structure, dynamics, development, differentiation and even apoptosis of cells are guided by physical stimuli feeding into a regulatory network integrating biochemical and mechanical signals. Cells are equipped with both, force-generating structures, and stress sensors including force-sensitive structural proteins or mechanosensitive ion channels. Pathways from force sensing to structural and transcriptional controls are not yet understood.

The goal of the proposed interdisciplinary project is to quantitatively establish such pathways, connecting the statistical physics and the mechanics to the biochemistry. We will measure and model the complex non-equilibrium mechanical structures in cells, and we will study how external and cell-generated forces activate sensory processes that (i) act (back) on the morphology of the cell structures, and (ii) lead to cell-fate decisions, such as differentiation. The most prominent stress-bearing and -generating structures in cells are actin/myosin based, and the most prominent mechanoactive and -sensitive cell types are fibroblasts in connective tissue and myocytes in muscle. We will first focus on actin/myosin bundles in fibroblasts and in sarcomeres in developing heart muscle cells. We will observe cells under the influence of exactly controlled external stresses. Forces on suspended single cells or cell clusters will be exerted by laser trapping and sensitively detected by laser interferometry. We furthermore will monitor mechanically triggered transcriptional regulation by detecting mRNA in the nucleus of mouse stem cells differentiating to cardiomyocytes. We will develop fluorescent mRNA sensors that can be imaged in cells, based on near-IR fluorescent single-walled carbon nanotubes.

Understanding mechanical cell regulation has far-ranging relevance for fundamental cell biophysics, developmental biology and for human health.","2425200","2014-06-01","2019-05-31"
"CELLO","From Cells to Organs on Chips: Development of an Integrative Microfluidic Platform","Jean-Louis Viovy","INSTITUT CURIE","We shall develop a microfluidic and microsystems toolbox allowing the  construction and study of complex cellular assemblies (“tissue or organ mimics on chip”), in a highly controlled and parallelized way.  This platform will allow the selection of specific cells from one or several  populations, their deterministic positioning and/or connection relative to each other, yielding  functional assemblies with a degree of complexity, determinism and physiological realism unavailable to current in vitro systems We shall in particular develop “semi-3D” architectures, reproducing the local 3D arrangement of tissues, but presenting at mesoscale  a planar and periodic arrangement facilitating high resolution stimulation and recording. This will provide biologists and clinicians with new experimental models able to bridge the gap between current in vitro systems, in which cells can be observed in parallel at high resolution, but lack the  highly ordered architecture  present in living systems, and in vivo models, in which observation and stimulation means are more limited.  This development will follow a functional approach, and gather competences and concepts from micr-nano-systems, surface science, hydrodynamics, soft matter and biology. We shall validate it on three specific applications, the sorting and study of circulating tumour cells for understanding metastases, the creation of “miniguts”, artificial intestinal tissue, for applications in developmental biology and cancerogenesis, and the in vitro construction of active and connected neuron arrays, for studying the molecular mechanisms of Alzheimer, and signal processing by neuron  networks.  This platform will also open new routes for drug testing, replacing animal models and reducing the health and economic risk of clinical tests, developmental biology , stem cells research. and regenerative medicine.","2260000","2013-07-01","2018-06-30"
"CEMAS","Controlling and Exploring Molecular Systems at the Atomic Scale with Atomic Force Microscopy","Gerhard Meyer","IBM RESEARCH GMBH","The objective of this project is to advance and use Atomic Force Microscopy (AFM) to explore the physical and chemical properties of single molecules and molecular systems with unprecedented spatial resolution. We will use AFM to develop atomically resolved molecular imaging with structural and chemical identification and investigate charge distribution and transfer in molecular systems. The AFM will allow the extension of seminal Scanning Tunneling Microscopy (STM) work on atoms/molecules on ultra-thin insulating films to thick insulating films, to control and explore single molecule chemistry processes in utmost detail. The whole work will be significantly based on the development and exploitation of novel atomic and molecular manipulation processes to control matter at the atomic scale, both for fabricating novel complex molecular nanostructures with atomic scale precision and understanding these systems, as well as for probe-tip functionalization to tailor tip-substrate interaction. Instrumental enhancements will focus on fabricating novel AFM sensors for simultaneous lateral and vertical force measurement and on developing a new original approach to increase the time resolution in AFM measurements. Due to the fundamental nature of this work we expect the long term impact of this work to be in surface science, chemistry, molecular electronics and life sciences. In the short term we expect to develop the AFM into a practical tool for chemical structure determination of unknown molecules and we will employ atomic manipulation and high resolution AFM imaging to image, modify and functionalize graphene edge structures with atomic scale precision with the prospect of exploring and developing novel molecular devices.","2496720","2011-12-01","2016-11-30"
"CEMYSS","Cosmochemical Exploration of the first two Million Years of the Solar System","Marc Chaussidon","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","One of the major outcomes of recent studies on the formation of the Solar System is the reconnaissance of the fundamental importance of processes which took place during the first 10 thousands to 2 or 3 millions years of the lifetime of the Sun and its accretion disk. Astrophysical observations in the optical to infrared wavelengths of circumstellar disks around young stars have shown the existence in the inner disk of high-temperature processing of the dust. X-ray observations of T-Tauri stars revealed that they exhibit X-ray flare enhancements by several orders of magnitude. The work we have performed over the last years on the isotopic analysis of either solar wind trapped in lunar soils or of Ca-, Al-rich inclusions and chondrules from primitive chondrites, has allowed us to link some of these astrophysical observations around young stars with processes, such as irradiation by energetic particles and UV light, which took place around the T-Tauri Sun.   The aim of this project is to make decisive progress in our understanding of the early solar system though the development of in situ high-precision isotopic measurements by ion microprobe in extra-terrestrial matter. The project will be focused on the exploration of the variations in the isotopic composition of O and Mg and in the concentration of short-lived radioactive nuclides, such as 26Al and 10Be, with half-lives shorter than 1.5 millions years. A special emphasis will be put on the search for nuclides with very short half-lives such as 32Si (650 years) and 14C (5730 years), nuclides which have never been discovered yet in meteorites. These new data will bring critical information on, for instance, the astrophysical context for the formation of the Sun and the first solids in the accretion disk, or the timing and the processes by which protoplanets were formed and destroyed close to the Sun during the first 2 million years of the lifetime of the Solar System.","1270419","2009-01-01","2013-12-31"
"CepBin","A sub-percent distance scale from binaries and Cepheids","Grzegorz PIETRZYNSKI","CENTRUM ASTRONOMICZNE IM. MIKOLAJAKOPERNIKA POLSKIEJ AKADEMII NAUK","We propose to carry out a project which will produce a decisive step towards improving the accuracy of the Hubble constant as determined from the Cepheid-SN Ia method to 1%, by using 28 extremely rare eclipsing binary systems in the LMC which offer the potential to determine their distances to 1%. To achieve this accuracy we will reduce the main error in the binary method by interferometric angular diameter measurements of a sample of red clump stars which resemble the stars in our binary systems. We will check on our calibration with similar binary systems close enough to determine their orbits from interferometry. We already showed the feasibility of our method which yielded the best-ever distance determination to the LMC of 2.2% from 8 such binary systems. With 28 systems and the improved angular diameter calibration we will push the LMC distance uncertainty down to 1% which will allow to set the zero point of the Cepheid PL relation with the same accuracy using the large available LMC Cepheid sample. We will determine the metallicity effect on Cepheid luminosities by a) determining a 2% distance to the more metal-poor SMC with our binary method, and by b) measuring the distances to LMC and SMC with an improved Baade-Wesselink (BW) method. We will achieve this improvement by analyzing 9 unique Cepheids in eclipsing binaries in the LMC our group has discovered which allow factor- of-ten improvements in the determination of all basic physical parameters of Cepheids. These studies will also increase our confidence in the Cepheid-based H0 determination. Our project bears strong synergy to the Gaia mission by providing the best checks on possible systematic uncertainties on Gaia parallaxes with 200 binary systems whose distances we will measure to 1-2%. We will provide two unique tools for 1-3 % distance determinations to individual objects in a volume of 1 Mpc, being competitive to Gaia already at a distance of 1 kpc from the Sun.","2360500","2016-11-01","2021-10-31"
"CHAMPAGNE","Charge orders, Magnetism and Pairings in High Temperature Superconductors","Catherine, Marie, Elisabeth PEPIN","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","For nearly thirty years, the search for a room-temperature superconductor has focused on exotic materials known as cuprates, obtained by doping a parent Mott insulator, and which can carry currents without losing energy as heat at temperatures up to 164 Kelvin. Conventionally three main players were identified as being crucial i) the Mott insulating phase, ii) the anti-ferromagnetic order and iii) the superconducting (SC) phase. Recently a body of experimental probes suggested the presence of a fourth forgotten player, charge ordering-, as a direct competitor for superconductivity. In this project we propose that the relationship between charge ordering and superconductivity is more intimate than previously thought and is protected by an emerging SU(2) symmetry relating the two. The beauty of our theory resides in that it can be encapsulated in one simple and universal “gap equation”, which in contrast to strong coupling approaches used up to now, can easily be connected to experiments.  In the first part of this work, we will refine the theoretical model in order to shape it for comparison with experiments and consistently test the SU(2) symmetry. In the second part of the work, we will search for the experimental signatures of our theory through a back and forth interaction with experimental groups. We expect our theory to generate new insights and experimental developments, and to lead to a major breakthrough if it correctly explains the origin of anomalous superconductivity in these materials.","1318145","2016-08-01","2021-07-31"
"CHANGE","New CHallenges for (adaptive) PDE solvers: the interplay of ANalysis and GEometry","Annalisa BUFFA","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The simulation of Partial Differential Equations (PDEs) is an indispensable tool for  innovation in science and technology.
Computer-based simulation of PDEs approximates unknowns defined on a geometrical entity such as the computational domain with all of its properties. Mainly due to historical reasons, geometric design and numerical methods for PDEs have been developed independently, resulting in tools that rely on different representations of the same objects.

CHANGE aims at developing innovative mathematical tools for  numerically solving PDEs and for geometric modeling and processing,   the final goal being  the definition of a common framework where geometrical entities and simulation are coherently integrated and where adaptive methods can be used to guarantee optimal use of computer resources, from the geometric description to the simulation. 
We will concentrate on two classes of methods for the discretisation of PDEs that are having growing impact: 
isogeometric methods and variational methods on polyhedral partitions. They are both  extensions of standard finite elements enjoying exciting features, but both lack of an ad-hoc geometric modelling counterpart. 
We will extend numerical methods to ensure robustness on the most general geometric models, and we will develop geometric tools to construct, manipulate and refine such models. Based on our tools, we will design an innovative adaptive framework, that jointly exploits  multilevel representation of  geometric entities and PDE unknowns. 
Moreover, efficient algorithms call for efficient implementation: the issue of the optimisation of our algorithms on modern computer architecture will be addressed. 
Our research (and the team involved in the project) will combine competencies in computer science, numerical analysis, high performance computing, and computational mechanics.   Leveraging our innovative tools, we will also tackle challenging numerical problems deriving from bio-mechanical applications.","2199219","2016-10-01","2021-09-30"
"CHAOS","C-H Acids for Organic Synthesis","Benjamin List","MAX PLANCK INSTITUT FUER KOHLENFORSCHUNG","Enantioselective Brønsted acid organocatalysis has the potential to revolutionize asymmetric synthesis. However, the commonly used chiral phosphoric acid catalysts are typically limited to certain, fairly reactive substrate classes such as imines. Recently, more active and stereoselective catalysts have been introduced, which rely on acidic N–H bonds, and which have significantly widened the scope of suitable substrates. Despite the considerable success of these catalysts, however, several important and highly attractive substrate classes still remain out of reach. The applicant’s group has now identified C–H acids as novel and highly promising candidates to tackle these long standing challenges. Here, a research program with three major goals is proposed: 1) broadly conceived synthetic studies will be undertaken, which are expected to give access to C–H acids with a wide range of acidity and steric confinement. 2) These C–H acids will be applied to address one of the most general limitations currently encountered in organocatalysis: The enantioselective conversion of small and unbiased substrates. 3) The developed C–H acids, which are expected to enable unprecedented acidities and catalytic activities, will be employed in the activation of increasingly less reactive electrophiles, for example aliphatic aldehydes but also esters and olefins, for which enantioselective organocatalytic reactions are currently very limited or even unknown. Overall, this research program will aim at the design, synthesis and application of C–H acids as platform for solving several long standing challenges in asymmetric organocatalysis. The introduction of C–H acids for organic synthesis is expected to enrich the toolbox of synthetic chemists in both academic and industrial laboratories.","2007500","2016-10-01","2021-09-30"
"CHEMAGEB","CHEMometric and High-throughput Omics Analytical Methods for Assessment of Global Change Effects on Environmental and Biological Systems","Roman Tauler Ferrer","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","We propose to develop new chemometric and high-throughput analytical methods to assess the effects of environmental and climate changes on target biological systems which are representative of ecosystems. This project will combine powerful chemometric and analytical high-throughput methodologies with toxicological tests to examine the effects of environmental stressors (like chemical pollution) and of climate change (like temperature, water scarcity or food shortage), on genomic and metabonomic profiles of target biological systems. The complex nature of experimental data produced by high-throughput analytical techniques, such as DNA microarrays, hyphenated chromatography-mass spectrometry or multi-dimensional nuclear magnetic resonance spectroscopy, requires powerful data analysis tools to extract, summarize and interpret the large amount of information that such megavariate data sets may contain. There is a need to improve and automate every step in the analysis of the data generated from genomic and metabonomic studies using new chemometric and multi- and megavariate tools. The main purpose of this project is to develop such tools. As a result of the whole study, a detailed report on the effects of global change and chemical pollution on the genomic and metabonomic profiles of a selected set of representative target biological systems will be delivered and used for global risk assessment. The information acquired, data sets and computer software will be stored in public data bases using modern data compression and data management technologies. And all the methodologies developed in the project will be published.","2454280","2013-04-01","2018-03-31"
"CHEMBIOSPHING","Chemical biology of sphingolipids: fundamental studies and clinical applications","Herman Steven Overkleeft","UNIVERSITEIT LEIDEN","""Sphingolipids are major components of the human cell and are involved in human pathologies ranging from lysosomal storage disorders to type 2 diabetes. Here, we propose to establish an integrated research program for the study of sphingolipid metabolism, in health and disease. We will combine state-of-the-art synthetic organic chemistry, bioorganic chemistry, analytical chemistry, molecular biology and biochemistry techniques and concepts and apply these in an integrated chemical biology approach to study and manipulate sphingolipid metabolism in vivo and in vitro, using human cells and animal models. The program is subdivided in three individual research lines that are interconnected both in terms of technology development and in their biological context. 1) We will develop modified sphinganine derivatives and apply these to study sphingolipid homeostasis in cells derived from healthy and diseased (Gaucher, Fabry, Niemann-Pick A/B disease) individuals/animal models. This question will be addressed in a chemical metabolomics/lipidomics approach. 2) We will develop activity-based probes aimed at monitoring enzyme activity levels of glycosidases involved in (glyco)sphingolipid metabolism, in particular the enzymes that - when mutated and thereby reduced in activity- are responsible for the lysosomal storage disorders Gaucher disease and Fabry disease. 3) We will develop well-defined enzymes and chaperone proteins for directed correction of sphingolipid homeostasis in Gaucher, Fabry and Niemann-Pick A/B patients, via a newly designed semi-synthetic approach that combines sortase-mediated ligation with synthetic chemistry. Deliverables are a better understanding of the composition of the sphingolipid pool that are at the basis of lysosomal storage disorders, effective ways to in situ monitor the efficacy of therapies (enzyme inhibitors, chemical chaperones, recombinant enzymes) to treat these and improved semi-synthetic proteins for enzyme replacement therapy.""","2999600","2012-06-01","2017-05-31"
"chemech","From Chemical Bond Forces and Breakage to Macroscopic Fracture of Soft Materials","Costantino CRETON","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Soft materials are irreplaceable in engineering applications where large reversible deformations are needed, and in life sciences to mimic ever more closely or replace a variety of living tissues. While mechanical strength may not be essential for all applications, excessive brittleness is a strong limitation. Yet predicting if a soft material will be tough or brittle from its molecular composition or structure relies on empirical concepts due to the lack of proper tools to detect the damage occurring to the material before it breaks. Taking advantage of the recent advances in materials science and mechanochemistry, we propose a ground-breaking method to investigate the mechanisms of fracture of tough soft materials. To achieve this objective we will use a series of model materials containing a variable population of internal sacrificial bonds  that break before the material fails macroscopically, and use a combination of advanced characterization techniques and molecular probes to map stress, strain, bond breakage and structure in a region ~100 µm in size ahead of the propagating crack. By using mechanoluminescent and mechanophore molecules incorporated in the model material in selected positions, confocal laser microscopy, digital image correlation and small-angle X-ray scattering we will gain an unprecedented molecular understanding of where and when bonds break as the material fails and the crack propagates, and will then be able to establish a direct relation between the architecture of soft polymer networks and their fracture energy, leading to a new molecular and multi-scale vision of macroscopic fracture of soft materials. Such advances will be invaluable to guide materials chemists to design and develop better and more finely tuned soft but tough and sometimes self-healing materials to replace living tissues (in bio engineering) and make lightweight tough and flexible parts for energy efficient transport.","2251026","2016-09-01","2021-08-31"
"ChemNav","Magnetic sensing by molecules, birds, and devices","Peter John Hore","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The sensory mechanisms that allow birds to perceive the direction of the Earth’s magnetic field for the purpose of navigation are only now beginning to be understood. One of the two leading hypotheses is founded on magnetically sensitive photochemical reactions in the retina. It is thought that transient photo-induced radical pairs in cryptochrome, a blue-light photoreceptor protein, act as the primary magnetic sensor. Experimental and theoretical support for this mechanism has been accumulating over the last few years, qualifying chemical magnetoreception for a place in the emerging field of Quantum Biology.

In this proposal, we aim to determine the detailed principles of efficient chemical sensing of weak magnetic fields, to elucidate the biophysics of animal compass magnetoreception, and to explore the possibilities of magnetic sensing technologies inspired by the coherent dynamics of entangled electron spins in cryptochrome-based radical pairs.

We will:
(a)  Establish the fundamental structural, kinetic, dynamic and magnetic properties that allow efficient chemical sensing of Earth-strength magnetic fields in cryptochromes.
(b)  Devise new, sensitive forms of optical spectroscopy for this purpose.
(c)  Design, construct and iteratively refine non-natural proteins (maquettes) as versatile model systems for testing and optimising molecular magnetoreceptors.
(d)  Characterise the spin dynamics and magnetic sensitivity of maquette magnetoreceptors using specialised magnetic resonance and optical spectroscopic techniques.
(e)  Develop efficient and accurate methods for simulating the coherent spin dynamics of realistic radical pairs in order to interpret experimental data, guide the implementation of new experiments, test concepts of magnetoreceptor function, and guide the design of efficient sensors.
(f)  Explore the feasibility of electronically addressable, organic semiconductor sensors inspired by radical pair magnetoreception.","2997062","2013-12-01","2018-11-30"
"CHEMPLAN","Astrochemistry and the Origin of Planetary Systems","Ewine Fleur Van Dishoeck","UNIVERSITEIT LEIDEN","When interstellar clouds collapse to form new stars and planets, the surrounding gas and dust become part of the infalling envelopes and rotating disks, thus providing the basic material from which new solar systems are made. Instrumentation to probe the physics and chemistry in low-mass star-forming regions has so far lacked spatial resolution. I propose here an integrated observational-modeling-laboratory program to survey protostars and disks on the relevant scales of 1-50 AU where planet formation takes place. The observations are centered on new data coming from the Atacama Large Millimeter / submillimeter Array (ALMA), and the analysis includes unique new data from key programs on Herschel, Spitzer and VLT that I am (co-)leading. The combination of millimeter and infrared data allows the full range of temperatures from 10-2000 K in star- and planet- forming regions to be probed, for both gas and solids. The molecular line data are used as diagnostics of physical parameters (such as UV field, cosmic ray ionization rate, kinematics, mixing, shock strength, grain growth, gas/dust ratios) as well as to follow the chemistry of water and complex organic molecules from cores to disks, which ultimately may be delivered to terrestrial planets. The implications for the history of volatile material in our own solar systen and exo-planetary atmospheres will be assessed by comparing models and data with cometary taxonomy and, ultimately, feeding them into planet population synthesis models. Altogether, this program will bring the link between interstellar chemistry and solar system and exo-planetary research to a new level.

The project will train four PhD students in a truly interdisciplinary environment in which they are exposed to all aspects of molecular astrophysics and have access to ample ALMA expertise, and it will prepare two postdocs for future faculty positions.","2499150","2012-07-01","2018-06-30"
"CHESS","Challenges in Extraction and Separation of Sources","Christian Patrice Jutten","UNIVERSITE GRENOBLE ALPES","Separation/extraction of sources are wide concepts in information sciences, since sensors provide information mixing and an essential step consists in separating or extracting useful information from unuseful one, called noise. In this project, we consider three challenges.

The first one is the multimodality. Indeed, with the multiplication of kinds of sensors, in many areas like biomedical signal processing, hyperspectral imaging, etc. there are many ways for recording the same physical phenomenon leading thus to multimodal data. Multimodality has been studied in the framework of human-computer interface or in data fusion, but never at the signal level. The objective is to provide a general framework for modeling classical multimodal properties, like complementarity, redundancy, equivalence, etc. as of function of source signals.

The second challenge is nonlinearity. Indeed, there exist a few cases where the mixtures are essentially nonlinear, e.g. with chemical sensors. The main objective is to enlarge results on identifiability conditions for new classes of nonlinearities and priors on sources.

The third challenge is the data size. For high-dimension data (e.g. EEG or MRI in brain imaging), separating all the sources is neither tractable nor relevant, since one would like to only extract the useful sources. Conversely, for a small number of sensors, especially smaller than the number of sources, it is again necessary to only focus on the useful signals. The main objective is to develop generic approaches able to only extract useful signals, based on simple reference signal, modeling weak properties of the useful signal.

Finally, validation and relevant modeling must be based on actual signals and problems. In this project, theoretical results and algorithms will be developed in interaction with applications in biomedical engineering (brain-computer interface, EEG, fMRI), chemical engineering, audio-visual scene analysis and hyperspectral imaging.","2499390","2013-03-01","2018-02-28"
"CHIMO","Chiral Morphogenesis - Physical Mechanisms of Actomyosin-Based Left/Right Symmetry Breaking in Biological Systems","Stephan Grill","TECHNISCHE UNIVERSITAET DRESDEN","The aim of this grant is to understand how cellular, tissue-scale and organismal left-right asymmetry arises
from the chirality of molecular constituents. In many instances the actomyosin cortex, a thin and
mechanically active layer of dynamically cross-linked filaments and molecular motors at the surface of cells,
drives the emergence of chiral morphogenetic events. In the nematode Caenorhabditis elegans, mesoscale
chiral active torques generated by this active layer establish the embryo’s left-right body axis. Here we want
to understand how mesoscale actomyosin active torques are generated at the molecular level, and how active
torque generation in the actomyosin surface drives chiral morphogenesis of cells, tissues and organisms.
Cells and tissues represent a new class of active chiral materials where both the force and the torque balance
need to be considered, and we will perform a systematic and cross-scale characterization of active chiral
biological matter. We will pursue an interdisciplinary approach at the interface of physics and biology. At the
molecular-scale, we will use optical tweezers to measure active torques generated by single molecules of the
molecular myosin and the actin polymerizing protein formin. At the cell-scale, we will reconstitute chiral
actomyosin flows in vitro and characterize chiral dynamics of single molecules in vivo. At the tissue-scale,
we will investigate chiral cell movements in a multicellular environment and unravel the physical basis of
chiral tissue flow in vertebrates. Theory is essential at all stages, and we will build a molecular-scale model
of actomyosin torque generation that will be coarse-grained to a generalized hydrodynamic description of
active chiral matter. This interdisciplinary and cross-scale approach will provide fundamentally new insights
into active chiral materials and the mechanisms by which left-right asymmetries arise in development.","2500000","2018-01-01","2022-12-31"
"CHIRALLCARBON","Chiral Allotropes of Carbon","Nazario Martín","UNIVERSIDAD COMPLUTENSE DE MADRID","The aim of the present project is to answer fundamental questions about how to introduce chirality into a variety of carbon nanostructures and how it modifies the properties in the search for new applications in materials science and nanotecnology. Thus, it describes a fundamental and technological research program designed to gain new knowledge for the development of novel covalent and supramolecular chiral carbon nanoforms, and their further chemical modification for the preparation of sophisticated supramolecular 3D nanoarchitectures. Our research activity should reinforce and integrate the strong position of Europe in the knowledge of carbon nanoforms.
This important scientific challenge has not been properly addressed so far due to the inherent difficulties to work on these materials and, particularly, to the lack of an efficient chemical protocol to prepare chiral carbon nanoforms.","2235000","2013-04-01","2019-03-31"
"CHLIP","""Understanding Halogenated Lipids: Synthesis, Mode of Action, Structural Studies, and Applications""","Erick Moran Carreira","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","""Among the various toxins isolated, the chlorosulfolipids are particularly intriguing because of their structural and stereochemical complexity. The mechanism of biological activity remains unknown. The lack of availability of the natural products has impaired more in-depth studies aimed at pharmacological, biological, and chemical characterization for proper evaluation of the risk for human health and their role in nature. The proposal takes as its basis this unusual class of natural products and delineates a multifaceted program of  inquiry involving: (1) structural characterization of the most complex chlorosulfolipid isolated to date, (2) conformational studies in solution of chlorinated lipids, (3) synthesis and study of brominated lipid analogs, (4) development of analytical methods for detection of these toxins in the environment, (5) the discovery and development of reagents and catalysts for asymmetric chlorination of olefins, (6) examination of lipid conformation in constrained media, (7) examination of the mechanism of anchimeric assistance by chlorides, and (8) applications to drug discovery.""","2233240","2013-03-01","2018-02-28"
"CHOMP","A Complete History of Massive Proto-Galaxies","James Dunlop","THE UNIVERSITY OF EDINBURGH","A key question in modern science is to explain how the present-day universe of galaxies evolved from the initial conditions measured in the micro-wave background at recombination. Over the next 5 years I propose to undertake a major program of research to address this issue, by discovering and studying directly the progenitors of today&apos;s massive galaxies during the first ~2 billion years of cosmic history, and hence performing critical tests of current theories of galaxy formation. It is now clear that to sample representative volumes of the high-redshift universe requires ultra-deep near-infrared, mid-infrared and sub-mm surveys covering over ~1 sq. degree. Until now this has not been possible, but this field is about to be revolutionized by the introduction of a new generation of wide-field facilities in the next year. Specifically, 2009 will see the commissioning of the new near-infrared VISTA survey telescope in Chile, the new SCUBA2 sub-mm camera on the JCMT in Hawaii, the far-infrared Herschel Space Observatory, and the near-infrared camera WFC3 in the Hubble Space Telescope. Now, through my leadership of the deepest of the new generation of wide-field infrared and submm surveys to be undertaken with these revolutionary new facilities, I am unusually well-placed to take an integrated approach to the study of galaxy formation/evolution reaching back, for the first time, into the epoch of re-ionisation, at redshifts z ~ 7 - 10. Through this application I request the level of support required to exploit these new and unique data in what is one of the most important and topical areas at the forefront of modern astronomical research. Investment in this research program will also help ensure that European astronomers are strongly positioned to exploit the James Webb Space Telescope (JWST), the Atacama Large Millimetre Array (ALMA), and future large telescopes (e.g. E-ELT) to study the physics of galaxy formation over virtually all of cosmic history.","2317255","2010-04-01","2016-03-31"
"CHROMIUM","CHROMIUM","Jennifer THOMAS","UNIVERSITY COLLEGE LONDON","Why the Universe is void of anti-matter is one of the remaining Big Questions in Science.One explanation is provided within the Standard Model by violation of Charge Parity (CP) symmetry, producing differences between the behavior of particles and their anti-particles.CP violation in the neutrino sector could allow a mechanism by which the matter-anti matter asymmetry arose.The objective of this proposal is to enable a step change in our sensitivity to CP violation in the neutrino sector. I have pioneered the concepts and led the deployment of a small prototype using a novel approach which could eventually lead to the construction of a revolutionary Mega-ton scale Water Cherenkov (WC) neutrino detector.The goal of my research program is to demonstrate the feasibility of this approach via the construction of an intermediate sized prototype with an expandable fiducial mass of up to 10-20kt. It will use a low-cost and lightweight structure, filled with purified water and submerged for mechanical strength and cosmic ray shielding in a 60m deep flooded mine pit in the path of Fermilab’s NuMI neutrino beam in N. Minnesota.The European contribution to this experiment will be profound and definitive.Applying the idea of fast timing and good position resolution of small photodetectors, already pioneered in Europe, in place of large-area photodetector, we will revolutionize WC design.The game-changing nature of this philosophy will be demonstrated via the proof of the detector construction and the observation of electron neutrino events form the NuMI beam.The successful completion of this R&D program will demonstrate a factor of up to 100 decrease in cost compared to conventional detectors and the proof that precision neutrino measurements could be made inside a few years rather than the presently needed decades.
The project describes a five year program of work amounting to a total funding request of €3.5M, including an extra €1M of equipment funds.","3500000","2016-10-01","2021-09-30"
"CHROMPHYS","Physics of the Solar Chromosphere","Mats Per-Olof Carlsson","UNIVERSITETET I OSLO","CHROMPHYS aims at a breakthrough in our understanding of the solar chromosphere by combining the development of sophisticated radiation-magnetohydrodynamic simulations with observations from the upcoming NASA SMEX mission Interface Region Imaging Spectrograph (IRIS).

The enigmatic chromosphere is the transition between the solar surface and the eruptive outer solar atmosphere.  The chromosphere harbours and constrains the mass and energy loading processes that define the heating of the corona, the acceleration and the composition of the solar wind, and the energetics and triggering of solar outbursts (filament eruptions, flares, coronal mass ejections) that govern near-Earth space weather and affect mankind's technological environment.

CHROMPHYS targets the following fundamental physics questions about the chromospheric role in the mass and energy loading of the corona:

- Which types of non-thermal energy dominate in the chromosphere and beyond?

- How does the chromosphere regulate mass and energy supply to the corona and the solar wind?

- How do magnetic flux and matter rise through the chromosphere?

- How does the chromosphere affect the free magnetic energy loading that leads to solar eruptions?

CHROMPHYS proposes to answer these by producing a new, physics based vista of the chromosphere through a three-fold effort:

- develop the techniques of high-resolution numerical MHD physics to the level needed to realistically predict and analyse small-scale  chromospheric structure and dynamics,

- optimise and calibrate diverse observational diagnostics by synthesizing these in detail from the simulations, and

- obtain and analyse data from IRIS using these diagnostics complemented by data from other space missions and the best solar telescopes on the ground.","2487600","2012-01-01","2016-12-31"
"CICERO","Cold Ion Chemistry - Experiments within a Rydberg Orbit","Frédéric MERKT","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","""To date no experiment has investigated ion-molecule reactions at temperatures significantly below about 20 K, for two reasons: (i) Cooling the translational and internal degrees of freedom of ions and molecules is extremely challenging. (ii) Even very weak stray electric fields accelerate the ions. A potential difference of only 1 mV across the reaction volume imparts a kinetic energy of 1 meV to ions, which corresponds to a temperature of about 12 K. Quantum mechanical effects arising from the translational and the frozen or hindered rotational motion of the reactants in the intermolecular potential are only expected to be significant below 20 K and have therefore not been observed yet in ion-molecule reactions, even for reactions involving the lightest ions and molecules. This proposal aims at developing a new experimental method to study ion-molecule reactions at temperatures down to 100 mK and to study ion-molecule reactions involving light species, with particular emphasis placed on the observation and quantification of quantum effects in low-temperature ion-molecule chemistry. To reach this goal, we will study the ion-molecule reactions within the orbit of a highly excited Rydberg electron, which will shield the reaction from stray fields without affecting its outcome. To reach very low collision energies, we will use a merged-beam approach relying on a surface-electrode Rydberg-Stark deflector. In the preparatory phase of this proposal, we have carried out a proof-of-principle measurement of the H2+ + H2 -> H3+ + H reaction below 1 K using a simplified version of the """"ideal"""" instrument and demonstrated the feasibility of our method. We now plan to exploit the full potential of our new approach and study important ion-molecule reactions in a temperature range thought until now to be experimentally inaccessible.""","2130088","2017-06-01","2022-05-31"
"CIF","Complex Interfacial Flows: From the Nano- to the Macro-Scale","Serafim Kalliadasis","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","A wide variety of natural phenomena and technological applications involve flow, transport and chemical reactions taking place on or near fluid-solid or fluid-fluid interfaces. From gravity currents under water and lava flows to heat and mass transport processes in engineering applications and to the rapidly developing field of microfluidics. Both equilibrium properties of a fluid and transportcoefficients are modified in the vicinity of interfaces. The effect of these changes is crucial in the behavior of ultra-thin fluidfilms and fluid motion in microchannels of micro-electromechanical systems, but is essential as well in macroscopic phenomena involving interfacial singularities, such as thin-film rupture and motion of three-phase contact lines associated e.g. with droplet spreading. Interface boundaries are mesoscopic structures. While material properties vary smoothly at macroscopic distances from an interface, gradients in the normal direction of conserved parameters, such as density, are steep with strong variations as the molecular scale in the neighborhood of the interface is approached. This brings about a contradiction between the need in macroscopic description and a necessity to take into consideration microscopic factors that come to influence the fluid motion and transport on incommensurately larger scales. The aim of the proposed research is to develop a class of novel continuous models bridging the gap between molecular dynamics and conventional hydrodynamics and applicable at mesoscopic distances from gas-liquid and fluid-solid interfaces. A combination of analytical techniques, numerical modeling and computer-aided multiscale analysis will be employed. The results of the proposed work will greatly contribute to the fundamental understanding of mesoscopic non-equilibrium phenomena in the vicinity of interfaces and to the development of novel computational methods combining the advantages of molecular and continuous models.","1273788","2010-04-01","2016-03-31"
"CIMNAS","Corrosion Initiation Mechanisms at the Nanometric/Atomic Scale","Philippe MARCUS","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The failure of metallic materials caused by corrosion strongly impacts our society with cost, safety, health and performance issues. The mechanisms of corrosion propagation are fairly well understood, and various means of mitigation are known even if research is still necessary to improve this knowledge or to develop corrosion protection for the application of new materials. The vision of CIMNAS is that a major breakthrough for corrosion protection lies in a deep understanding and control of the initiation stage triggering corrosion. Corrosion initiation takes place at the atomic/molecular scale or at a scale of a few nanometres (the nanoscale) on metal and alloy surfaces, metallic, oxidised or coated, and interacting with the corroding environment. The mission of CIMNAS is to challenge the difficulty of understanding corrosion initiation at the nanometric/atomic scale on such complex interfaces, ultimately aiming at designing more robust metallic surfaces via the understanding of corrosion mechanisms. The project is constructed on new ideas to achieve three knowledge breakthroughs, each answering a key question for the understanding of corrosion initiation on metal and alloy surfaces. It is envisioned that the model approach used and the achieved breakthroughs will open up a new horizon for research on corrosion initiation mechanisms at the nanoscale, and new opportunities for a knowledge-based design of novel corrosion protection technologies. Technologies presently at low TRL (Technology Readiness Level) will benefit from these breakthroughs. Resources will include a team of highly experienced and recognised researchers headed by the PI, a unique apparatus recently installed at the PI’s lab, integrating surface spectroscopy, microscopy, and electrochemistry for in situ measurements in a closed system, novel experimental approaches, and a strong complementarity of experiments and modelling.","1657056","2017-09-01","2021-08-31"
"CIO","Common Interactive Objects","Susanne Bødker","AARHUS UNIVERSITET","In CIO, common interactive objects are developed and explored to extend human control over the technological environment by human beings, both individually and together. CIO leads to a coherent framework of user interfaces to be applied in interaction design. Common interactive objects will provide a useful frame for furthering human computer interaction (HCI) theory, development of interaction design methods and the underlying technical platforms. Common interactive objects will empower users to better understand and develop the technologies they use. 
When carried through, the project offers new ways for people to construct and configure human physical and virtual environments, together, over time and within communities. 
The main objectives of CIO are to
1. develop the conception of common interactive objects in order to offer a new understanding of human-computer interaction, focusing on human control. 
2. develop support for building user interfaces in a coherent and unified framework. 
3. make common interactive objects that will empower users to better understand and develop the technologies they use. 
4. carry out ground-breaking research regarding the technological basis of common interactive objects with focus on malleability, control and shareability over time.
CIO is methodologically rooted in HCI. CIO’s research methods combine empirical, analytical, theoretical, and design approaches, all with focus on the relationship between common interactive objects and their human users. 
CIO presents the idea that common interactive objects may radically innovate our understanding of use and building user interfaces. The gains of CIO will be a coherent new, high-impact way of understanding and building HCI across physical and virtual structures, bringing control back to the users. The risks are in delivering this alternative in a manner that is able to confront the current strong commercial interests in the Internet-of-Things and the 'new' Artificial Intelligence","2398993","2017-12-01","2022-11-30"
"CISS","Chiral Induced Spin Selectivity","Ron Naaman","WEIZMANN INSTITUTE OF SCIENCE LTD","The overall objective is to fully understand  the Chiral Induced Spin Selectivity (CISS) effect, which was discovered recently. It was found that the transmission or conduction of electrons through chiral molecules is spin dependent. The CISS effect is a change in the pradigm that assumed that any spin manipulation requiers magnetic materials or materials with high spin-orbit coupling. These unexpected new findings open new possibilities for  applying chiral molecules in spintronics applications and may provide new insights on electron transfer processes in Biology.
The specific goals of the proposed research are
(i) To establish the parameters that affect the magnitude of the CISS effect.
(ii) To demonstrate spintronics devices (memory and transistors) that are based on the CISS effect.
(iii) To investigate  the role of CISS in electron transfer in biology related systems.
The experiments will be performed applying a combination of experimental methods including photoelectron spectroscopy, single molecule conduction, light-induced electron transfer, and spin specific conduction through magneto-electric devices.
The project has a potential to have very large impact on various fields from Physics to Biology. It will result in the establishment of chiral organic molecules as a new substrate for wide range of spintronics related applications including magnetic memory, and in determining whether spins play a role in electron transfer processes in biology.","2499998","2013-10-01","2018-09-30"
"CLEAN-ICE","Detailed chemical kinetic models for cleaner internal combustion engines","Frederique Battin-Leclerc","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The key objective of this project is to promote cleaner and more efficient combustion technologies through the development of theoretically grounded and more accurate chemical models. This is motivated by the fact that the current models which have been developed for the combustion of constituents of gasoline, kerosene, and diesel fuels do a reasonable job in predicting auto-ignition and flame propagation parameters, and the formation of the main regulated pollutants. However their success rate deteriorates sharply in the prediction of the formation of minor products (alkenes, dienes, aromatics, aldehydes) and soot nano-particles, which have a deleterious impact on both the environment and on human health. At the same time, despite an increasing emphasis in shifting from hydrocarbon fossil fuels to bio-fuels (particularly bioethanol and biodiesel), there is a great lack of chemical models for the combustion of oxygenated reactants. The main scientific focus will then be to enlarge and deepen the understanding of the reaction mechanisms and pathways associated with the combustion of an increased range of fuels (hydrocarbons and oxygenated compounds) and to elucidate the formation of a large number of hazardous minor pollutants. The core of the project is to describe at a fundamental level more accurately the reactive chemistry of minor pollutants within extensively validated detailed mechanisms for not only traditional fuels, but also innovative surrogates, describing the complex chemistry of new environmentally important bio-fuels. At the level of individual reactions rate constants, generalized rate constant classes and molecular data will be enhanced by using techniques based on quantum mechanics and on statistical mechanics.  Experimental data for validation will be obtained in well defined laboratory reactors by using analytical methods of increased accuracy.","1869450","2008-12-01","2013-11-30"
"CLIM","Computational Light fields IMaging","Christine GUILLEMOT","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","Light fields technology holds great promises in computational imaging. Light fields cameras capture light rays as they interact with physical objects in the scene. The recorded flow of rays (the light field) yields a rich description of the scene enabling advanced image creation capabilities from a single capture. This technology is expected to bring disruptive changes in computational imaging. However, the trajectory to a deployment of light fields remains cumbersome. Bottlenecks need to be alleviated before being able to fully exploit its potential. Barriers that CLIM addresses are the huge amount of high-dimensional (4D/5D) data produced by light fields, limitations of capturing devices, editing and image creation capabilities from compressed light fields. These barriers cannot be overcome by a simple application of methods which have made the success of digital imaging in past decades. The 4D/5D sampling of the geometric distribution of light rays striking the camera sensors imply radical changes in the signal processing chain compared to traditional imaging systems.

The ambition of CLIM is to lay new algorithmic foundations for the 4D/5D light fields processing chain, going from representation, compression to rendering. Data processing becomes tougher as dimensionality increases, which is the case of light fields compared to 2D images. This leads to the first challenge of CLIM that is the development of methods for low dimensional embedding and sparse representations of 4D/5D light fields. The second challenge is to develop a coding/decoding architecture for light fields which will exploit their geometrical models while preserving the structures that are critical for advanced image creation capabilities. CLIM targets ground-breaking solutions which should open new horizons for a number of consumer and professional markets (photography, augmented reality, light field microscopy, medical imaging, particle image velocimetry).","2461086","2016-09-01","2021-08-31"
"CLOTHILDE","CLOTH manIpulation Learning from DEmonstrations","Carmen TORRAS GENIS","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","Textile objects pervade human environments and their versatile manipulation by robots would open up a whole range of
possibilities, from increasing the autonomy of elderly and disabled people, housekeeping and hospital logistics, to novel
automation in the clothing internet business and upholstered product manufacturing. Although efficient procedures exist for
the robotic handling of rigid objects and the virtual rendering of deformable objects, cloth manipulation in the real world has
proven elusive, because the vast number of degrees of freedom involved in non-rigid deformations leads to unbearable
uncertainties in perception and action outcomes.

This proposal aims at developing a theory of cloth manipulation and carrying it all the way down to prototype implementation in our Lab. By combining powerful recent tools from computational topology and machine learning, we plan to characterize the state of textile objects and their transformations under given actions in a compact operational way (i.e., encoding task-relevant topological changes), which would permit probabilistic planning of actions (first one handed, then bimanual) that ensure reaching a desired cloth configuration despite noisy perceptions and inaccurate actions.

In our approach, the robot will learn manipulation skills from an initial human demonstration, subsequently refined through
reinforcement learning, plus occasional requests for user advice. The skills will be encoded as parameterised dynamical
systems, and safe interaction with humans will be guaranteed by using a predictive controller based on a model of the robot
dynamics. Prototypes will be developed for 3 envisaged applications: recognizing and folding clothes, putting an elastic
cover on a mattress or a car seat, and helping elderly and disabled people to dress. The broad Robotics and AI background
of the PI and the project narrow focus on clothing seem most appropriate to obtain a breakthrough in this hard fundamental
research topic.","2499149","2018-01-01","2022-12-31"
"CLOUDMAP","Cloud Computing via Homomorphic Encryption and Multilinear Maps","Jean-Sebastien Coron","UNIVERSITE DU LUXEMBOURG","The past thirty years have seen cryptography move from arcane to commonplace: Internet, mobile phones, banking system, etc. Homomorphic cryptography now offers the tantalizing goal of being able to process sensitive information in encrypted form, without needing to compromise on the privacy and security of the citizens and organizations that provide the input data.  More recently, cryptographic multilinear maps have revolutionized cryptography with the emergence of indistinguishability obfuscation (iO), which in theory can been used to realize numerous advanced cryptographic functionalities that previously seemed beyond reach. However the security of multilinear maps is still poorly understood, and many iO schemes have been broken; moreover all constructions of iO are currently  unpractical. 

The goal of the CLOUDMAP project is to make these advanced cryptographic tasks usable in practice, so that citizens do not have to compromise on the privacy and security of their input data. This goal can only be achieved by considering the mathematical foundations of these primitives, working ""from first principles"", rather than focusing on premature optimizations. To achieve this goal, our first objective will be to better understand the security of the underlying primitives of multilinear maps and iO schemes. Our second objective will be to develop new approaches to significantly improve their efficiency. Our third objective will be to build applications of multilinear maps and iO that can be implemented in practice.","2491266","2018-10-01","2023-09-30"
"CloudRadioNet","Cloud Wireless Networks: An Information Theoretic Framework","Shlomo Shamai Shitz","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","This five years research proposal is focused on the development of novel information theoretic concepts and techniques and their usage, as to identify the ultimate communications limits and potential of different cloud radio network structures, in which the central signal processing is migrated to the cloud (remote central units), via fronthaul/backhaul infrastructure links. Moreover, it is also directed to introduce and study the optimal or close to optimal strategies for those systems that are to be motivated by the developed theory. We plan to address wireless networks, having future cellular technology in mind, but the basic tools and approaches to be built and researched are relevant to other communication networks as well. Cloud communication networks motivate novel information theoretic views, and perspectives that put backhaul/fronthaul connections in the center, thus deviating considerably from standard theoretical studies of communications links and networks, which are applied to this domain. Our approach accounts for the fact that in such networks information theoretic separation concepts are no longer optimal, hence isolating simple basic components of the network is essentially suboptimal. The proposed view incorporates, in a unified way, under the general cover of information theory: Multi-terminal distributed networks; Basic and timely concepts of distributed coding and communications; Network communications and primarily network coding, Index coding, as associated with interference alignment and caching; Information-Estimation relations and signal processing, addressing the impact of distributed channel state information directly; A variety of fundamental concepts in optimization and random matrix theories. This path provides a natural theoretical framework directed towards better understanding the potential and limitation of cloud networks on one hand and paves the way to innovative communications design principles on the other.","1981782","2016-07-01","2021-06-30"
"CLUNATRA","Discovering new Catalysts in the Cluster-Nanoparticle Transition Regime","Ib CHORKENDORFF","DANMARKS TEKNISKE UNIVERSITET","The purpose of this proposal is to establish new fundamental insight of the reactivity and thereby the catalytic activity of oxides, nitrides, phosphides and sulfides (O-, N-, P-, S- ides) in the Cluster-Nanoparticle transition regime. We will use this insight to develop new catalysts through an interactive loop involving DFT simulations, synthesis, characterization and activity testing. The overarching objective is to make new catalysts that are efficient for production of solar fuels and chemicals to facilitate the implementation of sustainable energy, e.g. electrochemical hydrogen production and reduction of CO2 and N2 through both electrochemical and thermally activated processes. 
Recent research has identified why there is a lack of significant progress in developing new more active catalysts. Chemical scaling-relations exist among the intermediates, making it difficult to find a reaction pathway, which provides a flat potential energy landscape - a necessity for making the reaction proceed without large losses. My hypothesis is that going away from the conventional size regime, > 2 nm, one may break such chemical scaling-relations. Non-scalable behavior means that adding an atom results in a completely different reactivity. This drastic change could be even further enhanced if the added atom is a different element than the recipient particle, providing new freedom to control the reaction pathway. The methodology will be based on setting up a specifically optimized instrument for synthesizing such mass-selected clusters/nanoparticles. Thus far, researchers have barely explored this size regime. Only a limited amount of studies has been devoted to inorganic entities of oxides and sulfides; nitrides and phosphides are completely unexplored. We will employ atomic level simulations, synthesis, characterization, and subsequently test for specific reactions. This interdisciplinary loop will result in new breakthroughs in the area of catalyst material discovery.","2500000","2017-09-01","2022-08-31"
"CME","Concurrency Made Easy","Bertrand Philippe Meyer","POLITECNICO DI MILANO","The “Concurrency Made Easy” project is an attempt to achieve a conceptual breakthrough on the most daunting challenge in information technology today: mastering concurrency. Concurrency, once a specialized technique for experts, is forcing itself onto the entire IT community because of a disruptive phenomenon: the “end of Moore’s law as we know it”. Increases in performance can no longer happen through raw hardware speed, but only through concurrency, as in multicore architectures. Concurrency is also critical for networking, cloud computing and the progress of natural sciences. Software support for these advances lags, mired in concepts from the 1960s such as semaphores. Existing formal models are hard to apply in practice. Incremental progress is not sufficient; neither are techniques that place the burden on programmers, who cannot all be expected to become concurrency experts. The CME project attempts a major shift on the side of the supporting technology: languages, formal models, verification techniques. The core idea of the CME project is to make concurrency easy for programmers, by building on established ideas of modern programming methodology (object technology, Design by Contract) shifting the concurrency difficulties to the internals of the model and implementation.
The project includes the following elements.
1. Sound conceptual model for concurrency. The starting point is the influential previous work of the PI: concepts of object-oriented design, particularly Design by Contract, and the SCOOP concurrency model.
2. Reference implementation, integrated into an IDE.
3. Performance analysis.
4. Theory and formal basis, including full semantics.
5. Proof techniques, compatible with proof techniques for the sequential part.
6. Complementary verification techniques such as concurrent testing.
7. Library of concurrency components and examples.
8. Publication, including a major textbook on concurrency.","2482957","2012-04-01","2018-09-30"
"CMetC","Selective Carbon-Carbon Bond Activation: A Wellspring of Untapped Reactivity","Ilan Marek","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","The creation of new molecular entities and subsequent exploitation of their properties is central to a broad spectrum of research disciplines from medicine to materials. Most –if not all- of the efforts of organic chemists were directed to the development of creative strategies to built carbon-carbon and carbon-heteroatom bonds in a predictable and efficient manner. But is the creation of new bonds the only approach that organic chemistry should follow? Could we design the synthesis of challenging molecular skeleton no more through the construction of carbon-carbon bonds but rather through selective cleavage of carbon-carbon bonds (C-C bond activation)? The goal of this work is to develop powerful synthetic approaches for the selective C-C bond activation and demonstrate that it has the potential to be a general principle in organic synthesis for the regio-, diastereo- and even enantiomerically enriched preparation of adducts despite that C-C single bonds belong among the least reactive functional groups in chemistry. The realization of this synthetic potential requires the ability to functionalize selectively one C-C bond in compounds containing many such bonds and an array of functional groups. This site selective C-C bond activation is one of the greatest challenges that must be met to be used widely in complex-molecular synthesis. To emphasize the practicality of C-C bond activation, we will prepare in a single-pot operation challenging molecular framework possessing various stereogenic centers from very simple starting materials through selective C-C bond activation. Ideally, alkenes will be in-situ transformed into alkanes that will subsequently undergo the C-C activation even in the presence of functional group. This work will lead to ground-breaking advances when non-strained cycloalkanes (cyclopentane, cyclohexane) will undergo this smooth C-C bond activation with friendly and non toxic organometallic species.","2367495","2013-11-01","2018-10-31"
"CMR","Cosmic ray acceleration, magnetic field and radiation hydrodynamics","Anthony Raymond Bell","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Diffusive shock acceleration is widely acknowledged as the most likely source of cosmic rays and high energy particles. The basic macroscopic theory of how cosmic rays gain energy during multiple shock crossings is well known, but the microphysics of the interaction between cosmic rays (CR) and the MHD background fluid remained poorly understood before the recent discovery of a new non-resonant instability by which the CR precursor could greatly amplify the ambient magnetic field. The aims of the project are: 1) to develop the first self-consistent non-linear simulation of the CR/MHD interaction; to calculate the magnitude of the saturated magnetic field and the maximum energy to which CR are accelerated. We will characterise the structure of the amplified magnetic field and compare it with x-ray observations of the time-evolving outer shock of supernova remnants (SNR). We will investigate the effect of various orientations of the shock relative to the ambient magnetic field, the effect of non-diffusive transport on the energy spectrum and CR escape from the SNR, and how these match observation. 2) to extend the simulation to relativistic shocks as found in gamma-ray bursts (GRB) and active galactic nuclei (AGN); to establish whether the non-resonant instability operates effectively at relativistic shock velocities, whether it explains the large magnetic field found in GRB, and determine the maximum CR energy achieved by relativistic shocks. 3) to investigate high density shocks in GRB, x-ray flashes (XRF) and supernovae (SN) where radiative processes, pair production and other particle/photon and particle/particle interactions are important. We shall investigate CR acceleration on SN shock breakout and very young SNR as a possible source of very high energy CR.","900024","2010-05-01","2015-04-30"
"CoBCoM","Computational Brain Connectivity Mapping","Rachid DERICHE","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","One third of the burden of all the diseases in Europe is due to problems caused by diseases affecting brain. Although exceptional progress has been obtained for exploring it during the past decades, the brain is still terra-incognita and calls for specic research efforts to better understand its architecture and functioning.

CoBCoM is our response to this great challenge of modern science with the overall goal to develop a joint Dynamical Structural-Functional Brain Connectivity Network (DSF-BCN) solidly grounded on advanced and integrated methods for diffusion Magnetic Resonance Imaging (dMRI) and Electro & Magneto-Encephalography (EEG & MEG).

To take up this grand challenge and achieve new frontiers for brain connectivity mapping, we will develop a new generation of computational models and methods for identifying and characterizing the structural and functional connectivities that will be at the heart of the DSF-BCN. Our strategy is to break with the tradition to incrementally and separately contributing to structure or function and develop a global approach involving strong interactions between structural and functional connectivities. To solve the limited view of the brain provided just by one imaging modality, our models will be developed under a rigorous computational framework integrating complementary non invasive imaging modalities: dMRI, EEG and MEG.

CoBCoM will push far forward the state-of-the-art in these modalities, developing innovative models and ground-breaking processing tools to provide in-fine a joint DSF-BCN solidly grounded on a detailed mapping of the brain connectivity, both in space and time.

Capitalizing on the strengths of dMRI, MEG & EEG methodologies and building on the bio- physical and mathematical foundations of our new generation of computational models, CoBCoM will be applied to high-impact diseases, and its ground-breaking computational nature and added clinical value will open new perspectives in neuroimaging.","2469123","2016-09-01","2021-08-31"
"COBOM","Convective Boundary Mixing in Stars","Isabelle Baraffe","THE UNIVERSITY OF EXETER","Stellar evolution models are fundamental to nearly all fields of astrophysics, from exoplanet to galactic and extra-galactic research.
The heart of the COBOM project is to develop a global physical picture of fundamental mixing processes in stars in order to derive robust and predictive stellar evolution models.
The complex dynamics of flows at convective boundaries is a key process in stellar interiors that drives the transport of chemical species and heat, strongly affecting the structure and the evolution of many types of stars. The same physical processes can also drive transport of angular momentum, affecting the rotation evolution and the generation of magnetic field of stars. The treatment of mixing processes at convective boundaries (also referred to as overshooting) is currently one of the major uncertainties in stellar evolution theory. This mixing can dramatically affect the size of a convective core, the lifetime of major burning phases or the surface chemistry over a wide range of stellar masses.
The main objectives of this project are to (1) develop a global theoretical framework to describe mixing and heat transport at convective boundaries in stellar interiors, (2) derive new physically-based transport coefficients and parametrizations for one-dimensional stellar evolution models and (3) test the new formalisms against a wide range of observations.
We will accomplish these goals by performing the most comprehensive study ever performed of mixing processes in stars using a fundamentally new approach. We will combine the power of multi-dimensional fully compressible time implicit magneto-hydrodynamic simulations and rare event statistics, which are usually applied in finance or climate science.
The key strength of the project is to establish a direct link between multi-dimensional results and observations (asteroseismology, eclipsing binaries, color-magnitude diagrams) via the exploitation of 1D stellar evolution models.","2500000","2018-09-01","2023-08-31"
"COCAN","Complexity and Condition in Algebra and Numerics","Peter BÜRGISSER","TECHNISCHE UNIVERSITAT BERLIN","""This proposal connects three areas that are considered distant from each other: computational complexity, algebraic geometry, and numerics. In the last decade, it became clear that the fundamental questions of computational complexity (P vs NP) should be studied in algebraic settings, linking them to problems in algebraic geometry. Recent progress on this challenging and very difficult questions led to surprising progress in computational invariant theory, which we want to explore thoroughly. We expect this to lead to solutions of computational problems in invariant theory that currently are considered infeasible. The complexity of Hilbert's null cone (the set of """"singular objects'') appears of paramount importance here. These investigations will also shed new light on the foundational questions of algebraic complexity theory. As an essential new ingredient to achieve this, we will tackle the arising algebraic computational problems by means of approximate numeric computations, taking into account the concept of numerical condition.

A related goal of the proposal is to develop a theory of efficient and numerically stable algorithms in algebraic geometry that reflects the properties of structured systems of polynomial equations, possibly with singularities. While there are various heuristics, a satisfactory theory so far only exists for unstructured systems over the complex numbers (recent solution of Smale's 17th problem), which seriously limits its range of applications. In this framework, the quality of numerical algorithms is gauged by a probabilistic analysis that shows small average (or smoothed) running time. One of the main challenges here consists of a probabilistic study of random structured polynomial systems.  We will also develop and analyze numerical algorithms for finding or describing the set of real solutions, e.g., in terms of their homology. 
""","2297163","2019-01-01","2023-12-31"
"COCO2CASA","Modeling Stellar Collapse and Explosion: Evolving Progenitor Stars to Supernova Remnants","Hans-Thomas Janka","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","""This project intends to make groundbreaking progress towards the solution of one of the most pestering and long-standing riddles of stellar astrophysics, namely the question how massive stars explode as supernovae (SNe).
State-of-the-art simulations in two dimensions (2D) now yield neutrino-powered (through underenergetic) explosions for a growing variety of progenitors and thus support the delayed neutrino-heating mechanism. However, sophisticated, fully self-consistent, 3D simulations are still lacking, the spherical symmetry of the progenitor star models is becoming a serious handicap, and better exploitation of observational constraints of the SN mechanism is urgently needed.
For these reasons we plan a novel, comprehensive modeling approach, in which 3D hydrodynamics including all relevant microphysics will not only be employed for the launch phase of the SN blast wave by neutrino-energy deposition. Different from previous initiatives, 3D hydrodynamics will also be applied to the final stages of convective shell burning in the progenitor core before collapse in order to derive --for the first time-- self-consistent, multidimensional progenitor data for adopting them as initial conditions in the SN modeling. Moreover, the 3D explosion simulations will be continued consistently through the long-time evolution of the SN outburst into the gaseous remnant phase. This challenging approach promises fundamentally new insights into the processes that trigger and shape SN explosions and will revise our understanding of how SNe depend on the properties of their progenitor stars. Moreover, heading for a direct comparison of the derived theoretical models with nearby young SN remnants like Crab, Cassiopeia A, and SN 1987A, whose 3D morphology and composition are currently unfolded in stunning detail by multiwavelength observations, the project will lay the foundations of a powerful, innovative, and so far not exploited way of probing the physics deep inside the SN core.""","2898600","2014-02-01","2019-01-31"
"COCONIS","Coherent multidimensional spectroscopy of controlled isolated systems","Frank STIENKEMEIER","ALBERT-LUDWIGS-UNIVERSITAET FREIBURG","Fundamental quantum mechanical processes determine the properties of matter and their functionality. In order to understand complex processes such as light harvesting in photosynthesis and photovoltaics, a detailed knowledge of coherent effects in excitation and charge transfer processes and related dynamics is required. To a large extent, the complexity of the systems induces too many interactions and perturbations of the processes to isolate and understand individual mechanisms. Advanced experimental methods, capable of detecting quantum coherences, so far are not applicable to quantum state controlled molecular complexes isolated from the perturbing environment, due to the low density of such targets. In this project we will for the first time employ coherent femtosecond multidimensional spectroscopy to dilute isolated molecular complexes. For a specific heterogeneous synthesis we will use aggregation in superfluid helium at millikelvin temperatures. In order to reach the needed sensitivity we will setup a novel phase modulation technique including lock-in demodulation in combination with mass-resolved ionization and photoelectron detection. Advanced mathematical methods will furthermore be developed and applied, boosting efficient collection of multidimensional datasets. We will be able to (a) identify processes and coherent dynamics of excitation and charge transfer in fundamental heterogeneous complexes, in particular van der Waals bound donor acceptor complexes (b) elucidate coherence and dissipation effects in contact with tailored external baths, (c) investigate microsolvation, i.e. measure the evolution of dynamic properties as a function of attached solvent molecules, (d) determine collective effects like autoionization in dilute atomic gases or exciton annihilation in semiconductor systems, (e) implement compressed sensing in multidimensional data acquisition, (f) implement largely parallelized phase-cycling into real-time data acquisition.","2295000","2016-09-01","2021-08-31"
"CODE","Condensation in designed systems","Päivi Elina Törmä","AALTO KORKEAKOULUSAATIO SR","""Quantum coherent phenomena, especially marcoscopic quantum coherence, are among the most striking predictions of quantum mechanics. They have lead to remarkable applications such as lasers and modern optical technologies, and in the future, breakthroughs such as quantum information processing are envisioned. Macroscopic quantum coherence is manifested in Bose-Einstein condensation (BEC), superfluidity, and superconductivity, which have been observed in a variety of systems and continue to be at the front line of scientific research. Here my objective is to extend the realm of Bose-Einstein condensation into new conceptual and practical directions. I focus on the role of a hybrid character of the object that condenses and on the role of non-equilibrium in the BEC phenomenon. The work is mostly theoretical but has also an experimental part. I study two new types of hybrids, fundamentally different from each other. First, I consider pairing and superfluidity in a mixed geometry. Experimental realization of mixed geometries is becoming feasible in ultracold gases. Second, I explore the possibility of finding novel hybrids of light and matter excitations that may display condensation. By combining insight from these two cases, my goal is to understand how the hybrid and non-equilibrium nature can be exploited to design desirable properties, such as high critical temperatures. In particular, in case of the new light-matter hybrids, the goal is to provide realistic scenarios for, and also experimentally demonstrate, a room temperature BEC.""","1559608","2013-12-01","2018-11-30"
"CODITA","Cosmic Dust in the Terrestrial Atmosphere","John Maurice Campbell Plane","UNIVERSITY OF LEEDS","""This project addresses a fundamental problem – the size of the cosmic dust input to the earth’s atmosphere. Zodiacal cloud observations and spaceborne dust detection indicate a daily input of 100 – 300 tonnes, in agreement with the accumulation rates of cosmic elements (e.g. Ir, Pt) in polar ice cores and deep-sea sediments. In contrast, measurements in the middle atmosphere – by radar, lidar, high-flying aircraft and satellite remote sensing – indicate that the input is only 5 - 50 tonnes. The aim of CODITA is to resolve this huge discrepancy.

There are two reasons why this matters. First, if the upper range of estimates is correct, then vertical transport in the middle atmosphere must be considerably faster than generally believed; whereas if the lower range is correct, then our understanding of dust evolution in the solar system, and transport from the middle atmosphere to the surface, will need substantial revision. Second, cosmic dust particles enter the atmosphere at high speeds and in most cases completely ablate. The resulting metals injected into the atmosphere are involved in a diverse range of phenomena, including: formation of layers of metal atoms and ions; nucleation of noctilucent clouds; impacts on stratospheric aerosols and O3 chemistry (which need to be evaluated against the background of a cooling stratosphere and geo-engineering plans to increase sulphate aerosol); and fertilization of the ocean with bio-available Fe, which has potential climate feedbacks.

CODITA will use laboratory studies to target poorly understood aspects of this problem, such as the nature of the ablation process itself, the formation of meteoric smoke particles, and their role in ice nucleation and the freezing of polar stratospheric clouds. The results will be incorporated into a chemistry-climate model of the whole atmosphere, so that it will be possible, for the first time, to model the effects of cosmic dust self-consistently from the thermosphere to the surface.""","2484369","2012-04-01","2017-03-31"
"COGNET","Cognitive Networks for Intelligent Materials and Devices","John Boland","THE PROVOST, FELLOWS, FOUNDATION SCHOLARS & THE OTHER MEMBERS OF BOARD OF THE COLLEGE OF THE HOLY & UNDIVIDED TRINITY OF QUEEN ELIZABETH NEAR DUBLIN","""COGnitive NETwork (COGNET) is a new technology platform for materials, sensor and device design that exploits unique and hitherto unrecognised properties of random nanowire (NW) networks. These networks—comprised of metallic or semiconducting NWs connected to each other via junctions with controllably random property distributions—lead to new and unexpected levels of connectivity that are inherently scale dependent, creating opportunities for entirely new kinds of self-organised materials and devices. We propose to establish the ground rules for manipulating connectivity in NW networks.   By choosing appropriate NWs and incorporating junctions with the approprate properties COGNET will enable the fabrication of (i) intelligent materials, (ii) neural networks and (iii) memory devices.  Sequenced voltage pulse and back-gating techniques will in turn address and manipulate specific junctions or sets of junctions to demonstrate even higher density memory and in the case of neural networks, the possibility synaptic plasticity and self-learning.""","2497125","2013-06-01","2018-05-31"
"COIMBRA","Combinatorial methods in noncommutative ring theory","Agata Smoktunowicz","THE UNIVERSITY OF EDINBURGH","As noted by T Y Lam in his book, A first course in noncommutative rings, noncommutative ring theory is a fertile meeting ground for group theory (group rings), representation theory (modules), functional analysis (operator algebras), Lie theory (enveloping algebras), algebraic geometry (finitely generated algebras, differential operators), noncommutative algebraic geometry (graded domains), arithmetic (orders, Brauer groups), universal algebra (co-homology of rings, projective modules) and quantum physics (quantum matrices). As such, noncommutative ring theory is an area which has the potential to produce developments in many areas and in an efficient manner. The main aim of the project is to develop methods which could be applicable not only in ring theory but also in other areas, and then apply them to solve several important open questions in mathematics. The Principal Investigator, along with two PhD students and two post doctorates, propose to: study basic open questions on infinite dimensional associative noncommutative algebras; pool their expertise so as to tackle problems from a number of related areas of mathematics using noncommutative ring theory, and develop new approaches to existing problems that will benefit future researchers. A part of our methodology would be to first improve (in some cases) Bergman's Diamond Lemma, and then apply it to several open problems. The Diamond Lemma gives bases for the algebras defined by given sets of relations. In general, it is very difficult to determine if the algebra given by a concrete set of relations is non-trivial or infinite dimensional.  Our approach is to introduce smaller rings, which we will call platinum rings. The next step would then be to apply the Diamond Lemma to the platinum ring instead of the original rings. Such results would have many applications in group theory, noncommutative projective geometry, nonassociative algebras and no doubt other areas as well.","1406551","2013-06-01","2018-05-31"
"COLLMOT","Complex structure and dynamics of collective motion","Tamás Vicsek","EOTVOS LORAND TUDOMANYEGYETEM","Collective behaviour is a widespread phenomenon in nature and technology making it a very important subject to study in various contexts. The main goal we intend to achieve in our multidisciplinary research is the identification and documentation of new unifying principles describing the essential aspects of collective motion, being one of the most relevant and spectacular manifestations of collective behaviour. We shall carry out novel type of experiments, design models that are both simple and realistic enough to reproduce the observations and develop concepts for a better interpretation of the complexity of systems consisting of many organisms and such non-living objects as interacting robots. We plan to study systems ranging from cultures of migrating tissue cells through flocks of birds to collectively moving devices. The interrelation of these systems will be considered in order to deepen the understanding of the main patterns of group motion in both living and non-living systems by learning about the similar phenomena in the two domains of nature.  Thus, we plan to understand the essential ingredients of flocking of birds by building collectively moving unmanned aerial vehicles while, in turn, high resolution spatiotemporal GPS data of pigeon flocks will be used to make helpful conclusions for the best designs for swarms of robots. In particular, we shall construct and build a set of vehicles that will be capable, for the first time, to exhibit flocking behaviour in the three-dimensional space. The methods we shall adopt will range from approaches used in statistical physics and network theory to various new techniques in cell biology and collective robotics. All this will be based on numerous prior results (both ours and others) published in leading interdisciplinary journals. The planned research will have the potential of leading to ground breaking results with significant implications in various fields of science and technology.","1248000","2009-03-01","2015-02-28"
"COLMIN","A Google Earth Approach to Understanding Collagen Mineralization","Nico SOMMERDIJK","TECHNISCHE UNIVERSITEIT EINDHOVEN","Collagen mineralization in bone is one of the most crucial processes in our body as it supplies the skeleton on which we depend for support and protection. Bone’s impressive mechanical properties arise from the hierarchical organization of the organic collagen matrix that is mineralized with ultrathin, aligned inorganic crystals of carbonated hydroxyapatite. 
Despite its importance to the human body, relatively little is understood about collagen mineralization and how the proteins govern mineral growth with such precision. This is because the matrix development is a complex process with different stages that occur over multiple length scales and depends on many different components. 
I propose to obtain the first comprehensive picture of the collagen mineralization mechanism by unraveling its dynamics and structural details. It is not only of great fundamental importance, it also opens the way to the development of better biomaterials, as well as to strategies for the treatment of mineralization-related diseases.
I will achieve this ambitious goal by designing a dedicated tissue engineering platform that models real bone as closely as possible, and will allow application of multiple advanced analysis techniques. These I will employ in a “Google Earth” approach, studying the process from the micrometer to the nanometer scale, combining live cell imaging and “beyond state-of-the-art” electron microscopy with chemical and biochemical analysis to reveal the details of collagen mineralization with the highest spatial, temporal and molecular resolution thus far. Exploiting my extensive expertise in the field of biomineralization and advanced electron microscopy, COLMIN will provide a major step in understanding collagen formation and mineralization, and provide insights that will help to fight bone-related diseases. The advanced multidisciplinary methodology developed here will set a new standard for the advanced analysis of bone formation and other biological processes.","3498006","2019-01-01","2023-12-31"
"COLSTRUCTION","Numerical Design of Self Assembly of Complex Colloidal Structures","Daniel Frenkel","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","I propose to use computer simulations to predict the thermodynamic stability and kinetics of formation of three-dimensional structures of DNA-linked colloids. I then aim to go beyond simple binary structures and use simulation to explore novel strategies to build multi-component three-dimensional colloidal structures.  At present, the complexity of self-assembled colloidal crystals is limited: ordered structures with more than two distinct components are rare. To make more complex structures, particles should bind selectively to their designated neighbours. This may be achieved by coating colloids with single-stranded DNA that hybridises selectively with the complementary sequence on another colloid. However, there are many practical obstacles to go from there to the self assembly of multi-component structures. In order to make progress, we need to understand the factors that determine the thermodynamic stability and, even more importantly, the kinetics of formation of complex structures. Such a numerical study will require a wide range of numerical techniques, many of which do not yet exist.  As I have played a key role in the development of the numerical methods to study both the stability and the kinetics of formation of simple colloidal crystals, I am well positioned to make a breakthrough that should have important implications for experimental work in this field. My research will focus on DNA-linked colloidal systems, as this is an active area of experimental research. However, I stress that many of the techniques that I aim to develop are general. During the project, I aim to study the factors that influence the equilibrium phase diagram and the kinetics of passive and active self-assembly of (multi-component) DNA-colloid systems During the project, I aim to study the factors that influence the equilibrium phase diagram and the kinetics of  passive and active self-assembly of (multi-component) DNA-colloid systems","1863234","2008-11-01","2014-10-31"
"Com4Com","Collective modes in 4d-metal compounds and heterostructures","Bernhard Keimer","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Compounds of transition metals with 4d valence electrons (“4d metals”) play eminent roles in many areas of condensed matter physics ranging from unconventional superconductivity to oxide electronics, but fundamental questions about the interplay between the spin-orbit coupling and electronic correlations at the atomic scale remain unanswered. Momentum-resolved spectroscopies of collective electronic excitations yield detailed insight into the magnitude and spatial range of the electronic correlations, and have thus decisively shaped the conceptual understanding of quantum many-body phenomena in 3d-electron systems. We will devise and build a novel resonant inelastic x-ray scattering (RIXS) instrument capable of determining the dispersion relations of electronic collective modes in 4d-metal compounds with full momentum-space coverage, high energy resolution, and monolayer sensitivity. 

Data from this instrument will yield comprehensive information about the interaction parameters specifying the electronic Hamiltonians of 4d-electron materials, unique insight into the spin-orbital composition of their excited-state wavefunctions, and definitive tests of proposals to realize Kitaev models with spin-liquid states that are potentially relevant in topological quantum computation. The element-specificity of RIXS will also allow us to determine the microscopic exchange interactions in complex materials with both 3d and 4d valence electrons, and its high sensitivity will enable experiments on operational device structures comprising only a few monolayers. We will thus be able to tightly integrate momentum-resolved spectroscopy with state-of-the-art, monolayer-by-monolayer deposition methods of 4d metal-oxide films and heterostructures. The results will fuel a feedback loop comprising synthesis, characterization, and modeling, which will greatly advance our ability to design materials and devices whose functionality derives from the collective organization of electrons.","3176850","2016-01-01","2020-12-31"
"CoMoQuant","Correlated Molecular Quantum Gases in Optical Lattices","Hanns-Christoph NAEGERL","UNIVERSITAET INNSBRUCK","In a quantum engineering approach we aim to create strongly correlated molecular quantum gases for polar molecules confined in an optical lattice to two-dimensional geometry with full quantum control of all de-grees of freedom with single molecule control and detection. The goal is to synthesize a high-fidelity molec-ular quantum simulator with thousands of particles and to carry out experiments on phases and dynamics of strongly-correlated quantum matter in view of strong long-range dipolar interactions. Our choice of mole-cule is the KCs dimer, which can either be a boson or a fermion, allowing us to prepare and probe bosonic as well as fermionic dipolar quantum matter in two dimensions. Techniques such as quantum-gas microscopy, perfectly suited for two-dimensional systems, will be applied to the molecular samples for local control and local readout.
The low-entropy molecular samples are created out of quantum degenerate atomic samples by well-established coherent atom paring and coherent optical ground-state transfer techniques. Crucial to this pro-posal is the full control over the molecular sample. To achieve near-unity lattice filling fraction for the mo-lecular samples, we create two-dimensional samples of K-Cs atom pairs as precursors to molecule formation by merging parallel planar systems of K and Cs, which are either in a band-insulating state (for the fermions) or in Mott-insulating state (for the bosons), along the out-of-plane direction. 
The polar molecular samples are used to perform quantum simulations on ground-state properties and dy-namical properties of quantum many-body spin systems. We aim to create novel forms of superfluidity, to investigate into novel quantum many-body phases in the lattice that arise from the long-range molecular dipole-dipole interaction, and to probe quantum magnetism and its dynamics such as spin transport with single-spin control and readout. In addition, disorder can be engineered to mimic real physical situations.","2356117","2019-01-01","2023-12-31"
"COMP-DES-MAT","Advanced tools for computational design of engineering materials","Oliver Olivella","CENTRE INTERNACIONAL DE METODES NUMERICS EN ENGINYERIA","The overall goal of the project is to contribute to the consolidation of the nascent and revolutionary philosophy of “Materials by Design” by resorting to the enormous power provided by the nowadays-available computational techniques. Limitations of current procedures for developing material-based innovative technologies in engineering, are often made manifest; many times only a catalog, or a data basis, of materials is available and these new technologies have to adapt to them, in the same way that the users of ready-to-wear have to take from the shop the costume that fits them better, but not the one that fits them properly. This constitutes an enormous limitation for the intended goals and scope. Certainly, availability of materials specifically designed by goal-oriented methods could eradicate that limitation, but this purpose faces the bounds of experimental procedures of material design, commonly based on trial and error procedures.
Computational mechanics, with the emerging Computational Materials Design (CMD) research field, has much to offer in this respect. The increasing power of the new computer processors and, most importantly, development of new methods and strategies of computational simulation, opens new ways to face the problem. The project intends breaking through the barriers that presently hinder the development and application of computational materials design, by means of the synergic exploration and development of three supplementary families of methods: 1) computational multiscale material modeling (CMM) based on the bottom-up, one-way coupled, description of the material structure in different representative scales, 2) development of a new generation of high performance reduced-order-modeling techniques (HP-ROM), in order to bring down the associated computational costs to affordable levels, and 3) new computational strategies and methods for the optimal design of the material meso/micro structure arrangement and topology (MATO) .","2372973","2013-02-01","2018-01-31"
"COMP-MICR-CROW-MEM","Computational Microscopy of Crowded Membranes","Siewert Jan Marrink","RIJKSUNIVERSITEIT GRONINGEN","Cell membranes form a highly complex and heterogeneous mixture of membrane proteins and lipids. Understanding the protein-lipid interplay that gives rise to the lateral organisation principles of cell membranes is essential for life and health. Thus, investigations of these crowded membranes is emerging as a new and exceptionally exciting frontier at the crossroads of biology, life sciences, physics, and chemistry.

However, our current understanding of the detailed organisation of cellular membranes remains rather elusive. Characterisation of the structural heterogeneity in-vivo remains very challenging, owing to the lack of experimental methods suitable for studying these fluctuating nanoscale assemblies of lipids and proteins with the required spatio-temporal resolution. In recent years, computer simulations have become a unique investigatory tool for understanding the driving forces governing the lateral organisation of cellular membrane components and this “computational microscopy” has become indispensible as a complement to traditional microscopy methods.

In this ERC project I will, using advanced computational microscopy, study the interaction of lipids and proteins in complex, crowded, membrane patches, to enable the driving forces of membrane protein sorting and clustering to be unravelled at conditions closely mimicking real cellular membranes. The specific objectives are:

• To develop a novel computational microscopy framework for simulating biomolecular processes at multiple resolutions.
• To use this new computational microscopy framework to investigate the driving forces of membrane protein sorting and clustering.
• To provide a molecular view of realistic, crowded, biological membranes composed of hundreds of different lipids and proteins.

The outcomes will enable subsequent studies of many different types of cell membranes based on forthcoming lipidomics studies and progress in structural characterisation of membrane proteins.","2396585","2015-11-01","2020-10-31"
"COMPASP","Complex analysis and statistical physics","Stanislav Smirnov","UNIVERSITE DE GENEVE","""The goal of this project is to achieve breakthroughs in a few fundamental questions in 2D statistical physics, using techniques from complex analysis, probability, dynamical systems, geometric measure theory and theoretical physics.
Over the last decade, we significantly expanded our understanding of 2D lattice models of statistical physics, their conformally invariant scaling limits and related random geometries. However, there seem to be serious obstacles, preventing further development and requiring novel ideas. We plan to attack those, in particular we intend to:
(A) Describe new scaling limits by Schramm’s SLE curves and their generalizations,
(B) Study discrete complex structures and use them to describe more 2D models,
(C) Describe the scaling limits of random planar graphs by the Liouville Quantum Gravity,
(D) Understand universality and lay framework for the Renormalization Group Formalism,
(E) Go beyond the current setup of spin models and SLEs.
These problems are known to be very difficult, but fundamental questions, which have the potential to lead to significant breakthroughs in our understanding of phase transitions, allowing for further progresses.  In resolving them, we plan to exploit interactions of different subjects, and recent advances are encouraging.""","1995900","2014-01-01","2018-12-31"
"COMPASS","Colloids with complex interactions: from model atoms to colloidal recognition and bio-inspired self assembly","Peter Schurtenberger","LUNDS UNIVERSITET","Self-assembly is the key construction principle that nature uses so successfully to fabricate its molecular machinery and highly elaborate structures. In this project we will follow nature’s strategies and make a concerted experimental and theoretical effort to study, understand and control self-assembly for a new generation of colloidal building blocks. Starting point will be recent advances in colloid synthesis strategies that have led to a spectacular array of colloids of different shapes, compositions, patterns and functionalities. These allow us to investigate the influence of anisotropy in shape and interactions on aggregation and self-assembly in colloidal suspensions and mixtures. Using responsive particles we will implement colloidal lock-and-key mechanisms and then assemble a library of “colloidal molecules” with well-defined and externally tunable binding sites using microfluidics-based and externally controlled fabrication and sorting principles. We will use them to explore the equilibrium phase behavior of particle systems interacting through a finite number of binding sites. In parallel, we will exploit them and investigate colloid self-assembly into well-defined nanostructures. Here we aim at achieving much more refined control than currently possible by implementing a protein-inspired approach to controlled self-assembly. We combine molecule-like colloidal building blocks that possess directional interactions and externally triggerable specific recognition sites with directed self-assembly where external fields not only facilitate assembly, but also allow fabricating novel structures. We will use the tunable combination of different contributions to the interaction potential between the colloidal building blocks and the ability to create chirality in the assembly to establish the requirements for the controlled formation of tubular shells and thus create a colloid-based minimal model of synthetic virus capsid proteins.","2498040","2014-02-01","2019-01-31"
"COMPASS","COMPASS: Climate-relevant Ocean Measurements and Processes on the Antarctic continental Shelf and Slope","Karen HEYWOOD","UNIVERSITY OF EAST ANGLIA","Processes on the Antarctic continental shelf and slope are crucially important for determining the rate of future sea level rise, setting the properties and volume of dense bottom water exported globally, and regulating the carbon cycle. Yet our ability to model and predict these processes over future decades remains rudimentary. This deficiency in understanding originates in a lack of observations in this inaccessible region. The COMPASS project seeks to rectify that by exploiting new technology - autonomous marine vehicles called gliders - to observe, quantify and elucidate processes on the continental shelf and slope of Antarctica that are important for climate.

The COMPASS objective is to make a step-change in our quantitative understanding of: 
(i) the ocean front that marks the boundary between the Antarctic continental shelf and the open ocean, and its associated current system; 
(ii) the interaction between ocean, atmosphere and sea-ice on the Antarctic continental shelf; and 
(iii) the exchange of heat, salt and freshwater with the cavities beneath ice shelves. 

These goals will be met by a series of targeted ocean glider campaigns around Antarctica, spanning different flow regimes, including areas where warm water is able to access the continental shelf and influence ice shelves, areas where the continental shelf is cold and fresh, and areas where the continental shelf hosts cold, salty, dense water that eventually spills into the abyss. A unique circumpolar assessment of ocean properties and dynamics, including instabilities and mixing, will be undertaken. COMPASS will develop new technology to deploy a profiling glider into inaccessible environments such as Antarctic polynyas (regions of open water surrounded by sea-ice). As well as scientific breakthroughs that will feed into future climate assessments, improving projections of future sea level rise and global temperatures, COMPASS will deliver enhanced design for future ocean observing systems.","3499270","2017-09-01","2022-08-31"
"COMPAT","Complex Patterns for Strongly Interacting Dynamical Systems","Susanna Terracini","UNIVERSITA DEGLI STUDI DI TORINO","This project focuses on nontrivial solutions of systems of differential equations characterized by strongly nonlinear interactions. We are interested in the effect of the nonlinearities on the emergence of non trivial self-organized structures. Such patterns correspond to selected solutions of the differential system possessing special symmetries or shadowing particular shapes. We want to understand, from the
mathematical point of view, what are the main mechanisms involved in the aggregation process in terms of the global variational structure of the problem. Following this common thread, we deal with both with the classical N-body problem of Celestial Mechanics, where interactions feature attractive singularities, and competition-diffusion systems, where pattern formation is driven by strongly repulsive forces. More
precisely, we are interested in periodic and bounded solutions, parabolic trajectories with the final intent to build complex motions and possibly obtain the symbolic dynamics for the general N–body problem. On the other hand, we deal with elliptic, parabolic and hyperbolic systems of differential equations with strongly competing interaction terms, modeling both the dynamics of competing populations (Lotka-
Volterra systems) and other interesting physical phenomena, among which the phase segregation of solitary waves of Gross-Pitaevskii systems arising in the study of multicomponent Bose-Einstein condensates. In particular, we will study existence, multiplicity and asymptotic expansions of solutions when the competition parameter tends to infinity. We shall be concerned with optimal partition problems
related to linear and nonlinear eigenvalues","1346145","2014-02-01","2019-01-31"
"COMPECON","Complexity and Simplicity in Economic Mechanisms","Noam NISAN","THE HEBREW UNIVERSITY OF JERUSALEM","As more and more economic activity is moving to the Internet, familiar economic mechanisms are being deployed 
at unprecedented scales of size, speed, and complexity.  In many cases this new complexity becomes the defining 
feature of the deployed economic mechanism and the quantitative difference becomes a key qualitative one.  
A well-studied example of such situations is how the humble single-item auction suddenly becomes a 
billion-times repeated online ad auction, or even becomes a combinatorial auction with exponentially 
many possible outcomes.  Similar complexity explosions occur with various markets, with information 
dissemination, with pricing structures, and with many other economic mechanisms.  

The aim of this proposal is to study the role and implications of such complexity and to start 
developing a coherent economic theory that can handle it.  We aim to identify various measures of 
complexity that are crucial bottlenecks and study them. Examples of such complexities include the 
amount of access to data, the length of the description of a mechanism, its communication requirements, 
the cognitive complexity required from users, and, of course, the associated computational complexity.  
On one hand we will attempt finding ways of effectively dealing with complexity when it is needed, and on 
the other hand, attempt avoiding complexity, when possible, replacing it with ``simple'' alternatives 
without incurring too large of a loss.","2026706","2017-05-01","2022-04-30"
"ComplexiTE","An integrated multidisciplinary tissue engineering approach combining novel high-throughput screening and advanced methodologies to create complex biomaterials-stem cells constructs","Rui Luis Gonçalves Dos Reis","UNIVERSIDADE DO MINHO","New developments on tissue engineering strategies should realize the complexity of tissue remodelling and the inter-dependency of many variables associated to stem cells and biomaterials interactions. ComplexiTE proposes an integrated approach to address such multiple factors in which different innovative methodologies are implemented, aiming at developing tissue-like substitutes with enhanced in vivo functionality. Several ground-breaking advances are expected to be achieved, including: i) improved methodologies for isolation and expansion of sub-populations of stem cells derived from not so explored sources such as adipose tissue and amniotic fluid; ii) radically new methods to monitor human stem cells behaviour in vivo; iii) new macromolecules isolated from renewable resources, especially from marine origin; iv) combinations of liquid volumes mingling biomaterials and distinct stem cells, generating hydrogel beads upon adequate cross-linking reactions; v) optimised culture of the produced beads in adequate 3D bioreactors and a novel selection method to sort the beads that show a (pre-defined) positive biological reading; vi) random 3D arrays validated by identifying the natural polymers and cells composing the positive beads; v) 2D arrays of selected hydrogel spots for brand new in vivo tests, in which each spot of the implanted chip may be evaluated within the living animal using adequate imaging methods; vi) new porous scaffolds of the best combinations formed by particles agglomeration or fiber-based rapid-prototyping. The ultimate goal of this proposal is to develop breakthrough research specifically focused on the above mentioned key issues and radically innovative approaches to produce and scale-up new tissue engineering strategies that are both industrially and clinically relevant, by mastering the inherent complexity associated to the correct selection among a great number of combinations of possible biomaterials, stem cells and culturing conditions.","2320000","2013-05-01","2018-04-30"
"COMPLEXORDER","The Complexity Revolution: Exploiting Unconventional Order in Next-Generation Materials Design","Andrew GOODWIN","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The fundamental objective of the research described in this proposal is to lay the foundations for understanding how structural complexity can give rise to materials properties inaccessible to structurally-simple states. The long-term vision is a paradigm shift in the way we as chemists design materials—the “Complexity Revolution”—where we move to thinking beyond the unit cell and harness unconventional order to generate emergent states with entirely novel behaviour. The key methodologies of the project are (i) exploitation of the rich structural information accessible using 3D-PDF / diffuse scattering techniques, (ii) exploration of the phase behaviour of unconventional ordered states using computational methods, and (iii) experimental/computational studies of a broad range of materials in which complexity arises from a large variety of different phenemona. In this way, the project will establish how we might controllably introduce complexity into materials by varying chemical composition and synthesis, how we might then characterise these complex states, and how we might exploit this complexity when designing next-generation materials with unprecedented electronic, catalytic, photonic, information storage, dielectric, topological, and magnetic properties.","3362635","2018-10-01","2023-09-30"
"COMPLEXPLAS","Complex Plasmonics at the Ultimate Limit: Single Particle and Single Molecule Level","Harald Giessen","UNIVERSITAET STUTTGART","""Nano-optical investigations using plasmonic resonances have revolutionized optics in the last few years. The ability to concentrate light in subwavelength dimensions and to locally enhance the strength of the electromagnetic field in a tailored fashion opened several new fields in materials research, such as tailoring the linear and nonlinear properties of optical materials at will. So-called metamaterials allow now to design and realize unprecedented optical properties on the submicrometer level and hence tailor dispersion as well as real and imaginary parts of the linear and nonlinear refractive indices as a function of wavelength and wavevector.

Our ability to create two- and three-dimensional nanostructures with advanced fabrication technologies have led to the new era of complex plasmonics. We are able to tailor the spectral response of complex metallic nanostructures, including the creation of very sharp and narrow resonances. In combination with strong field localization and hence large dependence on the material properties of the nanostructure geometry and its surrounding, unique sensors with sensitivities close to fundamental limits should be within reach.

In my proposal, I would like to explore the ultimate limits of light-matter interaction using complex plasmonic nanostructures. I would like to apply them to different physical, chemical, and biological situations and undertake the first steps from fundamental insight into first applications. Namely, I would like to investigate complex plasmonics in four different contexts: single molecule reactions on complex surfaces, antenna-enhanced structural analysis of large single molecules, such as proteins, motion sensing of conformational changes of single molecules, as well as chiral sensing down to the single molecule level, hence ultimately being able to distinguish a single D-glucose molecule from its L-glucose enantiomer. This would bridge the gap between nanophysics, chemistry, and biology.""","2000000","2013-03-01","2018-02-28"
"COMPMUSIC","Computational models for the discovery of the world's music","Francesc Xavier Serra Casals","UNIVERSIDAD POMPEU FABRA","Current IT research does not respond to the world's multi-cultural reality. It could be argued that we are imposing the paradigms of our market-driven western culture also on IT and that current IT research results will only facilitate the access of a small part of the world’s information to a small part of the world's population. Most IT research is being carried out with a western centred approach and as a result, our data models, cognition models, user models, interaction models, ontologies, … are all culturally biased. This fact is quite evident in music information research, since, despite the world's richness in musical cultures, most of the research is centred on CDs and metadata of our western commercial music. CompMusic wants to break this huge research bias. By approaching musical information modelling from a multicultural perspective it aims at advancing our state of the art while facilitating the discovery and reuse of the music produced outside the western commercial context. But the development of computational models to address the world’s music information richness cannot be done from the West looking out; we have to involve researchers and musical experts immersed in the different cultures. Their contribution is fundamental to develop the appropriate multicultural musicological and cognitive frameworks from which we should then carry our research on finding appropriate musical features, ontologies, data representations, user interfaces and user centred approaches. CompMusic will investigate some of the most consolidated non-western classical music traditions, Indian (hindustani, carnatic), Turkish-Arab (ottoman, andalusian), and Chinese (han), developing the needed computational models to bring their music into the current globalized information framework. Using these music cultures as case studies, cultures that are alive and have a strong influence in current society, we can develop rich information models that can take advantage of the existing information coming from musicological and cultural studies, from mature performance practice traditions and from active social contexts. With this approach we aim at challenging the current western centred information paradigms, advance our IT research, and contribute to our rich multicultural society.","2443200","2011-07-01","2017-06-30"
"COMPSELF","Self-Organisation: From Molecules to Matter","David John Wales","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","This research proposal concerns the theory and computer simulation of self-organisation to predict properties and to design systems with specified characteristics. The key computational challenge is to explore the energy landscape for complex systems and make predictions to characterise efficient self-organisation on experimental time and length scales. Novel methodology is required to overcome the problems of broken ergodicity and rare events. The theoretical framework exploits stationary points of the potential energy landscape to access the required time and length scales. Applications include self-assembly of mesoscopic structures from coarse-grained building blocks and all-atom simulations of conformational changes in specific proteins and nucleic acids.

We aim to establish design principles for efficient self-assembly by developing novel tools for visualising and exploration of the corresponding landscape. Here, a key issue is how the interactions between the constituent particles determine the organisation of the energy landscape. Identifying which features lead to successful self-assembly and which disrupt such ordering will lead to a wide range of important applications, ranging from design of new materials to identifying new anti-viral drugs. The same methodology will be applied to detailed models of specific biomolecules, where self-organisation into alternative structures is associated with disease. Global optimisation will be employed in structure prediction for variable pathogens, such as human influenza virus. Pathways for folding and misfolding of specific proteins and nucleic acids will be characterised using novel rare events methodology, providing insight into intermediates that could serve as potential drug targets.","2069374","2011-03-01","2016-02-29"
"COMTESSA","Camera Observation and Modelling of 4D Tracer Dispersion in the Atmosphere","Andreas Stohl","NORSK INSTITUTT FOR LUFTFORSKNING STIFTELSE","COMTESSA will push back the limits of our understanding of turbulence and plume dispersion in the atmosphere by bringing together full four-dimensional (space and time) observations of a (nearly) passive tracer (sulfur dioxide, SO2), with advanced data analysis and turbulence and dispersion modelling.

Observations will be made with six cameras sensitive to ultraviolet (UV) radiation and three cameras sensitive to infrared (IR) radiation. The UV cameras will be built specifically for this project where high sensitivity and fast sampling is important. The accuracy of UV and IR retrievals will be improved by using a state-of-the art-3D radiative transfer model. 

Controlled puff and plume releases of SO2 will be made from a tower, which will be observed by all cameras, yielding multiple 2D images of SO2 integrated along the line of sight. The simultaneous observations will allow - for the first time - a tomographic reconstruction of the 3D tracer concentration distribution at high space (< 1 m) and time (>10 Hz) resolution. An optical flow code will be used to determine the eddy-resolved velocity vector field of the plume. Special turbulent phenomena (e.g. plume rise) will be studied using existing SO2 sources (e.g. smelters, power plants, volcanic fumaroles).

Analysis of the novel campaign observations will deepen our understanding of turbulence and tracer dispersion in the atmosphere. For instance, for the first time we will be able to extensively measure the concentration probability density function (PDF) in a plume not only near the ground but also at high-er altitudes; quantify relative and absolute dispersion; estimate the value of the Richardson-Obukhov constant, etc. We will also use the data to evaluate state-of-the-art LES and Lagrangian dispersion models and revise their underlying parameterizations.

COMTESSA’s vision is that the project results will lead to large improvements of tracer transport in all atmospheric models.","2800000","2015-11-01","2020-10-31"
"Con Espressione","Getting at the Heart of Things: Towards Expressivity-aware Computer Systems in Music","Gerhard Widmer","UNIVERSITAT LINZ","What makes music so important, what can make a performance so special and stirring? It is the things the music expresses, the emotions it induces, the associations it evokes, the drama and characters it portrays. The sources of this expressivity are manifold: the music itself, its structure, orchestration, personal associations, social settings, but also – and very importantly – the act of performance, the interpretation and expressive intentions made explicit by the musicians through nuances in timing, dynamics etc.
Thanks to research in fields like Music Information Research (MIR), computers can do many useful things with music, from beat and rhythm detection to song identification and tracking. However, they are still far from grasping the essence of music: they cannot tell whether a performance expresses playfulness or ennui, solemnity or gaiety, determination or uncertainty; they cannot produce music with a desired expressive quality; they cannot interact with human musicians in a truly musical way, recognising and responding to the expressive intentions implied in their playing.
The project is about developing machines that are aware of certain dimensions of expressivity, specifically in the domain of (classical) music, where expressivity is both essential and – at least as far as it relates to the act of performance – can be traced back to well-defined and measurable parametric dimensions (such as timing, dynamics, articulation). We will develop systems that can recognise, characterise, search music by expressive aspects, generate, modify, and react to expressive qualities in music. To do so, we will (1) bring together the fields of AI, Machine Learning, MIR and Music Performance Research; (2) integrate theories from Musicology to build more well-founded models of music understanding; (3) support model learning and validation with massive musical corpora of a size and quality unprecedented in computational music research.","2318750","2016-01-01","2020-12-31"
"CONCERTO","Intensity mapping of the atomic carbon CII line: the promise of a new observational probe of dusty star-formation in post-reionization and reionization epoch","Guilaine LAGACHE","UNIVERSITE D'AIX MARSEILLE","I propose for funding to construct a spectrometer to map in 3-D the intensity due to line emission, a
technique known as Intensity Mapping. Instead of detecting individual galaxies, this emerging technique
measures signal fluctuations produced by the combined emission of the galaxy population on large regions
of the sky in a wide frequency (i.e. redshift) band, and thus increases sensitivity to faint sources.
Capitalizing on a recent technology breakthrough, our intensity mapping experiment will measure the 3-D
fluctuations of the [CII] line at redshifts 4.5<z<8.5. [CII] is one of the most valuable star formation tracers
at high redshift. My project will answer the outstanding questions of whether dusty star-formation
contributes to early galaxy evolution, and whether dusty galaxies play an important role in shaping cosmic
reionization.
My team will first build, test, and finally install the instrument on the APEX antenna following an
agreement with APEX partners. The spectrometer will be based on the state-of-the-art development of new
arrays in the millimeter using Kinetic Inductance Detectors. Spectra (200-360 GHz) will be obtained by a
fast Martin-Puplett interferometer. Then, we will observe with CONCERTO a few square degrees and offer
a straight forward alternative for probing star formation and dust build-up in the early Universe. Finally,
CONCERTO will set to music the various cosmic evolution probes. Cross-correlation of the signals will be
used in particular to capture the topology of the end of reionization era.
CONCERTO will be one of two instruments in the world to perform intensity mapping of the [CII] line in
the short term. The novel methodology is extremely promising as it targets an unexplored observable
touching on some of the fundamental processes building the early universe. In the flourishing of new ideas
in the intensity-mapping field, CONCERTO lies at the forefront.","3499942","2019-01-01","2023-12-31"
"ConFluReM","Controlling Fluid Resistances at Membranes","Matthias WESSLING","DWI LEIBNIZ-INSTITUT FUR INTERAKTIVE MATERIALIEN EV","Today’s materials research in the field of synthetic membranes gives access to highly permeable and extremely selective membranes. However, their potential will remain ineffective as high and selective transport rates always go along with resistances emerging at the membrane fluid interface in the form diffusion limitations in the laminary boundary layers. In order to make full use of the very many new materials, also new means to control and minimize such fluid based resistances need to be developed. Yet another phenomena disturbs the full potential use of membranes: retained solutes, colloids and biological matter accumulates at the membrane interface and causes irreversible fouling and scaling.
The proposed research aims to develop a rigorous translational methodology to control and improve mass transport through the fluid/membrane interface. ConFluReM will establish Strategic Tools and New Instruments to:
(1)    comprehend and quantify the prevalent mass transport resistances in representative membrane separation processes,
(2)    synthesize and fabricate new nano-, micro- and mesoscale material and device systems as instruments to control and overcome the limitations of concentration polarization and fouling,
Strategic Tools are experimental and simulation methods to quantify and engineer the mass transport and hydrodynamical properties of the new membrane systems. These encompass flow imaging (flowMRI, microPIV and microfluidic transport studies) as well as computational fluidic dynamics (CFD and CFDEM). New Instruments are synthetic and fabrication means as well as process condition means to improve mixing at the membrane/fluid interface. These encompass (a) lateral patterning of chemical topology of the membrane surface by printing and stamping, (b) shaping the 3D geometry of channels using additive manufacturing techniques and (c) imposing dynamical gradients to destablize fluid side resistances.","2500000","2016-09-01","2021-08-31"
"CONFRA","Conformal fractals in analysis, dynamics, physics","Stanislav Smirnov","UNIVERSITE DE GENEVE","The goal of this project is to study conformally invariant fractal structures from the perspectives of analysis, dynamics, probability, geometry and physics, emphasizing interrelations of these fields.  In the last two decades such structures emerged in several areas: continuum scaling limits of 2D critical models in statistical physics (percolation, Ising model); extremal configurations for various problems in complex analysis (multifractal harmonic measures, coefficient growth of univalent maps, Brennan's conjecture); chaotic sets for complex dynamical systems (Julia sets, Kleinian groups).  Capitalizing on recent successes, I plan to continue my work in these areas, exploiting their interactions and connections to physics. I intend to achieve at least some of the following goals: * To establish that several critical lattice models have conformally invariant scaling limits, by building upon results on percolation and Ising models and finding discrete holomorphic observables. * To study geometric properties of arising fractal curves and random fields by connecting them to Schramm's SLE curves and Gaussian Free Fields. * To investigate massive scaling limits by describing them geometrically with generalizations of SLEs. * To lay mathematical framework behind relevant physical notions, such as Coulomb Gas (by relating height functions to GFFs) and Quantum Gravity (by identifying limits of random planar graphs with Liouville QGs).  * To improve known bounds in several old questions in complex analysis by studying multifractal spectra of harmonic measures.  * To estimate extremal behavior of such spectra by using holomorphic motions of (quasi) conformal maps and thermodynamic formalism.  * To understand nature of extremal multifractals for harmonic measure by studying random and dynamical fractals.  The topics involved range from century old to very young ones. Recently connections between them started to emerge, opening exciting possibilities for new developments in some long standing open problems.","1278000","2009-01-01","2013-12-31"
"CONSTANS","Control of the Structure of Light at the Nanoscale","Laurens Kuipers","TECHNISCHE UNIVERSITEIT DELFT","In the last decade, the fields of nanoplasmonics and photonic crystals have opened up the nanoscale for optical control. Both the flow and emission of light can be controlled at these small length scales, giving rise to new science and applications. Interestingly, freely propagating light beams can already contain nanoscale features, i.e. optical singularities. Little is known about this nanoscale structure of light.
I propose to (1) reveal the structure of light at the nanoscale and its interaction with geometrical structures or other light structures; and (2) achieve full spatio-temporal control of the nanoscale structure of light. Crucial to achieving these goals are technological innovations, which will be crosscutting objectives. These include the first nonlinear vectorial scanning near-field microscope and novel near-field probes allowing access to new combinations of vector fields.
This next step in the field of nano-optics is possible due to recent breakthroughs in the control and visualization of light at the nanoscale obtained in my group. I will combine newly acquired access to the vectorial nature of light with its active control to investigate how (deep-) subwavelength structures of light of different frequencies affect each other when coupled through a nonlinear interaction in a nanostructured material. In parallel I will focus on optical singularities. Because of their extreme size, small changes in their position will lead to huge effects in the local light fields, opening up potential for all-optical and therefore ultrafast control.
The research will lead to innovations in the visualization and control of light at the nanoscale, access to the magnetic component of light, nanoscale nonlinear optics and coherent control of light fields. The knowledge gain will be crucial for applications like ultrasensitive biosensors based on superchiral light, ultrafast magneto-optics and nanoscale quantum optics.","2493600","2014-03-01","2019-02-28"
"cool innov","Turning the concept of magnetocaloric cooling on its head","Oliver GUTFLEISCH","TECHNISCHE UNIVERSITAT DARMSTADT","Twenty years of research in magnetocaloric materials has failed to provide the necessary breakthrough that will lead to a commercial realisation of this technology and satisfy the urgent global need for more efficient refrigeration. We strongly believe that this is a result of looking in the wrong direction. The cool innov project will achieve this breakthrough by rethinking the whole concept of caloric cooling. We are rejecting the conventional idea of squeezing the best out of magneto-structural phase-change materials in relatively low magnetic fields, and instead we introduce a second stimulus in the form of pressure so that we can exploit, rather than avoid, the hysteresis that is inherent in these materials. The hysteresis will allow us to lock-in the magnetisation at saturation as the magnetising field is removed, so that magnetic fields persisting over a large area will no longer be required (instead, we can use a very focused field), and then demagnetise the material in a second step with an applied stress, enabling us to extract a lot more heat. In this case we only need to apply the magnetic field to a small volume of material, making it a completely new application for commercially available, high-temperature, YBCO-type, bulk superconducting permanent magnets. With the high-field, multi-stimuli approach proven, we will develop new magneto/mechanocaloric materials that match the new high-field, hysteresis-positive approach and start to fabricate novel heat-exchanger structures using additive manufacturing, so that we can combine a mechanically sound heat exchanger having a complex geometry with locally tailored, magneto/mechanocaloric properties. The success of cool innov will be game changing. We are being very ambitious in targeting a revolution in cooling technology, but if we succeed, we will have a huge impact on global energy consumption through greater efficiency, thanks to the novel energy materials that will be discovered within cool innov.","2499000","2017-10-01","2022-09-30"
"COOLART","Science-based Paradigm Shift for Metalworking Fluids - the Art of Cooling","Ekkard Brinksmeier","LEIBNIZ-INSTITUT FUR WERKSTOFFORIENTIERTE TECHNOLOGIEN-IWT","The overall goal of the project is to elaborate the scientific foundations for possible future innovations in the application of metalworking
fluids (MWFs). For most mechanical machining processes such as turning, milling, drilling, and grinding the use of MWFs is indispensable
as they perform several essential functions in material removal processes, such as cooling of workpiece and tool, reduction of heat
generation by lubrication, removal of chips and promotion of chemical reactions with the surface.
In the past decades the scientific understanding of MWF effectiveness could not keep up with the rapid developments in the industrial
application of MWFs. As a consequence of this lack of knowledge, today’s MWFs consist of up to 60 chemical components with partly
harmful properties and thus hazardous potential for the operator and the environment.
Hence, this project will aim towards scientific based innovations and suggests a four-mission approach for stimulating a paradigm shift in
MWF features and application. Research will include: a) fundamental research on physical, chemical, and microbial working mechanisms,
which allows MWF components e.g., with controlled chemical activity, b) new MWF design e.g., the development of MWFs which consist of
lubricating bacteria replacing today’s paradigms such as the usage of additives and oil in water emulsions, c) simplified MWF maintenance
and supply by e.g., in-process self-controlled chemical MWF composition and adaptation of the MWF supply system for temperature and
force reduction, and d) verification and transfer of the innovative, alternative systems e.g., on a test bench in a realistic environment being
part of a demonstration center. The distinguishing feature of the approach will be its cross-disciplinary and comprehensive nature.","2274600","2011-05-01","2016-04-30"
"COORDSPACE","Chemistry of Coordination Space: Extraction, Storage, Activation and Catalysis","Martin Schroder","THE UNIVERSITY OF NOTTINGHAM","The Applicant has an outstanding record of achievement and an international reputation for independent research across many areas of metal coordination chemistry. This high-impact and challenging Proposal brings together innovative ideas in coordination chemistry within a single inter- and multi-disciplinary project to open up new horizons across molecular and biological sciences, materials science and energy research. The Proposal applies coordination chemistry to the key issues of climate change, environmental and chemical sustainability, the Hydrogen Economy, carbon capture and fuel cell technologies, and atom-efficient metal extraction and clean-up. The vision is to bring together complementary areas and new applications of metal coordination chemistry and ligand design within an overarching and fundamental research program addressing: i. nanoscale functionalized framework polymers for the storage and activation of H2, CO2, CO, O2, N2, methane and volatile organic compounds; ii. new catalysts for the reversible oxidation and photochemical production of H2; iii) clean and selective recovery of precious metals (Pt, Pd, Rh, Ir, Hf, Zr) from process streams and ores. These research themes will be consolidated within a single cross-disciplinary and ambitious program focusing on the control of chemistry, reactivity and interactions within self-assembled confined and multi-functionalized space generated by designer porous framework materials. An AdG will afford the impetus and freedom via consolidated funding to undertake fundamental, speculative research with multiple potential  big-hits  across a wide range of disciplines. Via an extensive network of international academic and industrial collaborations, the Applicant will deliver major research breakthroughs in these vital areas, and train scientists for the future of Europe in an exciting, stimulating and curiosity-driven environment.","2492372","2008-12-01","2013-11-30"
"COPMAT","Full-scale COmputational design of Porous mesoscale MATerials","Sauro SUCCI","FONDAZIONE ISTITUTO ITALIANO DI TECNOLOGIA","The last decades have witnessed major progress in our understanding of the basic physics of soft matter materials. At the same time, microfluidics has also undergone spectacular theoretical and experimental progress. The confluence of such major advances spawns unprecedented opportunities for the design and manufacturing of new soft mesoscale materials, with promising applications in tissue engineering, photonics, catalysis and many others. COPMAT is targeted at making the most this opportunity through the pursuit of a single general goal: the full-scale simulation at nanometric resolution of micro-reactors for the design and synthesis of new tunable porous materials. In particular, we shall focus on the microfluidic design of: multi-jel materials, trabecular porous media and soft mesoscale molecules. We shall also explore new designs concepts based on unexplored microscale phenomena, such as the interaction between plasticity and nano-rugosity. The complex interplay between the highly non-linear rheology of  soft materials and the major experimental control parameters leads to an engineering design of formidable complexity, characterized  by a strong sensitivity of the macroscale material properties on the details of nanoscale interfacial interactions. COPMAT will tackle this formidable multiscale challenge through the deployment of an entirely new family of multiscale techniques, centered upon highly innovative extensions of  the Lattice Boltzmann method and its combinations with Immersed Boundary Method, Dissipative Particle Dynamics and Dissipative Voronoi Dynamics. The success of COPMAT will be gauged by its capability of inspiring and realizing the design of microfluidic devices for the synthesis of novel families of porous materials for bio-engineering applications. The new paradigm established by COPMAT for the computational design of soft materials is expected to extend well beyond the time-horizon of the project.","1880060","2017-10-01","2022-09-30"
"CORREL-CT","Correlative tomography","Philip Withers","THE UNIVERSITY OF MANCHESTER","Proposal summary (half page)
The vision is firstly, to develop correlative tomography to radically increase the nature and level of information (morphological, structural and chemical) that can be obtained for a 3D volume of interest (VoI) deep within a material or component by coupling non-destructive (3D+time) X-ray tomography with destructive (3D) electron tomography and, secondly to exploit this new approach to shed light on damage accumulation processes arising under demanding conditions. Successful completion of this project will provide new 3D & 4D insights across many areas and yield key experimental data for multiscale models.
Objective 1: To build the capability of correlative tomography
- To connect platforms across scales and modalities in order to track a VoI that may be located deep below the surface and to combine multiple techniques within a single platform.
- To add new facets to correlative tomography including
+ 3D chemical imaging
+ 3D crystal grain mapping
+ the local stress distribution
+ mechanical performance mapping at the VoI scale
Objective 2: To apply it to gain new insights into damage accumulation
Correlative tomography will provide a much richer multi-faceted hierarchical picture of materials behaviour from life science to food science from geology to cultural heritage. This project will focus specifically on identifying the nucleation, propagation and aggregation of damage processes in engineering materials.
- We will identify and track the mechanisms that control the progressive degradation of conventional bulk engineering materials operating under demanding conditions.
- We will examine the hierarchical strategies nature uses to control failure in natural materials through heterogeneous chemistry, morphology and properties. Alongside this we will examine the behaviour of man-made nano-structured analogues and whether we can exploit some of these strategies.","2926425","2016-11-01","2021-10-31"
"CORYPHEE","Cold Rydbergs: photoionization, electronic spectroscopy and electrostatic trapping","Frédéric Merkt","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Spectroscopic investigation of high (n &gt;&gt;20) molecular Rydberg states below and above the first adiabatic ionization threshold will be carried out with the aims of 1) obtaining fully resolved information on the vibrational, rotational, spin-orbit and hyperfine structures of these highly excited electronic states, 2) characterizing the role of nuclear spins in molecular photoionization, 3) determining the hyperfine structure of fundamental molecular cations at kHz resolution and accuracy by Rydberg series extrapolation, 4) measuring intervals between rovibrational levels of these molecular cations at sub MHz precision, 5) gaining a complete understanding, and providing an adequate description and classification, of angular momentum coupling (including nuclear spins) in high molecular Rydberg states, 6) testing theoretical predictions of the energy level structure of Rydberg molecules by ab initio multichannel quantum defect theory (MQDT) and of the rotational, vibrational and hyperfine levels of molecular cations by ab initio quantum chemistry and QED. The spectroscopic measurements using tunable narrow-band vacuum-ultraviolet and millimeter wave radiation sources will be performed on cold samples in supersonic beams as well as on trapped samples of translationally cold Rydberg atoms and molecules. To this end, our recent approach to trap H atoms in Rydberg states electrostatically (Hogan and Merkt, Phys. Rev. Lett. 100, 043001 (2008)) will be extended to molecules, and the possibility of transfering the trapped species from electrostatic traps to magnetic and optical traps will be explored.","1192395","2008-11-01","2013-10-31"
"COS-OCS","Carbonyl Sulphide: new ways of Observing the Climate System","Maarten KROL","WAGENINGEN UNIVERSITY","The future climate of our planet strongly depends on the capacity of the biosphere to sequester atmospheric CO2, and on the abundance of stratospheric sulphate aerosols (SSA). These aerosols form a layer that resides at about 16 km altitude that, contrary to CO2, has a cooling effect on climate. These two climate-regulating mechanisms are intricately linked to the atmospheric trace gas carbonyl sulphide (COS). 
COS is the most abundant sulphur compound in our atmosphere. The dominant COS source is biogenic activity in the ocean, while uptake by the terrestrial biosphere, and a small amount of destruction in the stratosphere, contribute to its removal. The COS loss to the biosphere could potentially be used to quantify photosynthetic CO2 uptake, while its stratospheric destruction is an important precursor for the formation of SSA. A deeper understanding of atmospheric COS variations would therefore signal a major step forward in our ability to diagnose CO2 uptake and SSA formation.
With this research program, I aim to fundamentally improve our limited understanding of the COS budget. The program combines innovative modelling and measurements. I aim to collect samples from aircraft, ship cruises, and stations across all latitudes, on which highly challenging analyses of COS and its isotopologues will be performed.  To characterise the important transition to the stratosphere, vertical COS profiles up to 30 km will be sampled with so-called “AirCores”. A larger spatial coverage will come from currently untapped satellite data of COS isotopologues. My program will integrate these measurements into the first multispecies and isotope-enabled inverse modelling framework for COS, building on techniques I developed during the past decade. The measurements and model together will allow breakthroughs in the coupled COS and CO2 budgets, and unlock the potential of COS as new climate diagnostic.","2462135","2017-09-01","2022-08-31"
"COSFORM","Cosmological Structure Formation in the Multiverse","John Peacock","THE UNIVERSITY OF EDINBURGH","This application proposes a programme of research directed at the outstanding puzzle of modern cosmology: the strangely small non-zero value of the vacuum density. This can be approached in three ways: (1) Evolution; (2) Revision of gravity; (3) Observer selection in the multiverse. The first two of these can be addressed by ongoing and future large galaxy surveys. Part of the research programme is directed at new ways of assuring robust measurements from these surveys of the main diagnostics of interest -- the effective equation of state of dark energy and the growth rate of density fluctuations. This will exploit and extend current work on systematics of galaxy properties as a function of large-scale environment in the cosmic web.

But so far such tests show no deviation from standard gravity and a cosmological constant. This fact drives interest in a multiverse solution, in which different causally disconnected domains may be able to possess different effective cosmological constants. This research will concentrate on the astrophysically interesting question of how galaxy formation would be affected by different levels of vacuum energy. This previously been addressed only by oversimplified analytic arguments, and it is possible that the exponential sensitivity of galaxy formation efficiency to the vacuum density could be very different to the simple estimates. Current claims that the multiverse approach predicts the right level for the cosmological constant would then be disproved. In any case, there is much of interest to be learned regarding the robustness of current theories of galaxy formation by 'stress-testing' them outside the rather restricted parameter regimes normally considered. The result will be a deeper understanding of the assembly of cosmic structure in our universe, as well as indications of how it might have proceeded in other members of an ensemble.","2191778","2015-10-01","2020-09-30"
"COSMIC-LAB","Star clusters as cosmic laboratories for Astrophysics, Dynamics and Fundamental Physics","Francesco Ferraro","ALMA MATER STUDIORUM - UNIVERSITA DI BOLOGNA","""Galactic Globular Clusters (GCs) are the most populous, old and dense stellar systems in the Galaxy. Their study addresses fundamental astrophysical  questions, ranging from the Galaxy formation, to stellar evolution and dynamics. With Cosmic-Lab we intend to use these  natural laboratories to perform three original experiments which will have a major  impact on several areas of modern Physics and  Astrophysics. To this aim we will adopt as test particles three classes of """"exotica"""" (namely blue stragglers - BSS, millisecond pulsars -MSPs, and  intermediate-mass black holes -IMBHs):

Exp 1 - """"Toward the definition of a dynamical clock for stellar systems"""". By exploiting our exceptional multi-wavelength database, we propose to use the observed properties of  BSS for defining  an innovative tool to measure the  degree of dynamical evolution of collisional stellar systems.

Exp 2 - """"Hunting for the most massive neutron stars (NSs): probing the equation of state of matter at nuclear densities"""" -  We propose to search for the companion stars to binary MSPs in a selected sample of GCs thus to  exploit the unique opportunity offered by these systems to measure the NS masses.  This  will  finally allow us to  determine the  upper limit to the NS mass and tightly constrain the equation of state of matter at the nuclear equilibrium density.

Exp 3 - """"IMBHs: the missing link in the formation of cosmic structures"""" - We propose to use a set of non-conventional data-analysis procedures  developed  by our group  in order to unveil IMBHs at the center of GCs.  Proving the  existence of these objects is crucial for understanding the formation of super-massive BHs, which are observed at the centre of all massive galaxies at any redshift, with a major impact on the comprehension of the formation and evolution of cosmic structures.""","1880000","2011-05-01","2016-04-30"
"Cosmic_Gas","Mapping the Cosmic Gas Supply with ALMA","Fabian WALTER","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","The molecular gas phase is the material in galaxies out of which stars form. As such, it is the quantity that controls the star formation rate of a galaxy, thereby the overall stellar mass build-up, and ultimately galaxy evolution through cosmic times. In contrast to studies of the stellar mass and star formation, characterizing this fuel supply in galaxies as a function of cosmic epoch is still in its infancy. The ALMA facility now redefines our ability to map out the cosmic cold gas supply, essentially unknown at present. This ERC proposal is based on extensive approved observational ALMA programs, led by the PI: ASPECS is the first-ever approved ALMA large (150h) program, aimed at providing a comprehensive view of the baryon cycle from gas to stars over cosmic time. ASPECS will provide 3D molecular scans in two ALMA bands of the Hubble Ultra Deep Field -- the iconic cosmological deep field. A second focus is the detailed characterization of the molecular gas content at z>6 in host galaxies of the most distant quasars via ALMA. This will assess the role of cold gas in the build-up of the first (t_Universe < 1 Gyr) massive cosmic structures in the Universe, again through significant approved ALMA programs led by the PI’s group. The studies outlined here will fully capitalize on the unparalleled capabilities of ALMA to map out the cosmic gas supply through cosmic history, and will provide crucial insights to define observational strategies for JWST (the PI is member of the European JWST/MIRI science team). Through his track record, past achievements in the field of galaxy evolution studies, and through the available proprietary data, the PI is uniquely positioned to lead this ambitious program, which will define the global state-of-the-art in cosmological galaxy evolution through high-redshift ISM studies.","2457500","2017-11-01","2022-10-31"
"COSMICISM","Characterising the interstellar medium of bright, lensed, star-forming galaxies across cosmic time","Robert Julian Ivison","THE UNIVERSITY OF EDINBURGH","The physical conditions of molecular gas in galaxies, and the impact of star formation and AGN on these conditions and on the emergent stellar IMF, are overarching themes in astrophysics. We are entering an era where numerical simulations of turbulent molecular gas can be informed and constrained by observations of such gas. I propose to investigate, theoretically and observationally, the impact of merger-driven star formation during a vital period in cosmic history, 1 < z < 3, when much of today’s stellar mass was formed. It is here that we must study Larson’s star-formation laws, and turbulence-regulated aspects of star formation, and look for possibly dramatic differences in the initial conditions of star formation, and the different IMF these may impose. These galaxies were significantly more gas-rich and turbulent than local starbursts, with different fragmentation histories and higher star-formation-rate densities (so more cosmic rays). They should yield cleaner signatures of a top-heavy IMF than local starbursts, where periods of ordinary star formation may have diluted such signatures. I will exploit strongly lensed starbursts to study powerful diagnostic rest-frame FIR cooling lines with Herschel's FTS and map velocity fields with JVLA/ALMA, moving beyond studies of integrated galaxy properties to study the activity within starbursts on sub-kpc scales, distinguishing between fueling mechanisms and testing Larson's relations. At this level of sophistication, the analysis of the ISM at z > 1 begins to be comparable to that possible at z ~ 0. Abundances - probed by multi-species, multi-J isotopologues and molecular diagnostics - will reveal the dominant form of nucleosynthesis enriching their ISM, and gravo-turbulent MHD simulations of gas fragmentation in cosmic-ray-dominated regions will determine how turbulent energy injection affects merger-driven systems, producing IMF libraries as functions of ISM conditions to determine the cosmological consequences.","2071721","2013-04-01","2018-03-31"
"COSMICLENS","Cosmology with Strong Gravitational Lensing","Frederic Yves Michel COURBIN","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Measuring cosmological distances has revolutionized our understanding of the Universe, and is still doing so! Early work in the 1920s led to the discovery of the expansion of the Universe. More precise distance measurements in the 90s with type-Ia supernovae revealed that this expansion is accelerating, with crucial consequences in cosmology and physics. Is the acceleration due to some repulsive form of dark energy? To Einstein's cosmological constant? Do we need to consider new physics? Answering these fundamental questions requires a precise measurement of the Hubble parameter, H0, which is my goal using the time delay (TD) method in strongly lensed quasars.

The TD method exploits well-known physics on galaxy-scales. It is one of the very few techniques that can yield H0 to <2% using a single methodology. It involves no calibration, and is truly independent of any other cosmological probe. Capitalizing on the successful pathfinders COSMOGRAIL (PI: Courbin) and H0LiCOW (PI: Suyu, CoI: Courbin) time has come to fully exploit TDs with an observational, modeling and technical boost, organized in 2 phases. 

Phase I will secure H0 to 2% using the current chain of analysis, with feasible enhancements beyond the current state-of the-art. This will confirm or refute the tension seen between H0 values with different cosmological probes. Phase II targets 1% precision, improving the FoM of Stage-IV cosmological surveys by 40%. The 4 proposed Work Packages can transform the field within the next 5 years by 1- implementing the first high-cadence photometric monitoring of lensed quasars to measure 50 new TDs, 2- providing new flexible non-parameteric lens models based on sparse regularization of the reconstructed source and lens mass/light distributions, 3- providing a modular end-to-end simulation framework to mock lensed systems from hydro-simulations and to evaluate in detail the impact model degeneracies on H0, 4- discovering new suitable lensed quasars in current surveys.","3129689","2018-10-01","2023-09-30"
"COSMIWAY","From the Milky Way to the cosmic large-scale structure","Carlos Silvestre Frenk","UNIVERSITY OF DURHAM","Wide field panoramic telescopes will become a major force in astronomy over the next decade. They will address a rich set of scientific problems, from ``killer asteroids'' to the cosmic dark energy. Pan-STARRS-1 (PS1), built by the University of Hawaii, is the first of this new generation of telescopes. European astronomers in Germany and the UK, including in the PI's host institute, make up a large fraction of the Science Consortium that, over the next 4 years, will exploit the data. This proposal is focused on the use of PS1 for cosmology. I propose a programme that combines state-of-the-art cosmological simulations and modelling with high-level analyses of the data. The goal is to test core assumptions of the standard cosmogonic model, LCDM, on scales and at epochs where it has not been tested before and where it can, in principle, be ruled out. At the same time, these tests will advance our understanding of the main constituents of our universe (dark matter and dark energy) and of the processes of galaxy formation and evolution. Two types of structure at opposite ends of the cosmological scale, the Milky Way and the large-scale distribution of galaxies at redshifts z<1.5, are ideally suited to this purpose. Studies of the Milky Way will test LCDM predictions for the hierarchical assembly of galaxies and the structure of their dark matter halos. Studies of the galaxy distribution will test LCDM predictions for the growth of structure and the connection between galaxies and dark matter. To link theory and data, I will construct mock catalogues using very large cosmological simulations and sophisticated modelling techniques. These catalogues will have a much broader applicability that just PS1 and I will make them publicly available using e-science techniques.","2266850","2011-05-01","2017-04-30"
"COSMOKEMS","EXPERIMENTAL CONSTRAINTS ON THE ISOTOPE SIGNATURES OF THE EARLY SOLAR SYSTEM","bernard BOURDON","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","This project aims at simulating the processes that took place in the early Solar System to determine how these processes shaped the chemical and isotope compositions of solids that accreted to ultimately form terrestrial planets. Planetary materials exhibit mass dependent and mass independent isotope signatures and their origin and relationships are not fully understood. This proposal will be based on new experiments reproducing the conditions of the solar nebula in its first few million years and on a newly designed Knudsen Effusion Mass Spectrometer (KEMS) that will be built for the purpose of this project. This project consists of three main subprojects: (1) we will simulate the effect of particle irradiation on solids to examine how isotopes can be fractionated by these processes to identify whether this can explain chemical variations in meteorites. We will examine whether particle irradiation can cause mass independent fractionation, (2) the novel KEMS instrument will be used to determine the equilibrium isotope fractionation associated with reactions between gas and condensed phases at high temperature. It will also be used to determine the kinetic isotope fractionation associated with evaporation and condensation of solids. This will provide new constraints on the thermodynamic conditions, T, P and fO2 during heating events that have modified the chemical composition of planetary materials. These constraints will also help identify the processes that cause the depletion in volatile elements and the fractionation in refractory elements observed in planetesimals and planets, (3) we will examine the effect of UV irradiation on chemical species in the vapour phase as an attempt to reproduce observed isotope compositions found in meteorites or their components. These results may radically change our view on how the protoplanetary disk evolved and how solids were transported and mixed.","3106625","2016-10-01","2021-09-30"
"COSMOS","Semiparametric Inference for Complex and Structural Models in Survival Analysis","Ingrid VAN KEILEGOM","KATHOLIEKE UNIVERSITEIT LEUVEN","In survival analysis investigators are interested in modeling and analysing the time until an event happens. It often happens that the available data are right censored, which means that only a lower bound of the time of interest is observed. This feature complicates substantially the statistical analysis of this kind of data. The aim of this project is to solve a number of open problems related to time-to-event data, that would represent a major step forward in the area of survival analysis.

The project has three objectives:

[1] Cure models take into account that a certain fraction of the subjects under study will never experience the event of interest.  Because of the complex nature of these models, many problems are still open and rigorous theory is rather scarce in this area.  Our goal is to fill this gap, which will be a challenging but important task.

[2] Copulas are nowadays widespread in many areas in statistics. However, they can contribute more substantially to resolving a number of the outstanding issues in survival analysis, such as in quantile regression and dependent censoring.  Finding answers to these open questions, would open up new horizons for a wide variety of problems.

[3] We wish to develop new methods for doing correct inference in some of the common models in survival analysis in the presence of endogeneity or measurement errors.  The present methodology has serious shortcomings, and we would like to propose, develop and validate new methods, that would be a major breakthrough if successful.

The above objectives will be achieved by using mostly semiparametric models.  The development of mathematical properties under these models is often a challenging task, as complex tools from the theory on empirical processes and semiparametric efficiency are required.  The project will therefore require an innovative combination of highly complex mathematical skills and cutting edge results from modern theory for semiparametric models.","2318750","2016-09-01","2021-08-31"
"CoSuN","Cooperative Phenomena in Supramolecular Nanostructures","Harry Laurence Anderson","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Many of the remarkable properties of molecular nanostructures are cooperative effects. A system is described as cooperative when it behaves differently from expectations based on the properties of its individual components. Multivalent cooperativity is crucial for biological molecular recognition, yet the factors determining the magnitude of this effect are poorly understood. Excitonic cooperativity is exploited in sensitive detectors for explosives, and is the basis of photosynthetic light harvesting. Electronic cooperativity is illustrated on the molecular scale by the phenomenon of aromaticity, and on a larger scale by metallic conductivity. Magnetic properties provide many examples of cooperativity. The magnitude of cooperative effects increases with the strength of coupling between the individual components, and with the number of coupled components. Cooperative systems exhibit sharp changes in behavior in response to small changes in conditions, such as transitions from free to bound, fluorescent to non-fluorescent, or conductive to insulating. The tendency towards an “all-or-nothing” response is often useful; in the limit of a very large ensemble, it leads to phase transitions. The CoSuN project will extend methodology developed in Oxford to create large monodisperse supramolecular nanostructures which are uniquely suited for exploring multivalent, excitonic and electronic cooperativity. The template-directed synthesis of these nanostructures is made possible by strong multivalent cooperativity, while the electronic coupling between the individual subunits results in other cooperative phenomena. This project will clarify understanding of cooperative molecular recognition. It will also help to solve some of the mysteries of photosynthesis and reveal the first molecular manifestations of coherent quantum mechanical phenomena, such as Aharonov-Bohm effects.","2452688","2013-05-01","2018-04-30"
"COTURB","Coherent Structures in Wall-bounded Turbulence","Javier Jiménez Sendín","UNIVERSIDAD POLITECNICA DE MADRID","Turbulence is a multiscale phenomenon for which control efforts have often failed because the dimension of the attractor is large. However, kinetic energy and drag are controlled by relatively few slowly evolving large structures that sit on top of a multiscale cascade of smaller eddies. They are essentially single-scale phenomena whose evolution can be described using less information than for the full flow. In evolutionary terms they are punctuated ‘equilibria’ for which chaotic evolution is only intermittent. The rest of the time they can be considered coherent and predictable for relatively long periods. Coherent structures studied in the 1970s in free-shear flows (e.g. jets) eventually led to increased understanding and to industrial applications. In wall-bounded cases (e.g. boundary layers), proposed structures range from exact permanent waves and orbits to qualitative observations such as hairpins or ejections. Although most of them have been described at low Reynolds numbers, there are reasons to believe that they persist at higher ones in the ‘LES’ sense in which small scales are treated statistically. Recent computational and experimental advances provide enough temporally and spatially resolved data to quantify the relevance of such models to fully developed flows. We propose to use mostly existing numerical data bases to test the various models of wall-bounded coherent structures, to quantify how often and how closely the flow approaches them, and to develop moderate-time predictions. Existing solutions will be extended to the LES equations, methods will be sought to identify them in fully turbulent flows, and reduced-order models will be developed and tested. In practical situations, the idea is to be able to detect large eddies and to predict them ‘most of the time’. If simple enough models are found, the process will be implemented in the laboratory and used to suggest control strategies.","2497000","2016-02-01","2021-01-31"
"COULOMBUS","Electric Currents in Sediment and Soil","Lars Peter Nielsen","AARHUS UNIVERSITET","""With COULOMBUS I will explore the new electronic world I recently found in marine sediment; a living world featuring transmission of coulombs of electrons over long distances through a grid of unknown origin and composition. This is a great challenge to science, and I will specifically

- Unravel function, expansion, resilience, and microbial engineering of the conductive grid
- Identify microbial and geological processes related to long distance electron transfer today and in the past
- Introduce the electron as a new element in biogeochemical and ecological models.
- Map the range of sediment and soil habitats featuring biogeoelectric currents

Incubations of marine sediment will serve as the “base camp” for the surveys. Here I consistently observe that current sources extending centimetres down deliver electrons for most of the oxygen consumption, and here my array of advanced microsensors and biogeochemical methods works well. My team will record electric currents and biogeochemical changes as we manipulate mechanical, chemical, and biological conditions, thereby getting to an understanding of the interplay between conductors, microorganisms, electron donors, electron acceptors, and minerals. Next we take the methods out in the sea to evaluate biogeoelectricity in situ using robots. Other aquatic environments will also be screened. The ultimate outdoor challenge will come as I lead the team into soils where surface potentials suggest biogeoelectric currents deep down. All observations, experiments, and models will be directed to answer the groundbreaking questions: What physics and microbial engineering can explain long distance electron conductance in nature? How do electric microbial communities evolve and how do they shape element cycling? What signatures of biogeoelectricity are left in the geological record of earth history? If I succeed I will have opened up many new exciting research routes for the followers.""","2155300","2012-03-01","2017-02-28"
"COUNTATOMS","Counting Atoms in nanomaterials","Gustaaf Van Tendeloo","UNIVERSITEIT ANTWERPEN","COUNTING ATOMS IN NANOMATERIALS Advanced electron microscopy for solid state materials has evolved from a qualitative imaging setup to a quantitative scientific technique. This will allow us not only to probe and better understand the fundamental behaviour of (nano) materials at an atomic level but also to guide technology towards new horizons. The installation in 2009 of a new and unique electron microscope with a real space resolution of 50 pm and an energy resolution of 100 meV will make it possible to perform unique experiments. We believe that the position of atoms at an interface or at a surface can be determined with a precision of 1 pm; this precision is essential as input for modelling the materials properties. It will be first applied to explain the fascinating behaviour of multilayer ceramic materials. The new experimental limits will also allow us to literally count the number of atoms within an atomic columns; particularly counting the number of foreign atoms. This will not only require experimental skills, but also theoretical support. A real challenge is probing the magnetic and electronic information of a single atom column. According to theory this would be possible using ultra high resolution. This new probing technique will be of extreme importance for e.g. spintronics. Modern (nano) technology more and more requires information in 3 dimensions (3D), rather than in 2D. This is possible through electron tomography; this technique will be optimised in order to obtain sub nanometer precision. A final challenge is the study of the interface between soft matter (bio- or organic materials) and hard matter. This was hitherto impossible because of the radiation damage of the electron beam. With the possibility to lower the voltage to 80 kV and possibly 50 kV, maintaining more or less the resolution, we will hopefully be able to probe the active sites for catalysis.","2000160","2010-01-01","2014-12-31"
"Counting conjectures","Counting conjectures and characters of almost simple groups","Gunter Malle","TECHNISCHE UNIVERSITAET KAISERSLAUTERN","This proposal has two major goals: to understand the irreducible complex characters of the finite almost simple groups, and to apply this knowledge to prove two longstanding famous conjectures in the representation theory of finite groups: the McKay conjecture and the Alperin Weight Conjecture.

The first goal requires the study of the action of outer automorphisms of finite groups of Lie type on their irreducible characters and the solution of extension problems. The determination of the irreducible characters of all almost simple groups is a fundamental task of group theory.
For the second goal, we will build on the recent reductions (by the PI and others) of both conjectures to assertions on characters of finite simple groups. To prove these assertions, one needs to construct certain equivariant bijections with respect to outer automorphisms, which will involve the results from the first goal.
Furthermore, we propose to extend the reduction of the McKay conjecture to include several refinements, in particular the block-wise version and
congruences of character degrees.

The project will involve the interplay of methods from the theory of algebraic groups, character sheaves, block theory and modular character theory.","1444200","2012-04-01","2017-03-31"
"CoupledNC","Coupled Nanocrystal Molecules: Quantum coupling effects via chemical coupling of colloidal nanocrystals","Uri BANIN","THE HEBREW UNIVERSITY OF JERUSALEM","Coupling of atoms is the basis of chemistry, yielding the beauty and richness of molecules and materials. Herein I introduce nanocrystal chemistry: the use of semiconductor nanocrystals (NCs) as artificial atoms to form NC molecules that are chemically, structurally and physically coupled. The unique emergent quantum mechanical consequences of the NCs coupling will be studied and tailored to yield a chemical-quantum palette: coherent coupling of NC exciton states; dual color single photon emitters functional also as photo-switchable chromophores in super-resolution fluorescence microscopy; electrically switchable single NC photon emitters for utilization as taggants for neuronal activity and as chromophores in displays; new NC structures for lasing; and coupled quasi-1D NC chains manifesting mini-band formation, and tailored for a quantum-cascade effect for IR photon emission.  A novel methodology of controlled oriented attachment of NC building blocks (in particular of core/shell NCs) will be presented to realize the coupled NCs molecules.  For this a new type of Janus NC building block will be developed, and used as an element in a Lego-type construction of double quantum dots (dimers), heterodimers coupling two different types of NCs, and more complex NC coupled quantum structures. To realize this NC chemistry approach, surface control is essential, which will be achieved via investigation of the chemical and dynamical properties of the NCs surface ligands layer. As outcome I can expect to decipher NCs surface chemistry and dynamics, including its size dependence, and to introduce Janus NCs with chemically distinct and selectively modified surface faces. From this I will develop a new step-wise approach for synthesis of coupled NCs molecules and reveal the consequences of quantum coupling in them. This will inspire theoretical and further experimental work and will set the stage for the development of the diverse potential applications of coupled NC molecules.","2499750","2017-11-01","2022-10-31"
"Couplet","Transient climate change in the coupled atmosphere--ocean system","Jonathan GREGORY","THE UNIVERSITY OF READING","The magnitude and impacts of many aspects of projected climate change due to anthropogenic emissions of greenhouse gases are expected to be greater for larger global mean surface temperature change. Although climate models have hugely improved, knowledge has grown and confidence increased, the climate feedback parameter, which determines the amount of global warming that results at equilibrium for a given radiative forcing (the heating due to greenhouse gases and other agents) is still very uncertain; for example, the range of equilibrium warming for a CO2 concentration of twice the pre-industrial level is 1.5-4.5 K, the same as estimated 25 years ago. It is widely assumed that we can evaluate the climate feedback parameter from the observed past or from an idealised model experiment with increased CO2, then use it to estimate global warming for future scenarios. However, research has revealed that, as well as being uncertain, the climate feedback parameter is not constant; it depends on the nature and magnitude of the forcing agent, it changes over time under constant forcing, it does not apply equally to spontaneous unforced climate variability, and it is not the same in the historical record and projections. The hypothesis of this project is that these reflect inadequacies of the global energy balance framework, which relates radiative forcing, climate feedback and ocean heat uptake to transient climate change. The objectives are therefore to develop a new framework for describing the variations of the coupled atmosphere--ocean climate system, by taking into account the relationships between the geographical patterns of change and its time-development in analyses of simulated and observed climate change, and to apply this framework to the analysis of historical climate change, in order to set refined constraints on the processes, pattern and magnitude of future CO2-forced climate change.","2127711","2018-10-01","2023-09-30"
"COXINEL","COherent  Xray source INferred from Electrons accelerated by Laser","Marie-Emmanuelle Couprie","SYNCHROTRON SOLEIL SOCIETE CIVILE","""Since the first laser discovery in 1960 and the first Free Electron Laser (FEL) in 1977, Linac based fourth generation light sources provide intense coherent fs pulses in the X-ray range for multidisciplinary investigations of matter. In parallel, Laser Wakefield Accelerator (LWFA) by using intense laser beams interacting with cm long plasmas can now provide high quality electron beams of very short bunches (few fs) with high peak currents (few kA). The so-called 5th generation light source aims at reducing the size and the cost of these FELs by replacing the linac by LWFA. Indeed, spontaneous emission from LWFA has already been observed, but the presently still rather large energy spread (1 %) and divergence (mrad) prevent from the FEL amplification. In 2012, two novel schemes in the transport proposed in the community, including my SOLEIL group, predict a laser gain increase by 3 or 4 orders of magnitudes. COXINEL aims at demonstrating the first lasing of an LWFA FEL and its detailed study in close interaction with future potential users. The key concept relies on an innovative electron beam longitudinal and transverse manipulation in the transport towards an undulator: a """"demixing"""" chicane sorts the electrons in energy and reduces the spread from 1 % to a slice one of 0.1%, and the transverse density is maintained constant all along the undulator (supermatching). Simulations based on the performance of the 60 TW laser of the Laboratoire d’Optique Appliquée and existing undulators from SOLEIL suggest that the conditions for lasing are fulfilled. The SOLEIL environment also possesses the engineering fabrication capability for the actual realization of these theoretical ideas, with original undulators and innovative variable permanent compact magnets for the transport. COXINEL will enable to master in Europe advanced schemes scalable to shorter wavelengths and pulses, paving the way towards FEL light sources on laboratory size, for fs time resolved experiments.""","2500000","2014-01-01","2018-12-31"
"CPDENL","Control of partial differential equations and nonlinearity","Jean-Michel Coron","UNIVERSITE PIERRE ET MARIE CURIE - PARIS 6","The aim of this 5,5 years project is to create around the PI a research group on the control of systems modeled by partial differential equations at the Laboratory Jacques-Louis Lions of the UPMC and to develop with this group an intensive research activity focused on nonlinear phenomena.

With the ERC grant, the PI plans to hire post-doc fellows and PhD students, to offer 1-to-3 months positions to confirmed  researchers, a regular seminar and workshops.

A lot is known on  finite dimensional control systems and  linear control systems modeled by partial differential equations. Much less is known for nonlinear control systems modeled by partial differential equations. In particular, in many important cases,  one does not know how to use the classical iterated Lie brackets which are so useful to deal with nonlinear control systems in finite dimension.

In this project, the PI plans to develop, with the research group, methods to deal with the problems of controllability and of stabilization for nonlinear systems modeled by partial differential equations, in the case where the nonlinearity plays a crucial role. This is for example the case where the linearized control system around the equilibrium of interest is not controllable or not stabilizable. This is also the case when the nonlinearity is too big at infinity and one looks for global results. This is also the case if the nonlinearity contains too many derivatives. The PI has already introduced some methods to deal with these cases, but a lot remains to be done. Indeed, many natural important and challenging problems are still open. Precise examples, often coming from physics, are given in this proposal.","1403100","2011-05-01","2016-09-30"
"CREAM4","Chemical Reaction Engineering by Additive Manufacturing of Mesoscale MetaMaterials","Johannes Gerardus Elisabeth GARDENIERS","UNIVERSITEIT TWENTE","""The management of mesoscale dynamics is the missing link in gaining complete control over chemical processes like heterogeneous catalysis. The ability to accurately position nanoscale active elements in cellular mesoscale (nm to µm-range) structures with high symmetrical order is instrumental in streamlining vital molecular or energetic paths. 3D periodicity in the structure that supports active or adsorption sites minimizes spatial variations in mass transport, whereas mesoscale control of the location of these sites gives a route to tuning activity and functionality. The introduction of mesoscale metamaterials expands the on-going trend in chemistry, of more and more dimensionally refined structured elements, a so to speak """"Moore's law in Process Intensification"""". The roadmap to higher process efficiency dictates a next, disruptive step in mastering manufacturing control at smaller dimensions. The proposed disruptive technology to realize the required mesoscale features is Additive Manufacturing, which is the only method offering the desired freedom in shape, symmetry and composition. More specifically, this project explores electrospinning methods with precise intra-wire control of the position of active sites and accurately tuneable 3D inter-wire distances. This is seen as the ideal technique to reach the mesoscale material target, as the method is scalable to practical device volumes. The main ingredients of the novel technology are microfluidic networks to line up nanoparticles, before electrospinning them with integrated micromachined nozzles, and depositing them accurately in the form of 3D nanowire networks, using integrated circuit collector electrodes. Flow-through, cellular materials which are highly homogeneous in size and composition, or with intentionally embedded gradients, having features designed at the mesoscale, will be investigated for applications in the fields of heterogeneous catalysis and solar energy capture and conversion.""","2500000","2017-09-01","2022-08-31"
"CREATIV","Creating Co-Adaptive Human-Computer Partnerships","Wendy Mackay","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","""CREATIV explores how the concept of co-adaptation can revolutionize the design and use of interactive software. Co-adaptation is the
parallel phenomenon in which users both adapt their behavior to the system’s constraints, learning its power and idiosyncrasies, and
appropriate the system for their own needs, often using it in ways unintended by the system designer.
A key insight in designing for co-adaptation is that we can encapsulate interactions and treat them as first class objects, called interaction
instruments This lets us focus on the specific characteristics of how human users express their intentions, both learning from and
controlling the system. By making instruments co-adaptive, we can radically change how people use interactive systems, providing
incrementally learnable paths that offer users greater expressive power and mastery of their technology.
The project offers theoretical, technical and empirical contributions. CREATIV will develop a novel architecture and generative principles for
creating co-adaptive instruments. The multi-disciplinary design team includes computer scientists, social scientists and designers as well
as ‘extreme users’, creative professionals who push the limits of their technology. Using participatory design techniques, we will articulate
the design space for co-adaptive instruments and build a series of prototypes. Evaluation activities include qualitative and quantitative
studies, in the lab and in the field, to test hypotheses and assess the success of the prototypes.
The initial goal of the CREATIV project is to fundamentally improve the learning and expressive capabilities of advanced users of creative
software, offering significantly enhanced methods for expressing and exploring their ideas. The ultimate goal is to radically transform
interactive systems for everyone by creating a powerful and flexible partnership between human users and interactive technology.""","2458996","2013-06-01","2018-05-31"
"CRESUCHIRP","Ultrasensitive Chirped-Pulse Fourier Transform mm-Wave Detection of Transient Species in Uniform Supersonic Flows for Reaction Kinetics Studies under Extreme Conditions","Ian SIMS","UNIVERSITE DE RENNES I","This proposal aims to develop a combination of a chirped-pulse (sub)mm-wave rotational spectrometer with uniform supersonic flows generated by expansion of gases through Laval nozzles and apply it to problems at the frontiers of reaction kinetics.
The CRESU (Reaction Kinetics in Uniform Supersonic Flow) technique, combined with laser photochemical methods, has been applied with great success to perform research in gas-phase chemical kinetics at low temperatures, of particular interest for astrochemistry and cold planetary atmospheres. Recently, the PI has been involved in the development of a new combination of the revolutionary chirped pulse broadband rotational spectroscopy technique invented by B. Pate and co-workers with a novel pulsed CRESU, which we have called Chirped Pulse in Uniform Flow (CPUF). Rotational cooling by frequent collisions with cold buffer gas in the CRESU flow at ca. 20 K drastically increases the sensitivity of the technique, making broadband rotational spectroscopy suitable for detecting a wide range of transient species, such as photodissociation or reaction products. 
We propose to exploit the exceptional quality of the Rennes CRESU flows to build an improved CPUF instrument (only the second worldwide), and use it for the quantitative determination of product branching ratios in elementary chemical reactions over a wide temperature range (data which are sorely lacking as input to models of gas-phase chemical environments), as well as the detection of reactive intermediates and the testing of modern reaction kinetics theory. Low temperature reactions will be initially targeted; as it is here that there is the greatest need for data. A challenging development of the technique towards the study of high temperature reactions is also proposed, exploiting existing expertise in high enthalpy sources.","2100230","2016-09-01","2021-08-31"
"CRIPHERASY","Critical Phenomena in Random Systems","Giorgio Parisi","UNIVERSITA DEGLI STUDI DI ROMA LA SAPIENZA","This project aims to get a theoretical understanding of the most important large-scale phenomena in classical and quantum disordered systems. Thanks to the renormalization group approach the critical behaviour of pure systems is under very good control; however disordered systems are in many ways remarkably peculiar (think for example to non-perturbative phenomena like Griffiths singularities), often the conventional approach does not work and many crucial issues are still unclear. My work aims to fill this important hole in our understanding of disordered systems. I will concentrate my efforts on some of the most important and studied systems, i.e. spin glasses, random field ferromagnets (that are realized in nature as diluted antiferromagnets in a field), Anderson and Mott localization (with possible experimental applications to Bose-Einstein condensates and to electron glasses), surface growth in random media (KPZ and DLA models). In this project I want to pursue a new approach to these problems. I aim to compute in the most accurate way the properties of these systems using the original Wilson formulation of the renormalization group with a phase space cell analysis; this is equivalent to solving a statistical model on a hierarchical lattice (Dyson-Bleher-Sinai model). This is not an easy job. In the same conceptual frame we plan to use simultaneously very different techniques: probabilistic techniques, perturbative techniques at high orders, expansions around mean field on Bethe lattice and numerical techniques to evaluate the critical behaviour. I believe that even this restricted approach is very ambitious, but that the theoretical progresses that have been done in unveiling important features of disordered systems suggest that it will be possible to obtain solid results.","2098800","2010-01-01","2014-12-31"
"CRIPTO","CRIPTO: Cryptography Research Involving Practical and Theoretical Outlooks","Nigel Smart","UNIVERSITY OF BRISTOL","In this project I will investigate four interrelated topics in cryptography from both a theoretical and practical perspective. Each topic is chosen such that it not only provides a testing ground for more general ideas, but it also is grounded in specific examples which can help guide the general principles. Each topic has the potential to make dramatic advances, both on the subject of cryptography itself and how it is used and deployed in the real world.

We will be investigating application domains as diverse as cloud computing, electronic voting, protocols for trusted computing and privacy preserving methodologies. Each topic will be tackled however with common tools of provable security, and testing via implementation. In addition we aim to extend the tool box of techniques available to the cryptographer in terms of analysis and development methodologies, by being guided by the above application domains.

This research will have a transformative affect on the subject of cryptography and how it is deployed in the real world. We aim to demonstrate that previous “blue-skies” research can have direct practical benefit in applications, by researching in a pipeline of theory-to-practice. In addition we aim to feed back the practical knowledge learned into new theoretical models which capture more realistically the scenarios faced in practice, thus making the pipeline two-way.

Finally, the proposal builds on a wealth of knowledge and experience built up at Bristol over the last ten years in these explicit sub-areas. My group at Bristol is not only the best place to execute this ambitious programme of work, but, due to the unique combination of theoretical and practical perspectives we offer, possibly the only place capable of working on these interrelated fronts.","2102041","2011-10-01","2016-09-30"
"CRITISUP2","Criticality and Dual Superfluidity","christophe SALOMON","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Low temperature matter exhibits a spectacular variety of highly ordered states that occur through phase transitions. In quantum systems, phase transitions and associated critical phenomena constitute a central issue of modern physics. Wilson’s theory of renormalization showed that very different physical systems could be unified under the same universality class characterized by critical exponents. The high degree of control offered by ultracold atom experiments sets them as an ideal platform for the investigation of phase transitions and critical phenomena.
CRITISUP2 aims at exploring criticality in superfluid spin ½ Fermi gases where the interplay between temperature spin polarization and interactions is at the origin of a rich phase diagram and a variety of phase transitions. We will measure the corresponding static and dynamic critical exponents, and search for the long-sought Fulde-Ferrell-Larkin-Ovchinnikov (FFLO) phase predicted over 50 years ago. We will also study the phase diagram and critical counterflow of dual Bose-Fermi superfluids which have emerged as a new paradigm of quantum matter. Cutting-edge Bold Diagrammatic Monte Carlo and new resummation methods, developed in-house, will be confronted to the experiments on the one hand, and provide answers to debated questions on the other.
The expected outcomes of CRITISUP2 will constitute a major leap forward relevant for several fields of modern physics, ranging from condensed-matter to astrophysics, nuclear physics, and high energy physics.","2246536","2017-05-01","2022-04-30"
"CRITMAG","Critical Behaviour in Magmatic Systems","Jonathan David Blundy","UNIVERSITY OF BRISTOL","Crustal magmatism is periodic on a very wide range of timescales from pulses of continental crustal growth, through formation of granite batholiths, to eruptions from individual volcanic centres. The cause of this periodicity is not understood. I aim to address this long-standing geological problem through a combination of experiments, petrological methods and numerical models via a novel proposal that periodicity arises because of the highly non-linear ( critical ) behaviour of magma crystallinity with temperature in a series of linked crustal magma reservoirs. The ultimate objective is to answer five fundamental questions: &quot; Why is crustal magmatism episodic? &quot; How are large batholiths formed of rather similar magmas over long periods of time? &quot; How do large bodies of eruptible magma develop that can lead to huge, caldera-forming eruptions? &quot; What controls the chemistry of crustal magmas? Why are some compositions over-represented relative to others? &quot; What is the thermal structure beneath volcanic arcs and how does it evolve with time? The project will address these questions through case studies of three contrasted active volcanoes: Nevado de Toluca, Mexico; Soufriere St Vincent, Lesser Antilles; and Mount Pinatubo, Philippines. For each volcano I will use experimental petrology to constrain the phase relations of the most recently erupted magma as a function of pressure, temperature, volatile content and oxygen fugacity in the shallow, sub-volcanic storage region. I will also carry out high-pressure phase equilibria on coeval Mg-rich basaltic rocks from each area with the aim of constraining the lower crustal conditions under which the shallow magmas were generated and use diffusion chronometry to constrain the frequency of magmatic pulses in the sub-volcanic reservoirs. The project will result in a quantum leap forwards in how experimental and observational petrology can be used to understand magmatic behaviour beneath hazardous volcanoes","2959518","2010-04-01","2015-03-31"
"CROSS","Cryogenic Rare-event Observatory with Surface Sensitivity","Andrea Ernesto Guido GIULIANI","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","CROSS will set the grounds for large-scale experiments searching for neutrinoless double beta decay with zero background at an exposure scale of ~1 tonne x year and with very high energy resolution – about 1.5‰ – in the region of interest. These features will enable searching for lepton number violation with unprecedented sensitivity, penetrating in prospect the direct-ordering region of the neutrino masses. CROSS will be based on arrays of TeO2 and Li2MoO4 bolometers enriched in the isotopes of interest 130Te and 100Mo, respectively. There are strong arguments in favor of these choices, such as the high double beta transition energy of these candidates, the easy crystallization processes of TeO2 and Li2MoO4, and the superior bolometric performance of these compounds in terms of energy resolution and intrinsic purity. The key idea in CROSS is to reject surface events (a dominant background source) by pulse-shape discrimination, obtained by exploiting solid-state-physics phenomena in superconductors. The surfaces of the crystals will be coated by an ultrapure superconductive aluminium film, which will act as a pulse-shape modifier by delaying the pulse development in case of shallow energy depositions, exploiting the long quasi-particle life-time in aluminium. This method will allow getting rid of the light detectors used up to now to discriminate surface alpha particles, simplifying a lot the bolometric structure and achieving the additional advantage to reject also beta surface events, which unfortunately persist as an ultimate background source if only alpha particles are tagged. The intrinsic modularity and the simplicity of the read-out will make CROSS easily expandable. The CROSS program is focused on an intermediate experiment with 90 crystals, installed underground in the Canfranc laboratory, which will be not only extremely competitive in the international context but also a decisive step to demonstrate the enormous potential of CROSS in terms of background.","3146598","2018-01-01","2022-12-31"
"CROWDED-PRO-LIPIDS","Computational Perspective to Dynamical Protein-Lipid Complexes under Crowded Conditions","Ilpo Tapio Vattulainen","TTY-SAATIO","""One of the great challenges is to understand how cellular functions emerge in cell membrane systems. Unlocking this mystery is the key to the vast majority of human diseases. The current view is based on a static picture where membrane proteins in protein-poor membranes interact with a few specific lipids, while in reality the situation is much more complicated. This ambitious project aims for a breakthrough by changing the present paradigm. The objective is to focus on the dynamical interplay between lipids and proteins under crowded conditions, paving the way for understanding the dynamics of lipid-protein complexes and their resulting functions. The objectives are outstanding and contain a high risk, with exceptional gain. The main goal is better understanding of the physical principles that give rise to cellular functions, with a strong impact to clarify the relevance of dynamical lipid-protein interactions in cellular processes related to health and disease. For this purpose, the grand themes chosen for this project are lipoproteins coupled to cardiovascular disease (“good” and “bad” cholesterol) and the function of especially cholesterol and glycolipids with membrane proteins. In order to meet these goals, the applicant employs state-of-the-art simulation techniques that comprise quantum-mechanical, classical atomistic and coarse-grained simulation methods to elucidate the complex biological phenomena associated with lipid-protein systems. The simulations cover atomistic and molecular details, over time scales from femtoseconds up to milliseconds. The theory & simulation group lead by PI comprises expertise in a truly cross- and multi-disciplinary manner, and it strongly collaborates with some of the leading experimental teams in biomedical sciences, cell biology, structural biology, and membrane biophysics.""","1920334","2012-05-01","2017-04-30"
"CryptoCloud","Cryptography for the Cloud","David Daniel Rene Pointcheval","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Many companies have already started the migration to the Cloud and many individuals share their personal informations on social networks. Unfortunately, in the current access mode, the provider first authenticates the client, and grants him access, or not, according to his rights in the access-control list. Therefore, the provider itself not only has total access to the data, but also knows which data are accessed, by whom, and how: privacy, which includes secrecy of data (confidentiality), identities (anonymity), and requests (obliviousness), should be enforced.

The industry of the Cloud introduces a new implicit trust requirement: nobody has any idea at all of where and how his data are stored and manipulated, but everybody should blindly trust the providers. Privacy-compliant procedures cannot be left to the responsibility of the provider: however strong the trustfulness of the provider may be, any system or human vulnerability can be exploited against privacy. This presents too huge a threat to tolerate. The distribution of the data and the secrecy of the actions must be given back to the users. It requires promoting privacy as a global security notion.

A new generation of secure multi-party computation protocols is required to protect everybody in an appropriate way, with privacy and efficiency: interactive protocols will be the core approach to provide privacy in practical systems.

Privacy for the Cloud will have a huge societal impact since it will revolutionize the trust model: users will be able to make safe use of outsourced storage, namely for personal, financial and medical data, without having to worry about failures or attacks of the server. It will also have a strong economic impact, conferring a competitive advantage on Cloud providers implementing these tools.","2168261","2014-06-01","2019-05-31"
"CRYTERION","Cryogenic Traps for Entanglement Research with Ions","Rainer Blatt","UNIVERSITAET INNSBRUCK","Quantum computers offer a fundamentally new way of information processing. Within the scope of this proposal, quantum information processing with an ion trap quantum computer will be investigated. With the new combination of cryogenic technology and ion traps for quantum computing we intend to build a quantum information processor with strings of up to 50 ions and with two-dimensional ion arrays for an investigation of deterministic many-particle entanglement. The cryogenic traps will be applied for quantum simulations, for fundamental investigations concerning large-scale entanglement and for precision measurements enhanced by quantum metrology techniques employing entangled particles.","2200000","2008-12-01","2013-11-30"
"CRYVISIL","Crystalline and vitreous silica films and their interconversion","Hans-Joachim Freund","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Silicon is the most abundant element in the earth’s crust. Its oxide, silica (SiO2) is the basis for most minerals of the earth’s crust, and also for a number of technological applications ranging from window glass, via electronics to catalysis. The structure of crystalline materials such as quartz or silica-based minerals is well understood due to the application of scattering techniques such as x-ray or neutron diffraction, for example, which allow accurate structure determinations. Silica, however, also forms glasses, which are amorphous or vitreous. Its structure is not well understood. In fact, diffraction techniques have only been able to deliver pair correlation functions, which reveal the density of a material around a given atom, but do not allow a detailed reconstruction of the atomic structure as in the case of crystalline materials. Until recently, a real space image of a silica glass with atomic resolution had not been recorded. Using scanning probe techniques applied to a thin silica film grown atomically flat on a metal substrate, it has been possible to reveal, for the first time, an atomically resolved image of vitreous silica. Both, a crystalline as well as a vitreous phase have been imaged. With this system, it is now possible to address the transition from a vitreous state to a crystal-line in real space by developing a scanning probe microscope that allows the study of its structure over a wide range of temperatures ranging from cryogenic temperatures to 1500 K. It is the purpose of this grant application to build such a device and apply it to the crystal-glass transition and the study of vibrational properties. This instrument may also be used to address a number of scientific problems related to other glass-formers, such as borates and the influence of silica modifications by atom doping, for example.","2484375","2016-01-01","2020-12-31"
"CUNDA","Causality Relations Using Nonlinear Data Assimilation","Peter Jan VAN LEEUWEN","THE UNIVERSITY OF READING","A major problem in understanding complex nonlinear geophysical systems is to determine which processes drive which other processes, so what the causal relations are. 

Several methods to infer nonlinear causal relations exist, but often lead to different answers, often perform hypothesis testing on causality, need long stationary time series, can be misleading if an unknown process drives the processes under study, or, if a numerical model is used, reflect model causality instead of real-world causality. Furthermore methods that use the governing evolution equations directly lead to intractable high-dimensional integrals.

In this proposal I will tackle these problems by firstly embedding causality into a Bayesian framework, moving from testing causality to estimating causality strength and its uncertainty in a systematic way. Knowledge from several causality methods can be combined, new knowledge can be brought in systematically, and time series can be short.   Furthermore, new knowledge can be incorporated into the existing knowledge basis, and 
several methods can be combined in a consistent manner. Secondly, a new formulation to infer causal strength exploring evolution equations that avoids high-dimensional integrals will be explored. Thirdly, numerical models are combined with observations by exploring fully nonlinear data assimilation to study real-world causality.

I will test the new techniques on simple models and then apply them to a high-resolution model 
of the ocean area around South Africa where the Southern Ocean, the Indian Ocean, and the Atlantic Ocean meet.
This area plays a crucial role in the global circulation of heat and salt by bringing warm and salty Indian Ocean
water into the Atlantic in a highly turbulent manner. The techniques allow to infer what sets this interocean transport, 
the turbulent local dynamics or the global climate-related dynamics, crucial for understanding 
the functioning of the ocean in the climate system.","2597754","2016-09-01","2021-08-31"
"CyberCare","Integrated Sensing Architectures and Tools for Health Care","Giovanni De Micheli","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","This proposal addresses high-risk, high-reward research of integrated sensing and computing architectures, as well as of models, methods and tools for their design and operation. Such architectures provide the bridge between bio-systems and information processing systems, where a bio-system is an abstraction of a human in terms of biophysical parameters. Breakthroughs in data acquisition, processing and decision making support will enable new smart-health applications.

The essential research goals of this proposal are: biophysical data acquisition by novel programmable integrated sensor arrays and their design and test using a modular and structured architecture; data processing in situ and/or remotely using application-specific hardware and/or embedded software; a new robust synthesis methodology for data processing units based on a new logic structure; models, abstractions and software tools for reasoning about the acquired data, to validate health conditions and/or to provide remedies (i.e., therapy). The results of this research will be embodied in a demonstrator showing the effectiveness of these combined technologies in first-aid medical care. 

The outcome of this research will have a deep and broad impact on health care, because it will improve diagnosis and therapy in a variety of cases. Namely, it will boost the quality and quantity of the acquired biophysical data, possibly in real time, by leveraging multiple sensing modalities and dedicated computing architectures. The use of formal methods for design, data evaluation and decision making support will enhance the quality of the diagnostic platforms and will ease their qualification and adoption. Moreover, the integration of sensing and electronics and their in-field programmability will reduce production cost and lower the barrier of adoption, thus providing for better and more affordable health care means.","2086740","2016-01-01","2020-12-31"
"CyberGenetics","Cybergenetics: Theory and Design Tools for Biomolecular Control Systems","Mustafa KHAMMASH","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","We propose to develop a new theory and design tools for the estimation and real-time control of living cells. The control systems designed using these tools will precisely and robustly steer the dynamic behavior of living cells in real time to achieve desired objectives.  Cells would be controlled either collectively at the population level, or individually as single cells. The control systems achieving this regulation will be realized either on a digital computer that is interfaced with living cells, or using de novo genetic circuits that are introduced into the cells where they are designed to function as molecular control systems. Our methods will explicitly confront the numerous challenges brought about by the special environment of the cell including nonlinearity, stochasticity, cell-to-cell variability, metabolic burden, etc. The theory and methods developed in this project will thus enable the systematic, rational, and effective feedback control of living cells at the gene level, and will lay the foundation for a new corresponding body of knowledge which we call ``Cybergenetics''.  It will also open new research directions in the areas of control theory and estimation.

We also propose to design three cybergenetic control systems, each addressing an important application in biotechnology or therapeutics. In the first, the controller will use light and nutrient supply to precisely regulate gene expression and cell growth in E. coli to achieve high protein and low biomass production rates. The second involves multiple feedback controllers regulating in parallel a large number of single stem cells, and leading to their differentiation to desired fates, e.g. beta cells, with potential for therapeutic applications. Finally, we will engineer into living cells dynamic molecular control systems. Such controllers can be used to monitor physiological variables and secrete biological effectors in a feedback fashion for the treatment of diseases like Type 1 diabetes.","2499887","2017-08-01","2022-07-31"
"D-TXM","Diffraction Based Transmission X-ray Microscopy","Henning Friis Poulsen","DANMARKS TEKNISKE UNIVERSITET","The aim of this project is to develop a diffraction based transmission X-ray microscope, d-TXM, for non-destructive structural characterization of polycrystalline materials such as metals, ceramics, semiconductors, dust, soil and rocks, and for R&D applications in e.g. the energy-, electronics- and environmental sectors. Uniquely, d-TXM will be able to visualise the grains inside 100 micrometer thick specimens with a spatial resolution of 10-30 nm. Up to a thousand grains may be mapped simultaneously in three dimensions with respect to morphology, phase, orientation and local stress-state. Furthermore, the method will be sufficiently fast to enable the acquisition of 3D movies of the time evolution of the structure in nano-materials and components during synthesis, processing or operation.
During the last decade the applicant pioneered and matured a set of X-ray based methods for 3D studies of polycrystals on the micrometre scale. For this achievement, he is recognized as a worldwide leading figure in X-ray instrumentation for structural materials, situated at a nodal point between materials, X-ray physics, applied mathematics and crystallography. The underlying vision of d-TXM is similar to this past work, but in terms of optics the microscopy approach is radically different and the spatial resolution will be two orders of magnitude better.
In this project, the scientific potential will be demonstrated by means of applications to selected issues in metallurgy. Being able to directly observe the evolution of the individual crystalline elements, our understanding of processes such as plasticity and phase evolution can be greatly enhanced.
Dissemination to other fields will take place via an advisory board of future users and a workshop. Continuity of the project is ensured by the technique being implemented at the European Synchrotron Research Facility.","2499860","2012-10-01","2017-09-30"
"D5S","Direct Statistical Simulation of the Sun and Stars","Steven Tobias","UNIVERSITY OF LEEDS","This proposal (D5S) addresses a key problem of astrophysics – the origin of magnetic activity in the sun and solar-type
stars. This is a problem not only of outstanding theoretical importance but also significant practical impact – solar activity has
major terrestrial consequences. An increase in activity can lead to an increase in the number and violence of solar flares and
coronal mass ejections, with profound consequences for our terrestrial environment, causing disruption to satellites and
power. Predictions of magnetic activity are highly desired by government and industry groups alike. A deep understanding of
the mechanisms leading to solar magnetic activity is required. The variable magnetic field is generated by a dynamo in the
solar interior. Though this mechanism is known to involve the interaction of magnetohydrodynamic (MHD) turbulence with
rotation, no realistic model for dynamo action currently exists. D5S utilises two recent significant breakthroughs to construct
new models for magnetic field generation in the sun and other solar-type stars. The first of these involves an entirely new
approach termed Direct Statistical Simulation (DSS) (developed by the PI), where the statistics of the astrophysical flows are
solved directly (enabling the construction of more realistic models). This approach is coupled to a breakthrough (recently
published by the PI in Nature) in our understanding of the physics of MHD turbulence at the extreme parameters relevant to
solar interiors. D5S also uses the methodology of DSS to provide statistical subgrid models for Direct Numerical Simulation
(DNS). This will increase the utility, fidelity and predictability of such models for solar magnetic activity. Either of these new
approaches, taken in isolation, would lead to significant progress in our understanding of magnetic field generation in stars.
Taken together, as in this proposal, they will provide a paradigm shift in our theories for solar magnetic activity.","2499899","2018-10-01","2023-09-30"
"DAL","DAL: Defying Amdahl's Law","Andre Seznec","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","Multicore processors have now become mainstream for both general-purpose and embedded computing. Instead of working on improving  the architecture of the  next generation multicore, with the DAL project, we deliberately anticipate the  next few generations of  multicores.

While multicores featuring 1000's of cores might become  feasible around 2020, there are strong indications that sequential programming style will continue to be dominant. Even future mainstream parallel applications will exhibit large sequential sections.  Amdahl's law  indicates that high performance on these sequential sections is needed to enable overall high performance on the whole application. On many (most) applications, the effective performance of future computer systems using a  1000-core processor chip  will significantly depend on their performance on both sequential code sections and single thread.

We envision that,  around 2020, the processor chips will feature a few complex cores  and many (may be 1000's) simpler, more silicon and power effective cores.

In the DAL  research project, we will explore the microarchitecture techniques that will be needed to enable high performance on such heterogeneous processor chips. Very high performance will be required on both sequential sections -legacy sequential codes, sequential sections of parallel applications- and critical threads on parallel applications -e.g. the main thread controlling the application.  Our research will  focus on enhancing single process performance. On the microarchitecture side, we will explore both a radically  new approach, the sequential accelerator,  and more conventional processor architectures. We will also study how to exploit heterogeneous multicore architectures to enhance  sequential thread performance.","2398542","2011-04-01","2016-03-31"
"DALDECS","Development and Application of Laser Diagnostic Techniques for Combustion Studies","Lars Eric Marcus Aldén","LUNDS UNIVERSITET","This project is directed towards development of new laser diagnostic techniques and a deepened physical understanding of more established techniques, aiming at new insights in phenomena related to combustion processes. These non-intrusive techniques with high resolution in space and time, will be used for measurements of key parameters, species concentrations and temperatures. The techniques to be used are; Non-linear optical techniques, mainly Polarization spectroscopy, PS. PS will mainly be developed for sensitive detection with high spatial resolution of &quot;new&quot; species in the IR region, e.g. individual hydrocarbons, toxic species as well as alkali metal compounds. Multiplex measurements of these species and temperature will be developed as well as 2D visualization. Quantitative measurements with high precision and accuracy; Laser induced fluorescence and Rayleigh/Raman scattering will be developed for quantitative measurements of species concentration and 2D temperatures. Also a new technique will be developed for single ended experiments based on picosecond LIDAR. Advanced imaging techniques; New high speed (10-100 kHz) visualization techniques as well as 3D and even 4D visualization will be developed. In order to properly visualize dense sprays we will develop Ballistic Imaging as well as a new technique based on structured illumination of the area of interest for suppression of multiple scattering which normally cause blurring effects. All techniques developed above will be used for key studies of phenomena related to various combustion phenomena; turbulent combustion, multiphase conversion processes, e.g. spray combustion and gasification/pyrolysis of solid bio fuels. The techniques will also be applied for development and physical understanding of how combustion could be influenced by plasma/electrical assistance. Finally, the techniques will be prepared for applications in industrial combustion apparatus, e.g. furnaces, gasturbines and IC engines","2466000","2010-02-01","2015-01-31"
"DAMESYFLA","Electroweak Symmetry Breaking, Flavor and Dark
Matter: One Solution for Three Mysteries","Guido Martinelli","SCUOLA INTERNAZIONALE SUPERIORE DI STUDI AVANZATI DI TRIESTE","In the next five years, experiments will give us a unique opportunity to unravel the mysteries of Electroweak Symmetry Breaking, Flavor and Dark Matter. The LHC at CERN will push the Energy frontier well into the TeV region and shed light on electroweak symmetry breaking. The LHCb experiment,  super-B factories and other dedicated experiments, also in the lepton sector, will push forward the Intensity frontier and test the Standard Model description of flavor and CP violation with unprecedented accuracy.  Earth- and space-based experiments will push forward the Astroparticle frontier, in particular direct and indirect searches for Dark Matter. My goal is to identify a coherent explanation of the three mysteries, as complete and as unique as possible, by combining the vast information coming from the Energy, Intensity and  Astroparticle frontiers. This requires a global strategy, making use of highly qualified competences in the relevant branches of theory and phenomenology. I will put together some of the leading particle theorists operating in SISSA, Padua and Rome into a unique and extraordinarily strong team. The variety of competences,  ranging from phenomenological fits and data interpretation to unified models and fundamental theories, will be used to interpret the results coming from a wide range of experiments and to formulate a coherent framework to account for them. With the essential contribution of the researchers paid on the project funds, the project will catalyze results going much beyond what the team members could individually achieve. The main support requested to the ERC is for hiring six experienced researchers, the rest of the funds are for optimizing the effectiveness of the team and the research environment.","1439400","2011-04-01","2017-03-31"
"DAMIC-M","Unveiling the Hidden: A Search for Light Dark Matter with CCDs","Paolo PRIVITERA","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Dark matter (DM) is a ubiquitous yet invisible presence in our universe. It dictated how galaxies formed in the first place, and now moves stars around them at puzzling speeds. The DM mass in the universe is known to be five times that of ordinary matter; yet its true nature remains elusive. 

Weakly interacting massive particles (WIMPs), relics from the early universe, are a compelling explanation chased by sensitive experiments in deep underground laboratories. However, searches for heavy WIMPs (≈100 times the proton mass), the most theoretically natural candidates, have been so far unsuccessful. Nor has evidence for such heavy particles yet been found at the CERN Large Hadron Collider. Alternative scenarios are now under scrutiny, such as the existence of a hidden sector of lighter DM particles that interact, differently than WIMPs, also with electrons. 

DAMIC-M (Dark Matter In CCDs at Modane) will search beyond the heavy WIMP paradigm by detecting nuclear recoils and electrons induced by light DM in charge-coupled devices (CCDs). The 0.5 kg detector will be installed at the Laboratoire Souterrain de Modane, France. In this novel and unconventional use of CCDs, which are commonly employed for digital imaging in astronomical telescopes, the ionization charge will be detected in the most massive CCDs ever built with exquisite spatial resolution (15 μm x 15 μm pixel). The crucial innovation in these devices is the non-destructive, repetitive measurement of the pixel charge, which results in the high-resolution detection of a single electron and unprecedented sensitivity to light DM (≈ eV energies are enough to free an electron in silicon). By counting individual charges in a detector with extremely low leakage current – a combination unmatched by any other DM experiment – DAMIC-M will take a leap forward of several orders of magnitude in the exploration of the hidden sector, a jump that may be rewarded by serendipitous discovery.","3349563","2018-09-01","2023-08-31"
"DAMOCLES","Simulating Non-Equilibrium Dynamics of Atmospheric Multicomponent Clusters","Hanna Vehkamäki","HELSINGIN YLIOPISTO","Atmospheric aerosol particles play a key role in regulating the climate, and particulate matter is responsible for most of the 7 million deaths per year attributed to air pollution. Lack of understanding of aerosol processes, especially the formation of ice crystals and secondary particles from condensable trace gases, hampers the development of air quality modelling, and remains one of the major uncertainties in predicting climate. 
The purpose of this project is to achieve a comprehensive understanding of atmospheric nanocluster and ice crystal formation based on fundamental physico-chemical principles. We will use a wide palette of theoretical methods including quantum chemistry, reaction kinetics, continuum solvent models, molecular dynamics, Monte Carlo simulations, Markov chain Monte Carlo methods, computational fluid dynamics, cluster kinetic and thermodynamic models. We will study non-equilibrium effects and kinetic barriers in atmospheric clustering, and use these to build cluster distribution models with genuine predictive capacity.
Chemical ionization mass spectrometers can, unlike any other instruments, detect the elemental composition of many of the smallest clusters at ambient low concentrations. However, the charging process and the environment inside the instrument change the composition of the clusters in hitherto unquantifiable ways. We will solve this problem by building an accurate model for the fate of clusters inside mass spectrometers, which will vastly improve the amount and quality of information that can be extracted from mass spectrometric measurements in atmospheric science and elsewhere.
DAMOCLES will produce reliable and consistent models for secondary aerosol and ice particle formation and growth. This will lead to improved predictions of aerosol concentrations and size distributions, leading to improved air quality forecasting, more accurate estimates of aerosol indirect climate forcing and other aerosol-cloud-climate interactions.","2390450","2016-06-01","2021-05-31"
"DAMREG","Pushing the Frontier of Brittlness
Damage Resistant Glasses","Tanguy Gilles Michel Rouxel","UNIVERSITE DE RENNES I","""In order to improve the strength of a glass part (flat display, window, lens, fiber, etc.), most investigations so far were devoted to thermal and chemical surface treatments aimed at generating compressive stresses at the surface. The DAMREG project focuses on the incidence of the glass composition and atomic network structure on the mechanical properties, and specifically on the cracking and fracture behavior, and is based on the experience and expertise of the PI on the structure-property relationships in glass science. This project proposes to address the fundamental issue of glass brittleness in a new paradigm of thinking, questioning the usefulness of the standard fracture toughness parameter, with emphasis on the surface flaw generation process (multiscale approach), and aims at determining novel routes to improve the mechanical performance of glass further promoting innovative applications. DAMREG involves revisiting the fundamental fracture mechanics concepts, the preparation of novel glass compositions, and nanoscale physico-chemical and mechanical characterization. So far most glass fracture studies focused on the crack tip behavior, and were limited to vitreous silica. A crack acts as a lever arm for the stress so that the singular stress at the tip is proportional to the crack length and inversely proportional to the square-root of the tip radius (provided this has a meaning). Since a crack can hardly be cured or shielded at ambient, the presence of a sharp crack is already detrimental. On the contrary to this approach, DAMREG is aimed at understanding the crack initiation process, and the main objective is to define some roadmap to design glasses (composition, thermo-mechanical treatments etc.) with better damage (initiation) resistance.""","1821596","2013-06-01","2018-05-31"
"DARCLIFE","Deep subsurface Archaea: carbon cycle, life strategies, and role in sedimentary ecosystems","Kai-Uwe Hinrichs","UNIVERSITAET BREMEN","Archaea are increasingly recognized as globally abundant organisms that mediate important processes controlling greenhouse gases and nutrients. Our latest work, published in PNAS and Nature, suggests that Archaea dominate the biomass in the subseafloor. Their unique ability to cope with extreme energy starvation appears to be a selecting factor. Marine sediments are of crucial importance to the redox balance and climate of our planet but the regulating role of the deep biosphere remains one of the great puzzles in biogeochemistry. The unique and diverse sedimentary Archaea with no cultured representatives, so-called benthic archaea, are key to understanding this system. Their presumed ability to degrade complex recalcitrant organic residues highlights their relevance for the carbon cycle and as potential targets for biotechnology. I propose to study the role of benthic archaea in the carbon cycle and in the deep biosphere and to explore their life strategies. This task requires an interdisciplinary frontier research approach at the scale of an ERC grant, involving biogeochemistry, earth sciences, and microbiology. Central to my research strategy is the information contained in structural and isotopic properties of membrane lipids from benthic archaea, an area of research spearheaded by my lab. In-depth geochemical examination of their habitat will elucidate processes they mediate. Metagenomic analysis will provide a phylogenetic framework and further insights on metabolism. At the Archaeenzentrum in Regensburg, we will grow model Archaea under a set of environmental conditions and examine the impact on cellular lipid distributions in order to develop the full potential of lipids as proxies for studying nearly inaccessible microbial life. Attempts to enrich benthic archaea from sediments will complement this approach. This frontier research will constrain the role of benthic archaea in the Earth system and examine the fundamental properties of life at minimum energy.","2908590","2010-04-01","2015-03-31"
"DARE","Soil   Foundation   Structure  Systems Beyond  Conventional Seismic    Failure   Thresholds: Application  to  New  or  Existing Structures  and  Monuments","George Gazetas","NATIONAL TECHNICAL UNIVERSITY OF ATHENS - NTUA","The main goal of the proposed research is to investigate the possibility of allowing  below-ground  support systems to respond to strong seismic shaking by going beyond a number of  thresholds  that would conventionally imply failure and are today forbidden by codes. Such  thresholds  include : (a) sliding at the soil-foundation interface ; (b) separation and uplifting of a shallow foundation from the soils ; (c) mobilization of  bearing capacity   failure mechanism for shallow foundations ; (d) structural yielding of pile foundations ; (e) combination of some of the above.  Whereas under static loading conditions a slight  exceedance  of such thresholds leads to failure, the oscillatory nature of seismic shaking will  allow  such exceedances for a short period of time, with perhaps no detrimental or irreparable consequences.  The latter take the form of permanent foundation displacements, rotations, or  injuries , which the designer will aspire to confine within rational limits. The motivation and the need for this research has come from : (i) observations of actual behaviour in a variety of earthquakes ; conspicuous examples : the permanent tilting , overturning, and often survival of numerous buildings on extremely soft soil in Adapazari during the Kocaeli 1999 earthquake ; (ii) the foundation design of a number of critical structures (e.g., major bridge pier, air control tower, tall monuments, elevated water tanks,) against large seismic actions ; the disproportionately large overturning moment and/or base shear force of such slender structures can hardly be faced with today s conventional foundation methods, (iii) the need to seismically retrofit and rehabilitate older structures and historical monuments;  (iv) structural yielding of pile foundations is now detectable (thanks to technological advances), thus eliminating one of the reasons for avoiding it.","2399992","2008-12-01","2013-10-31"
"DARK","Dark Matters","Joseph Ivor Silk","UNIVERSITE PIERRE ET MARIE CURIE - PARIS 6","This interdisciplinary proposal spans theoretical astrophysics and particle physics by addressing  the need to provide astrophysical expertise to the particle astrophysics community in the area of dark matter and dark energy research. A new dialogue will be developed via collaborations involving expertise in  astronomy, statistics and particle physics that centre on fundamental aspects of the nature of the  contents of the universe. Theoretical predictions will be refined to pursue the quest for dark matter using novel experiments designed to detect the direct signatures of dark matter in our galactic halo via  scattering and indirect via annihilations into high energy particles and photons.  Dark matter and dark energy will be studied by cosmic microwave background temperature fluctuations and structure formation constraints. The former  probe is contaminated by inadequately  understood foregrounds that will be examined to extract clues to new physics in the very early universe, an especially timely research frontier in view of the anticipated data from the Planck satellite. The latter is rendered difficult by the highly complex interface of star and galaxy formation. This will be studied by emphasizing development of feedback prescriptions, an ingredient that plays a central role in the current paradigm for galaxy formation and complements ultradeep searches with the new generation of telescopes. The overall goal, namely to leverage via theory on the unprecedented experimental efforts that are underway to address dark sector issues in the emerging field of particle astrophysics, is achievable at relatively modest cost.","2499990","2011-06-01","2017-05-31"
"Dark-OsT","Experimental Searches for Oscillating and Transient effects from the Dark Sector","Dmitry Budker","JOHANNES GUTENBERG-UNIVERSITAT MAINZ","The objective of the proposed project is to pioneer a magnetometry-based experimental framework for the detection of time-varying signatures of the ‘dark sector’. This novel approach will enable systematic searches for particles contributing to the dark matter and for dark-energy components.

The nature of dark matter and that of dark energy are among the central open problems in modern physics. There are only few experimental bounds and so far no conclusive observations of dark-sector particles or fields. Experiments enabling a direct coupling to the dark sector and thus a systematic search for and study of the contributing particles and fields would open up new vistas for areas ranging from particle physics to astrophysics and cosmology, and would in particular provide insights into the physics beyond the Standard Model.

Here, we propose a framework for such experimental searches based on high-precision magnetometers, and networks thereof. Our approach is distinct from existing efforts in two ways. First, it will enable searches for so-far unexplored couplings to ultra-light bosonic particles present in the Universe that could be components of dark matter and/or dark energy, in particular axions and axion-like particles (ALPs). Second, we will develop and use devices and methods tailored to search for oscillating and transient, rather than time-independent, effects. Specifically, we will use nuclear magnetic resonance (NMR) techniques for detecting spin precession caused by background axion and ALP dark matter, and geographically separated magnetometers for identify transient effects, such as crossing domain walls of ALP fields, which have been proposed as a possible dark-energy component.

The devices and methods developed in the framework of this project will provide the essential components for unique searches for a broad class of dark-matter and dark-energy candidates and might enable the key experiments to understanding the dark sector.","2474875","2016-08-01","2021-07-31"
"DARKLIGHT","ILLUMINATING DARK ENERGY WITH THE NEXT GENERATION OF COSMOLOGICAL REDSHIFT SURVEYS","Luigi Guzzo","UNIVERSITA DEGLI STUDI DI MILANO","Galaxy redshift surveys have been central in establishing the current successful cosmological model. Reconstructing the large-scale distribution of galaxies in space and time, they provide us with a unique probe of the basic constituents of the Universe, their evolution and the background fundamental physics. A new generation of even larger surveys is planned for the starting decade, with the aim of solving the remaining mysteries of the standard model using high-precision measurements of galaxy clustering. These entail the nature of the “dark sector” and in particular the origin of the accelerated cosmic expansion. While data accumulation already started, the needed analysis capabilities to reach the required percent levels in both accuracy and precision are not ready yet.

I propose to establish a focused research group to develop these tools and optimally analyze the new data, while being directly involved in their collection. New techniques as redshift-space distortions and well-known but still debated probes as galaxy clusters will be refined to a new level. They will be combined with more standard methods as baryonic acoustic oscillations and external data as CMB anisotropies. Performances will be validated on mock samples from large numerical simulations and then applied to state-of-the-art data with enhanced control over systematic errors to obtain the best achievable measurements.

These new capabilities will be decisive in enabling ongoing and future surveys to tackle the key open problems in cosmology: What is the nature of dark energy? Is it produced by an evolving scalar field? Or does it rather require a modification of the laws of gravity? How does it relate to dark matter? What is the role of neutrinos?  The answer to these questions may well revolutionize our view of physics.","1723600","2012-05-01","2017-10-31"
"DARWIN","Deep mm-Wave RF-CMOS Integrated Circuits","Michel Steyaert","KATHOLIEKE UNIVERSITEIT LEUVEN","Wireless and mobile communication systems have become an important part of our daily environment. Since the introduction of the GSM-network in the early nineties, different wireless applications such as WiFi, Bluetooth, GPS, etc. have been brought into the market. This has become possible due to the high integration of integrated circuits in relatively cheap technologies. Besides the digital signal processing, those wireless applications require complex analog circuits operating at very high frequencies (RF circuits). In the early days these were implemented as discrete components or standalone ICs in expensive technologies such as GaAs, InP and SiGe. Due to the research towards nanometer CMOS technologies, and due to improved RF circuit techniques, RF-CMOS has been introduced since the mid nineties. The intention of this research project is to take the next big leap forward in wireless applications, i.e. the exploration and research, based on the vast RF-CMOS knowledge already existing, towards the Extremely High Frequencies which is above 70 GHz up to 300GHz, with wavelengths close to 1 mm. The research project is a logical evolution of the RF-CMOS research knowledges of the team. For that the &quot;natural evolution&quot; acronym DARWIN (Deep mm-Wave RF CMOS Integrated Circuits (with the M of CMOS inverted (W)) is choosen. Implementing circuit techniques in standard CMOS technologies at those frequencies is again an enormous challenge and will open a lot of new opportunities and applications towards the future due to possibilities in safety monitoring, e.g. collision radar detection for automobiles at 77 GHz, the need for high data-rate telecommunication systems, with capacity of 1-10 Gbps, and imaging for medical and security systems. The goal of the proposed project is to perform the necessary fundamental basic research to be able to implement these 70-300 GHz applications in CMOS technology (45 nm and below).","2042640","2009-01-01","2013-12-31"
"DCENSY","Doping, Charge Transfer and Energy Flow in Hybrid Nanoparticle Systems","Uri Banin","THE HEBREW UNIVERSITY OF JERUSALEM","We target a frontier in nanocrystal science of combining disparate materials into a single hybrid nanosystem. This offers an intriguing route to engineer nanomaterials with multiple functionalities in ways that are not accessible in bulk materials or in molecules. Such control of novel material combinations on a single nanoparticle or in a super-structure of assembled nanoparticles, presents alongside with the synthesis challenges, fundamental questions concerning the physical attributes of nanoscale systems. My goals are to create new highly controlled hybrid nanoparticle systems, focusing on combinations of semiconductors and metals, and to decipher the fundamental principles governing doping in nanoparticles and charge and energy transfer processes among components of the hybrid systems. The research addresses several key challenges: First, in synthesis, combining disparate material components into one hybrid nanoparticle system. Second, in self assembly, organizing a combination of semiconductor (SC) and metal nanoparticle building blocks into hybrid systems with controlled architecture. Third in fundamental physico-chemical questions pertaining to the unique attributes of the hybrid systems, constituting a key component of the research. A first aspect concerns doping of SC nanoparticles with metal atoms. A second aspect concerns light-induced charge transfer between the SC part and metal parts of the hybrid constructs. A third related aspect concerns energy transfer processes between the SC and metal components and the interplay between near-field enhancement and fluorescence quenching effects. Due to the new properties, significant impact on nanocrystal applications in solar energy harvesting, biological tagging, sensing, optics and electropotics is expected.","2499000","2010-06-01","2015-05-31"
"DDD","Diffusive Droplet Dynamics in multicomponent fluid systems","Detlef Lohse","UNIVERSITEIT TWENTE","Liquid-liquid extraction - the transfer of a solute from one solvent to another - is a core process in chemical technology and analysis. The current challenge is to miniaturise the analyte extraction process and to optimize the extraction recovery and preconcentration factor. Lacking a priori calculations, this is now often done by trial-and-error. However, to control and optimize the extraction processes, it is crucial to quantitatively understand the diffusive droplet dynamics in multicomponent fluid systems. This is essential and urgently needed not only for modern liquid-liquid extraction processes for diagnostics & microanalysis, for droplet microfluidics, or in the paint & coating industry, but on larger scales also in remediation industry, in chemical technology, or in food processing. These applications of droplets governed by diffusion include cases of immersed droplets in the bulk & on a surface, single & multicomponent droplets & solvents, and cases with high droplet number density. In spite of their relevance, multiphase & multicomponent fluid systems with relevant diffusive droplet dynamics are poorly understood.

The objective of DDD is a breakthrough: to fill this gap and to come to a quantitative understanding of diffusive droplet dynamics, thus illuminating the fundamental fluid dynamics of diffusive processes of immersed (multicomponent) (surface) droplets on multiple scales. To achieve this objective, we will perform a number of key controlled experiments and numerical simulations for idealized setups on 9 orders of magnitude in length scale, allowing for one-to-one comparison between experiments and numerics/theory. It is now time to bridge the gap from modern fluid dynamics to process-technology, colloidal & interface science, from nano/microscopic and purely diffusively governed droplets to macroscopic ones and from single droplets to multiple & multi-component droplets, to arrive at multiscale high-precision chemical engineering for droplets.","2937500","2017-05-01","2022-04-30"
"DECLIC","Exploring the Decoherence of Light in Cavities","Serge Haroche","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The transition from quantum to classical is an essential issue in physics. At a practical level, quantum information thrives to build large quantum systems for tasks in communication or computing beyond the reach of classical devices. At the fundamental level, the question is whether there exists, in addition to environment-induced decoherence, another mechanism responsible for the disappearance of state superpositions at the macroscopic scale. Harmonic oscillators coupled to qubits are ideal to probe the limits of the quantum domain. Among various versions of this system, microwave Cavity Quantum Electrodynamics coupling Rydberg atoms to superconducting cavities has developed tools of un-matched sensitivity and precision. Building on these advances and on the development of deterministic atomic sources, DECLIC proposes to explore the dynamics of fields trapped in cavities and to study their decoherence under various perspectives. It will implement novel ways to generate non-classical states with large photon numbers stored in one cavity or non-locally split between two. DECLIC will record the gradual evolution of these states towards classicality and locality. Along this way, it will explore promising processes such as quantum random walks and collective photonic effects leading to non-classical interferometry breaking the standard quantum limit. Beyond witnessing decoherence, DECLIC will investigate ways to manipulate and control it, either by implementing feedback procedures steering the field towards targeted states, or by engineering artificial environments protecting against decoherence specific states of light. These experiments will provide invaluable clues for the understanding of other oscillator-qubit systems exploring the quantum to classical boundary.","2500000","2010-02-01","2016-01-31"
"DECODA","Tensor Decomposition for Data Analysis with Applications to Health and Environment","Pierre Comon","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""Multi-Way factor Analysis (MWA) is attracting growing interest in many disciplines of engineering, as described in this proposal. Because the applications are much more numerous than those that serve as focus for this project, the tools developed in the framework of the project will have major impact. MWA is probably the simplest extension of the well-known (linear) Factor Analysis. However, despite its extremely wide panel of applications and its apparently simple expression, it still, surprisingly, lacks theoretical background. The reason is that this identification problem disguises challenges of unexpected magnitude. In fact, several tensor problems still remain open for several decades, and the difficulties should not be overlooked. Yet, the lack of identifiability results (existence, uniqueness) prevents the design of efficient numerical algorithms.
The first objective is to address these theoretical problems and develop appropriate identification algorithms. Multilinear models underlying MWA are shown to be closely related to tensor algebra and multivariate polynomials, so that tools can be borrowed from Algebraic Geometry, with the goal of developing theoretical solutions and numerical algorithms. The second objective is to apply these solutions to practical problems in various realms of application. In particular, it centers on creating modified models, better matched to analysis of health (e.g., EEG) or environmental data (e.g., water resources, microbial ecosystems), to analyzing their identifiability, and to developing corresponding identification algorithms. The final goal is to design a device able to detect and track suspect or toxic molecules in river or tap waters.
This very challenging research proposal is positioned in close collaboration with specialists in the above-mentioned application fields, and is at the core of European population health.""","1500000","2013-09-01","2018-08-31"
"DEEP","Deep Earth Elastic Properties and a Universal Pressure Scale","Daniel Frost","UNIVERSITAET BAYREUTH","Knowledge of the physical and chemical state of the Earth s inner silicate mantle is central to our understanding of plate tectonics, mantle convection, magma generation and the composition of the Earth as a whole. The key to this knowledge is the ability to interpret studies of seismic wave velocities through the deep Earth using laboratory measurements of mineral sound velocities at high pressures and temperatures. Scientists have for many years measured these properties as a function of pressure but due to the experimental difficulties the majority of studies have been performed only at room temperature. Large extrapolations of these data to mantle temperatures are required to link seismic velocity observations with physical and chemical properties of mantle rocks, resulting in large uncertainties that obscure firm conclusions.  An additional uncertainty arises because there is currently no primary scale for accurately measuring pressure at high temperatures. In this study mineral sound velocities and densities will be measured in the diamond anvil cell at simultaneous high pressures and high temperatures to at least 50 GPa and 1300K. This will be possible by a pioneering combination of Brillouin scattering spectroscopy, to measure sound velocities, and single crystal X-ray diffraction determinations of density. By making both types of measurements on the same sample while it is maintained at a constant pressure and temperature, the pressure can be independently measured. These absolute pressure determinations will be used to derive a new universal pressure scale for use at high temperatures. Sound velocities of the major mantle minerals will be determined at high temperatures and absolute pressures thereby drastically decreasing the uncertainties in velocity calculations for rock assemblages at deep mantle conditions. The resulting data will be employed to finally interpret a host of seismic observations made at both global and local scales.","2079888","2009-02-01","2014-07-31"
"DeepInternal","Going Deep and Blind with Internal Statistics","Michal IRANI","WEIZMANN INSTITUTE OF SCIENCE LTD","Unsupervised visual inference can often be performed by exploiting the internal redundancy inside a single visual datum (an image or a video). The strong repetition of patches inside a single image/video provides a powerful data-specific prior for solving a variety of vision tasks in a “blind” manner: (i) Blind in the sense that sophisticated unsupervised inferences can be made with no prior examples or training; (ii) Blind in the sense that complex ill-posed Inverse-Problems can be solved, even when the forward degradation is unknown.

While the above fully unsupervised approach achieved impressive results, it relies on internal data alone, hence cannot enjoy the “wisdom of the crowd” which Deep-Learning (DL) so wisely extracts from external collections of images, yielding state-of-the-art (SOTA) results.  Nevertheless, DL requires huge amounts of training data, which restricts its applicability. Moreover, some internal image-specific information, which is clearly visible, remains unexploited by today's DL methods. One such example is shown in Fig.1.

We propose to combine the power of these two complementary approaches – unsupervised Internal Data Recurrence, with Deep Learning, to obtain the best of both worlds. If successful, this will have several important outcomes including:
• A wide range of low-level & high-level inferences (image & video).
• A continuum between Internal & External training – a platform to explore theoretical and practical tradeoffs between amount of available training data and optimal Internal-vs-External training.
• Enable totally unsupervised DL when no training data are available.
• Enable supervised DL with modest amounts of training data.
• New applications, disciplines and domains, which are enabled by the unified approach.
• A platform for substantial progress in video analysis (which has been lagging behind so far due to the strong reliance on exhaustive supervised training data).","2466940","2018-05-01","2023-04-30"
"deepSLice","Deciphering the greenhouse gas record in deepest ice using continuous sublimation extraction / laser spectrometry","Hubertus Fischer","UNIVERSITAET BERN","The recent anthropogenic global warming makes a detailed understanding of coupling processes between climate and biogeochemical cycles of pressing importance. The atmospheric archive of air bubbles enclosed in polar ice cores provides the only direct record of greenhouse gas changes in the past, and the key to understanding the related changes in biogeochemical cycles and climate/greenhouse gas feedbacks. 

Crucial questions about greenhouse gas variability on very short (decadal) and very long (orbital) time scales still remain open. To answer these questions, the ice core community has proposed new drilling projects with the goal of nearly doubling the time span of the available ice core record to the last 1.5 million years and of covering the entire Holocene greenhouse gas record in unprecedented decadal resolution. These goals have one thing in common: due to glacier flow most of this record will only be found in a very thin layer in the bottom-most ice of the cores. Completely new analytical approaches are needed to unlock the atmospheric archive in this ice in order to gain high-resolution, high-precision measurements, while at the same time drastically reducing sample consumption compared to established techniques.

The deepSLice project will make such a step change in ice core analytics by developing a novel coupled Continuous Sublimation Extraction-Quantum Cascade Laser Spectrometer system. It will allow us to simultaneously measure CO2, CH4 and N2O concentrations as well as the isotopic composition of CO2 on air samples of only 1-2 ml at standard pressure and temperature, reducing the required sample size by one order of magnitude. This non-destructive analysis will make it also possible for the complete air sample to be recollected after analysis and used for other measurements. This method will be applied to existing and new ice cores in order to study past changes in greenhouse gases and the underlying biogeochemical cycles in unparalleled detail.","2255788","2015-10-01","2020-09-30"
"DELPHI","Deterministic Logical Photon-Photon Interactions","Philippe Grangier","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The main objective of this proposal is to design and implement a novel scheme for efficient, deterministic, lossless photon-photon interactions, and to exploit it to achieve logical processing and quantum measurements on optical light beams. For that purpose, we will create, study and exploit a new transparent medium, based on the transient excitation of Rydberg polaritons, where the optical non-linearities are so large that they can act at the single photon level. These techniques will be applied to perform quantum measurements and manipulations of light beams. This will include the deterministic generation of single photons and optical Schrödinger's cat states, the implementation of quantum non-demolition (QND) measurements for the photon number and the parity operators, and the demonstration of controlled-phase and controlled-not quantum gates. These operations will be implemented in the optical domain, where they can be combined with efficient propagation in free space or in optical fibers, and with high efficiency detectors already available, in order to open an avenue towards a fully deterministic quantum engineering of light.","2496000","2010-01-01","2014-12-31"
"DEMOVE","DECODING THE NEURAL CODE OF HUMAN MOVEMENTS FOR A NEW GENERATION OF MAN-MACHINE INTERFACES","Dario Farina","UNIVERSITAETSMEDIZIN GOETTINGEN - GEORG-AUGUST-UNIVERSITAET GOETTINGEN - STIFTUNG OEFFENTLICHEN RECHTS","The generation of a movement is the combination of discrete events (action potentials) generated in the brain, spinal cord, nerves, and muscles. These discrete events are the result of ion exchanges across membranes, electrochemical mechanisms, and active ion pumping through energy expenditure. The ensemble of spike trains discharged in the various parts of the neuromuscular system constitutes the neural code for movements. Recording and interpretation of this code provides the means for decoding the motor system. The main limitation in the investigation of the motor system is the current impossibility of detecting and processing in the intact human, during natural movements, the activity of a sufficiently large number of motor neurons and sensory afferents (neural code) to associate a functional meaning to the cellular mechanisms that ultimately determine a movement. This limitation in turn impedes to answer to many fundamental questions on the control of human movements. These questions have tremendous implications in the development of man-machine interface systems. In this project, we propose the development of advanced electrode systems for in-vivo electrophysiological recordings from nerves and muscles in humans and new computational methods/models for extracting functionally significant information on human movement from these recordings. The highly innovative focus is that of providing the link between the cellular mechanisms and the behavior of the whole motor system in the intact human, i.e. to build the bridge between the neural and functional understanding of movement. On the basis of these new technologies, we aim at answering open questions in movement neuroscience and using novel principles for man-machine interaction. Specific applications in man-machine interaction will be related to neurorehabilitation technologies, such as functional electrical stimulation, myoelectric and peripheral neural prostheses.","2431473","2011-07-01","2016-06-30"
"DEPENDENTCLASSES","Model theory and its applications: dependent classes","Saharon Shelah","THE HEBREW UNIVERSITY OF JERUSALEM","Model theory deals with general classes of structures (called models).
Specific examples of such classes are: the class of rings or the class of
algebraically closed fields.

It turns out that counting the so-called complete types over models in the
class has an important role in the development of model theory in general and
stability theory in particular.
Stable classes are those with relatively few complete types (over structures
from the class); understanding stable classes has been central in model theory
and its applications.

Recently, I have proved a new dichotomy among the unstable classes:
Instead of counting all the complete types, they are counted up to conjugacy.
Classes which have few types up to conjugacy are proved to be so-called
``dependent'' classes (which have also been called NIP classes).
I  have developed (under reasonable restrictions) a ``recounting theorem'',
parallel to the basic theorems of stability theory.

I have started to develop some of the basic properties of this new approach.
The goal of the current project is to develop systematically the theory of
dependent classes. The above mentioned results give strong indication that this
new theory can be eventually as useful as the (by now the classical) stability
theory. In particular, it covers many well known classes which stability theory
cannot treat.","1748000","2014-03-01","2019-02-28"
"DG-PESP-CS","Deterministic Generation of Polarization Entangled single Photons Cluster States","David Gershoni","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","Measurement based quantum computing is one of the most fault-tolerant architectures proposed for quantum information processing. It opens the possibility of performing quantum computing tasks using linear optical systems. An efficient route for measurement based quantum computing utilizes highly entangled states of photons, called cluster states. Propagation and processing quantum information is made possible this way using only single qubit measurements. It is highly resilient to qubit losses. In addition, single qubit measurements of polarization qubits is easily performed with high fidelity using standard optical tools. These features make photonic clusters excellent platforms for quantum information processing.
Constructing photonic cluster states, however, is a formidable challenge, attracting vast amounts of research efforts. While in principle it is possible to build up cluster states using interferometry, such a method is of a probabilistic nature and entails a large overhead of resources. The use of entangled photon pairs reduces this overhead by a small factor only.
We outline a novel route for constructing a deterministic source of photonic cluster states using a device based on semiconductor quantum dot. Our proposal follows a suggestion by Lindner and Rudolph. We use repeated optical excitations of a long lived coherent spin confined in a single semiconductor quantum dot and demonstrate for the first time practical realization of their proposal. Our preliminary demonstration presents a breakthrough in quantum technology since deterministic source of photonic cluster, reduces the resources needed quantum information processing. It may have revolutionary prospects for technological applications as well as to our fundamental understanding of quantum systems.
We propose to capitalize on this recent breakthrough and concentrate on R&D which will further advance this forefront field of science and technology by utilizing the horizons that it opens.","2502974","2016-06-01","2021-05-31"
"DIADEM","Domain-centric Intelligent Automated Data Extraction Methodology","Georg Gottlob","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","This proposal is in the area of automated web data extraction and web data management. The aim of our project is to provide the logical, methodological, and algorithmic foundations for the knowledge-based extraction of structured data from web sites belonging to specific domains, such as estate agents, restaurants, travel agencies, car dealers, and so on. One core part of this will be a comprehensive multi-dimensional logical data model that will be used to simultaneously represent both the content of a large website, its structure, inferred user-interaction patterns and all meta-information and knowledge (factual and rule-based) that is necessary to automatically perform the desired extraction tasks. I envision that, based on these new foundations, we will be able to build extremely powerful systems that autonomously explore websites of a given domain, understand their structure and extract and output richly structured data in formats such as XML or RDF. We aim at systems that take as input a URL of a website in a given domain, automatically explore this site and deliver as output a structured data set containing all the relevant information present on that site. As an example, imagine a system specialized in the real-estate domain, that receives as input the URL of any real-estate agent, explores the site automatically and outputs richly structured records of all properties that are currently advertised for sale or for rent on the many web pages of this site. We plan to develop and implement at least two such systems for two different domains, including the one mentioned. The breakthrough in automatic data extraction that we are striving for would enable a quantum leap for two interrelated technologies which are the hottest next topics in web search: vertical search, that is, web search in specialized domains, and object search, that is, the search for web data objects rather than web pages.","2402846","2010-04-01","2015-03-31"
"DIFFERENTIALGEOMETR","Geometric analysis, complex geometry and gauge theory","Simon Kirwan Donaldson","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","The proposal is for work in Geometric Analysis aimed at two different problems. One is to establish necessary and sufficient conditions for the existence of extremal metrics on complex algebraic manifolds. These metrics are characterised by conditions on their curvature tensor a paradigm being the Riemannian version of the Einstein equation of General Relativity The standard conjecture is that the right condition should be the stability of the manifold, a condition defined entirely in the language of algebraic geometry. But there are very few cases where this conjecture has been verified. The problem comes down to proving the existence of a solution to highly nonlinear partial differential equation. The aim is to advance this theory by a detailed study of interesting but more amenable cases, for example where there is a large symmetry group. The second problem is to develop new invariants and structures associated to a particular class of manifolds of dimension 6 and 7 (with holonomy SU(3) and G2). These would be derived from the solutions of versions of the Yang-Mills equation over the manifolds, in a similar manner to familiar theories in 3 and 4 dimensions. In higher dimensions there are fundamental new difficulties to overcome to set up a theory rigorously and the main point of this part of the proposal is to attack these. It is likely that the new structures, if they do exist, will have interesting connections to other developments in this general area, involving string theory and algebraic geometry.","1501361","2010-04-01","2015-03-31"
"DIGISMART","Multifunctional Digital Materials Platform for Smart Integrated Applications","Elvira Fortunato","UNIVERSIDADE NOVA DE LISBOA","DIGISMART creates new avenues into two main areas: 1) processing nanomaterials/nanostructures applied to electronic devices by exploring a new digital multifunctional direct laser writing (LDW) method for in situ synthesis of small-sized nanomaterials/nanofilms micro-patterned growth by selective photothermal decomposition of semiconductors, dielectrics and conductors precursors and 2) provide simultaneously multifunction to single based metal oxide devices (like thin film transistors, the workhorses for large area electronics having electron, charge and color modulation), as the basic unit to promote systems’ integration by exploring the use of new advanced materials with unique multi-functionalities using low cost process solutions. 
This new fabrication process will be very useful for low-cost, eco-friendly, and efficient fabrication of nanostructures and thin films-integrated microelectronic devices due to its low-power, simple setup as well as excellent reliability. This new and disruptive concept will be achieved with low cost and non-toxic materials (new metal oxides, MO semiconductors, conductors, dielectrics and electrochromics free of In and Ga) associated to a low cost process multifunctional platform technology (ALL-IN-ONE TOOL) well supported by high-resolution nano-characterization techniques. With DIGISMART new and unexplored materials will be produced as well as to boost the original properties of conventional materials in order to contribute to the needs for low cost and flexible electronics. If we succeed to embed some level of intelligence in every object, this would change electronics and it would change society, ranging from embedded window displays to a wide range of biomedical electronics, just to mention a few and this is what the Internet of Things is looking for.","3495250","2019-01-01","2023-12-31"
"DiluteParaWater","Long-Lived Nuclear Magnetization in Dilute Para-Water","Geoffrey Bodenhausen","ECOLE NORMALE SUPERIEURE","The magnetization of hydrogen nuclei in H2O constitutes the basis of most applications of magnetic resonance imaging (MRI.) Only ortho-water, where the two proton spins are in states that are symmetric with respect to permutation, features NMR-allowed transitions. Para-water is analogous to para-hydrogen, where the two proton spins are anti-symmetric with respect to permutation. The objective of this proposal is to render para-H2O accessible to observation. Several strategies will be developed for its preparation and observation in solids, liquids and gas phase, with yields up to 33%. When diluted in acetonitrile at room temperature, we found that Tortho(H2O) = 6 s. Based on experiments on H2C groups where Tpara/Tortho > 37, we conservatively estimate that Tpara/Tortho > 10 for H2O, so that we expect Tpara = 60 s. Dilution in aprotic solvents inhibits the exchange of protons and extends the lifetimes t(H2O) of water molecules from ca. 1 ms in pure water to 10 s and beyond, so that proton exchange does not hamper the use para-water. The ratio Tpara/Tortho of H2O depends on temperature, viscosity, paramagnetic agents, etc., which affect intra- and inter-molecular dipole-dipole interactions, chemical shift anisotropy, and spin rotation. In cases where proton exchange significantly shortens the lifetime of para-H2O, we shall prepare and observe para-ethanol and aqueous solutions of para-glycine, which cannot suffer from proton exchange, and allow similar perspectives as para-water. In conventional MRI, contrast stems mostly from spatial variations of T1 and T2. By monitoring the ratio Tpara/Tortho as a function of spatial coordinates, it will be possible to obtain a novel type of contrast. In suitable phantoms and porous media, para-water will allow us to characterize slow transport phenomena such as flow, diffusion, and electrophoretic mobility. The study of transport phenomena will become possible over longer time intervals, lower velocities or greater distances.","2500000","2014-01-01","2018-12-31"
"DIME","Disequilibirum metamorphism of stressed lithosphere","Bjørn Jamtveit","UNIVERSITETET I OSLO","Most changes in mineralogy, density, and rheology of the Earth’s lithosphere take place by metamorphism, whereby rocks evolve through interactions between minerals and fluids. These changes are coupled with a large range of geodynamic processes and they have first order effects on the global geochemical cycles of a large number of elements.
In the presence of fluids, metamorphic reactions are fast compared to tectonically induced changes in pressure and temperature. Hence, during fluid-producing metamorphism, rocks evolve through near-equilibrium states.  However, much of the Earth’s lower and middle crust, and a significant fraction of the upper mantle do not contain free fluids. These parts of the lithosphere exist in a metastable state and are mechanically strong. When subject to changing temperature and pressure conditions at plate boundaries or elsewhere, these rocks do not react until exposed to externally derived fluids. 
Metamorphism of such rocks consumes fluids, and takes place far from equilibrium through a complex coupling between fluid migration, chemical reactions, and deformation processes. This disequilibrium metamorphism is characterized by fast reaction rates, release of large amounts of energy in the form of heat and work, and a strong coupling to far-field tectonic stress. 
Our overarching goal is to provide the first quantitative physics-based model of disequilibrium metamorphism that properly connects fluid-rock interactions at the micro and nano-meter scale to lithosphere scale stresses. This model will include quantification of the forces required to squeeze fluids out of grain-grain contacts for geologically relevant materials (Objective 1), a new experimentally based model describing how the progress of volatilization reactions depends on tectonic stress (Objective 2), and testing of this model by analyzing the kinetics of a natural serpentinization process through the Oman Ophiolite Drilling Project (Objective 3).","2900000","2015-09-01","2021-08-31"
"DIOLS","Long chain diols as novel organic proxies for paleoclimate reconstructions","Stefan Schouten","STICHTING NIOZ, KONINKLIJK NEDERLANDS INSTITUUT VOOR ONDERZOEK DER ZEE","""Accurate reconstructions of past climate changes are essential to understand e.g. the sensitivity of Earth’s climate to global increases in atmospheric greenhouse gasses such as CO2. For these reconstructions it is vital to have proxies which are well constrained and are able to provide robust quantitative estimates. However, it has become clear that currently used proxies are sometimes associated with large uncertainties and thus more proxies are needed in order to perform reliable paleoclimate reconstructions.

In my group we are developing new proxies based on so-called long chain diols. These compounds are synthesized by several groups of algae and occur abundantly in present day oceans as well as ancient sediments. Initial results have shown that one set of compounds, the 1,14-diols, can be used to reconstruct past primary productivity and upwelling conditions. Excitingly, the distribution of another set of compounds, the long chain 1,13- and 1,15-diols, show a strong relationship with sea surface temperature and can be used to reconstruct past sea surface temperatures in several parts of the oceans. Finally, culture experiments indicate that the stable carbon isotopic composition of diatoms producing 1,14-diols is strongly related to CO2 concentrations, raising the possibility that it may be used to reconstruct ancient pCO2 levels.

In this ERC proposal I want to develop, test and apply these exciting new proxies in order to provide robust and accurate reconstructions of past oceans. To this end this ERC project is subdivided in several different subprojects, each designed to investigate long chain diol proxies but from a different perspective, in particular cultivation and molecular biology, organic geochemistry, (paleo)limnology and paleoceanography. The combination of these subprojects will result in a highly multidisciplinary project needed to make progress in the development of these unique proxies.""","2499982","2014-03-01","2019-02-28"
"DIOPHANTINE PROBLEMS","Integral and Algebraic Points on Varieties, Diophantine Problems on Number Fields and Function Fields","Umberto Zannier","SCUOLA NORMALE SUPERIORE","Diophantine problems have always been a central topic in Number Theory, and have shown deep links with other basic mathematical topics, like Algebraic and Complex Geometry. Our research plan focuses on some issues in this realm, which are strictly interrelated. In the last years the PI and collaborators obtained several results on integral and algebraic points on varieties, which have inspired much subsequent research by others, and which we plan to develop further. In particular:
We plan a further study of integral points on varieties, and applications to Algebraic Dynamics, a possibility which has emerged recently.
We plan to study further the so-called `Unlikely intersections'. This theme contains celebrated issues like the Manin-Mumford conjecture. After work of the PI with Bombieri and Masser in the last 10 years, it has been the object of much recent work and also of new conjectures by R. Pink and B. Zilber. Here a new method has recently emerged in work of the PI with Masser and Pila, which also leads (as shown by Pila) to signi_cant new cases of the Andr_e-Oort conjecture. We intend to pursue in this kind of investigation, exploring further the range of the methods.
Finally, we plan further study of topics of Diophantine Approximation and Hilbert Irreducibility, connected with the above ones in the contents and in the methodology.","928500","2011-02-01","2016-01-31"
"DIPOLAR ROTOR ARRAY","Regular Arrays of Artificial Surface-Mounted Dipolar Molecular Rotors","Josef Michl","USTAV ORGANICKE CHEMIE A BIOCHEMIE, AV CR, V.V.I.","We propose a feasibility demonstration of an unprecedented concept: preparation of regular two-dimensional arrays of artificial surface-mounted dipolar molecular rotors and control of their coherent motion by the application of an outside electric field.  The proposal involves a highly interdisciplinary endeavor, which requires experience in synthesis (preparation of molecular rotors), surface chemistry (assembly of rotors into arrays on surfaces), surface spectroscopy and scanning microscopy (characterization of rotor arrays on surfaces), and theory (modeling of rotor dynamics).  The principal investigator is presently actively working and publishing in all of these subdisciplines.","2457600","2009-02-01","2014-07-31"
"DISCONV","DISCRETE AND CONVEX GEOMETRY: CHALLENGES, METHODS, APPLICATIONS","Imre Barany","MAGYAR TUDOMANYOS AKADEMIA RENYI ALFRED MATEMATIKAI KUTATOINTEZET","Title: Discrete and convex geometry: challenges, methods, applications
Abstract: Research in discrete and convex geometry, using tools from combinatorics, algebraic
topology, probability theory, number theory, and algebra, with applications in theoretical
computer science, integer programming, and operations research. Algorithmic aspects are
emphasized and often serve as motivation or simply dictate the questions. The proposed
problems can be grouped into three main areas: (1) Geometric transversal, selection, and
incidence problems, including algorithmic complexity of Tverberg's theorem, weak
epsilon-nets, the k-set problem, and algebraic approaches to the Erdos unit distance problem.
(2) Topological methods and questions, in particular topological Tverberg-type theorems,
algorithmic complexity of the existence of equivariant maps, mass partition problems, and the
generalized HeX lemma for the k-coloured d-dimensional grid. (3) Lattice polytopes and random
polytopes, including Arnold's question on the number of convex lattice polytopes, limit
shapes of lattice polytopes in dimension 3 and higher, comparison of random polytopes and
lattice polytopes, the integer convex hull and its randomized version.","1298012","2011-04-01","2017-03-31"
"DISCRETECONT","From discrete to contimuous: understanding discrete structures through continuous approximation","László Lovász","EOTVOS LORAND TUDOMANYEGYETEM","Important methods and results in discrete mathematics arise from the interaction between discrete mathematics and ``continuous'' areas like analysis or geometry. Classical examples of this include topological methods, linear and semidefinite optimization generating functions and more. More recent areas stressing this connection are the theory of limit objects of growing sequences of finite structures (graphs, hypergraphs, sequences), differential equations on networks, geometric representations of graphs. Perhaps most promising is the study of limits of growing graph and hypergraph sequences. In resent work by the Proposer and his collaborators, this area has found highly nontrivial connections with extremal graph theory, the theory of property testing in computer science, to additive number theory, the theory of random graphs, and measure theory as well as geometric representations of graphs. This proposal's goal is to explore these interactions, with the participation of a number of researchers from different areas of mathematics.","739671","2009-01-01","2014-06-30"
"DISCSIM","Hydrodynamical simulations of protoplanetary discs in the era of ALMA imaging","Catherine Clarke","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","""This is a proposal for an ambitious programme of state of the art hydrodynamical simulations, designed to answer some key questions about the role of disc self-gravity in planet formation. The programme is also designed so as to maximize the synergy with  the new observational constraints from high resolution imaging data from the Atacama Large Millimetre Array that will become  available  over the  timescale of the grant. The five year programme should provide definitive answers about whether planet formation is able to get going during the early, self-gravitating phase of disc evolution and how, if so, it would affect the further evolution of the disc. The topic of gravitational disc fragmentation as a route to planet formation is currently in a state of crisis, with recent simulations undermining what had become a consensus view on the subject. A dedicated and carefully constructed approach, as detailed here, is required to solve this problem.""","1892844","2014-02-01","2019-01-31"
"Disorder Control","Tuning Disorder in Chalcogenides 
to realize Advanced Functional Devices","Matthias Wuttig","RHEINISCH-WESTFAELISCHE TECHNISCHE HOCHSCHULE AACHEN","Better performance of future computers and communication equipment requires substantially higher speeds of switching devices at lower energy consumption. Those requirements can only be achieved by substantial improvement of the transport properties of the materials employed. The transport of charge and heat is strongly influenced by disorder. In recent years we have found a unique class of crystalline materials which combines an exceptionally high, yet tuneable degree of disorder with remarkable transport properties. This class includes the best phase change materials, superconductors with an unconventional coupling mechanism, good thermoelectrics, as well as known topological insulators. For these different phenomena disorder is either very beneficial or – if unconditioned - rather detrimental. Hence we need to be able to control disorder in these materials to tailor their properties.
Exploring this concept requires the ability to understand, eliminate or harness the effects of disorder. Recently we have demonstrated an Anderson-type transition from insulating to metallic behaviour upon annealing. However, to fully utilize these ideas it is mandatory to realize devices with a more directly controllable degree of disorder. Within the framework of this project, we will develop a tuneable Anderson insulator to delocalize charge carriers. This allows us to address a) the transition from an insulator to a metal, the impact of disorder on superconductors (b) and topological insulators (c) and finally d) the ability to control thermoelectric properties by tuneable electronic disorder. From the results to be obtained we expect consequences for a wide range of materials listed in our “treasure map”, with promising new technological applications in various devices.","2186000","2014-03-01","2019-02-28"
"DISQUA","Disorder physics with ultracold quantum gases","Massimo Inguscio","LABORATORIO EUROPEO DI SPETTROSCOPIE NON LINEARI","Disorder is ubiquitous in nature and has a strong impact on the behaviour of many physical systems. The most celebrated effect of disorder is Anderson localization of single particles, but many other more complex phenomena arise in interacting, many-body systems. A full understanding of how disorder affects the behavior of quantum systems is still missing, also because of the unavoidable presence of nonlinearities, dissipation and thermal effects that make a careful exploration of real condensed-matter systems very difficult. In this project we want to fully exploit the unprecedented potentialities offered by ultracold atomic quantum gases to explore some of the present challenges for our understanding of the physics of disorder. These systems offer indeed the possibility of controlling to a great extent crucial parameters such as the type of disorder, the nonlinearities due to interactions, the temperature and density, the dimensionality, the quantum statistics. A variety of advanced diagnostic techniques allow to gain detailed information on the static and dynamic properties of the system. The potentialities of atomic quantum gases for the study of disorder have already showed up in recent breakthrough experiments. The project aims at an experimental exploration, supported by advanced theory, of the current issues in disordered quantum systems. We will investigate a few frontier themes of general interest: 1) Anderson localization and the interplay of disorder and a weak interaction; 2) strongly correlated, disordered bosonic systems; 3) disordered, interacting fermionic systems. In the research we will employ atomic Bose and Fermi gases with tunable interactions and advanced diagnostic techniques that we have recently contributed to develop. A successful completion of the project will push forward our understanding of the behaviour of quantum systems with disorder, with a potentially large impact on many fields of physics.","2500000","2010-03-01","2015-02-28"
"DM","Dirac Materials","Alexander Balatsky","KUNGLIGA TEKNISKA HOEGSKOLAN","""The elegant Dirac equation, describing the linear dispersion (energy/momentum) relation of electrons at relativistic speeds, has profound consequences such as the prediction of antiparticles, reflection less tunneling (Klein paradox)  and others. Recent discovery of graphene and topological insulators (TI) highlights the scientific importance and technological promise of materials with “relativistic Dirac dispersion"""" of electrons for functional materials and device applications with novel functionalities. One might use term ‘Dirac materials’  to encompass a subset of (materials) systems in which the low energy phase space for fermion excitations is reduced compared to conventional band structure predictions (i.e. point or lines of nodes vs. full Fermi Surface).

Dirac materials are characterized by universal low energy properties due to presence of the nodal excitations.  It is this reduction of phase space due to additional symmetries that can be turned on and off that opens a new door to functionality of Dirac materials.

We propose to use the sensitivity of nodes in the electron spectrum of Dirac materials to induce controlled modifications of the Dirac points/lines via band structure engineering in artificial structures and via inelastic scattering processes with controlled doping. Proposed research will expand our theoretical understanding and guide design of materials and engineered geometries   that allow tunable energy profiles of Dirac carriers.""","1700000","2013-04-01","2018-03-31"
"DMIDAS","Astrophysical constraints on the identity of the dark matter","Carlos Silvestre FRENK","UNIVERSITY OF DURHAM","The identity of the dark matter is a fundamental problem in Physics whose solution will have major implications for cosmology, astronomy and particle physics. There is compelling evidence that the dark matter consists of elementary particles created shortly after the Big Bang, but searches for them in the laboratory and from astrophysical sources have proved inconclusive. The currently favoured candidate is cold dark matter or CDM. This forms the basis of the standard model of cosmology, LCDM, whose predictions, dating back to the 1980s, turned out to agree remarkably well with observations covering a staggering range of epochs and scales, from the temperature structure of the cosmic microwave background radiation to the large-scale pattern of galaxy clustering. Yet, this agreement is not exclusive to CDM: models based on other types of particles -- warm, self-interacting or asymmetric, for example -- agree equally well with these data but differ on scales smaller than individual bright galaxies. These are the scales targeted in this application in which we propose a comprehensive investigation of small-scale structure, with the aim of testing dark matter candidates, by focusing on three key astrophysical diagnostics: strong gravitational lensing, dwarf galaxies and stellar halos. We propose a joint theoretical and observational programme exploiting three major developments: SWIFT, a new code developed at Durham that will enable cosmological hydrodynamics simulations an order of magnitude larger than is possible today; SuperBIT, an innovative balloon-borne wide-field imaging telescope that will collect gravitational lensing data for hundreds of galaxy clusters; and DESI, a spectro-photometric survey that will acquire 10 times more spectra of stars in the Milky Way than previous surveys. The particle models that we will consider have predictive power and are disprovable. Our programme has the potential to rule out many dark matter particle candidates, including CDM.","2493439","2018-10-01","2023-09-30"
"DMMCA","Discrete Mathematics: methods, challenges and applications","Noga Alon","TEL AVIV UNIVERSITY","Discrete Mathematics is a fundamental mathematical discipline as well as an essential component of many mathematical areas, and its study has experienced an impressive growth in recent years. Some of the main reasons for this growth are the broad applications of tools and techniques from extremal and probabilistic combinatorics in the rapid development of theoretical Computer Science, in the spectacular recent results in Additive Number Theory and in the study of basic questions in Information Theory. While in the past many of the basic combinatorial results were obtained mainly by ingenuity and detailed reasoning, the modern theory has grown out of this early stage, and often relies on deep, well developed tools, like the probabilistic method, algebraic, topological and geometric techniques. The work of the principal investigator, partly jointly with several collaborators and students, and partly in individual efforts, has played a significant role in the introduction of powerful algebraic, probabilistic, spectral and geometric techniques that influenced the development of modern combinatorics. In the present project he aims to try and further develop such tools, trying to tackle some basic open problems in Combinatorics, as well as significant questions in Additive Combinatorics, Information Theory, and theoretical Computer Science. Progress on the problems mentioned in this proposal, and the study of related ones, is expected to provide new insights on these problems and to lead to the development of novel fruitful techniques that are likely to be useful in Discrete Mathematics as well as in related areas.","1061300","2008-12-01","2013-11-30"
"DNA MACHINES","Nanomachines based on interlocked DNA architectures","Michael Famulok","RHEINISCHE FRIEDRICH-WILHELMS-UNIVERSITAT BONN","DNA-nanotechnology has created different topologies, including replicable ones, nanomachines, patterns, logic gates, or algorithmic assemblies. Interlocked double-stranded (ds) DNA-architectures like catenanes or rotaxanes, wherein individual components can be set in motion in a controlled manner have not been accessible. These molecules represent long-sought devices for nanorobotics and nanomechanics because they possess a unique mechanical bonding motif, not available to conventional building blocks. The project will apply an unprecedented, simple, and modular interlocking paradigm for double-stranded (ds) circular DNA geometries that we have developed in preliminary studies. This will now be taken several crucial steps forward by generating unconventional DNA-, protein-, aptamer-, and ribozyme hybrid architectures containing interlocked structures wherein the motion of individual components can be controlled in many different ways. We will design, construct, and evaluate switchable autonomous DNA-nanomachines that function as rotational motors, muscles, or switches for powering and manipulating nanoscale components. The DNA machines envisaged in this project will be applied, for example, in synthetic supramolecular self-assembly systems that emulate complex biological machines like motor proteins, nucleic acid polymerases, or ATPases. In addition, they will be developed for multiple purposes in biosensing, logic-gate- and memory circuit assembly, and catalysis. This efficient method for constructing interlocked dsDNA nanostructures opens the exciting possibility of conjoining the area of lifesciences with that of nanomechanical engineering, paving entirely new avenues for nanotechnology. The project is highly interdisciplinary and will open a new field with enormous innovative potential and implications ranging from chemistry to synthetic biology, and from the life sciences to nano-engineering.","2499522","2011-03-01","2016-02-29"
"DOC","The Dawn of Organic Chemistry","Cecilia CECCARELLI","UNIVERSITE GRENOBLE ALPES","Terrestrial life is based on organic chemistry, on the complex combination of relatively small molecules containing less than 50 atoms of carbon and other elements in smaller quantities. Some of these bricks, notably amino acids, are found in meteoritic and cometary material, a fact (among others) which led the Nobel laureate C. de Duve to conclude that “the seeds of life are universal” and “life is an obligatory manifestation of matter, written into the fabric of the Universe”.
The objective of the DOC project is to understand the dawn of organic chemistry, namely the start of organic chemistry in systems similar to the progenitor of the Solar System, with the ultimate goal to understand how organic chemistry builds up and evolves in these systems and, consequently, to understand how universal the chemical seeds of life are.
To achieve this objective, I propose to build a reliable theory for the organic chemistry in nascent Solar type systems, by combining in a tightly coordinated way new ground-breaking astronomical observations, quantum chemistry computations, astrochemical/chemi-physical models and sophisticated analysis tools. The DOC project is based on (i) a mine of first-class data from already awarded Large Programs at IRAM and from a plethora of smaller proposals at IRAM, ALMA and APEX, (ii) new state-of-the-art quantum chemistry computations to understand astrochemistry reactions at the molecular level, and (iii) models and tools to fully exploit the new data and computations.
My ambition is to provide a reliable theory not only for the astrochemical and the star and planet formation communities, but also for the extragalactic one. Indeed, the new highly sensitive spectral observations from facilities like IRAM, ALMA and, in the future, SKA will inevitably contain lines from many organic molecules. DOC ambition is to ultimately allow us to understand how organic chemistry unfolds in the Universe.","2454368","2017-10-01","2022-09-30"
"DOMINOCAT","Asymmetric Organodomino Catalysis","Dieter Enders","RHEINISCH-WESTFAELISCHE TECHNISCHE HOCHSCHULE AACHEN","""Nature is able to carry out numerous parallel reactions transforming countless chemical compounds in a single cell. For the stereoselective synthesis of complex, biologically relevant molecules, such as pharmaceuticals and agrochemicals, chemists both in academia and industry typically still use “stop and go” procedures with all its disadvantages, such as the isolation and purification of intermediates after each step causing a lot of waste and high costs.

Since around the turn of the millennium, the research area of organocatalysis has grown with a breathtaking speed and can now be seen as a third pillar of catalysis beside bio- and metal catalysis. Very recently, organocatalytic multi-component domino reactions came into the focus of synthetic chemistry by mimicking Nature’s enzymatic cascades.

Based on our pioneering contributions in this field of asymmetric organodomino catalysis we aim at developing novel simple, triple, quadruple and even more complex organocascade protocols as well as sequential one-pot-procedures thereby significantly advancing the strategic arsenal for the synthesis of complex biologically active enantiopure compounds bearing multiple stereocenters. Spectroscopic and kinetic studies, especially high resolution ESI-MS measurements which opens an insight into a running domino reaction and accompanying theoretical calculations (in cooperation) will allow a deep mechanistic understanding. The combination of organocatalysis with metal, especially gold catalysis will pave the way for completely new environmentally benign and “green” enantioselective multi-component one-pot techniques scalable for future industrial applications.""","2435600","2013-02-01","2018-01-31"
"DOPPLER","Domain-optimised parallelisation by polymorphic language embeddings and rewritings","Martin Maria Anton Nikolaus Odersky","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Concurrent and parallel programming are becoming indispensable for
exploiting modern hardware. Because possible speed increases of single
processors have reached their limit, increasing transistor count will
yield more, but not necessarily faster cores, and this for the
foreseeable future. This means that, from now on, parallelism in
software will have to double every 18 months to keep up with
hardware. This problem has been identified as the ``Popular Parallel
Programming'' grand challenge by the computer architecture community.

The proposed project will research new ways to solve this challenge.
We start with a set of domain-specific languages which naturally admit
a high degree of parallelism. The domain specific languages are
integrated in a common host language using polymorphic language
embeddings. Such embeddings provide a high degree of abstraction,
which gives considerable freedom in their actual representation and
implementation. The new direction taken by this proposal is to combine
polymorphic embeddings with optimizing rewritings in a staged
compilation process. This can lead to highly parallel and efficient
implementations on a variety of heterogeneous hardware.","2392400","2011-04-01","2016-03-31"
"DQSIM","Discrete Quantum Simulator","Dieter Meschede","RHEINISCHE FRIEDRICH-WILHELMS-UNIVERSITAT BONN","We propose to build a two-dimensional (2D) discrete quantum simulator based on ensembles of ultracold neutral atoms. In this system all degrees of freedom will be controlled at the quantum limit: the number and positions of the atoms, as well as their internal (qubit) and vibrational states. The dynamics is implemented by discrete steps of spin-dependent transport combined with controlled cold collisions of the atoms.

Although numerous theoretical studies have considered this architecture as the most promising route to quantum simulation, it has not yet been realized experimentally in all essential aspects.

This simulator allows us to study dynamical properties of single-particle and many-body systems in engineered 2D environments. In single particle discrete systems, also known as quantum walks, we plan to investigate transport properties connected to graphene-like Dirac points, and localization phenomena associated with disorder. In the many-particle setting we will realize 2D cluster states as needed for measurement-based quantum computation, as well as simple quantum cellular automata.","2575573","2012-04-01","2017-03-31"
"DrEAM","Directed Evolution of Artificial Metalloenzymes for In Vivo Applications","Thomas WARD","UNIVERSITAT BASEL","In the past decade, artificial metalloenzymes (AMs) have emerged as an attractive alternative to the more traditional enzymes and homogeneous catalysts. Such hybrid catalysts result from the incorporation of an abiotic metal cofactor within a macromolecule (protein or oligonucleotide). Artificial metalloenzymes combine attractive features of both homogeneous catalysts and enzymes, including the possibility to genetically optimize the catalytic performance of new-to-nature organometallic reactions. Can artificial metalloenzymes become as catalytically efficient as naturally-evolved metalloenzymes, even in complex biological mixtures? Herein, we outline our efforts to address this challenge by localizing and evolving AMs within the periplasm of Escherichia coli. 
To achieve this objective, we will exploit AMs based on the biotin-streptavidin technology. Four subprojects have been tailored to address the challenges: i) knock-out deleterious components present in the periplasm; ii) improve the cofactor uptake through the outer-membrane; iii) engineer streptavidin to boost the AM’s performance; and iv) rely both on screening and selection strategies to evolve AMs in vivo. Relying on auxotrophs, we will demonstrate the potential of AMs to complement metabolic pathways. Only E. coli auxotrophs containing an evolved AM capable of producing the vital aminoacid-precursor will survive the stringent selection pressure. We have identified several selectable aminoacid precursors which can be produced by metathesis (indole, precursor of tryptophan), enone reduction (keto valine, precursor of valine) and allylic substitution (prephenate, precursor of tyrosine and phenylalanine). In a Darwinian evolution spirit, we anticipate that applying selection pressure will allow to evolve AMs to unprecedented catalytic performance.
The main deliverable of the DrEAM is an engineered and evolvable E. coli strain capable of performing in vivo reaction cascades combining AMs and natural enzymes.","2490700","2016-10-01","2021-09-30"
"DREAMS","Development of a Research Environment for Advanced Modelling of Soft matter","Vincenzo Barone","SCUOLA NORMALE SUPERIORE","""DREAMS aims at developing an integrated theoretical-computational approach for the efficient description of linear and non-linear spectroscopies of several classes of organic probes, dispersed in polymeric matrices that range in complexity from simple polyolefins all the way to large biomolecules (proteins and polysaccharides).
In order to reach this objective, developments along the following lines are required: (i) elaboration of new theoretical models, to expand the scope of currently available treatments; (ii) definition of specific treatments for intermediate regions / regimes in the context of space- and time-multiscale descriptions; (iii) algorithmic implementation of the developed models / protocols in computational codes and, (iv) their efficient integration allowing for seamless flow of information and easy use by non-specialists.
A crucial asset for the success of the planned theoretical-computational developments is represented by an extensive network of solid collaborations with leading experimental groups, that will be involved in the synthesis and characterization of the different chromophore / matrix systems, as well as in the in-depth characterization of their spectroscopic responses. These interactions will thus allow for a stringent and exhaustive validation of the capabilities required of a general and versatile computational tool; at the same time, the experimental groups will make full use of advanced theoretical interpretations in the context of a real-world technological problem.
In summary, DREAMS relies on a carefully planned combination of theoretical developments, computational implementations, and interactions with experimentalists, in order to achieve a novel and cutting-edge result, namely to provide the scientific community with a set of computational tools that will make possible the simulation and prediction of response and spectroscopic properties of multi-component materials.""","2152600","2013-02-01","2018-01-31"
"DRIVEN","Field driven materials for functions, dissipation, and mimicking Pavlovian adaptation","Olli Ikkala","AALTO KORKEAKOULUSAATIO SR","During the recent years, biological materials have extensively inspired materials scientists towards new properties, e.g., for composites, photonics, and wetting. The future grand challenge is to mimic biological active materials towards new properties that commonly have not been connected with man-made materials. Due to the biological complexity, conceptually new approaches are needed in materials science. In the project DRIVEN, field-driven dissipative out-of-equilibrium self-assemblies are developed in the colloidal and molecular scale. In the proposal, instead of using chemical fuels to drive dissipative self-assemblies, which is ubiquitous in Nature, imposed fields are here used to drive the system out-of-equilibrium towards new assemblies and functions.  The project show steps with growing risks towards highly ambitious new materials mimicking aspects from active biological materials.","2499999","2017-10-01","2022-09-30"
"DropletControl","Controlling the orientation of molecules inside liquid helium nanodroplets","Henrik Stapelfeldt","AARHUS UNIVERSITET","In this project I will develop and exploit experimental methods, based on short and intense laser pulses, to control the spatial orientation of molecules dissolved in liquid helium nanodroplets. This idea is, so far, completely unexplored but it has the potential to open a multitude of new opportunities in physics and chemistry. The main objectives are:

1) Complete control and real time monitoring of molecular rotation inside liquid helium droplets, exploring superfluidity of the droplets, the possible formation of quantum vortices, and rotational dephasing due to interaction of the dissolved molecules with the He solvent.

2) Ultrafast imaging of molecules undergoing chemical reaction dynamics inside liquid helium droplets, exploring rapid energy dissipation from reacting molecules to the helium solvent, transition between mirror forms of chiral molecules, strong laser field processes in He-solvated molecules, and structure determination of non crystalizable proteins by electron or x-ray diffraction.

I will achieve the objectives by combining liquid helium droplet technology, ultrafast laser pulse methods and advanced electron and ion imaging detection. The experiments will both rely on existing apparatus in my laboratories and on new vacuum and laser equipment to be set up during the project.

The ability to control how molecules are turned in space is of fundamental importance because interactions of molecules with other molecules, atoms or radiation depend on their spatial orientation. For isolated molecules in the gas phase laser based methods, developed over the past 12 years, now enable very refined and precise control over the spatial orientation of molecules. By contrast, orientational control of molecules in solution has not been demonstrated despite the potential of being able to do so is enormous, notably because most chemistry occurs in a solvent rather than in a gas of isolated molecules.","2409773","2013-05-01","2018-04-30"
"DuaLL","Duality in Formal Languages and Logic - a unifying approach to complexity and semantics","Mai Gehrke","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Dualities between algebraic and topological structure are pervasive in mathematics, and toggling back and forth between them has often been associated with important breakthroughs. The main objective of this project is to bring this important tool to bear on a number of subjects in theoretical computer science thereby advancing, systematising, and unifying them.

One subject of focus is the search for robust extensions of the theory of regular languages. A powerful technical tool for classifying regular languages and proving decidability results is  Eilenberg-Reiterman theory, which assigns classes of finite monoids or single profinite algebras to classes of languages. Recent results by the PI and her co-authors show that the theory may be seen as a special case of Stone duality for Boolean algebras with operators. We want to:
- Develop an Eilenberg-Reiterman theory beyond regular languages with the goal of obtaining new tools and separation results for Boolean circuit classes, an active area in the search for lower bounds in complexity theory.
-Systematise and advance the search for robust generalisations of regularity to other structures such as infinite words, finite and infinite trees, cost functions, and words with data.

The second subject of focus is the development of duality theoretic methods for logics with categorical semantics. We want to approach the problem incrementally:
-  View duality for categorical semantics through a spectrum of intermediate cases going from regular languages over varying alphabets, Ghilardi-Zawadowski duality for finitely presented Heyting algebras, and the Bodirsky-Pinsker topological Birkhoff theorem to Makkai's, Awodey and Forssell's, and Coumans' recent work on first-order logic duality, thus unifying topics in semantics and formal languages.

Our main tools come from Stone duality in various forms including the Jonsson-Tarski canonical extensions and profinite algebra, and from universal algebra and category theory.","2348938","2015-09-01","2020-08-31"
"DUPLEX","Programmable Plastics","Christopher Alexander Hunter","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The unique properties of nucleic acids have made them the material of choice for complex nanofabrication. High fidelity formation of duplexes via non-covalent interactions between complementary sequences provides a straightforward approach to molecular programming of multicomponent self-assembly processes. The structure of the nucleic acid backbone and bases can be changed without destroying these properties, suggesting that there are all kinds of unexplored polymeric structures that will also show sequence selective duplex formation. This proposal investigates this rich new area at the interface of supramolecular, biological and polymer chemistry. The appeal of nucleic acids is that we can dial up any desired sequence via chemical solid phase synthesis or via biological template synthesis. With recent advances in polymerisation processes, which proceed under mild conditions compatible with non-covalent chemistry, we are now in a position to develop comparable processes for synthetic polymers. This proposal explores a ground-breaking approach to the synthesis of polymeric systems equipped with defined sequences of recognition sites. The aim is to establish protocols for routine solid phase synthesis of one class of oligomer, which can be used to template the synthesis of different classes of oligomer. This template chemistry will provide tools for polymerisation of conventional monomers using templates to determine the sequence of recognition sites and hence incorporate the selective recognition properties of nucleic acids into bulk polymers like polystyrene. The ability to program polymers with recognition information will open the way to new materials of unprecedented complexity and functionality with applications in all areas of nanotechnology where precise control over macromolecular structure and supramolecular organisation will be used to program mechanical, photochemical and electronic properties into sophisticated assemblies that rival biology.","2457947","2013-03-01","2019-02-28"
"DUSTYGAL","The formation of massive galaxies: the roles of dust-obscured starbursts and AGN activity","Ian Robert Smail","UNIVERSITY OF DURHAM","I propose an integrated programme to determine the role of dust-obscured starburst activity and AGN growth in the formation and evolution of  galaxies.  This programme will exploit three, new cutting-edge observational facilities: the SCUBA-2 submillimetre camera, which will provide the first panoramic surveys of luminous, but highly obscured, sources out to the highest redshifts;  the Atacama Large Millimeter Array (ALMA), which will provide sub-kpc imaging of the distribution of dust and gas within these sources to understand the physics of their activity;  the e-MERLIN radio telescope, which will map the distribution of star formation and AGN activity at sub-kpc scales in these systems.  I lead major international surveys on all three facilities and I propose to develop and combine these projects to provide a single focused programme to understand the processes which trigger obscured activity at high redshifts and their role in determining the properties of galaxies at the present day.","2050123","2013-01-01","2018-12-31"
"DYCON","Dynamic Control and Numerics of Partial Differential Equations","Enrique Zuazua","FUNDACION DEUSTO","This project aims at making a breakthrough contribution in the broad area of Control of Partial Differential Equations (PDE) and their numerical approximation methods by addressing key unsolved issues appearing systematically in real-life applications. 
To this end, we pursue three objectives: 1) to contribute with new key theoretical methods and results, 2) to develop the corresponding numerical tools, and 3) to build up new computational software, the DYCON-COMP computational platform, thereby bridging the gap to applications.
The field of PDEs, together with numerical approximation and simulation methods and control theory, have evolved significantly in the last decades in a cross-fertilization process, to address the challenging demands of industrial and cross-disciplinary applications such as, for instance, the management of natural resources, meteorology, aeronautics, oil industry, biomedicine, human and animal collective behaviour, etc. Despite these efforts, some of the key issues still remain unsolved, either because of a lack of analytical understanding, of the absence of efficient numerical solvers, or of a combination of both. 
This project identifies and focuses on six key topics that play a central role in most of the processes arising in applications, but which are still poorly understood: control of parameter dependent problems; long time horizon control; control under constraints; inverse design of time-irreversible models; memory models and hybrid PDE/ODE models, and finite versus infinite-dimensional dynamical systems.
These topics cannot be handled by superposing the state of the art in the various disciplines, due to the unexpected interactive phenomena that may emerge, for instance, in the fine numerical approximation of control problems. The coordinated and focused effort that we aim at developing is timely and much needed in order to solve these issues and bridge the gap from modelling to control, computer simulations and applications.","2065125","2016-10-01","2021-09-30"
"DYNACEUTICS","Remote control healing: Next generation mechano-nano-therapeutics","Alicia El Haj","THE UNIVERSITY OF BIRMINGHAM","Imagine if doctors could heal patients via remote control. Following simple injections into regions of the body, they could activate internal cells by an external bandage. In this way, they could remotely control the ways tissue heal. This Advanced grant sets out to understand, design and develop the mechano-nano-magnetic platform that will underpin this therapeutic strategy for the future – DYNACEUTICS. 
Key receptors have been identified such as ion channels, integrins and growth factors which respond to mechanical cues on the membrane and activate downstream pathways. How do we ‘bottle’ an agonist like a drug which can influence or regulate mechano-sensors on the membrane and can be controlled remotely? This project tackles this complex interdisciplinary question through breakthrough nanotechnologies. We aim to expand and develop a platform technology using magnetic particle tagging which will allow us to direct cells for therapeutic purposes. 
Specifically, we aim 
• to identify mechano-receptor binding sites on stem and mature cells which will enable remote activation of signalling pathways via magnetic fields, 
• to design and test magnetic particles with tailored tagging strategies using single cell through 3D human organoid models to in vivo disease models, 
• to tailor and design external remote control devices  
• to create clinically relevant treatment modalities for remote control healing. 
This proposal presents a unique opportunity to launch a new dynamic treatment platform, DYNACEUTICS, which we propose will extend the therapeutic horizon and provide a new form of remote controlled healing.","2499068","2019-01-01","2023-12-31"
"DYNALLO","Towards a Dynamical Understanding of Allostery","Peter Hamm","UNIVERSITAT ZURICH","Allostery is a fundamental concept Nature uses to regulate the affinity of a certain substrate to an active site of a protein by binding a ligand to a distant allosteric site. We will design experimental tools to gain an atomistic understanding of the conformational transitions that give rise to allostery. We will approach the problem from two distinctively different directions. First, we will initiate conformational transitions of proteins that per se are not photoswitchable, by cross-linking two sites of an allosteric protein with a photo-switchable azobenzene-moiety to initiate a conformational transition similar to ligand binding. We will use ultrafast infrared spectroscopy to time-resolve the conformational transition. Second, we will experimentally verify a frequently expressed hypothesis that allosteric and active site communicate by exchange of vibrational energy. To that end, we will design a versatile approach that allows us to locally deposit vibrational energy at essentially any site in a protein (e.g. through pumping of an optical chromophore that undergoes ultrafast internal conversion), and to detect its appearance at any other site by using vibrational transitions as local thermometers. Thereby, we will map out a network of connectivity in a given protein. Both approaches will applied both to one and the same protein family. One concrete example are PDZ domains, which are among the smallest allosteric proteins, and for which the connection between allostery and vibrational energy flow has been made explicit, based on computer simulations. We will eventually test this hypothesis experimentally, and provide the foundation for a description of allostery that is on an equal footing as our current understanding of protein folding.","2400000","2010-02-01","2015-07-31"
"DYNAMIN","Dynamic Control of Mineralisation","Fiona Meldrum","UNIVERSITY OF LEEDS","This project will take inspiration from biomineralisation to achieve exceptional, dynamic control over crystallisation processes.  

Understanding the fundamental mechanisms which govern crystallisation promises the ability to inhibit or promote crystallisation as desired, and to tailor the properties of crystalline materials towards a huge range of applications. Biomineralisation provides a perfect precedent for this approach, where organisms achieve control currently unparalleled in synthetic systems. This is achieved because mineralisation occurs within controlled environments in which an organism can interact with the nascent mineral.

Thanks to recent advances in microfabrication techniques and analytical methods we finally have the tools required to bring such control to the laboratory. DYNAMIN will exploit microfluidic and confined systems to study and interact with crystallisation processes with outstanding spatial and temporal resolution. Flowing droplet devices will be coupled to synchrotron techniques to investigate and control nucleation, using soluble additives and nucleating particles to direct the crystallisation pathway. Static chambers will be used to interact with crystallisation processes over longer length and time scales to achieve spatio-temporal control to rival that in biomineralisation, while a unique confined system – titania nanotubes – will enable the study and control of organic-mediated mineralisation, using fresh reagents and proteinases to interact with the process. Finally, a key biogenic strategy will provide the inspiration to develop a simple and potentially general method to trigger and control the transformation of amorphous precursor phases to single crystal products.

This will generate a new framework for studying and controlling crystallisation processes, where these new skills will find applications in sectors ranging from the Chemical Industry, to Healthcare, Advanced Materials, Formulated Products and the Environment.","2632375","2018-09-01","2023-08-31"
"DYNAMINT","Dynamics of Probed, Pulsed, Quenched and Driven Integrable Quantum Systems","Jean-Sébastien CAUX","UNIVERSITEIT VAN AMSTERDAM","This proposal intends to develop and apply a new-generation theoretical toolbox for understanding the rich dynamics of strongly-interacting many-body quantum sytems subjected to destabilizing manipulations bringing them very far from equilibrium. 

In atomic systems, condensed matter and nanophysics settings, quantum matter is nowadays routinely pushed beyond the traditional low-energy/linear response/thermal equilibrium paradigms. Some experiments even clearly highlight the need to revise basic fundamental quantum statistical mechanics notions such as ergodicity, relaxation and thermalization in order to explain their behaviour. Theory must thus urgently revise its textbooks and develop new interpretations and capabilities for offering concrete, quantitative phenomenology. 

This proposal is focused on a set of systems at the very center of this strongly-correlated, experimentally realizable far-from-equilibrium spectacle: integrable models of quantum spin chains, interacting gases confined to one spatial dimension, and quantum dots. Building up on recent theoretical breakthroughs in dynamical correlations and post-quench steady states, this proposal aims to shed a new light on the fundamental principles at the heart of many-body quantum dynamics. It will implement a broad and ambitious research agenda consisting of synergetic projects from mathematically formal thought experiments all the way to phenomenologically applied practical calculations. The types of protocols to be studied include probes creating high-energy excitations, pulses inducing changes beyond linear response, quenches causing sudden global reorganizations, all the way to drivings completely metamorphozing the physical states. 

The result will be to provide reliable, experimentally relevant and urgently-needed theoretical `anchoring points' in our general understanding of the physics underlying far-from-equilibrium strongly-interacting quantum matter.","2444446","2017-09-01","2022-08-31"
"DYNAMO","Dynamical processes in open quantum systems: pushing the frontiers of theoretical spectroscopy","Angel Secades Rubio","UNIVERSIDAD DEL PAIS VASCO/ EUSKAL HERRIKO UNIBERTSITATEA","""Scope """"Energy Materials. In  this project we develop new concepts for building a novel theoretical framework (the ab-initio non-equilibrium dynamical modelling tool”) for understanding, identifying, and quantifying the different contributions to energy harvesting and storage as well as describing transport mechanisms in natural light harvesting complexes, photovoltaic materials, fluorescent proteins and artificial (nanostructured) devices by means of theories of open quantum systems, non-equilibrium processes and electronic structure. We address cutting-edge  applications along three major scientific challenges: i) characterize matter out of equilibrium, ii) control material processes at the electronic level and tailor material properties, iii) master energy and information on the nanoscale. The long-term goal is developing a set of theoretical tools for the quantitative prediction of energy transfer phenomena in real systems.
We will provide answers to the following questions: What are the design principles from the environment-assisted quantum transport in photosynthetic organisms that can be transferred to nanostructured materials such as organic photovoltaic materials and biomimetic materials? What are the fundamental limits of excitonic transport properties such as exciton diffusion lengths and recombination rates? What is the role of quantum coherence in the energy transport in photosynthetic complexes and photovoltaic materials?  What is the role of spatial confinement in water and proton transfer through porous membranes (nano-capillarity)?
The ground-breaking nature of the project lies in being the first systematic development and application of the theories of open quantum systems   and quantum optimal control within an ab-initio framework (time-dependent-density functional theory). The project will open new methodological, applicative and theoretical horizons of research.""","1877497","2011-04-01","2016-03-31"
"DYNAMOX","Charge carrier dynamics in metal oxides","Majed CHERGUI","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Transition metal (TM) oxides (TiO2, ZnO, NiO) are large gap insulators that have emerged as highly attractive materials over the past two decades for applications in photocatalysis, solar energy conversion, etc., all of which rely on the generation of charge carriers, their evolution and their eventual trapping at defects or a self-trapped excitons. Despite the huge interest for such materials, the very nature of the elementary electronic excitations (Frenkel, Wannier or charge transfer exciton) is still not established, nor is the way these excitations evolve after being created: excitonic polaron or charged polaron. Finally, the electron and hole recombine is also not clearly established because of issue of defects and trapping. 
In order to tackle these issues, here we implement novel experimental tools that would provide us with hitherto inaccessible information about the charge carrier dynamics in TM oxides. Of importance is the ability to detect both the electrons and the holes. Some of these tools have been developed in the PI’s group: i) Ultrafast X-ray absorption spectroscopy (XAS) will provide information about the final metal d-orbitals and about the structural changes around it; ii) Ultrafast X-ray emission (XES) will provide information about hole states. While these two approaches are ideal element-selective ones, the localization of the electron at metal atoms represents a small proportion of the electron population. Therefore, ultrafast Angle-resolved photoemission spectroscopy (ARPES) will be used to map out the band structure changes in the system and the evolution of the conduction band electrons. Ultrafast 2-dimensional (2D) UV (<400nm) transient absorption spectroscopy allows the mapping of the time evolution of both the valence and the conduction bands by its ability to pump and probe above the band gap. Last, Fourier Transform visible 2D spectroscopy will allow the probing of gap state dynamics at high time resolution.","2482305","2016-10-01","2021-09-30"
"DYNAPORE","Dynamic responsive porous crystals","Matthew ROSSEINSKY","THE UNIVERSITY OF LIVERPOOL","The project addresses the long-term vision of man-made materials with chemical selectivity and functional efficiency produced by dynamic structural flexibility. These materials are not intended as protein mimics; they are however inspired by nature’s use of flexible rather than rigid systems, with their ability to dynamically restructure around guests and thus perform highly specific chemistry. Such materials would transform chemical processes through their precision, for example by reorganising to accelerate each step of a cascade reaction without reagent or product inhibition. The road to this vision is blocked as we do not have the methodology and understanding to control such materials.
The aim is to develop synergic, multidisciplinary experimental and computational capability to harness the dynamics of flexible crystalline porous solids for function, demonstrated in separation and catalysis. This will enable design and synthesis of materials that controllably adopt distinct structures according to their chemical environment to optimise performance. We will create a new workflow that integrates understanding of the structure-composition-dynamics-property relationship into the materials design and discovery process. This workflow builds on proof-of-concept in (i) chemical control of dynamical restructuring in flexible crystalline porous materials and in the use of dynamics to (ii) enhance function and (iii) guide synthesis.
Crystalline flexible porous materials are selected because crystallinity maximises the atomic-scale understanding generated, which is transferable to other materials classes, whilst porosity permits sorption and organisation of guests that controls function.
This inorganic materials chemistry project develops integrated capability in chemical synthesis (new metal-organic frameworks and linkers), computation (prediction and evaluation of structure and dynamical guest response), characterisation (e.g. by diffraction) and measurement of function.","2493425","2016-10-01","2021-09-30"
"e-Cat","The Electron as a Catalyst","Armido STUDER","WESTFAELISCHE WILHELMS-UNIVERSITAET MUENSTER","Is the electron a catalyst in synthesis? This fundamental question will be addressed within the frame of the suggested ERC-project. Brönsted acid catalysis is well established in organic synthesis. The electron, as compared to the proton about 1800 times smaller and omnipresent, is currently not recognized as a potential catalyst in synthesis. The concept of using the electron as a catalyst is nearly unexplored. In the suggested, challenging project this kind of catalysis and its potential in synthesis will be the target of the investigations. The aim is to establish catalysis with the electron as an independent research branch in organic synthesis. To this end, the generality and broad applicability of the concept has to be documented. Different reactions, which are currently conducted as non-chain reactions by using transition metals as redox catalysts, will be performed via electron-catalyzed radical chain processes. In view of the foreseen shortage of transition metals we consider the development of transition-metal-free chemistry as important. Guided by Mother Nature we plan to develop synthetic dehydrogenases. Unactivated aliphatic sites in complex substrates will be selectively oxidized to the corresponding alkenes. Remote regioselective C-H functionalization in complex molecules comprising C-C- and C-X-bond formation will be investigated and also transition-metal-free radical arene and alkene C-H functionalization will be explored. Furthermore, the potential of electron-catalysis in asymmetric synthesis will be elucidated. Preparative and kinetic experimental studies will be supported by theoretical chemistry, new methods for initiation of electron-catalyzed processes will be developed and also mechanistic studies will be performed.","2490000","2016-10-01","2021-09-30"
"E-DESIGN","Artificial designer materials","Peter LILJEROTH","AALTO KORKEAKOULUSAATIO SR","Constructing designer materials where the atomic geometry, interactions, magnetism and other relevant parameters can be precisely controlled is becoming reality. I will reach this aim by positioning every atom with the tip of a scanning probe microscope, or by using molecular self-assembly to reach the desired structures. I will realize and engineer several novel quantum materials hosting exotic electronic phases: 2D topological insulators in metal-organic frameworks (MOF) and 2D topological superconductors in hybrid molecule-superconductor structures. These classes of materials have not yet been experimentally realized but could enable novel spintronic and quantum computing devices. In addition, we will realize a tuneable platform for quantum simulation in solid-state artificial lattices, which could open a whole new area in this field.

I will employ a broad experimental approach to reach the above targets by utilizing molecular self-assembly and scanning probe microscopy -based atom/molecule manipulation. The systems are characterized using low-temperature atomic force microscopy (AFM) and scanning tunneling microscopy (STM). My group is one of the leading groups in these topics globally. We have initial results on the topics discussed in this proposal and are thus in a unique position to make ground-breaking contributions in realizing designer quantum materials.

The artificial designer materials we study are characterized by the engineered electronic response with atomically precise geometries, lattice symmetries and controlled interactions. Such ingredients can result in ultimately controllable materials that have large, robust and quick responses to small stimuli with applications in nanoelectronics, flexible electronics, high-selectivity and high-sensitivity sensors, and optoelectronic components. Longer term, the biggest impact is expected through a profound change in the way we view materials and what can be achieved through designer materials approach.","2374922","2018-09-01","2023-08-31"
"E-DUALITY","Exploring Duality for Future Data-driven Modelling","Johan SUYKENS","KATHOLIEKE UNIVERSITEIT LEUVEN","Future data-driven modelling is increasingly challenging for many systems due to higher complexity levels, such as in energy systems, environmental and climate modelling, traffic and transport, industrial processes, health, safety, and others. This requires powerful concepts and frameworks that enable the design of high quality predictive models. In this proposal E-DUALITY we will explore and engineer the potential of duality principles for future data-driven modelling. An existing example illustrating the important role of duality in this context is support vector machines, which possess primal and dual model representations, in terms of feature maps and kernels, respectively. Within this project, besides using existing notions of duality that are relevant for data-driven modelling (e.g. Lagrange duality, Legendre-Fenchel duality, Monge-Kantorovich duality), we will also explore new ones. Duality principles will be employed for obtaining a generically applicable framework with unifying insights, handling different system complexity levels, optimal model representations and designing efficient algorithms. This will require taking an integrative approach across different research fields. The new framework should be able to include e.g. multi-view and multiple function learning, multiplex and multilayer networks, tensor models, multi-scale and deep architectures as particular instances and to combine several of such characteristics, in addition to simple basic schemes. It will include both parametric and kernel-based approaches for tasks as regression, classification, clustering, dimensionality reduction, outlier detection and dynamical systems modelling. Higher risk elements are the search for new standard forms in modelling systems with different complexity levels, matching models and representations to system characteristics, and developing algorithms for large scale applications within this powerful new framework.","2492500","2018-10-01","2023-09-30"
"E-SWARM","Engineering Swarm Intelligence Systems","Marco Dorigo","UNIVERSITE LIBRE DE BRUXELLES","Swarm intelligence is the discipline that deals with natural and artificial systems composed of many individuals that coordinate using decentralized control and self-organization. In this project, we focus on the design and implementation of artificial swarm intelligence systems for the solution of complex problems. Our current understanding of how to use swarms of artificial agents largely relies on rules of thumb and intuition based on the experience of individual researchers. This is not sufficient for us to design swarm intelligence systems at the level of complexity required by many real-world applications, or to accurately predict the behavior of the systems we design. The goal of the E-SWARM is to develop a rigorous engineering methodology for the design and implementation of artificial swarm intelligence systems. We believe that in the future, swarm intelligence will be an important tool for researchers and engineers interested in solving certain classes of complex problems. To build the foundations of this discipline and to develop an appropriate methodology, we will proceed in parallel both at an abstract level and by tackling a number of challenging problems in selected research domains. The research domains we have chosen are optimization, robotics, networks, and data mining.","2016000","2010-06-01","2015-05-31"
"EARLY","Early phases of galaxy evolution","Olivier Claude Jacques Le Fevre","UNIVERSITE D'AIX MARSEILLE","This project is aimed to support a comprehensive survey of the early phases of galaxy evolution to better understand how galaxies formed and evolved. The goal is to focus on the redshift range 2.5<z<7, based on a new ultra-deep spectroscopic survey just approved by ESO, the largest ‘Large Program’ ever on the VLT with 648 hours allocated, for which I am the PI.  In a ground-breaking effort, this Legacy survey will assemble 12,000 galaxies in total with VLT-VIMOS, over 1deg² in 3 fields (COSMOS, ECDFS, VVDS-02), to study the formation and evolution of galaxies at this key epoch. It will increase by one order of magnitude the total number of spectral measurements ever obtained in this redshift range, including >300 new galaxies with z>5, considerably increasing our knowledge of the galaxy population at these epochs.
This large sample of galaxies will enable a number of detailed studies,  most importantly: (i) the evolution of the global star formation rate and the build-up of the mass for different galaxy populations, (ii) the study of very young galaxies and search for the long sought PopIII stellar populations, (iii) the evolution of the morphology of galaxies as they are building up, (iv)  the clustering of galaxies to infer the mass growth of underlying dark matter halos, (v) the contribution of mergers to the mass growth of galaxies .
The support from the ERC will be used to conduct efficient exploitation of this leading survey. The survey observations and data processing will be performed with new generation methods. The data will be secured and accessed via an open database. Dedicated analysis tools will be developed to firmly establish the properties of galaxies at these epochs. This will lead to the elaboration of a comprehensive scenario for early galaxy evolution.
In supporting this project the ERC will help maintain European leadership in this field, developing expertise for the coming new infrastructure at ESO and ESA.","2478615","2011-04-01","2016-03-31"
"EARLYHUMANIMPACT","How long have human activities been affecting the climate system?","Carlo Barbante","UNIVERSITA CA' FOSCARI VENEZIA","Human activities are altering the global climate system at rates faster than ever recorded in geologic time. Ample observational evidence exists for anthropogenic climate change including measured increased in atmospheric CO2, temperature and sea level rise. Biomass burning causes CO2 emissions of ~50% of those from fossil-fuel combustion and so are highly likely to influence future climate change. However, aerosols continue to be the least understood aspect of the modern climate system and even less is known about their past influence. Anthropogenic aerosols may have altered the global climate system for thousands of years as suggested by comparing late-Holocene greenhouse-gas concentrations to those from previous interglacials. The decrease in the spatial extent of forests beginning ~8000 yrs BP may be related to early agricultural activity including forest clearance through burning which should leave a quantifiable signal in climate proxies.
We pioneered a ground-breaking technique for determining a specific molecular marker of biomass burning (levoglucosan) which can record past fire in ice cores and lake sediments. The research incorporates continuous ice and lake core climate records from seven continents with parallel histories of fire regime. These can provide essential insight into the interplay between climate and human activity, especially with the advent of agriculture. Key objectives include:
1) How does biomass burning change through time and space?
2) How do climate parameters respond to or correlate with changes in biomass burning?
3) Did fires increase ~8000  and/or  ~5000 years ago?
4) Can natural and anthropogenic fires be differentiated? If so, how do fires and associated climate change ascribed to human activity differ from natural biomass burning?","2370767","2011-07-01","2016-12-31"
"EARTHGROWTH","The construction of Planet Earth","Bernard John Wood","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The first 150 M.yrs of Earth evolution were the most dramatic in the history of the planet, setting the scene for the development of plate tectonics, the Earth’s magnetic field and the origin of life. By the end of this period Earth had separated into core, mantle, crust and atmosphere and the Moon had formed by, it is presumed, a giant impact near the end of accretion. The aim of this project is to quantify the processes by which, during Earth’s earliest evolution, the chemical elements were distributed into different geological reservoirs, to determine the timings and conditions under which this partitioning occurred and to determine how Earth’s interior reached its current composition and oxidation state. The principal method involves experiments at high temperatures (1400-3000K) and high pressures (0-25 GPa, equivalent to 0-700 km depth) in which the silicate materials of Earth’s mantle, crust and core are equilibrated with one another and with a gas phase under controlled conditions. Elements which constrain the major processes of growth and differentiation of the Earth are added to each experiment in trace concentrations similar to those found on Earth. After the experiment the products (typically 2-60 mgm) are sectioned and their chemical compositions determined by microanalysis.  By varying the experimental conditions the dependence of the geochemical behaviour of the different elements on physical conditions such as pressure, temperature and oxidation state will be determined. These measurements of chemical fractionations between different phases are complemented by experimentally-measured isotopic fractionations between the same phases. These will enable us to interpret the observed isotopic differences between Earth and primitive planetary material (as represented by chondritic meteorites) in terms of the processes which formed our planet.","2498761","2011-04-01","2017-03-31"
"EASY","Ejection Accretion Structures in YSOs (EASY)","Thomas RAY","DUBLIN INSTITUTE FOR ADVANCED STUDIES","For a number of reasons, in particular their proximity and the abundant range of diagnostics to determine their characteristics, outflows from young stellar objects (YSOs) offer us the best opportunity of discovering how astrophysical jets are generated and the nature of the link between outflows and their accretion disks. Models predict that the jet is initially launched from within 0.1 to a few au of the star and focused on scales at most ten times larger. Thus, even for the nearest star formation region, we need high spatial resolution to image the “central engine” and test current models.  
With these ideas in mind, and the availability of a whole new set of observational and computational resources, it is proposed to investigate the origin of YSO jets, and the jet/accretion zone link, using a number of highly novel approaches to test magneto-hydrodynamic (MHD) models including: 
(a) Near-infrared interferometry to determine the spatial distribution and kinematics of the outflow as it is launched as a way of discriminating between competing models.  
(b) A multi-epoch study of the strength and configuration of the magnetic field of the parent star to see whether model values and geometries agree with observations and the nature of its variability. 
(c) Examining, through high spatial resolution radio observations, how the ionized component of these jets are collimated very close to the source and how shocks in the flow can give rise to low energy cosmic rays.
(d) Use the James Webb Space Telescope (JWST) and, in particular, the Mid-Infrared Instrument (MIRI) and Near-Infrared Spectrograph (NIRSpec) to investigate with high spatial resolution atomic jets from protostars that are still acquiring most of their mass. In addition, we will study how accretion is affected by metallicity by studying young solar-like stars in the low metallicity Magellanic Clouds. 
In all cases the required observational campaigns have been approved.","1853090","2017-10-01","2022-09-30"
"ECC SCIENG","Error-correcting codes and their applications in Science and Engineering","Mohammad Amin Shokrollahi","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Error correcting codes are combinatorial objects which have traditionally been used to enhance the transmission of data on unreliable media. They have experienced a phenomenal growth since their birth some fifty years ago. Today, everyday tasks such as listening to a CD, accessing the hard disk of an electronic device, talking on a wireless phone, or downloading files from the Internet are impossible without the use of error-correcting codes. Though traditional communication still occupies centerstage in the realm of applied coding theory, emerging applications are changing the rules of the game, and calling for a new type of coding theory capable of addressing future needs. These are not limited to physical applications, however. In fact, coding theory is an integral part of solutions offered by researchers outside traditional physical communication to solve fundamental problems of interest, such as the complexity of computation, reliable transfer of bulk data, cryptographic protocols, self correcting software, signal processing, or even computational biology.While research in the past fifty years has put traditional coding theory on firm theoretical grounds, emerging applications are in need of new tools and methods to design, analyze, and implement coding technologies capable of dealing with future needs. This is the main concern of the present proposal. To strike the right balance between length and impact we have identified five areas of research that span the full spectrum of coding theory ranging from fundamental theoretical aspects to practical applications. We set out to develop new theoretical and practical models for the design and analysis of codes, and explore new application areas hitherto untouched. A unique feature of this proposal is our choice of the tools, ranging from classical areas of algebra, combinatorics, and probability theory, to ideas and methods from theoretical computer science.","1959998","2009-04-01","2013-03-31"
"ECCLES","Emergent Constraints on Climate-Land feedbacks in the Earth System","Peter COX","THE UNIVERSITY OF EXETER","The Land Biosphere is a critical component of the Earth System, linking to climate through multiple feedback processes. Understanding these feedback processes is a huge intellectual challenge. In part because of the pioneering work of the PI (Cox et al., 2000), many of the climate projections reported in the IPCC 5th Assessment Report (AR5) now include climate-carbon cycle feedbacks. However the latest Earth System Models (ESMs) continue to show a huge range in the projected responses of the land carbon cycle over the 21st century. This uncertainty threatens to undermine the value of these projections to inform climate policy.  This project (ECCLES) is designed to produce significant reductions in the uncertainties associated with land-climate interactions, using the novel concept of Emergent Constraints - relationships between future projections and observable variations in the current Earth System that are common across the ensemble of ESMs. Emergent Constraints have many attractive features but chief amongst these is that they can make ensembles of ESMs more than the sum of the parts - allowing the full range of ESM projections to be used collectively, alongside key observations, to reduce uncertainties in the future climate. The project will deliver: (i) a theoretical foundation for Emergent Constraints; (ii) new datasets on the changing function of the land biosphere; (iii) Emergent Constraints on land-climate interactions based on observed temporal and spatial variations; (iv) a new generation of scientists expert in land-climate interactions and Emergent Constraints. ECCLES will benefit from the expertise and experience of the PI, which includes training as a theoretical physicist, an early career developing models of the land biosphere for ESMs, and a current career in a department of mathematics where he is at the forefront of efforts to develop and apply the concept of Emergent Constraints (Cox et al., 2013, Wenzel et al., 2016).","2249834","2017-10-01","2022-09-30"
"ECOF","Electroactive Donor-Acceptor Covalent Organic Frameworks","Thomas Bein","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","The effective conversion of light into chemical or electrical energy is one of the major challenges of humanity during the 21st century. Organic bulk heterojunctions of polymers or aggregates of small molecules combining donor- and acceptor-functionality offer promising prospects for effective light-induced energy conversion. In order to efficiently utilize the solar energy, interpenetrating networks of donor- and acceptor components are often required. While impressive advances have been achieved in organic photovoltaics systems, so far a deterministic control of their nanoscale morphology has been elusive. It would be a major breakthrough to develop model systems with well-defined periodic, interpenetrating networks of electron donor- and acceptor-phases. It is the goal of this project to create such highly defined model systems, to enhance our understanding of the relationship between the electronic and structural parameters and the resulting light-induced charge carrier dynamics. To pursue this challenge, we base our strategy on the recently discovered conceptual paradigm of Covalent Organic Frameworks (COFs). COFs are a class of highly porous, organic crystalline materials that are held together by covalent bonds between molecular building blocks. In a concerted team effort with organic chemists, we will create COFs with different π-stacked heteroaromatic electron donor- and acceptor moieties, thus forming highly ordered interpenetrating networks for light-induced charge separation. This interdisciplinary program is unique as we join the forces of top-level organic synthesis with advanced nanoscience and in-depth physical characterization in one team.","2431728","2013-03-01","2019-02-28"
"ECOGAL","Star Formation and the Galactic Ecology","Ian Bonnell","THE UNIVERSITY COURT OF THE UNIVERSITY OF ST ANDREWS","We will construct the first self-consistent models of star formation that follow the galactic scale flows
where molecular clouds form yet still resolve the star formation and feedback events down to sub-parsec scales.
By following the full galactic ecology, the life cycle of gas from the interstellar medium into stars and their radiative and kinematic output back into
the galaxy, we will develop a comprehensive theory of star formation. The link between the large-scale dynamics of the galaxy and the
small-scale star formation provides the ground-breaking nature of this proposal.
Star formation produces a wide range
of outcomes in nearby molecular clouds yet on large scales yields star formation rates that are strongly correlated to galactic-scale gas densities.
These observed  properties of star forming galaxies have inspired a plethora of theoretical ideas, but until now there has been
no means of testing these analytical theories.
We will use galactic-disc simulations to determine how  molecular clouds form through self-gravity, spiral shocks and/or
cloud-cloud collisions. We will use these self-consistent models of molecular clouds to follow the local gravitational collapse to
form individual stars and stellar clusters.
We will include ionisation, stellar winds and supernovae into the ISM to study how feedback can  support
or destroy molecular clouds,  as well as triggering successive generations of young stars.
We will also conduct Galactic bulge scale simulations to
model how gas flows into, and star formation occurs in, the Galactic centre.
The primary goals of this proposal are to understand what determines the
local and global  rates,  efficiencies and products of star formation in galaxies, and to develop
a complete theory of star formation that can be applied to galaxy formation and cosmology.","2210523","2012-05-01","2018-04-30"
"ECSYM","Events, Causality and Symmetry-the next-generation semantics","Glynn Winskel","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Headed by Principal Investigator Glynn Winskel, the project ECSYM assembles a world-leading team of theoretical computer scientists and mathematicians. Their  goal: to build the next-generation semantics---a new mathematical  foundation with which to understand and analyze computation of the complexity we  begin to see today. The proposal arises in  answer  to the anomalies found in today's theories of computation and  to the commonality and  need  for shared  techniques  becoming apparent across a range of  seemingly disparate areas, through  security protocols, systems biology, model checking, computational games, types and proof.The evidence points to a new intensional semantics,  one in which the current distinctions between operational and denotational semantics disappear.  It leads to the project ECSYM (Events, Causality and SYMmetry). The project marries the vision of Scott and Strachey, who sought a comprehensive semantics of computation, with Petri's  analysis of computation, as emergent from  local causal structure between basic events.  A key insight is the increased expressivity a treatment of behavioural symmetry brings to causal models, to the types, processes, operations and applications they can  support.","2347999","2011-05-01","2017-04-30"
"EDAX","Beating Complexity through Selectivity:Excited state Dynamics from Anti-Stokes and non-linear resonant inelastic X-ray scattering","Alexander Föhlisch","UNIVERSITAET POTSDAM","The key to move from a mere description of static materials properties to the determination and control of functionality and chemistry lies in understanding dynamic pathways through multidimensional energy landscapes.

Through my efforts over the last decade my group accomplished breakthroughs towards the required excited states selectivity and to follow dynamic pathways with resonant inelastic soft x-ray scattering (RIXS) in three aspects: Non-linear RIXS for materials science to boost scattering efficiency. Time resolved and Anti-Stokes RIXS for back-ground free detection of excited states. Sub-natural line width RIXS to map out potential energy surfaces at selected atoms. 

In the ERC research I link these unfolding fields and combine them with ab-initio treatment of excited states to create unprecedented back-ground free X-ray probes of excited states of matter and their dynamics. This will be femtosecond time resolved Anti-Stokes RIXS for excited state selectivity with transform limited pulses and doubly resonant soft X-ray 4 wave mixing to determine multi-center dynamics on atomic scales.

The ERC grant will answer long standing questions on the governing principles of functionality, regarding chemical pathways and energy landscapes in molecules as well as phase transitions, driven phases and emergence in functional materials.

These novel approaches become only now feasible through the unprecedented brilliance of Free-Electron Lasers and the efforts of my group over the last decade. The ERC grant will establish proof-of-principle at brilliant soft X-ray Free-Electron Lasers that will be followed by world leading, ideal conditions implemented at the European XFEL.","2496250","2016-01-01","2020-12-31"
"EDIFICE","Changes in the geomagnetic dipole (Earth Dipole Field Intensity from Cosmogenic Elements)","Jean-Pierre, Michel Valet","INSTITUT DE PHYSIQUE DU GLOBE DE PARIS","Ancient records of the geomagnetic field intensity provide the unique source of information on the evolution of the geodynamo. The paleomagnetic data contain a broad spectrum of dipole moment fluctuations with polarity reversals and excursions that typically occur during periods of very low field intensity, but the amplitude and the timing of the variations as well as critical features remain debated. The variability of the dipole with rapid fluctuations combined with long-term changes must be clarified to understand what controls the dipole strength, why it fluctuates and what is the cause of polarity reversals. Much has been learned for the past 30 years from records of paleointensity relying on natural remanent magnetization of sediments and lava flows, but large uncertainties persist and major features of the field remain poorly documented, pointing out the limits of the approach. As an alternative to magnetization, changes in geomagnetic intensity can be reconstructed from the production of cosmogenic 10Be. The 10Be production can be measured with confidence from sedimentary sequences. Our main objective is to build up a worldwide database of the dipole field changes for the past 5 Ma by acquiring high resolution records of 10Be production from a worldwide set of selected sediment cores. The Accelerator mass spectrometry national facility « ASTER» at CEREGE dedicated to 10Be measurements offers this unique opportunity.  Accurate time control will be obtained by astronomical calibration of paleoenvironmental records. In parallel, we will focus on the short-term field changes occurring during geomagnetic reversals. This will be addressed by combining detailed paleomagnetic records of reversals from volcanic sequences with high resolution 10Be measurements from marine cores that recorded the same reversals. Predictions of numerical geodynamo simulations will be tested against the data.","2499600","2014-06-01","2019-05-31"
"eDrop","Droplet Photoelectron Imaging","Ruth Signorell Luckhaus","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Angle-resolved photoelectron spectroscopy of aerosol droplets (“droplet photoelectron imaging”) is a novel approach to study fundamental aspects of the electron dynamics in liquids and across interfaces. Our recent proof-of-principle studies demonstrate that droplet photoelectron imaging not only complements, but also significantly extends the range of accessible information over established methods. Two aspects are unique to droplets: Firstly, the droplet size can be varied over a wide range from submicrons to microns. While large droplets provide overlap with liquid microjet and bulk studies, small droplets offer additional control by acting as efficient optical resonators. These optical cavity effects can be exploited to control where in the droplet the photoelectrons are generated; e.g. surface versus volume. Secondly, comprehensive information about photoelectron kinetic energy and angular distributions can be obtained fast and in a straightforward way by velocity map imaging.

Building on our proof-of-principle studies, we propose to exploit the versatility of the droplet approach to address fundamental questions regarding electron dynamics in liquids and across interfaces: Can this new tool provide the missing data for low-energy electron scattering in water and other liquids and resolve the issue of the “universal curve”? How do slow electrons scatter across liquid-gas and buried liquid-liquid/solid interfaces and how does this depend on the composition and curvature of the interface? How is the ultrafast relaxation dynamics of electrons following above-band-gap excitation influenced by electron scattering and confinement effects? Low-energy electron scattering is a determining factor in radiation chemistry and biology and a central aspect of the solvated electron dynamics, while interfacial processes play a key role in atmospheric aerosols. Droplet photoelectron imaging opens up new ways to study such phenomena.","2500000","2018-11-01","2023-10-31"
"eEDM","A laser-cooled molecular fountain to measure the electron EDM","Edward Allen Hinds","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","I propose to build an instrument that cools YbF molecules to microK temperature using laser light, and throws them up as a fountain in free fall.  This will be used to detect CP-violating elementary particle interactions that caused our universe to evolve an excess of matter over antimatter  These interactions cause the charge distribution of the electron to be slightly non-spherical and it is this property, the permanent electric dipole moment (EDM), that the ultracold molecules will sense.

Laser cooling of any molecule is very new, with first results emerging from a few laboratories including mine. Developing a fountain of molecules will be a major advance in the state of the art.  As well as being the key to the new EDM instrument, this will be important in its own right because ultracold molecules have major applications in chemistry, quantum information processing and metrology.

In the fountain, the electron spin of each molecule will be polarized. On applying a perpendicular electric field, the spins will precess in proportion to the EDM.  At present the (warm) YbF molecules in my lab precess for only 1ms. This gives us world-leading sensitivity, but has not been sufficient to detect the CP-violating forces being sought. The fountain however will achieve precession times of almost a second, giving over 1000x more rotation.  The increase in sensitivity should reveal a clear EDM, providing information about the fundamental laws of physics, and the important CP-violating physics of the early universe, which is currently not understood.

By advancing the preparation of ultracold molecules, this project will address a key question in particle physics and cosmology: the nature of CP-violating physics beyond the standard model. The approach is radically different from standard accelerator physics and complements it.  The sensitivity is sufficient to detect some proposed new forces that are beyond the reach of any current collider experiment.","2409629","2013-02-01","2018-01-31"
"EFT4LHC","An Effective Field-Theory Assault on the Zeptometer Scale: Exploring the Origins of Flavour and Electroweak Symmetry Breaking","Matthias Neubert","JOHANNES GUTENBERG-UNIVERSITAT MAINZ","""Questions about the origins of electroweak symmetry breaking and of the striking hierarchies ob-served in the spectrum of fermion masses and mixing angles are among the most pressing problems in fundamental physics. While the Large Hadron Collider at CERN was built to explore the physics of electroweak symmetry breaking on tiny distance scales of an attometer, the absence of clear hints for new particles in existing high-energy physics experiments suggests that new phenomena might only occur at distances still smaller than this. What if the LHC discovers a Higgs boson and nothing else? It has recently been realized that significantly shorter distances of only a few zeptometer (10^-21 m) can be probed indirectly in precision measurements of rare weak decay processes and of the couplings of the Higgs boson. Exploring nature at these scales never before accessible to mankind requires breakthrough advances in theory.

I propose a broad theoretical approach to precision physics in and beyond the Standard Model based on effective field-theory tools. In the context of warped extra-dimension models, the genuine quantum structure of fundamental physics will be probed in loop-mediated processes, including Higgs-boson production and decay as well as rare flavour-changing neutral current processes. These explorations will be complemented by highest-precision calculations of important collider-physics processes, such as Higgs, top, and electroweak gauge-boson production in association with jets, which for the first time will be performed without recourse to phenomenological models. The multi-loop anomalous dimensions required for these calculations will also provide a deeper understanding of the structure of infrared singularities of scattering amplitudes in non-abelian gauge theories. The results obtained from the research described in this proposal are likely to reveal the deep common origins of the flavour structure and electroweak symmetry breaking.""","2109600","2012-01-01","2016-12-31"
"ELAB4LIFE","eLab4Life: Electr(ochem)ical Labs-on-a-Chip for Life Sciences","Albert Van Den Berg","UNIVERSITEIT TWENTE","We propose the development of new electrochemical techniques for health and life sciences applications in Lab-on-a-Chip devices. A Scanning ElectroChemical Microscope (SECM) will be used to study surface properties, such as local consumption and/or release of electroactive chemical compounds by (single) cells by electrochemical sensing, new detection methods for proteins using redox cycling, and new separation methods for DNA exploiting nanoscale electrical field gradients. The ability to generate and control electrical fields (and gradients) at the scale of the size of biomolecules using nanostructures, and the simple translation of novel electrical methods into practical Lab-on-a-Chip devices will create a breakthrough in bioanalytical methods. The knowledge and expertise obtained from SECM experimentation will be used to design and realize Labs-on-a-Chip that can be used for efficient production of drugs by electrofused cells, for early biomarker detection using nanowires and nano-spaced electrodes (Point-of-Care application), and rapid DNA analysis using nanofluidic structures. Besides this, the results can have great benefits for study of embryonic cell growth and for advanced tissue engineering. The results will be translated into devices and systems that can be used in Point-of-Care (POC) applications and will bring this area a big step closer to successful commercialization.","2382442","2008-12-01","2013-10-31"
"ELBM","Frontiers for multi-scale computational fluid dynamics","Ilya Karlin","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","""Computational fluid dynamics remains challenged with the complexity of fluid motion on all scales from atmospheric phenomena down to flows in micro- and nano-devices. The lattice Boltzmann method (LBM) has been conceived to replace the conventional methods of computational fluid dynamics. Due to its computational efficiency and simplicity in handling complex geometries, LBM was only partly successful in simulating incompressible flows. However, it faced stiff challenges in other domains of fluid dynamics due to low isotropy of the lattice and lack of stability. Recently, a new generation of entropic lattice Boltzmann models (ELBM) restored second law of thermodynamics in the lattice Boltzmann kinetics and made lattice Boltzmann unconditionally stable. Armed with new higher-order entropic lattices, ELBM project will open up high Reynolds number flows, compressible flows, multi-phase and micro flows and other domains for fast and efficient simulations. New ELBM models retain all the advantages of LBM in terms of efficiency, parallelism, and handling of complex geometries. This project will serve as unique source of largest possible benchmark simulations and engineering applications in fluid dynamics; thus challenging or even replacing the most advanced methods of computational fluid dynamics as well as particle methods in micro flows.""","1656800","2012-01-01","2016-12-31"
"ELE","Evolving Language Ecosystems","Jan VITEK","CESKE VYSOKE UCENI TECHNICKE V PRAZE","The ELE project will study the foundational principles of programming  language evolution and develop practical tools and technologies for supporting the evolution of complete ecosystems. If successful, ELE will drastically decrease the cost of evolution and avoid the need to invent completely new languages every time there is a shift in hardware trends or in programming methodology. Instead, ELE will allow evolution of languages  and will support migration of code and knowledge bases.  The project proceeds along two major axes.  The first axis is language dynamics where new features and new capabilities are added to a preexisting language. This requires changing, at the same time, the language's specification, it's semantics, and the language's implementation, the compiler and interpreter that runs code written in the language as well the runtime libraries that provide basic capabilities. The second axis for evolution is language statics where new rules are added to enforce novel programming disciplines and where existing code artifacts are adapted to new semantics. These axes are not entirely disjoint, as static restrictions, such as a new type system, can feedback into the implementation by providing behavioral information that can be exploited by a compiler.","3234000","2016-10-01","2021-09-30"
"ELECSPECIONS","Electronic spectra of cold, large interstellar ions","John Paul Maier","UNIVERSITAT BASEL","The purpose of this project is to measure for the first time gas-phase spectra of large carbon containing cations, at low temperatures, which are of astrophysical importance. Knowledge of electronic spectroscopy of such molecules is also of pertinence in a number of areas of chemistry and physics, enabling their identification in planetary atmospheres, flames and plasmas and as intermediates in chemical reaction dynamics. This project is interdisciplinary, bridging the areas of chemistry, physics, and astrophysics. It encompasses state of the art techniques of chemical physics and aims at obtaining information to solve the long standing enigma in observational astronomy, the identification of some of the molecules causing absorption of starlight in diffuse interstellar clouds. The project uses ion trapping technology, whereby mass-selected species are held in a radio-frequency field and the vibrations and rotations are relaxed by collisions with cold helium to typical interstellar temperatures of 10-30 K. The electronic spectra of a number of cations, selected on the basis of their special properties including those of bare carbon chains, rings and fullerenes, polycyclic aromatic hydrocarbon cations and their protonated forms will be measured using new detection schemes. The first approach is based on photo-induced charge transfer which is turned on upon laser excitation and the second uses the possibility of rare-gas complexation in the ground but not excited state.","1898624","2010-05-01","2015-04-30"
"ELECTRA","Electrochemically induced Asymmetry: from materials to molecules and back","Alexander KUHN","INSTITUT POLYTECHNIQUE DE BORDEAUX","Asymmetry is a very common feature of many systems, objects and molecules, that we use in our daily life. Actually, it is in a majority of cases the absolutely crucial ingredient for conferring a useful property to a system, a prominent example being the chiral nature of pharmaceutically active compounds. Chemists have developed various approaches to generate asymmetry, from the molecular to the macroscopic scale, but are still facing major challenges when exploring efficient alternative physico-chemical concepts for symmetry breaking. The global aim of ELECTRA is to propose so far unexplored and versatile strategies, based on the unconventional use of electrochemical phenomena, to generate asymmetry in chemical systems at different length scales.
Investigating simultaneously wired and wireless electrochemistry will open up unique possibilities for advancing the topic of asymmetry generation in an original and cross-disciplinary way. We will determine the utility of these strategies in the frame of two major challenges that are: 
-unconventional detection, separation and synthesis of enantiomers, based on chiral encoded metal phases, very recently pioneered by us; 
-design and characterization of Janus systems with complex structures and reactivity 
Carefully designed experiments at the forefront of electrochemical science will first enable us to gain a better understanding of the different mechanisms involved in symmetry breaking. An optimization by exploring new concepts with respect to their efficiency, yield and selectivity is the next step. This will prepare for the choice of the most innovative approaches of symmetry breaking, in view of the numerous highly relevant applications, ranging from analysis to catalysis and energy conversion. Furthermore, due to the interdisciplinary character of asymmetry, the findings of this project will not only have a major impact in various areas of chemistry, but will also be very interesting for physics and biology.","2415849","2017-09-01","2022-08-31"
"ELICOS","Enantioselective Light-induced Catalysis for Organic Synthesis","Thorsten Bach","TECHNISCHE UNIVERSITAET MUENCHEN","Photochemical reactions are induced by ultraviolet or visible light and they enable the construction of molecular skeletons that are difficult or even impossible to access by any other means. In order to secure a widespread application of these reactions in organic synthesis and in order to make the respective products available for high-tech applications, it is required that photochemical reactions can be performed enantioselectively, i.e. that the three-dimensional architecture of the product molecules is unambiguously defined. Despite some progress in recent years, this challenge has as yet remained unsolved. It is the objective of this proposal to develop catalytic methods that allow the enantioselective synthesis of a broad variety of compound classes by light-induced reactions. The mode of action of potential catalysts is based on energy transfer or on wavelength-selective excitation. In the former scenario, the substrate-catalyst complex will be held together by non-covalent interactions, which position the substrate for an efficient energy transfer and for an enantioselective approach of the reagent. In the latter approach, acid-base interactions in the substrate-catalyst complex will lead to a significant bathochromic absorption shift of the substrate, enabling a selective excitation within the complex and a subsequent enantioselective reaction pathway within a spatially defined chiral environment. Both approaches include the option to employ sunlight as the exclusive energy source to drive enantioselective processes without the need to apply external heat or pressure.","2357901","2016-01-01","2020-12-31"
"ELVER","Engineering with Logic and Verification: Mathematically Rigorous Engineering for Safe and Secure Computer Systems","Peter SEWELL","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Computer systems have become critical to modern society, but they are pervasively subject to security flaws and malicious attacks, with large-scale exposures of confidential data, denial-of-service and ransom attacks, and the threat of nation-state attackers: they are trusted, but are far from trustworthy. This is especially important for the major pan-industry components of our information infrastructure: processors, programming languages, operating systems, etc.

The basic problem is that conventional engineering techniques suffice only to make systems that *usually* work. The usual test-and-debug development methods, with poorly specified abstractions described in prose, lack the mathematical rigour of other engineering disciplines - yet the huge investment in legacy systems and skills makes it hard to improve.

ELVER will develop *mathematically rigorous* methods for specifying, testing, and reasoning about *real systems*, focussed on the core mechanisms used by hardware and software to enforce security boundaries.  It will establish mathematical models for the industry ARM architecture, used pervasively in mobile phones and embedded devices, and the CHERI research architecture, which protects against many attacks. Using these, ELVER will build tools for analysis of system software, develop techniques for mathematical proof of safety and security properties, and explore improved systems programming languages.  ELVER will build on successful collaborations with ARM, IBM, and the C/C++ ISO standards committees.  It will directly impact mainstream processor architectures, languages, and development methods, smoothly complementing existing methods while simultaneously enabling longer-term research towards the gold standard of provably secure systems.

ELVER will thus demonstrate the feasibility and benefits of a more rigorous approach to system engineering, putting future systems on more solid foundations, and hence making them safer and more secure","2473844","2018-10-01","2023-09-30"
"ELYCHE","Electron-scale dynamics in chemistry","Mauro Nisoli","POLITECNICO DI MILANO","The target of the proposal is the first experimental demonstration of attosecond coherent control of electron motion in many-particle systems. The past decade has seen remarkable advances in the field of coherent control of chemical reactions thanks to the application of femtosecond technology; I propose to use the emerging attosecond technology to achieve coherent control of photodissociation reactions on a purely electronic scale. I will mainly concentrate on molecules with biological interest. The success of the project will be based on the possibility to initiate and control the sub-femtosecond electronic motion in large molecules, by using high-intensity isolated attosecond pulses. Such electron motion precedes and determines the subsequent nuclear rearrangement, which ultimately leads to the chemical change. In this way it will be possible to control in a direct way the outcome of a chemical reaction, which is one of the central problems in modern chemistry. A crucial benchmark of the project, substantially beyond the current state-of-the-art in Attosecond Science, will be the experimental demonstration of attosecond pump / attosecond-probe measurements, which for the present are not technically feasible. Electron dynamics will be measured, with attosecond resolution, in many-particle systems, ranging from simple molecules to complex bio-molecules.
The application of attosecond pulses and the development of attochemistry techniques for the investigation of the primary electronic steps of chemical processes, is a completely new and challenging research field, with tremendous prospects for both fundamental research and technology. In particular, the attosecond coherent control of charge localization in bio-molecules can offer unique information on the mechanisms at the basis of biological signal transmission or on the processes leading to damaging of complex biological molecules (from polypeptides to proteins and DNA).","2446200","2009-04-01","2014-03-31"
"EMERGENCE","The Emergence of Structure during the Epoch of Reionization","Martin Gerhard Otto Haehnelt","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Early on the Universe consisted of a near-uniform mixture of hydrogen, helium, dark matter and radiation.  The emergence of structure from a stochastic background of fluctuations in the
period between 400.000 years and 1 billion years  is the main subject of this proposal.  This era saw the  formation of the first autonomous sources of radiation, stars and black holes.  This `renaissance' of light led to the heating, reionization  and pollution  of the Intergalactic Medium  with metals.

We will unravel   how the hydrogen in Universe progressed from substantially neutral to highly ionized  by detailed comparison  of cosmological hydro-simulations of the intergalactic Medium (IGM) and galaxy formation    including continuum and resonant Lyman-alpha radiative  transfer with QSO absorption spectra and   LBG/ Lyman-alpha emitter surveys and other data.
This will help us to make the most out the wealth  of information which will be provided  by  new observational missions  and surveys which have just begun (or are just about to begin) to report results (UKIDDS, VISTA, Planck, Herschel, COS@HST, LOFAR, ALMA). In this way we expect to make  decisive contributions  to the expected transformation of  our understanding of this exciting period in the history of the Universe.

Measurements of the matter power spectrum on scales from 1Mpc to a Gpc from  Lyman-alpha forest, weak gravitational lensing, and galaxy survey data contain important information of the nature of dark matter and the mass and number of species of neutrinos.  Particularly exciting is the possibility to significantly push the limit  on 'how cold' dark matter is. To robustly  answer  the question, whether the free-streaming of dark matter suggested to solve the dwarf-galaxy problem  of the cold dark matter  paradigm is consistent with  Lyman-alpha forest data, is another key goal of this proposal.","1975121","2013-05-01","2019-04-30"
"EMERGRAV","Emergent Gravity, String Theory and the Holographic Principle","Erik Peter Verlinde","UNIVERSITEIT VAN AMSTERDAM","The study of black hole physics and string theory are leading to a novel perspective on gravity and space-time. The old frameworks are replaced by a new paradigm in which gravity is understood as an emergent phenomenon. A central role in this revolution is played by the holographic principle put forward by ‘t Hooft. It states that the microscopic information associated with the physical world can be stored on the boundary of space. From this holographic viewpoint I have recently derived the familiar laws of Newton and Einstein using only first principles. Gravity appears as an entropic force caused by changes in information associated with matter. With this ERC proposal I am aiming to build a research group that will further develop this new entropic view on gravity. The powerful string theoretic tools, such as the holographic correspondence between gauge theory and gravity, will be used to illuminate and further clarify gravity’s entropic origin.  In addition, I plan to investigate the implications of the emergence of the gravitational force for the areas in which gravity plays a crucial role, in particular cosmology. For instance, the entropic viewpoint is expected to shed new light on the nature of dark energy and possibly dark matter. It may also lead to a new perspective on the other fundamental forces, since the notions of inertia and mass need to be reconsidered as well. The understanding of gravity as an emergent phenomenon will also influence and benefit from the conceptual ideas developed in condensed matter physics, such as the recently discovered connection between quantum critical electron systems and black hole horizons. The university of Amsterdam and the Netherlands provide an excellent environment for a successful completion of these goals.","2033983","2011-04-01","2016-03-31"
"EMIS","An Intense Summer Monsoon in a Cool World, Climate and East Asian Monsoon during Interglacials with a special emphasis on the Interglacials 500,000 years ago and before","André, Léon Berger","UNIVERSITE CATHOLIQUE DE LOUVAIN","Asian monsoon is a spectacular occurrence in the climate system. What make it so powerful are the combination of thermal contrast between the World s largest landmass (Eurasian continent) and ocean basin (the Indo-Pacific Ocean) and the presence of the World s largest ridge, the Tibetan Plateau. Climatologically, monsoon regions are the most convectively active areas and account for the majority of global atmospheric heat and moisture transport.  Moreover, the economy, culture and rhythms of life of 60% of humanity are critically influenced by the evolution and variability of the Asian monsoon. The need to better understand the monsoon leads inevitably to the close inspection of its activity during the geological times to provide a long-term perspective from which any future change may be more effectively assessed. Our research proposal aims to understand the seeming paradox of the exceptionally intense East Asian summer monsoon (actually the strongest over the last one million years) which occurred during the relatively cool interglacial (MIS-13), 500,000 years ago. This will be done using first a model of intermediate complexity (LOVECLIM) to achieve a number of sensitivity experiments to the astronomical forcing, the Eurasian and North American ice sheets, the Tibetan Plateau and the Ocean. Ocean-atmosphere coupled general circulation models will then be used to confirm the main processes underlined by LOVECLIM, in particular those related to the wave train topographically induced by the Eurasian ice sheet, to the Tibetan Plateau, to the sea-surface temperature and to their role in reinforcing the East Asian summer monsoon. This monsoon of MIS-13 will be compared with the monsoon which occurred during the other interglacials of the upper Pleistocene and Holocene (about the last 700,000 years). All simulation results will be compared with the available proxy records, in particular-but not exclusively-those coming from the loess-soil sequences in China.","893880","2008-11-01","2013-10-31"
"eNANO","FREE ELECTRONS AS ULTRAFAST NANOSCALE PROBES","Javier Garcia de Abajo","FUNDACIO INSTITUT DE CIENCIES FOTONIQUES","With eNANO I will introduce a disruptive approach toward controlling and understanding the dynamical response of material nanostructures, expanding nanoscience and nanotechnology in unprecedented directions. Specifically, I intend to inaugurate the field of free-electron nanoelectronics, whereby electrons evolving in the vacuum regions defined by nanostructures will be generated, guided, and sampled at the nanoscale, thus acting as probes to excite, detect, image, and spectrally resolve polaritonic modes (e.g., plasmons, optical phonons, and excitons) with atomic precision over sub-femtosecond timescales. I will exploit the wave nature of electrons, extending the principles of nanophotonics from photons to electrons, therefore gaining in spatial resolution (by relying on the large reduction in wavelength) and strength of interaction (mediated by Coulomb fields, which in contrast to photons render nonlinear interactions ubiquitous when using free electrons). I will develop the theoretical and computational tools required to investigate this unexplored scenario, covering a wide range of free-electron energies, their elastic interactions with the material atomic structures, and their inelastic coupling to nanoscale dynamical excitations. Equipped with these techniques, I will further address four challenges of major scientific interest: (i) the fundamental limits to the space, time, and energy resolutions achievable with free electrons; (ii) the foundations and feasibility of pump-probe spectral microscopy at the single-electron level; (iii) the exploration of quantum-optics phenomena by means of free electrons; and (iv) the unique perspectives and potential offered by vertically confined free-electrons in 2D crystals. I will face these research frontiers by combining knowledge from different areas through a multidisciplinary theory group, in close collaboration with leading experimentalists, pursuing a radically new approach to study and control the nanoworld.","1899788","2018-12-01","2023-11-30"
"ENCOPOL","Encoding information into polymers","Roeland NOLTE","STICHTING KATHOLIEKE UNIVERSITEIT","The amount of information trafficking internet nowadays is enormous and will increase further in the near future. It can be expected that in the next decennia the current technologies to store and process data will no longer suffice and that other strategies to handle information have to be developed. One approach is to explore chemical routes, which nature has also followed during evolution: our brain can store and handle very large amounts of data and process them in a way silicon-based computers cannot do. Although brain-like chemical computers are still far beyond reach, it is of interest to explore how atom and molecule-based systems that can write, read, and store information might be designed and constructed. In this proposal we aim at developing a new technology to write, store, and read information, i.e. on a single polymer chain with the help of a molecular machine that is inspired by the hypothetical device (Turing machine) proposed by the British mathematician Alan Turing in 1936 as the general basis for the operation of a computer. The molecular machine is composed of a chiral catalytic cage compound (tape-head) that moves unidirectionally along a chiral polymer chain (tape) while writing a binary code in the form of (R)- (symbol 0) and (S)- (symbol 1) epoxide functions. This writing process is controlled by light or electrons. The information on the tape will be read by single molecule spectroscopy using a reading device that is also based on a chiral cage compound. It moves along the encoded tape and produces left- or right-handed polarized fluorescence light depending on whether it reads a 0 (R-epoxide) or 1 (S-epoxide). As part of this project we will also make the first steps towards chemical computing by arranging two circular tapes (one left-handed and the other one right-handed), each with an attached writing head, in a teller set-up, which allows them to be addressed separately with light according to a set of instructions (Minsky machine).","2498076","2017-07-01","2022-06-30"
"ENERGYSURF","Surfaces of Energy Functional Metal Oxides","Geoffrey Thornton","UNIVERSITY COLLEGE LONDON","Many important interactions near the surfaces of energy functional metal oxide surfaces are yet to be established at the atomic level. How bonds are formed and broken in photocatalysis, the role of the metal and oxide in a supported metal catalyst, the mechanism of energy flow from atom to atom after photoexcitation in a photovoltaic device are just some of the open questions. The underlying motivation to generate answers is clear: it provides an opportunity to improve technology associated with light harvesting and energy-related catalysis. But the fundamental science required is extremely challenging and is only just starting to yield some detailed answers. In this highly ambitious project we will tackle three major issues associated with the surface chemistry of energy functional metal oxides-- three grand challenges. In doing so, we will make use of a unique range of experimental techniques. One will focus on solid-gas interactions in studies of Au on CeO2, employing scanning tunneling microscopy (STM), scanning tunnelling spectroscopy and STM-inelastic electron tunnelling spectroscopy to answer questions about the active site of water gas shift catalysis and the mechanism by which the substrate reversibly exchanges oxygen.  The second challenge will probe the structures of interfaces between water and ZnO and TiO2, using surface X-ray diffraction and STM. This will include the adsorption of dye mimics from solution. In the third challenge, femtosecond time resolved photoemission using pump probe will be used to unravel the details of energy dissipation following a UV pulse absorption by ZnO and TiO2 substrates and their interaction with water. This is directly related to processes associated with photocatalysis. We expect that this ERC project will revolutionise our understanding of energy functional surfaces based on metal oxides and ultimately lead to key breakthroughs in the design of advanced devices.","2364681","2011-10-01","2017-09-30"
"ENIGMA","ENIneering MAterial properties with advanced laser direct writing","Peter KAZANSKY","UNIVERSITY OF SOUTHAMPTON","Ultrafast laser material processing is approaching its limits in terms of ability to produce innovative materials with
compositional and structural consistency. The main idea of this project is to remove barriers to product development and go
beyond state-of-the-art by applying tailored and few-cycle laser pulses (FCLPs) for engineering of materials.
In this project I will investigate the interaction between intense ultra-short light pulses and matter at or below the wavelength
scale reaching states of matter found only deep planetary conditions.A key goal of the project is to exploit these extreme
conditions for synthesising unique material phases with on-demand optical and electronic properties, and progress photonic
devices with utilizing FCLP advantages: control over the bond scissoring density; efficient and highly localized energy
deposition; seeding of self-organized nanostructures; manipulation of spatio-temporal coupling.
Currently, a key limitation is plasma scattering that diminishes the performance of engineered materials. The question I will
address is whether control of ultra-short pulses can lead to ways around this limitation. The control of self-organization
process will revolutionize the field of data storage by achieving record high 100 TB/cm3 densities, high writing speed and
practically unlimited lifetime. I will radically improve the performance of printed flat optics with perfected nanostructures
engineered from nano- to macro-scale and capable of replacing conventional optics significantly advancing photonic devices
used in high-resolution microscopy, consumer electronics, and high-power laser applications. I envisage obtaining exotic
material phases such as metallic phases of silicon and tailored metallic nanoparticles in silicate glass. Hence this project will
push the frontiers of laser material processing to unprecedented precision and will develop novel family of devices that will
feed into the future of optics, electronics and computing","2499957","2019-01-01","2023-12-31"
"ENRICO","Enrichment of Components at Interfaces and Mass Transfer in Fluid Separation Technologies","Hans Hasse","TECHNISCHE UNIVERSITAET KAISERSLAUTERN","Techniques for separating fluid mixtures are important in many industries like the chemical and pharmaceutical industry. The most relevant of these separation techniques, like distillation and absorption, are based on mass transfer over fluid interfaces. Results from molecular thermodynamics, which have recently become available, show that for many industrially important mixtures a strong enrichment of components occurs at the fluid interface. There is a striking congruence between shortcomings of the present design methods for fluid separations and the occurrence of that enrichment. It is the central hypothesis of the present research that the enrichment leads to a mass transfer resistance of the fluid interface which has to be accounted for in fluid separation process design. The fact that it is presently neglected causes unnecessary empiricism and inconsistencies in the design. ENRICO will advance the knowledge on the enrichment of components at fluid interfaces using a novel combination of two independent theoretical methods, namely molecular simulations with force fields on one side and density gradient theory coupled with equations of state on the other. This will enable reliable predictions of the occurrence of the enrichment and its magnitude. These results will be combined with the theory of irreversible thermodynamics to establish for the first time a model for the mass transfer resistance of the interface due to the enrichment. On that basis, a new approach for designing fluid separation processes will be developed in ENRICO, which will lead to more efficient and robust designs. The theoretical results will be validated by experiments from laboratory to pilot plant scale, and the benefits of the new approach will be demonstrated. ENRICO will thus establish a link between molecular physics and engineering practice. The results from ENRICO will have a major impact on chemical engineering world-wide and change the way fluid separation processes are designed.","2498750","2016-10-01","2021-09-30"
"EntangleGen","Entanglement Generation in Universal Quantum Dynamics","Markus OBERTHALER","RUPRECHT-KARLS-UNIVERSITAET HEIDELBERG","A paradigm example of precise predictions in complex systems is the universal scaling of correlation functions close to phase transitions, with their associated critical exponents. The extension of this concept to time dependent problems has been studied in the classical regime as well as in the quantum regime. A clean experimental confirmation of this prediction in a quantum system as well as of its connection to non-local entanglement generation is the defined goal of this project.
The experimental system builds on atomic Bose-Einstein condensates with precisely controlled internal degrees of freedom. Their physics can be mapped onto extensively studied spin systems in the large-collective-spin limit. While the mean evolution of these large spins is well captured by classical descriptions, the detailed study of the fluctuations can reveal particle entanglement. The technology for such high-precision measurements has been pioneered by the PI, demonstrating entanglement in spin-squeezed as well as non-gaussian entangled states.
In this project one-dimensional gases will be realized allowing for the implementation of a spin system revealing a quantum phase transition. While the spatial spin-spin correlation functions can already be detected, the future experimental development concerns the implementation of non-demolition/weak measurements of the spin degree of freedom. This makes time-time and time-space correlation functions for the first time accessible, as a necessary prerequisite for the envisaged studies of universal dynamics out of equilibrium and the experimental confirmation of non-local entanglement. Observation of scale invariance in the then available full correlation landscape will allow the verification of the presence of a non-thermal fixed point.
The successful demonstration will lead to a paradigm shift in the description of quantum dynamics in complex systems and will also open up new routes for generating quantum resources for quantum metrology.","2390000","2016-10-01","2021-09-30"
"EPCABO","Engineered Protein Capsids as Artificial Bacterial Organelles","Donald Michael Hilvert","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Many proteins self-assemble into regular, shell-like, polyhedral structures. Protein capsids are useful, both in nature and in the laboratory, as molecular containers for diverse cargo molecules, including proteins, nucleic acids, metal nanoparticles, quantum dots, and low molecular weight drugs.  They can consequently serve as delivery vehicles, bioimaging agents, reaction vessels, and templates for the controlled synthesis of novel materials.  Here, we will apply our experience with protein design and laboratory evolution to extend the properties of protein containers to create practical, non-viral encapsulation systems for applications in the test tube and in living cells. Specifically, we will adapt the icosohedral cage structures formed by Aquifex aeolicus lumazine synthase (AaLS) to engineer increasingly sophisticated supramolecular complexes for use as delivery vehicles, nanoreactors and, ultimately, as bacterial organelles. Our principal aims are to: (a) tailor AaLS capsids for selective encapsulation of a broad range of macromolecular guests; (b) develop AaLS capsids as delivery vehicles for medical and imaging applications; (c) design simplified, functional mimics of carbon-fixing carboxysomes; (d) evolve redox active organelles for metabolizing aliphatic alcohols; and (e) engineer artificial organelles for the detoxification of polychlorinated phenols. We anticipate that these experiments will lead to a deeper understanding of the principles underlying the construction, function and evolution of natural protein microcompartments. At the same time, they will establish powerful strategies for creating tailored assemblies for practical applications in delivery and catalysis.","1889200","2013-03-01","2018-12-31"
"EPIC","Evolving Program Improvement Collaborators","Mark HARMAN","UNIVERSITY COLLEGE LONDON","EPIC will automatically construct Evolutionary Program Improvement Collaborators (called Epi-Collaborators) that suggest code changes that improve software according to multiple functional and non-functional objectives. The Epi-Collaborator suggestions will include transplantation of code from a donor system to a host, grafting of entirely new features `grown' (evolved) by the Epi-Collaborator, and identification and optimisation of tuneable `deep' parameters (that were previously unexposed and therefore unexploited).

A key feature of the EPIC approach is that all of these suggestions will be underpinned by automatically-constructed quantitative evidence that justifies, explains and documents improvements. EPIC aims to introduce a new way of developing software, as a collaboration between human and machine, exploiting the complementary strengths of each; the human has domain and contextual insights, while the machine has the ability to intelligently search large search spaces. The EPIC approach directly tackles the emergent challenges of multiplicity: optimising for multiple competing and conflicting objectives and platforms with multiple software versions.


Keywords:
Search Based Software Engineering (SBSE),
Evolutionary Computing,
Software Testing,
Genetic Algorithms,
Genetic Programming.","2159035","2017-10-01","2022-09-30"
"EPiR","The Chemical Basis of RNA Epigenetics","Thomas CARELL","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","The genetic code consists of a defined sequence of four canonical nucleosides and the sequence of these bases carries the blueprints of all life on earth. It is apparent that this sequence information alone is not sufficient to explain how a multicellular organism can establish specialized cells like the 200 known cells types of a human body. For this, a second information layer is required and since a few years, it is apparent that this information layer is strongly based on Chemistry. Histones, DNA and RNA are the target of sophisticated chemical modification, which establishes a second layer of information. This proposal aims at decoding this chemical code on RNA, specifically on messenger RNA. More than 150 chemical derivatives of RNA nucleosides are known and many more await discovery. We are proposing a program to study RNA modifications and to decipher their function. We are expecting to answer key questions about the structures, the distribution and the biological function of numerous known and yet unknown chemical RNA derivatives that brings us deep into unexplored areas of science. Since some of the modified bases equip RNA with yet unknown reactivity we will study the functional aspects of reactive bases, which allows us to tackle some of the most important questions associated with the RNA world theory. Our results will move Europe into a central position in the new field of RNA epigenetics, which has the potential to become the next big wave in science 63 years after the discovery of the double helix structure. We are perfectly positioned to move the field forward. We established quantitative methods to study modified bases that uncovered a new biological principles in cancer cells (Carell, Carmeliet, Lamberts Nature 2016) and we contributed to the RNA world theory (Carell Science 2016). We applied proteomic tools to learn about the function of new DNA bases that we partially even discovered (Carell, Vermeulen Cell 2013; Carell Nat. Chem. Biol. 2014).","2486375","2017-10-01","2022-09-30"
"EPoCH","Exploring and Preventing Cryptographic Hardware Backdoors: Protecting the Internet of Things against Next-Generation Attacks","Christof PAAR","RUHR-UNIVERSITAET BOCHUM","The digital landscape is currently undergoing an evolution towards the Internet of Things. The IoT comes with a dramatically increased threat potential, as attacks can endanger human life and can lead to a massive loss of privacy of (European) citizens. A particular dangerous class of attacks manipulates the cryptographic algorithms in the underlying hardware. Backdoors in the cryptography of IoT devices can lead to system-wide loss of security.  This proposal has the ambitious goal to comprehensively understand and counter low-level backdoor attacks. The required research consists of two major modules:

1) The development of an encompassing understanding of how hardware manipulations of cryptographic functions can actually be performed, and what the consequences are for the system security.  Exploring attacks is fundamental for designing strong countermeasures, analogous to the role of cryptanalysis in cryptology.

2) The development of hardware countermeasures that provide systematic protection against malicious manipulations. In contrast to detection-based methods which dominate the literature, our approach will be pro-active. We will develop solutions for instances of important problems, including hardware reverse engineering and hardware hiding. Little is known about the limits of and optimum approaches to both problems in specific settings.

Beyond prevention of hardware Trojans, the research will have applications in IP protection and will spark research in the theory of computer science community.","2498286","2016-10-01","2021-09-30"
"EPOCHS","The Formation of the First Galaxies and Reionization with the James Webb Space Telescope","CHRISTOPHER CONSELICE","THE UNIVERSITY OF NOTTINGHAM","Within the first few hundred million years after the Big-Bang the first galaxies and stars were born.  Sometime soon after, these first objects produced enough energetic photons to reionization the neutral gas in the universe. This frontier of early galaxy assembly has not yet been observed, but will be uncovered by deep imaging and spectroscopy taken with the James Webb Space Telescope (JWST).  Key problems include: how the very first galaxies were assembled, and evolved, in their first few Gyr, and the history of reionization.  With this ERC funded EPOCHS project I will lead a major effort to investigate these questions using JWST GTO time discovering galaxies before, during, and after the epoch of reionization.   This proposal has three interconnected and complementary themes: (i) Identifying the first galaxies and characterizing their UV luminosities, stellar masses, and star formation rates at 7<z<12.  JWST imaging and spectroscopy will allow us to make significant progress beyond the current state of the art, and to use these measures to test models of the earliest galaxy assembly. (ii) Using these galaxies we will map the process of reionization: the sources of it, and the time-scale of its onset and duration.  Using new diagnostics we will address uncertainties that currently plague this calculation, including escape fractions and the number of ionizing photons, using UV emission lines, spectral shapes, and measuring hardness ratios with radiative transfer models. (iii) We will measure the rest-frame optical structures of galaxies at 3<z<7 to reveal the formation modes of galaxies when they assembled their first masses and structures. We will determine how and when compact galaxies, mergers, dissipative formation in star forming disks, and the formation of bulges and disks are occurring.  This includes measuring the formation history of internal components in 3<z<7 galaxies, allowing us to examine how quenching is occurring ‘inside-out’ or ‘outside-in’.","1951138","2020-05-01","2025-04-30"
"EPOS CRYSTALLI","Epitaxial thin-film organic semiconductor crystals and devices","Paul Heremans","INTERUNIVERSITAIR MICRO-ELECTRONICA CENTRUM","""Today, organic semiconductor devices are severely limited by the strong disorder in the amorphous or polycrystalline semiconductor films. This disorder is in fact due to the nature of the films, and is NOT an intrinsic molecular property. Indeed, single-crystal organic semiconductors are known, and display exciting characteristics and high performance. Unfortunately, they are today only grown as individual objects, not applicable to integrable thin-film transistors (TFT), solar cells (OPV), and light-emitting diodes (OLED) or transistors (OLET).
In this project, we propose a radical shift in the film formation of organic semiconductors, to master the nucleation and growth of highly crystalline thin films on arbitrary surfaces. We propose several possible templates for crystal growth, control of nucleation sites and new techniques to impose gradients in supersaturation of the environment from which the molecules condense in a growing crystal. Fundamental understanding of the thin-film crystal forming processes will be acquired by in-situ monitoring, and by modelling of nucleation and growth processes. We will apply similar methodologies to hetero-epitaxy of thin-film crystals, i.e. growth of crystalline layers of different types of molecules, and to doping of crystals.  This will open a gateway to use the immense libraries of organic semiconducting molecules for application in high-performance crystalline heterojunction devices.
Proof-of-principle devices will complement the materials science study and establish new research domains. We propose integrable crystalline TFTs, as these are also useful to further probe the physics of crystalline organic semiconductors. Crystalline heterojunction OPVs promise combined high exciton diffusion lengths and carrier mobilities. We will explore the benefits of crystallinity in heterojunction OLEDs and OLETs towards higher current densities and brightness, which may lead to the elusive electrically pumped organic laser.""","2499408","2013-01-01","2017-12-31"
"EQFT","Emergence from Quantum Frustration and Topology","Radu COLDEA","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Highly-correlated many-body quantum states often emerge from correlations between strongly interacting electrons. The proposed research will experimentally explore emergent properties of quantum materials in the presence of strong correlations and spin-orbit coupling, when the spin and orbital angular momentum of electrons are strongly entangled. This is a largely experimentally unexplored regime where theoretical guidance suggests a fertile ground to potentially discover completely new types of correlated quantum behaviour, ranging from quantum spin liquids, where a local spin flip creates multiple exotic quasiparticles with fractional quantum numbers, to novel forms of magnetic order, with counter-rotating spin spirals or spontaneously formed periodic arrangements of spin vortices, to magnetic quasiparticles with topological properties. High applied magnetic fields will be used to stabilize novel magnetic phases with the potential to discover new universality classes for field-driven quantum phase transitions. Single crystals of spin-orbit dominated quantum materials, with key ingredients to exhibit correlated quantum behaviour, will be synthesized and their magnetic states will be probed using the latest advances in neutron and resonant x-ray diffraction and spectroscopy techniques that allow unprecedented high-sensitivity mapping of the static and dynamic correlations in space and time (or momentum and energy). The results will be compared with the latest theoretical models of many-body correlated quantum states with spin-orbit entanglement. This research will establish the experimental manifestation and manipulation of magnetic quasiparticles with topological character and help build a systematic understanding of the organizing principles that govern emergent quantum phases of matter in the unexplored regime of strong correlations and spin-orbit entanglement.","2500000","2018-10-01","2023-09-30"
"eQG","Exceptional Quantum Gravity","Hermann NICOLAI","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Motivated by the overwhelming success of symmetry concepts in formulating basic laws of physics, the present proposal (eQG) seeks to develop a new symmetry based approach to the problem of reconciling Quantum Mechanics and Einstein’s General Relativity into a consistent theory of Quantum Gravity, regarded by many as the greatest challenge of contemporary theoretical physics. The need for such a theory is most pressing for the resolution of black hole singularities and the Big Bang, but it is equally crucial to the search for a consistent UV completion of the Standard Model of Particle Physics and the unification of the fundamental interactions. eQG aims to tackle this problem from a new perspective, bringing together very different strands of development: on the one hand, by paying particular attention to recent advances in our understanding of cosmological singularities and the evidence for novel infinite-dimensional duality symmetries near the singularity that has emerged in supergravity and string theory, and to recent progress in formulating ‘exceptional geometries’ transcending Riemannian geometry; on the other hand, by exploiting insights from modern canonical quantisation towards a better understanding of the basic degrees of freedom and the dynamics of quantum space-time. The main focus of eQG will be the ‘maximally extended’ exceptional hyperbolic Kac–Moody symmetry E10, whose uniquely distinguished status makes it a prime candidate symmetry for unifying the known dualities of string and M theory, for a conceptually precise scenario of emergent (quantum) space and time near the singularity, and finally, for replacing supersymmetry as a guiding principle for unification. Consequently, the principal goal of eQG will be to explore how this symmetry can define a theory of quantum gravity, how it acts on its fundamental degrees of freedom, what the special features are of the quantised theory, and what physical predictions can be derived from it.","1918750","2017-12-01","2022-11-30"
"EQU","Exploring  the Quantum Universe","Jan Ambjørn","KOBENHAVNS UNIVERSITET","""One of the main unsolved problems in theoretical  physics today is to reconcile the theories of  general relativity and quantum mechanics. The starting point of this proposal is a new background-independent theory of quantum gravity, which has been constructed from first principles as a sum over space-time histories and has already passed its  first non-trivial tests. The theory can be investigated analytically as well as by Monte Carlo simulations. The aim is to verify that it is a viable theory of quantum gravity. Thus we want to show that it has the correct long-distance behaviour (classical Einstein gravity) and to investigate its short-distance behaviour in detail. We expect new physics to show up at the shortest distances, physics which might help us understand the origin of our universe and why the universe looks the way we observe today.""","2187286","2012-07-01","2017-06-30"
"EQUEMI","Entanglement and Quantum Engineering with optical Microcavities","Jakob Reichel","SORBONNE UNIVERSITE","I propose to leverage the unique properties of optical fiber Fabry-Perot (FFP) microcavities pioneered by my group to advance the field of quantum engineering. We will take quantum-enhanced measurement from its current proof-of-principle state to a true metrological level by applying cavity-based spin squeezing to a compact atomic clock, aiming to improve the clock stability beyond one part in 10^-13 in one second. In a new experiment, we will generate multiparticle entangled states with high metrological gain by applying cavity-based entanglement schemes to alkaline earth-like atoms, the atomic species used in today’s most precise atomic clocks. In a second phase, a miniature quantum gas microscope will be added to this experiment, creating a rich new situation at the interface of quantum information, metrology, and cutting-edge quantum gas research. Finally, we will further improve the FFP microcavity technology itself to enable novel atom-light interfaces with a currently unavailable combination of strong coupling, efficient fiber coupling, and open access. This will open new horizons for light-matter interfaces not only in our experiments, but also in our partner groups working with trapped ions, diamond color centers, semiconductor quantum dots, carbon nanotubes and in quantum optomechanics.","2422750","2015-10-01","2020-09-30"
"EQUIARITH","Equidistribution in number theory","Philippe Michel","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The purpose of this proposal is to investigate from various perspectives some equidistribution problems associated with homogeneous spaces of arithmetic type: a typical problem (basically solved) is the distribution of the set of representations of a large integer by an integral quadratic form. Another harder problem is the study of the distribution of special points on Shimura varieties. In a different direction (linked with quantum chaos), the study of the concentration of Laplacian (Maass) eigenforms or of sections of holomorphic bundles is related to similar problems.  Given X such a space and G>L the underlying algebraic group and its corresponding lattice L,  the above questions boil down to studying the distribution of H-orbits x.H (or more generally H-invariant measures)on the quotient L\G for some subgroups H. This question may be studied different methods:  Harmonic Analysis (HA): given a function f on  L\G one studies the period integral of f along x.H. This may be done by automorphic methods. In favorable circumstances, the above periods are related to L-functions which one may hope to treat by methods from analytic number theory (the subconvexity problem).  Ergodic Theory (ET): one studies the properties of weak*-limits of the measures supported by x.H using rigidity techniques: depending on the nature of H, one might use either rigidity of unipotent actions or the more recent rigidity  results for torus actions in rank >1.  In fact, HA and ET are intertwined and complementary : the use of ET in this context require a substantial input from number theory and HA, while ET lead to a soft understanding of several features of HA. In addition, the Langlands correspondence principle make it possible to pass from one group G to another. Based on earlier experience, our goal is to develop these interactions systematically and to develop new approaches to outstanding arithmetic problems :eg. the subconvexity problem or the Andre/Oort conjecture.","866000","2008-12-01","2013-11-30"
"ESCADA","Energy-optimized Symmetric Cryptography by Algebraic Duality Analysis","Joan DAEMEN","STICHTING KATHOLIEKE UNIVERSITEIT","The main scientific contribution of this project will be a breakthrough in the understanding of cryptanalytic and side channel attacks of symmetric cryptosystems. We will do this by a unification of attacks that will a stepping stone to the holy grail of symmetric cryptography: provable security of concrete cryptosystems. The main real-world impact is that we will build cryptosystems that are much more efficient than those used today while having the same strength. Depending on the platform, higher efficiency translates to lower energy/power (in-body sensors, contactless payment cards etc.), but also lower latency (authentication for e.g car brakes or airbags) and/or lower heat dissipation (on-the-fly encryption of high bandwidth data streams). In a software implementation it simply means less CPU cycles per byte.

We build our cryptosystems as modes, on top of block ciphers or permutations. For these primitives we adopt the classical technique of iterating a simple round function (more rounds means more security but less efficiency). We focus on round functions of algebraic degree 2. Their relative simplicity will allow a unification of all cryptanalytic attacks that exploit propagation of affine varieties and polynomial ideals (their dual) through the rounds and to precisely estimate their success rates. Moreover, we will design modes that strongly restrict the exposure of the primitive(s) to attackers and that permit security reductions to specific properties of the underlying primitive(s) in a formally verifiable way. In comparison to the classical pseudorandom and ideal permutation models, this will allow reducing the number of rounds while preserving security with high assurance. We will also study side channel attacks of our round functions and ways to defend against them. We will make ASIC prototypes and implement novel efficient countermeasures against side channel attacks and use this to evaluate their effectiveness in practice.","2500000","2018-10-01","2023-09-30"
"ESig","Creating rigorous mathematical and computational tools that can summarise high dimensional data streams in terms of their effects","Terence John Lyons","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The Calculus of differential equations has proved to be a very powerful tool for describing the interrelationships between systems. That understanding has transformed many aspects of our world. This success has now reached an important limitation. As the systems we seek to understand increase in dimension and complexity, oscillatory and complex order information becomes much more important, and on normal computational scales the systems of interest often fail to fit the smooth Newtonian paradigm.

Mathematical tools that go beyond that smooth paradigm, and particularly Ito's extension of calculus to systems that have an additional Brownian component, have proved enormously valuable and have helped raised Stochastic Mathematics to the centre of the subject in a period of little more than 60 years. It has provided some of the most important applications of mathematics (spanning Neuroscience, Finance, Engineering, Image processing) over the second half of the last century.

In the late 1990s a new tool, the theory of rough paths, began to emerge. The mathematical aspects have been developed strongly by probability theorists to describe couplings between systems that are completely outside the Ito framework, by analysts to understand the solutions to certain non-linear vector valued PDEs, by classical analysts interested in the non-linear Fourier transform, and by those desiring to go beyond Monte Carlo techniques by choosing carefully chosen and representative scenarios instead of random ones. Several excellent texts now exist.

Key to this progress has been the combination of new definitions with strong rigorous results that underpin the concepts. The flow is still very active, and new tools, particularly the signature of a path, and the expected signature have a strong mathematical basis (eg. Annals of Math, Jan 2010) and potential as tools in pure and applied mathematics.

This proposal would allow the PI to create the momentum for completely new applications.","1814301","2012-06-01","2017-05-31"
"ESSOG","Extracting science from surveys of our Galaxy","James Jeffrey Binney","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","""The goal is to put in place the infrastructure required to extract the promised science for large surveys of our Galaxy that are underway and will culminate in ESA's Cornerstone Mission Gaia. Dynamical models are fundamental to this process because surveys are heavily biased by the Sun's location in the Galaxy. Novel dynamical models will be built and novel methods of fitting them to the data developed. With their help we will be able to constrain the distribution of dark matter in the Galaxy. By modelling the chemical and dynamical evolution of the Galaxy we expect to be able to infer much information about how the Galaxy was assembled, and thus test the prevailing cosmological paradigm. During the grant period we will be applying our tools to ground-based surveys, but the first version of the Gaia Catalogue will become available at the end of the grant period, and our goal is to have everything ready and tested for its prompt exploitation.""","1954460","2013-04-01","2018-03-31"
"ESTIA","Exponential sums, translation invariance, and applications","Trevor Dion WOOLEY","UNIVERSITY OF BRISTOL","Title: Exponential Sums, Translation Invariance, and Applications. 

Short : Exponential sums are fundamental throughout (analytic)  number theory, and are key to the robustness of applications in theoretical computer science, cryptography, and so on. They are the primary tool for testing equidistribution (apparent “randomness”) of number theoretic sequences. For a century, bounds for such sums of degree 3 or more have fallen far short of those conjectured to hold.

The landscape for exponential sums changed decisively in late 2010, when the proposer devised the “efficient congruencing” method. As a result, mean value estimates associated with translation invariant systems are now within a whisker of the main conjectures. Very significant progress has resulted in such Diophantine applications as Waring's problem, the validity of the Hasse principle for systems of diagonal equations, and equidistribution of polynomial sequences mod 1.

It is little understood in the wider community that efficient congruencing offers a fundamentally new approach to estimating moments of Fourier coefficients of wide generality, with hitherto inaccessible applications. We propose:

(i) to generalise efficient congruencing to approximately translation invariant systems, and explore consequent applications to Diophantine problems such as Waring's problem, restriction problems from discrete Fourier analysis, and bounds for the Riemann zeta function within the critical strip;

(ii) to extend the method to the multidimensional setting relevant to the investigation of local-global principles for spaces of rational morphisms from rational curves to diagonal hypersurfaces;

(iii) to explore the application of efficient congruencing over function fields where the ground field is a finite field, in particular as a vehicle for establishing estimates of use in randomness extractors;

(iv) to investigate the potential use of higher degree translation invariance in generalising Gowers norms.","1873483","2016-09-01","2021-08-31"
"ESUX","Electron Spectroscopy using Ultra Brilliant X-rays - a program for the advancement of state-of-the-art instrumentation and science","Nils Ove Tor Mårtensson","UPPSALA UNIVERSITET","The progress of materials science depends critically on the access to advanced characterization methods. During the last decades there has been an almost revolutionary development of modern X-ray based tools. We are today at a turning point in the development of synchrotron radiation based electron spectroscopy, where the program of the PI has maintained a leading position since 20 years. New techniques are evolving parallel to the development of a new generation of ultra-brilliant synchrotron radiation (SR) facilities. Electron spectroscopy is one of the most important techniques for the advancement of materials science and is one of the corner stones for the research at SR facilities. It is of highest priority to introduce new types of instruments to push the spectroscopy into new domains of time, spatial, energy and angular resolution.

We have recently accomplished a break-through in this field with a new type of electron analyzer, the ArTOF instrument, capable of increasing the energy resolution down to the micro-eV range with a simultaneous increase of the transmission of almost three orders of magnitude, compared to the earlier instruments. In addition the emission angles of all electrons are determined, with high precision and within a wide cone. This allows us to obtain three dimensional electronic structure information in real time. The present ERC proposal defines an ambitious program to fully exploit the new possibilities in urgent fields of research: In situ time resolved electronic band structure of organic crystals for electronic applications, time resolved studies of 3D band structure of solids and new 2D materials (graphene, topological insulators), electron structure and dynamics of materials for solar cell applications, and in other important research fields. The research program will also adapt the new technique to take advantage of new opportunities opened by emerging ultra-brilliant X-ray sources.","2486128","2013-02-01","2018-01-31"
"ETMD_ICEC","Efficient pathways to neutralization and radical production enabled by environment","Srulek Moritz CEDERBAUM","RUPRECHT-KARLS-UNIVERSITAET HEIDELBERG","The relaxation of electronically excited systems like atoms and molecules embedded in environment is a key question in chemistry and photobiology and the underlying physical mechanisms have been widely studied. The applicant has predicted theoretically that there are two efficient electronic processes between a system and its neighbors even if the system itself is in its electronic ground state and does not have excess energy, making clear that an important gap exists in our current understanding of systems embedded in environment. There is extensive future potential in filling this gap and we aim at exploring and exploiting it in systems of physical, chemical, and biological interest. The discovered electron-transfer mediated decay (ETMD) [several variants of ETMD have been already verified experimentally] and intermolecular Coulombic electron capture (ICEC) mechanisms give rise to a plethora of surprising electronic phenomena relevant to many fields. In radiation damage, for instance, differently charged cations and electrons are produced by ionization and Auger processes, and ETMD and ICEC continue to be operative, neutralize cations and produce radicals in the environment also after the known mechanisms of radiation damage have ceased to operate. This leads to severe additional damage. Knowing the factors influencing the impact of ETMD and ICEC most, we can exploit this knowledge to be able to suppress or enhance this impact. Such a control is invaluable in many cases, may be even in radiation therapy. We are certain that the novel and fundamental ETMD and ICEC processes can be exploited to probe and control systems embedded in an environment. Such a breakthrough necessitates the advancement of current methodologies far beyond the state-of-the-art, and can only be achieved by the close collaboration of a highly motivated strong team of scientists over a long period of time. The support by the ERC will substantially contribute to the realization of this vision.","2500000","2016-10-01","2021-09-30"
"EUREC4A","Elucidating the Role of Clouds-Circulation Coupling in Climate","Sandrine Bony","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","This proposal focuses on two of climate science’s most fundamental questions: How sensitive is Earth's surface temperature to radiative forcing? and What governs the organization of the atmosphere into rain bands, cloud clusters and storms? These seemingly different questions are central to an ability to assess climate change on regional and global scales, and are in large part tied to a single and critical gap in our knowledge: A poor understanding of how clouds and atmospheric circulations interact.
To fill this gap, my goal is to answer three questions, which are critical to an understanding of cloud-circulation coupling and its role in climate: (i) How strongly is the low-clouds response to global warming controlled by atmospheric circulations within the first few kilometres of the atmosphere? (ii) What controls the propensity of the atmosphere to aggregate into clusters or rain bands, and what role does it play in the large-scale atmospheric circulation and in climate sensitivity? (iii) How much do cloud-radiative effects influence the frequency and strength of extreme events?
I will address these questions by organising the first airborne field campaign focused on elucidating the interplay between low-level clouds and the small-scale and large-scale circulations in which they are embedded, as this is key for questions (i) and (ii), by analysing data from other field campaigns and satellite observations, and by conducting targeted numerical experiments with a hierarchy of models and configurations.
This research stands a very good chance to reduce the primary source of the forty-year uncertainty in climate sensitivity, to demystify long-standing questions of tropical meteorology, and to advance the physical understanding and prediction of extreme events. EUREC4A will also support, motivate and train a team of young scientists to exploit the synergy between observational and modelling approaches to answer pressing questions of atmospheric and climate science.","3013334","2016-08-01","2021-07-31"
"EVOKES","Explosive Volcanism in the Earth System","Donald Bruce Dingwell","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","Volcanism, is a vital factor in the Earth system. Molten silicates are a major transport agent in the differentiation and interaction of lithosphere, hydrosphere, atmosphere, and biosphere. Further, the immediate consequences of volcanic eruptions on all scales - local, regional and global - are issues of direct practical relevance to mankind as they are measured in lives, infrastructure and the environment. Volcanism is the result of a complex interplay of physico-chemical processes operating at varying efficiencies during ascent, differentiation and eruption of magma. As a result, volcanic phenomena span a range from effusive to explosive. The largest explosive events are repeatedly responsible for global impact on the Earth System, yet it is precisely these events that, due to their explosive character, are relatively inaccessible for direct scientific investigation. A major opportunity in accessing such systems has been provided by recent technological advances permitting the experimental investigation of volcanism. Experimental volcanology operates directly under volcanic conditions of time, pressure, temperature, and state; a near-unique opportunity in the solid earth sciences. Based on experimental volcanology, this project aims to provide mechanistic models of magmatic/volcanic processes and their impact on the Earth System. Four priority areas are selected as those needing most urgent attention. These are: 1) Quantification of the rheology of magma/lava for parameterisation of stress-strain relationships in numerical simulations of eruptive events. 2) Mechanistic understanding explosive failure of magma for the interpretation of volcanic hazard monitoring. 3) Development of quantitative methods for inferring eruptive physics from the physico-chemical fossil records (thermal, magnetic, chemical) preserved in volcanic lavas. 4) Experimental characterisation of the physical, chemical and biological properties and impact of volcanic ash on the earth system.","2991058","2010-04-01","2017-07-31"
"EvoTrap","Mechanisms to emerge and replicate  the first sequence information of life in geothermal microfluidics of early Earth","Dieter BRAUN","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","Can we reconstruct in the lab the onset of molecular evolution? To trigger the autonomous emergence of the first oligonucleotide sequences, we will explore non-equilibrium boundary conditions and selective mechanisms to host the fast progressing prebiotic replication chemistry of oligonucleotides. We will explore novel water-fog microfluidic settings to boost the replication and selection of the first RNA sequences. The findings aims to enable the creation of primitive life forms in the lab, starting from simple molecules in heated rock pores of early Earth.

Autonomous replication and metabolism. We will expand our thermal gradient expertise to host three replication chemistries. Using 3D printed microfluidics, we will mimick conditions in pores on early Earth. Thermophoresis will select long over short strands, accumulate small food molecules and strands will be separated by thermal convection and novel mechanisms in water-air systems. With respective collaboration partners, we will drive the replication from RNA ribozymes (Joyce), base-by-base RNA replication (Szostak) and EDC activated DNA ligation (Richert) and monitor the results with Illumina sequencing and TOF LC/MS. The ligation will be also explored with Taq ligase since we expect a cooperative replication dynamics with hypercycle-like characteristics. Thermal gradients will drive early metabolism to boost RNA polymerization and select ATP over ADP to drive modern biochemistry.

Sequence selection in low pressure water-air systems. Oligonucleotides bind to water-air interfaces. and can be accumulated 800-fold by heat-driven capillary flows. Based on this, we expect interesting selection effects under microfluidic boiling, fog formation and recondensation dynamics. The settings are tested for sequence selective hydro-gelation of RNA/DNA and enhanced replication chemistry. The temperature of boiling water will be limited below 60°C by using air pressures <200mbar, mimicking very early Earth conditions.","2364500","2018-10-01","2023-09-30"
"EXACTYMER","ADVANCED NANOMEMBRANES FOR EXACT POLYMER PRODUCTION","Andrew LIVINGSTON","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","The production of synthetic polymers with precisely defined monomer sequences – exact polymers, which I call “exactymers” – is highly challenging. Iterative synthesis, in which specific monomers are added one-at-a-time to the end of a growing polymer chain, affords exquisite control over the final sequence, but requires accurate purification of the growing polymer with each and every cycle. EXACTYMER will create new super-stable, ultra-selective nanomembranes, with high permeances, enabling rapid, repeated purifications, which will transform exactymer fabrication. Multiple growing polymer chains will be attached to a central hub molecule to create a macromolecular homostar with enhanced molecular size, promoting accurate separation of the growing exactymer from reaction debris via nanomembrane processing. Automation and engineering will enable rapid, accurate and precise cycles of exactymer chain growth. EXACTYMER objectives will be achieved through curiosity-driven research into (1) the creation of nanomembranes with exquisite molecular selectivity between growing homostars and monomer plus reaction debris; (2) advancing the chemistry of iterative synthesis by creating strategies for step-wise growth of polyethers, polysiloxanes, and polyesters, and side chain functionalised monomers of these species; (3) combining iterative chemistry and nanomembranes together in an automated homostar nanofiltration platform, and; (4) exploring the use of exactymers in healthcare, nanotechnology and information storage. EXACTYMER will undertake pioneering research at the boundaries of membrane technology, polymer synthesis, process engineering and nanotechnology. The most profound anticipated outcome is a new capability to produce synthetic polymers, over 20 monomers in length, with exactly defined monomer sequences to an unprecedented accuracy, at multi-gram scale. New scientific insights will derive from the properties and performances of these newly accessible molecules.","2499814","2018-07-01","2023-06-30"
"EXCATRO","In-situ experiments on the chemical composition of high altitude aerosols and clouds in the tropical upper troposphere and lower stratosphere","Stephan Hans Paul Borrmann","JOHANNES GUTENBERG-UNIVERSITAT MAINZ","""Clouds and aerosols are the most important component of the global climate system, but the least understood. The general goal of EXCATRO is to enhance the understanding of TROPICAL clouds/aerosols at high altitudes, and in particular to quantify the ORGANIC and ANTHROPOGENIC contributions. This is of fundamental importance for the global climate and ozone chemistry since (1) the highest solar energy input occurs in the tropics, (2) the fastest (convective) transport of trace substances and pollutants from the ground towards the stratosphere takes place here, and (3) the highest tropical clouds largely control how much water vapor and aerosols enter the global stratosphere.

Most of the processes and effects of tropical high clouds/aerosols critically depend on the chemical composition of individual particles. As very few direct in-situ measurements are available, I propose with EXCATRO: (1) to devise state-of-the-art in-situ instrumentation for the study of cloud/aerosol particle chemical composition from the Russian high altitude aircraft M-55 """"Geophysica"""", and (2) to perform research flights within the Indian Monsoon and other tropical regions. In-situ, online-remote-controlled aerosol particle mass spectrometers as well as specific particle collection devices will allow measurements inside tropical (cirrus) clouds and aerosols as high as 20 km so as to gain essential insight into the processes underlying their origin, the formation of the stratospheric aerosol and of precipitation. Major emphasis is placed on the ORGANIC content of the measured aerosol, its sources and anthropogenic fraction, in particular from ground releases in the Indian Monsoon region and Southeast Asia.

The in-situ data will provide key input for the numerical simulation of clouds, and satellite product verification. EXCATRO will thus provide knowledge crucial to gaining a full picture of the role of clouds and aerosols in influencing the planet's atmospheric chemistry and climate.""","2748500","2013-03-01","2019-02-28"
"EXCHANGE","Magnetism at the time and length scale of the Exchange interaction","Theodorus Henricus Maria Rasing","STICHTING KATHOLIEKE UNIVERSITEIT","The aim of EXCHANGE is to achieve a breakthrough in the understanding of magnetism and magnetic phase transitions on the time and length scale of the exchange interaction, the strongest force in magnetism. This will be achieved by developing and applying novel, beyond the state-of-the-art femtosecond X-ray and picosecond THz techniques, in combination with laboratory based ultrafast optical techniques and close interaction with new theoretical developments.

Magnetism is essentially a phenomenon of angular momentum and the interpretation  of magnetic order is based on the concept of exchange interaction. So far, the understanding of the physics of magnetism, including its dynamics, has only been achieved for systems close to their thermodynamic equilibrium. Magnetism at the length and time scale of the exchange interaction, that is to say, at nanometer length and femtosecond time scales, is completely unknown. Yet, future magnetic data storage aims at Tbit densities switched at THz rates, exactly this regime.

Recent developments of a new generation of femtosecond X-ray and picosecond THz free electron lasers create the opportunity, now for the first time, to experimentally visualize the transfer of angular momentum under strongly nonequilibrium conditions and thereby provide a so far inaccessible view to the strongest and most fundamental force in magnetism, the exchange interaction. When successful, this will strongly advance the frontiers of knowledge in the Physics of Magnetism, with a high potential to impact contemporary technologies for recording and processing magnetically stored information.","2495180","2014-05-01","2019-04-30"
"EXCIPOL","Exciton-Polaritons: New Physics and Long Term Applications","Maurice Skolnick","THE UNIVERSITY OF SHEFFIELD","This proposal combines novel experimentation and physical insight with state-of-the-art advances in technology to establish the field of exciton-polariton physics in major new directions. The new physics takes advantage of unique polariton properties including very light mass, strong non-linearities, bosonic character and direct access to density, phase and quantum statistics. The major goals are:

1. Transform the field into the regime of non-classical polariton physics. Major steps forward will include the polariton blockade where one polariton prevents the passage of the next, and very fast 10-100 GHz single photon sources, opening the way to realisation of a variety of strongly correlated photon phenomena in a solid state system.
2. Achieve a quantum phase transition in a system with strong inter-particle interactions, with particular opportunities deriving from the non-equilibrium nature of the polariton system.
3. In the many particle regime, create non-dispersing polariton wave-packets, study collisions and create the first polariton circuits, capitalising on advantageous soliton and condensate properties.

As well as the polariton area, the project will impact on several broader fields: semiconductor physics in revealing new interaction phenomena on the nanoscale, quantum optics and information science in the realisation of very fast single photon sources and quantum circuit functions, and new high density collective phase physics towards exploitation as opto-electronic logic gates and circuits. Advances in technology will be crucial to enable the new directions. They will include fabrication of highly uniform cavities using innovation in crystal growth, the pioneering of a new type of polariton system, waveguide polaritons, and the use of open cavities to permit the application of very short wavelength periodic potentials. These technology goals are challenging but achievable, and have potential to enable major advances over the next 5 to 10 years.","2100000","2013-02-01","2018-01-31"
"EXCITON","Advanced Measurement and Control of Exciton Diffusion for Next Generation Organic Semiconductor Optoelectronics","Ifor David William Samuel","THE UNIVERSITY COURT OF THE UNIVERSITY OF ST ANDREWS","There is great interest in organic materials with semiconducting electronic properties.  This arises from both a scientific point of view (how can a plastic be a semiconductor?) and a technological point of view as these materials can be used to make light-emitting diodes, lasers and solar cells.  The performance of all these devices is strongly affected by exciton diffusion, a process that is little studied or understood (particularly compared with charge transport) largely because of the lack of reliable measurement techniques.  The purpose of this proposal is to make a breakthrough in the measurement, understanding and control of exciton diffusion in organic semiconductors, and so create a new generation of materials and devices with enhanced performance due to control of exciton diffusion.  The key elements of the study are first to develop and validate advanced measurements of exciton diffusion.  This will open up the whole topic of exciton “transport” and provide the tools for us (and others) to explore the physics of exciton diffusion and how it is affected by a range of factors relating to the structure of the materials and how they are processed.  The following phase of work will use information about the main factors affecting exciton diffusion to develop strategies for controlling it.  A particular challenge is to increase exciton diffusion which will then lead to improved efficiency of organic solar cells.  We aim to address this both by applying the structure-property relations we develop and by developing directional exciton transfer, including quantum coherent energy transfer.  This is an unconventional approach to improving organic solar cells, which could not only improve their efficiency, but also greatly simplify their structure, leading to a breakthrough in their manufacturability. Control of exciton diffusion arising from the proposed research will also lead to strategies for increasing the efficiency of organic light-emitting diodes and lasers.","2100000","2013-04-01","2019-03-31"
"ExCoMet","CONTROLLING AND MEASURING RELATIVISTIC MOTION OF MATTER WITH ULTRAINTENSE STRUCTURED LIGHT","Fabien, Hervé, Jean QUERE","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","Femtosecond lasers can now provide intensities such that the light field induces relativistic motion of large ensembles of electrons. The ultimate goal of this Ultra-High Intensity (UHI) Physics is the control of relativistic motion of matter with light, which requires a deep understanding of this extreme regime of laser-matter interaction. Such a control holds the promise of major scientific and societal applications, by providing ultra-compact laser-driven particle accelerators and attosecond X-ray sources. Until now, advances in UHI Physics have relied on a quest for the highest laser intensities, pursued by focusing optimally-compressed laser pulses to their diffraction limit. In contrast, the goal of the ExCoMet project is to establish a new paradigm, by demonstrating the potential of driving UHI laser plasma-interactions with sophisticated structured laser beams–i.e. beams whose amplitude, phase or polarization are shaped in space-time.
Based on this new paradigm, we will show that unprecedented experimental insight can be gained on UHI laser-matter interactions. For instance, by using laser fields whose propagation direction rotates on a femtosecond time scale, we will temporally resolve the synchrotron emission of laser-driven relativistic electrons in plasmas, and thus gather direct information on their dynamics. We will also show that such structured laser fields can be exploited to introduce new physics in UHI experiments, and can provide advanced degrees of control that will be essential for future light and particles sources based on these interactions. Using Laguerre-Gauss beams, we will in particular investigate the transfer of orbital angular momentum from UHI lasers to plasmas, and its consequences on the physics and performances of laser-plasma accelerators. This project thus aims at bringing conceptual breakthroughs in UHI physics, at a time where major projects relying on this physics are being launched, in particular in Europe.","2250000","2016-10-01","2021-09-30"
"EXMOLS","Excited electronic states in extended molecular systems","Richard Friend","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The directed movement of electronic excitations in molecular materials lies at the heart of photosynthesis and also in nanoscale synthetic materials systems used for electronic applications.  Efficient materials systems must span many length scales; from nm molecular dimensions, to the 10 nm length scale of Coulomb interactions at 300 K in molecular systems, to the macroscopic dimensions of biological structures and of synthetic electronic devices.  There is now tantalising evidence that efficient biological and synthetic systems use ultrafast coherent electronic state evolution to couple molecular and macroscopic length scales, which requires special structural arrangements over intermediate length scales of 10 nm and more. 
EXMOLS will develop a new platform to study and control electronic excitations in extended molecular systems using DNA  assembly methods to construct functional molecular semiconductor stacks. DNA-assembly takes the place here of the protein structure assembly of chromophores within photosynthetic systems. In contrast to current synthetic molecular systems that have little control beyond simple heterojunctions, these DNA-assembled structures will allow for the precise placement of molecules within stack-structures of dimension 5 nm or more, which will allow for the definition of precise electronic couplings and energetic landscapes, within extended artificial molecular systems.  
New transient optical spectroscopy will track wavefunction evolution from 10fs. These will allow for the study of a range of emergent electronic phenomena on the 5-100nm length scale including, charge delocalisation, coherent electron-hole separation, singlet exciton fission, resonant energy transfer across the organic-inorganic interface and topologically protected electronic excitations. 
EXMOLS is a fundamental science project, but will also deliver real design rules for practical molecular-scale devices, from solar cells, to LEDs, to spintronics, to solar fuels.","2499836","2015-10-01","2020-09-30"
"EXOCONDENSE","Climate Dynamics of Exoplanets with Condensible Atmospheres","Raymond Thomas PIERREHUMBERT","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Condensible substances, which undergo a phase change from gaseous to liquid or solid condensed form, have a profound impact on planetary atmospheres, and are central to the determination of most key aspects of a planet's climate. The three phases of water operating in Earth's present climate provide the archetype for condensible processes in climate dynamics, but the dawning age of exoplanet discovery and characterization requires that the understanding of phase change effects be expanded far beyond the situations familiar from the study of Earth's climate, or indeed of the climate of any Solar System planet.  The goal of this project is to pioneer the advances needed to understand condensible climate dynamics for the vastly broader range of condensible substances, thermodynamic and planetary configurations presented by the growing catalogue of exoplanets.  The emphasis will be on the smaller range of planets (super-Earth to Earth mass or size class), which need not have hydrogen-dominated atmospheres and therefore present a richer and highly challenging variety of possible condensible behavior. This class of planets includes all planets that are potentially habitable for Earthlike life, but even planets that are far from habitable shed light on essential features of planets in the Universe, and will be studied. The work will embrace both small scale buoyancy-driven turbulent convection and planetary scale circulations. Idealized numerical simulations, buttressed by theoretical analysis will be employed. Particular emphasis will be put on aspects of exoclimate that are amenable to probing by current observations and improved observational techniques likely to become available in the coming decade. Such properties include cloud properties observable through transit-depth spectra and dayside/nightside temperature and composition contrasts observable through phase curve observations.","2492565","2017-10-01","2022-09-30"
"EXOMOL","ExoMol: molecular line lists for exoplanet atmospheres","Charles Jonathan Penrose Tennyson","UNIVERSITY COLLEGE LONDON","The discovery of extrasolar planets was one of the major advances of the last two decades. Over 400 planets have now been detected and astronomers are beginning to characterise their composition and physical characteristics. To do this requires huge quantities of spectroscopic data most of which is not available from laboratory studies. The interdisciplinary ExoMol project will provide a
comprehensive solution to this problem by providing spectroscopic data on all the molecular transitions of importance in the atmospheres of exoplanets. This data will be widely applicable to other problems and will be used for studies on cool stars, brown dwarfs and circumstellar environments. ExoMol will also be used by scientists who study spectra
of hot molecules in a other situations such as combustions.

A mixture of first principles and empirically tuned quantum mechanical methods will be used to compute comprehensive and huge (up to 100 billion
transitions) line lists. Novel methodologies will be developed for treating larger molecules such as methane and nitric acid. The success of ExoMol
will rely on these developments and the use of state-of-the-art computing.

The ExoMol database will from part of the EU Virtual Atomic and Molecular Data Centre (VAMDC). A program to promote application of the data by a wide variety of users will be initiated.","2472032","2011-05-01","2016-04-30"
"EXOPLANETBIO","Exoplanet atmospheres as indicators of life: From hot gas giants to Earth-like planets","Ignas Snellen","UNIVERSITEIT LEIDEN","This is a proposal to use a new ground-breaking spectroscopic technique to study the atmospheres of extrasolar planets. Understanding planet atmospheric processes and their evolutionary histories is crucial for unambiguously identifying biomarker gases, and forms the main driver behind the enormous surge in exoplanet atmospheric research. 

I propose to lead a program using the new VLT CRIRES+ instrument, that will focus on the new ground-breaking developments in ground-based high-dispersion spectroscopy, in which my work plays a leading role. We successfully determined the dominant spectroscopically-active species in hot Jupiter atmospheres (e.g. Brogi, Snellen et al. Nature 2012), provided the first evidence for high altitude winds (Snellen et al. Nature 2010), and determined for the first time the spin-rotation rate of a young gas-giant planet (Snellen et al. Nature 2014) – pioneering a technique that combines high-dispersion spectroscopy with high-contrast imaging.

The new CRIRES+ spectrograph at the VLT (2017) will have a revolutionary impact in the field, changing the main focus of current atmospheric research from hot 1000-1500 K gas giants to cooler 400-700 K Neptunes and Super-Earths. With this new instrument, I will 1) make a large inventory of planet spin rates as function of planet mass and age, 2) probe the atmospheres of cool super-Earths above the cloud-deck for the first time, solving for their bulk compositions. 3) determine the vertical and longitudinal atmospheric temperature profiles of hot Jupiters, and obtain a complete inventory of the C and O bearing molecules in their upper atmospheres. 4) I will for the first time probe isotope-ratios in exoplanet atmospheres. This project will be an important stepping stone in developing high-dispersion spectroscopic techniques for studying Earth-like exoplanets with the European Extremely Large Telescope.","2300962","2016-09-01","2021-08-31"
"EXPANDERS","Expander Graphs in Pure and Applied Mathematics","Alexander Lubotzky","THE HEBREW UNIVERSITY OF JERUSALEM","Expander graphs are finite graphs which play a fundamental role in many areas of computer science such as: communication networks, algorithms and more.  Several areas of deep mathematics have been used in order to give explicit constructions of such graphs e.g. Kazhdan property  (T) from representation theory of semisimple Lie groups, Ramanujan conjecture from the theory of automorphic forms and more.  In recent years, computer science has started to pay its debt to mathematics: expander graphs are playing an increasing role in several areas of pure mathematics.  The goal of the current research plan is to deepen these connections in both directions with special emphasis of the more recent and surprising application of expanders to group theory, the geometry of 3-manifolds and number theory.","1082504","2008-10-01","2014-09-30"
"EXPRESSIVE","EXPloring REsponsive Shapes for Seamless desIgn of Virtual Environments 
be retained","Marie-Paule Renée Cani","INSTITUT POLYTECHNIQUE DE GRENOBLE","Despite our great expressive skills, we humans lack an easy way of communicating the 3D shapes we imagine, and even more so when it comes to dynamic shapes. Over centuries humans used drawing and sculpture to convey shapes. These tools require significant expertise and time investment, especially if one aims to describe complex or dynamic shapes. With the advent of virtual environments one would expect digital modeling to replace these traditional tools. Unfortunately, conventional techniques in the area have failed, since even trained computer artists still create with traditional media and only use the computer to reproduce already designed content.
Could digital media be turned into a tool, even more expressive and simpler to use than a pen, to convey and refine both static and dynamic 3D shapes? This is the goal of this project. Achieving it will make shape design directly possible in virtual form, from early drafting to progressive refinement and finalization of an idea. To this end, models for shape and motion need to be totally rethought from a user-centered perspective . Specifically, we propose the new paradigm of responsive 3D shapes – a novel representation separating morphology from isometric embedding – to define high-level, dynamic 3D content that takes form, is refined, moves and deforms based on user intent, expressed through intuitive interaction gestures.
Scientifically, while the problem we address belongs to Computer Graphics, it calls for a new convergence with Geometry, Simulation and Human Computer Interaction. In terms of impact, the resulting “expressive virtual pen” for 3D content will not only serve the needs of artists, but also of scientists and engineers willing to refine their thoughts by interacting with prototypes of their objects of study, educators and media aiming at quickly conveying their ideas, as well as anyone willing to communicate a 3D shape This project thus opens up new horizons for science, technology and society.","2498116","2012-04-01","2017-03-31"
"ExQuiSid","Extreme Quantum Matter in Solids","Christian PFLEIDERER","TECHNISCHE UNIVERSITAET MUENCHEN","Quantum stochastic processes in solids, representing many-body systems par excellence, are believed to lead to extreme forms of quantum entanglement and non-local correlations (extreme quantum matter), that offer a well-defined starting point for an understanding of a wide range of anomalous materials properties, as well as emergent electronic phases such as magnetically mediated superconductivity or partial spin and charge order. While overwhelming experimental evidence clearly suggests a breakdown of traditional concepts such as well-defined quasi-particle excitations, the striking present-day disagreement between experiment and theory may be traced to the lack of experimental information on the spectrum of quantum stochastic many-body processes in solids in the low-energy and low-temperature limit close to and far from equilibrium.

ExQuiSid will advance the understanding of the nature of extreme quantum matter in the most extensively studied model systems, notably simple magnetic materials (insulators and metals) tuned through a quantum phase transition. For the proposed studies my group has implemented a new generation of methods covering for the first time neutron spectroscopy with an unprecedented nano-eV resolution even under large magnetic fields, transverse-field vector magnetometry, calorimetry and transport down to milli-Kelvin temperatures, and, ultra-high purity single-crystal growth combined with advanced materials characterisation. 

ExQuiSid will (i) solve long-standing mysteries in model-systems of extreme quantum phase transitions, (ii) experimentally enable and permit pioneering studies on the creation, nature and classification of non-equilibrium quantum matter in solids at ultra-low energies and temperatures, and (iii) experimentally enable and permit pioneering studies of quantum matter driven periodically out of equilibrium to identify dynamical quantum instabilities and dynamical quantum phases such as many body localisation.","2500000","2018-06-01","2023-05-31"
"ExtComb","Extremal Combinatorics: existence, counting and typical structure","Daniela KUHN","THE UNIVERSITY OF BIRMINGHAM","A central theme of extremal combinatorics is the interplay and relationship between the parameters of combinatorial objects. The first and most immediate question which arises in this context is that of the (i) existence of objects with a given set of parameters. Once this has been answered, the next step is to seek for (ii) the number of such objects - i.e. to ask for a counting result. This is of central importance in the context of many combinatorial questions arising in statistical physics. A very effective approach here is to seek asymptotic results - rather than exact formulas. This asymptotic approach sometimes makes it possible to go even further and ultimately uncover the (iii) typical structure of the objects in such a given class. 
 
In this project, we will consider the above perspective with a focus on inter-related topics involving combinatorial designs, decompositions, Latin squares as well as matchings in graphs and hypergraphs. The project themes have close connections e.g. to statistical physics, probability, algebra and theoretical computer science.
 
A common feature of the structures considered in this proposal is that the constraints describing them are of a ""global nature"". This makes their study extremely challenging. However, recently initiated methods have opened up completely new avenues, bringing questions within reach that were considered inaccessible until now. (In fact, one of the objectives involves the study of algebraic structures which had been conjectured not even to exist.)

The aim of the project is the development of general tools and approaches which make the asymptotic study of such structures far more accessible. These tools will be mostly of a probabilistic nature. Indeed, the probabilistic perspective has already been the driving force behind recent advances which underpin the proposal. But it seems that overall, this development is still in its early stages - a situation we aim to address in the current project.","1797111","2019-01-01","2023-12-31"
"ExtendGlass","Extending the range of the glassy state: Exploring structure and property limits in metallic glasses","Alan Lindsay GREER","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Metallic glasses (MGs), among the most actively studied metallic materials, have attractive mechanical properties (high elastic limit) but show work-softening and lack ductility. Recent work suggests the as-cast state of MGs can be much altered by thermomechanical treatments: rejuvenation (to higher energy) offers improved plasticity (perhaps even desirable work-hardening); relaxation (to lower energy) offers access to ultrastable states. Work of the PI has just shown that even simple thermal cycling can induce rejuvenation comparable with that from heavy plastic deformation, while elastic stress cycling can accelerate annealing. The research aims to extend the range of glassy states and to explore the consequences of unusual states, particularly for mechanical properties and for phase stability/crystallization. One possible limit to rejuvenation is the onset of fast crystallization. This regime will be studied for its relevance to crystallization of melts of low glass-forming ability, of interest to fill a gap in existing crystal-growth theory and for application in phase-change memory. Nine work-packages address these and further issues: exploitation of inhomogeneity in MGs to improve properties and enable processing, e.g. to permit stress relief without accompanying undesirable embrittlement; probing the maximum extent of anisotropy in MGs and the links between anisotropic structure and flow. Complementing the many mechanical and structural studies, molecular-dynamics simulations will be used to identify local events relating to rejuvenation/relaxation, to characterize (at atomic level) the anisotropy induced by anelastic strain and viscoplastic flow, to characterize the processes at the solid/liquid interface in pure-metal systems to understand crystal-growth mechanisms, especially why growth of ccp metals is so fast (and glass-forming ability very low). From preliminary results, it is expected that properties can be widened much beyond those of as-cast MGs.","2434090","2016-10-01","2021-09-30"
"f-ex","f-block hydrocarbon interactions: exploration; exploitation","Polly ARNOLD","THE UNIVERSITY OF EDINBURGH","Understanding, controlling, and predicting the subtle interactions that hydrocarbons form with metals is a major challenge in molecular science, and a key technology enabler in areas such as homogeneous catalysis, drug recognition, polymer properties, and metal recovery. For the f-block, it is important due to the urgent need for clean access to critical elements such as neodymium, and the safe handling of nuclear waste. However, technical challenges of paramagnetism, radiotoxicity, and relativistic effects, make quantifying and exploiting f-block hydrocarbon interactions very hard using traditional methods or calculations alone. 

We have used organometallic systems to study two types of poorly understood hydrocarbon interactions with f-block metal cations: arene binding which is stronger, yet controversial in terms of its electronic demands, and neutral hydrocarbon C-H bonding which is weaker, yet crucially reaction controlling. 

f-ex sets out a new way to experimentally measure and define these subtle hydrocarbon interactions. It then exploits the stored electrons in the metal-arene motif as a new method to control these powerful Lewis acidic metals for new hydrocarbon C-element bond formation and inert hydrocarbon C-H bond cleavage, with the ultimate aim of viable, low-energy hydrocarbon functionalisations. 

Uniquely, we will extend our organometallic work to the more difficult transuranic elements, and exploit high pressure solution (and single crystal) work to enhance and interrogate intermolecular C-H binding. The targets of this combined study now offer high scientific impact by demonstrating fundamental bonding insight and ground-breaking structures and reactions.

Unprecedented new insight also derives from incorporating new techniques, e.g. high-pressure solution and single crystal work, and transuranic organometallic chemistry.","2456120","2017-10-01","2022-09-30"
"F-IMAGE","Seismic Functional Imaging of the Brittle Crust","Michel CAMPILLO","UNIVERSITE GRENOBLE ALPES","Despite the dramatic impact of earthquakes, the physics of their onset and the short-term behavior of fault are still poorly understood. Using existing high quality seismic observations, we propose to develop a novel functional imaging of the brittle crust to clarify not only structural properties but also the dynamics of faults. We will analyze spatio-temporal changes of elastic properties around fault zones to highlight the interplay between changes in the host rocks and fault slip. Imaging the damage structure around faults and its evolution requires new seismological methods. With novel methods to image the highly heterogeneous fault regions, we will provide multi-scale descriptions of fault zones, including their laterally variable thicknesses and depth dependence. In parallel we will image temporal changes of seismic velocities and scattering strength. External natural forcing terms (e.g. tides, seasonal hydrologic loadings) will be modeled to isolate the signals of tectonic origin.  This will also allow us to monitor the evolving seismic susceptibility, i.e. a measure of the proximity to a critical state of failure. Improved earthquake detection techniques using ‘deep machine learning’ methods will facilitate tracking the evolution of rock damage. The imaging and monitoring will provide time-lapse images of elastic moduli, susceptibility and seismicity. The observed short-time changes of the materials will be included in slip initiation models coupling the weakening of both the friction and the damaged host rocks. Laboratory experiments will shed light on the transition of behavior from granular (shallow fault core) to cohesive (distant host rock) materials. Our initial data cover two well-studied fault regions of high earthquake probability (Southern California and the Marmara region, Turkey) and an area of induced seismicity (Groningen). The derived results and new versatile imaging and monitoring techniques can have fundamental social and economic impacts.","2434743","2017-10-01","2022-09-30"
"FACT","Factorizing the wave function of large quantum systems","Eberhard Gross","THE HEBREW UNIVERSITY OF JERUSALEM","This proposal puts forth a novel strategy to tackle large quantum systems. A variety of highly sophisticated methods such as quantum Monte Carlo, configuration interaction, coupled cluster, tensor networks, Feynman diagrams, dynamical mean-field theory, density functional theory, and semi-classical techniques have been developed to deal with the enormous complexity of the many-particle Schrödinger equation. The goal of our proposal is not to add another method to these standard techniques but, instead, we develop a systematic way of combining them. The essential ingredient is a novel way of decomposing the wave function without approximation into factors that describe subsystems of the full quantum system. This so-called exact factorization is asymmetric. In the case of two subsystems, one factor is a wave function satisfying a regular Schrödinger equation, while the other factor is a conditional probability amplitude satisfying a more complicated Schrödinger-like equation with a non-local, non-linear and non-Hermitian “Hamiltonian”. Since each subsystem is necessarily smaller than the full system, the above standard techniques can be applied more efficiently and, most importantly, different standard techniques can be applied to different subsystems. The power of the exact factorization lies in its versatility. Here we apply the technique to five different scenarios: The first two deal with non-adiabatic effects in (i) molecules and (ii) solids. Here the natural subsystems are electrons and nuclei. The third scenario deals with nuclear motion in (iii) molecules attached to semi-infinite metallic leads, requiring three subsystems: the electrons, the nuclei in the leads which ultimately reduce to a phonon bath, and the molecular nuclei which may perform large-amplitude movements, such as current-induced isomerization, (iv) purely electronic correlations, and (v) the interaction of matter with the quantized electromagnetic field, i.e., electrons, nuclei and photons.","2443932","2019-09-01","2024-08-31"
"FAILFLOW","Failure and Fluid Flow in Porous Quasibrittle Materials","Gilles Pijaudier-Cabot","UNIVERSITE DE PAU ET DES PAYS DE L'ADOUR","This project focuses on fluid flow in porous materials with evolving microstructure in the context of civil engineering applications and geomechanics. When the distribution of cracks and the distribution of pore size evolve in concrete and rocks, the influence on the permeability in the case of a single or a multiphase fluid flow needs some in depth investigation. A recent review of state of the art in modelling progressive mechanical breakdown and associated fluid flow in heterogeneous rock shows that little is known on the coupled effects between micro cracking and the intrinsic permeability of a solid phase.  The present project intends to tackle this relationship between mechanical breakdown and associated fluid flow in the context of poromechanics extended to non local modelling. In particular, we will investigate how the internal length which plays a pivotal role at the inception and propagation of material failure may interact with the permeability, what enhanced Darcy-like relationship might be derived in order to apprehend such effects and how to model fluid flow in tight porous materials. The models will be extended to complex and multicomponent systems reproducing as closely as possible the behaviour of real fluids in order to understand and to describe the thermodynamical behaviour due to confinement such as modification of phase transitions and capillary condensation. The principal investigator of this project is a specialist in the field of continuum damage mechanics, failure due to strain and damage localisation. He has been the founder and among the major promoters of non local damage modelling, which is today a state of the art model in computational structural failure analyses. After a decade of research on durability problems for which he was elected at Institut Universitaire de France, his research interests recently turned toward petroleum engineering, the focus of the research team he joined two years ago at université de Pau.","1490200","2008-12-01","2013-11-30"
"FairSocialComputing","Foundations for Fair Social Computing","Krishna GUMMADI","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Social computing represents a societal-scale symbiosis of humans and computational systems, where humans interact via and with computers, actively providing inputs to influence and being influenced by, the outputs of the computations. Recently, several concerns have been raised about the unfairness of social computations pervading our lives ranging from the potential for discrimination in machine learning based predictive analytics and implicit biases in online search and recommendations to their general lack of transparency on what sensitive data about users they use or how they use them.

In this proposal, I propose ten fairness principles for social computations. They span across all three main categories of organizational justice, including distributive (fairness of the outcomes or ends of computations), procedural (fairness of the process or means of computations), and informational fairness (transparency of the outcomes and process of computations) and they cover a variety of unfairness perceptions about social computations. 

I describe the fundamental and novel technical challenges that arise when applying these principles to social computations. These challenges are related to operationalization (measurement), synthesis and analysis of fairness in computations. Tackling these requires applying methodologies from a number of sub-areas within CS, including learning, datamining, IR, game-theory, privacy, and distributed systems. 

I discuss our recent breakthroughs in tackling some of these challenges, particularly our idea of fairness constraints, a flexible mechanism that allows us to constrain learning models to synthesize fair computations that are non-discriminatory, the first of our ten principles. I outline our plans to build upon our results to tackle the challenges that arise from the other nine fairness principles. Successful execution of the proposal will provide the foundations for fair social computing in the future.","2487500","2018-07-01","2023-06-30"
"FASTER","""Fundamental Studies of the Sources, Properties and Environmental Behaviour of  Exhaust Nanoparticles from Road Vehicles""","Roy Harrison","THE UNIVERSITY OF BIRMINGHAM","""Despite intensive abatement efforts, airborne particulate matter remains a major public health issue with costs across the European Union estimated at 600 billion euros in 2005.  Road traffic remains one of the major sources of particulate matter, and diesel emissions are by far the largest source of atmospheric nanoparticles in urban areas.  Semi-volatile organic compounds emitted largely in the condensed matter phase are a major component of diesel emissions, and as primary particles are advected from their road traffic source, the semi-volatile compounds vaporise and are oxidised, forming a greater mass of secondary organic aerosol (SOA).  However, the semi-volatile compounds are extremely poorly characterised as they are not resolved by traditional gas chromatographic methods, presenting an unresolved complex mixture (UCM).  For this reason, despite being a major precursor of SOA, such compounds are often poorly represented or completely omitted from atmospheric chemistry-transport models.  This proposal is concerned with applying new two dimensional gas chromatographic methods to characterisation of the UCM at a molecular level which will be followed by studies of the physico-chemical properties of representative components of the semi-volatile emissions.  The very abundant nucleation nanoparticle mode of diesel emissions is comprised almost entirely of semi-volatile organic material and hence these particles are progressively lost from the atmosphere by evaporation.  Until now, there has been insufficient knowledge of the properties of the semi-volatile components to model this behaviour reliably.  Such processes will be quantified through both controlled laboratory studies and carefully designed field measurements.  Numerical models on both a street canyon and a neighbourhood (5x5 km) scale will be developed to simulate the key processes, such that spatial patterns and size distributions will be predicted, and compared with independent measurements.""","2394959","2013-04-01","2018-03-31"
"FASTER","Faster magic-angle spinning leads to a resolution revolution in biological solid-state NMR","Beat Hugo Meier","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Solid-state NMR has recently made a significant impact on structural biology by providing atomic-resolution structures of several, previously uncharacterized proteins. A particularly relevant example is the Amyloid-beta (Aβ) peptide linked to Alzheimer’s disease where we determined the atomic-resolution structure of Aβ(1-42) and of the Osaka mutant of Aβ(1-40).
A spectral resolution revolution is now in reach that will enable solid-state NMR to address new frontiers in structural biology. The applications mentioned above are based on 13C-detected spectroscopy. Proton-detected experiments, although clearly more sensitive thanks to the high gyromagnetic ratio of 1H, have found few applications so far, due to the poor resolution of 1H spectra caused by the 1H-1H dipolar interaction. The proton resolution can be enhanced by employing faster rotation of the sample, i.e. higher MAS (magic-angle spinning) frequencies. Presently accessible MAS frequencies are already faster than the ones of any other man-made object. A significant improvement is still attainable in our view. Increasing the MAS frequency to 200-250 kHz will improve the spectral quality to favorably compare with solution NMR for larger proteins, including fully protonated systems. In addition, the amount of sample required is reduced by almost two orders of magnitude, to approx. 100 μg, compared to the about 10 mg needed in 13C-detected experiments. This removes an important bottleneck in sample-preparation. The resolution and sensitivity gain will allow the structural characterization of e.g. disease-relevant amyloids or membrane proteins with higher precision. Moreover, this approach will enable the investigation of complex systems, which presently elude structural characterization. The resolution revolution brought about by fast spinning shall thus represent a breakthrough since it will open new horizons for solving urgent biological and medical questions.","2173375","2017-10-01","2022-09-30"
"FAULT-ADAPTIVE","Fault-Adaptive Monitoring and Control of Complex Distributed Dynamical Systems","Marios Polycarpou","UNIVERSITY OF CYPRUS","""The emergence of networked embedded systems and sensor/actuator networks has facilitated the development of advanced monitoring and control applications, where a large amount of sensor data is collected and processed in real-time in order to activate the appropriate actuators and achieve the desired control objectives. However, in situations where a fault arises in some of the components (e.g., sensors, actuators, communication links), or an unexpected event occurs in the environment, this may lead to a serious degradation in performance or, even worse, to an overall system failure. There is a need to develop a systematic framework to enhance the reliability, fault-tolerance and sustainability of complex distributed dynamical systems through the use of fault-adaptive monitoring and control methods. The work proposed here will contribute to the development of such a framework with emphasis on applications related to critical infrastructure systems (e.g., power, water, telecommunications and transportation systems). It will provide an innovative approach based on the use of networked intelligent agent systems, where the state of the infrastructure is monitored and controlled by a network of sensors and actuators with cooperating agents for fault diagnosis and fault tolerant control. A hierarchical fault diagnosis architecture will be developed, with neighbouring fault diagnosis agents cooperating at a local level, while transmitting their information, as needed, to a regional monitoring agent, responsible for integrating in real-time local information into a large-scale “picture” of the health of the infrastructure. A key motivation is to exploit spatial and temporal correlations between measured variables using learning methods, and to develop the tools and design methodologies that will prevent relatively “small” faults or unexpected events from causing significant disruption or complete system failures in complex distributed dynamical systems.""","2035200","2012-04-01","2018-03-31"
"FCC","Functional Coordination Chemistry","David Parker","UNIVERSITY OF DURHAM","To address critical problems in the natural sciences by developing the chemistry of metal coordination complexes, harnessing the unique ground and excited state properties of the lanthanide (III) ions to impart the required function into the complexes and their conjugates. This requires PI-led collaborative work in molecular design and instrumentation development.

Objectives
1. To measure changes in the concentration of essential bioactive species in particular compartments of plant and animal cells, including chiral species. This requires the creation of targeted luminescent probes that relay an optical signal to the observer, signalling changes in the concentration of these species, with high spatial and temporal control. In parallel, circularly polarised emission microscopy will be pioneered.
2. To enable 19F magnetic resonance studies to be undertaken much more widely by devising functional paramagnetic 19F-magnetic resonance probes, with fast relaxation and enhanced chemical shift dispersion.","2498552","2011-06-01","2016-05-31"
"FCCA","Five Challenges in Computational Anatomy","Darryl Holm","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","New medical imaging technologies encode human anatomy in a wide variety of data structures. Computational Anatomy (CA) offers an approach to synthesize this plethora of data by comparison of anatomical features using smooth invertible transformations specific to the data structure.
This proposal is for work to develop new mathematical and numerical methods for image analysis in the framework of CA, aimed at meeting the following five challenges in image analysis for the comparison and interpolation of shapes in biomedical images of the heart and the brain.
1. Data structure: Develop a unified approach for registering images encoded in a wide variety of data structures. The unifying concept in our approach is the momentum map, a fundamental concept from the theory of Lie group transformations.
2. Data fusion: After placing the transformations of the variety of data structures into the same conceptual framework using momentum maps, synthesise (fuse) their multiple modalities of information by accounting for the different transformation properties of the different data structures under smooth invertible maps.
3. Multiple resolutions: Develop the momentum map framework to enable registration of data at multiple resolutions by concatenating the Lie group transformations that define the momentum maps.
4. Time-varying (4D) images:  Treat time-varying images in this geometric framework by matching snapshots in time using geodesic splines that interpolate the image snapshot from one time to another. Quantity the effects of noise and uncertainty in 4D image analysis.
5. Changes in image topology: Extend the transformative approach to allow changes in topology in passing between images by using the method of metamorphosis.
Simply put, the five challenges are: to 1. register images of different data structures and 2. combine them, even at 3. different resolutions; then do the same things with 4. splines and 5. metamorphosis, including noise.","1740000","2011-05-01","2017-04-30"
"FEALORA","""Feasibility, logic and randomness in computational complexity""","Pavel Pudlák","MATEMATICKY USTAV AV CR V.V.I.","""We will study fundamental problems in complexity theory using means developed in logic, specifically, in the filed of proof complexity. Since these problems seem extremely difficult and little progress has been achieved in solving them, we will prove results that will explain why they are so difficult and in which direction theory should be developed.

Our aim is to develop a system of conjectures based on the concepts of feasible incompleteness and pseudorandomness. Feasible incompleteness refers to conjectures about unprovability of statements concerning low complexity computations and about lengths of proofs of finite consistency statements. Essentially, they say that incompleteness in the finite domain behaves in a similar way as in the infinite. Several conjectures of this kind have been already stated. They have strong consequences concerning separation of complexity classes, but only a few special cases have been proved. We want to develop a unified system which will also include conjectures connecting feasible incompleteness with pseudorandomness. A major part of our work will concern proving special cases and relativized versions of these conjectures in order to provide evidence for their truth. We believe that the essence of the fundamental problems in complexity theory is logical, and thus developing theory in the way described above will eventually lead to their solution.""","1259596","2014-01-01","2018-12-31"
"FEEC-A","Finite Element Exterior Calculus and Applications","Ragnar Winther","UNIVERSITETET I OSLO","""The finite element method is one of the most successful techniques for designing numerical methods for systems of partial differential equations (PDEs). It is not only a methodology for developing numerical algorithms, but also a mathematical framework in which to explore their behavior. The finite element exterior calculus (FEEC) provides a new structure that produces a deeper understanding of the finite element method and its connections to the partial differential equation being approximated. The goal is to develop discretizations which are compatible with the geometric, topological, and algebraic structures which underlie well-posedness of the partial differential equation. The phrase FEEC was first used in a paper the PI wrote for Acta Numerica in 2006, together with his coworkers, D.N. Arnold and R.S. Falk. The general philosophy of FEEC has led to the design of new algorithms and software developments, also in areas beyond the direct application of the theory. The present project will be devoted to further development of the foundations of FEEC, and to direct or indirect use of FEEC in specific applications. The ambition is to set the scene for a nubmer of new research directions based on FEEC by giving ground-braking contributions to its foundation. The aim is also to use FEEC as a tool, or a guideline, to extend the foundation of numerical PDEs to a variety of problems for which this foundation does not exist. The more application oriented parts of the project includes topics like numerical methods for elasticity, its generalizations to more general models in materials science such as viscoelasticity, poroelasticity, and liquid crystals, and the applications of these models to CO2 storage and deformations of the spinal cord.""","2059687","2014-02-01","2019-01-31"
"FEEDBACK","ACCRETING BLACK HOLES AND COSMIC FEEDBACK","Andrew Christopher Fabian","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","""The black hole at the centre of every massive galaxy bulge appears to
play a controlling role in the final stellar mass of that bulge. This
is especially true for the most massive galaxies at the centres of
cool core clusters where X-ray observations clearly show energy being
fed back into the surrounding gas which would otherwise cool and form
more stars at high rates. Although the total energy budget for this
phenomenon is clear, the details by which the processes operate are
not. This proposal aims to further our understanding of how accreting
black holes generate and feed back energy into the surrounding gas,
how it is dissipated and establishes a heating / cooling balance and
how the black hole, galaxy and feedback co-evolve. The work will focus
on the broad iron line and X-ray reflection spectral and reverberation
components which we have discovered in the spectra of accreting black
holes, on X-ray imaging spectroscopy of cool core clusters of galaxies
and on the filamentary nebulosity commonly observed around the central
galaxy in such clusters.  The work will involve X-ray observations
together with data from other wavebands where relevant. We shall also
consider theoretical aspects of the problem and  interpret
the whole phenomenon. The current X-ray telescopes (Chandra, XMM and
Suzaku) are now mature and we understand how to use them optimally,
particularly with longer exposures.  Major new data are coming from
NuSTAR (launched 2012), which is now opening up the hard X-ray band,
and from ASTRO-H (to be launched 2014) which will open up the iron-K
band to high spectral resolution and thus direct measurement of the
velocity field of the hot and cold components of AGN Feedback.""","2498064","2014-02-01","2019-01-31"
"FEMMES","FerroElectric Multifunctional tunnel junctions for MEmristors and Spintronics","Agnès Yvonne Georgette Barthélémy","UNIVERSITE PARIS-SUD","The aim of the project FEMMES is to study the interplay between charge/spin tunneling and ferroelectricity in Ferroelectric Tunnel Junctions (FTJs) composed of two electrodes separated by a ferroelectric tunnel barrier. It will address fundamental issues such as the influence of interfaces and small thicknesses on the ferroelectricity, the dependence of the charge and spin tunneling on the ferroelectric orientation (electroresistance), the impact of the ferroelectricity of the barrier on the magnetism and spin polarisation of the electrodes.
I  propose to exploit FTJs  and the intrinsic low-power of “ferroelectric writing”, to obtain:
1) a low-power electrical control of spin polarized electron sources for spintronics in FTJs with magnetic electrodes.
2) memristive FTJs mimicking the plasticity of synapses for an exploitation in neuromorphic analog circuits.
This will be achieved by a synergetic approach combining:
- ab initio calculations to determine the most appropriate combination of ferroelectric materials and     electrodes and to obtain a complete description of the impact of the ferroelectric character on the transport properties.
- the growth of selected heterostructures and extensive characterization of their structural, ferroelectric and magnetic properties.
- the patterning of junctions (at the µm and nm scale) and the investigation of their transport and magnetotransport properties.
- the evaluation and optimization of the potential of FTJs as electrically tunable spin sources for spintronics and memristors for neuromorphic circuits.","2148796","2011-04-01","2016-03-31"
"FEMTO/NANO","Nonequilibrium phenomena at femtosecond/nanometer scale","Mikhail Katsnelson","STICHTING KATHOLIEKE UNIVERSITEIT","Nanoscale objects like magnetic molecules and clusters, quantum dots, and graphene, bring us novel physical concepts. Recently, the temporal scale of the order of tens of femtoseconds (femtoscale) became available and new physical phenomena associated with this time scale, such as laser-induced electron and magnetic phase transitions, were discovered. The theoretical background for understanding this new physics is still rather poor. This temporal scale, like the spatial nanoscale is intermediate between micro- and macroworld making the standard approaches developed in micro- and macrophysics not suitable anymore.  Essentially new theoretical ideas and methods are necessary for its description, especially in a combination with the spatial nanoscale. The aim of this project is to provide such a background via detailed studies of key problems, and open the way for new practical applications.

Based on a combination of analytical and computational theoretical approaches (most of them were suggested by us), we plan to study systematically time-dependent many-body phenomena at the femto/nano scale. We will develop a theory of nonequilibrium magnetic interactions and spin dynamics of nanosystems and apply it to molecular magnets and clusters at metal surfaces and at graphene. We will study the physics of graphene and “artificial graphene” (array of semiconducting quantum dots) in strongly time-dependent electric fields (laser-induced ultrafast dynamics).

This list covers the crucial problems in this new field (nonequilibrium spin dynamics, calculation of response functions crucial for pump-probe experiments, new physics in highly excited graphene and graphene-like systems) and the success of the project will represent a breakthrough in our understanding of the nanoworld, with very important perspectives for applications, namely, for the drastic miniaturization of basic elements and enhancing speed of basic operations in electronics.","1637630","2013-10-01","2018-09-30"
"FERLODIM","Atomic Fermi Gases in Lower Dimensions","Christophe Salomon","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The complex interplay between Coulomb repulsion and Fermi statistics in two dimensional systems is responsible for some of the most dramatic phenomena encountered in solid state physics (High critical temperature superfluidity, Fractional Quantum Hall Effect,..). However, despite decades of efforts, many questions regarding these systems are still unsolved.  In FERLODIM, we plan to take advantage of recent progress in ultracold gases, to simulate several fundamental Hamiltonians describing these many-body systems in 1 and 2 dimensions. We will realize two ultra-cold atom machines allowing for a full characterization of the many-body wave function of an ensemble of interacting fermions in periodic potentials, called optical lattices.  Our experiments will rely on a high resolution imaging system allowing both for single atom detection and the possibility of tailoring optical potentials of arbitrary shape and geometry.  This unique design will allow us to address a variety of physical situations, depending on the geometry of the light induced potentials. One-dimensional problems will be addressed, from spin chains to Luttinger liquids.  In pure two dimensional configurations, we will investigate the link between the repulsive Hubbard model, superfluidity and the Mott insulator transition, as well as frustration effects in periodic potentials. Finally we will explore the physics of interacting fermions under rotation in the lowest Landau level, and the connection with fractional Quantum Hall systems.","2050000","2009-01-01","2013-12-31"
"FFlowCCS","Fluid Flow in Complex and Curved Spaces","Hans Jürgen Herrmann","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","In many natural and industrial situations, fluids in cavities, membranes or pipes of complex shape grow as well as modify the structures through which they flow. This leads to important challenges both fundamental, like biological morphogenesis, and practical, like the motion of nano-electromechanical systems (NEMS). We seek to substantially advance the understanding of the resulting shapes and instabilities. Our approach will focus on numerical methods, validated through theoretical and experimental analysis.
Mathematically fluid-structure interactions involve ambitious moving boundary problems, where structure and fluid flow feedback on one another in complex ways. Detailed analysis requires precise modeling of coupling between very strongly de¬forming elasto-plastic solids and fluid flow in intricately curved spaces and solving both iteratively many times. To address this computational challenge, significant innovations will be implemented, including the use of novel erosion laws, the insertion of spatial curvature and metric directly into the equations of motion of the fluid, and special methods to handle the singular behaviour at kinks and constrictions. Our fluid solvers will be new variants of Lattice Boltzmann Models (LBM) coupled to temperature and concentration fields. The accuracy of the methods will be quantitatively validated by experiments.
An unconventional hydrodynamic formulation for electronic currents will provide big advantages. We will develop LBM solvers for quantum and relativistic fluids and in particular create a Lattice Wigner model and couple it to the molecular dynamics of the support.
Our method will open new horizons for the design of continuously regenerating filters, for shape optimi¬zation of heat exchangers and catalysts and for the engineering of electronic devices. Our approach will also shed light on sand avalanches in oil extraction, on aspects of folding in living matter, and on electromechanical instabilities.","2200000","2013-01-01","2017-12-31"
"FilAtmo","Laser Filamentation for Probing and Controlling Atmospheric Processes","Jean-Pierre, Louis Wolf","UNIVERSITE DE GENEVE","The prevention of damaging weather phenomena like floods, hail and lightning strikes has been a dream for centuries. We propose a highly innovative approach relying on laser filaments for both triggering and guiding lightning and produce water condensation in the atmosphere. Filaments are self-sustained light strings of typ. 100 um diameter and hundreds of meters length in air, bear very high intensities and are electrically conductive through molecular ionization.

The filamentation process in air was considered until recently as resulting from the dynamic balance between the optical Kerr effect and defocusing by the self-generated plasma. Our unexpected discovery, last year, that filaments are governed by negative higher-order Kerr effect (HOKE), opened both basic physical questions about the stabilization mechanism and new opportunities to optimize the envisioned applications to lightning triggering and cloud condensation.

We propose first to study in the laboratory the physical origin of the alternated signs of HOKE in gases, which are suspected to stem from populated bound states. Coherently controlling these bound states in rare gases and air will allow us to tailor the HOKE inversion, and consequently to control the filament process itself. Optimal pulse shapes will then be sought by adaptive (closed loop) techniques to maximize the plasma density and lifetime in filaments for lightning control applications. Similar coherent control approaches will be performed for optimizing the complex photochemistry that leads to water vapor condensation in the atmosphere.

We will then apply the optimal pulse shapes to real scale field experiments. To this end we intend to use the mobile TW laser from the Teramobile consortium, which we are part of, in order to perform two extensive campaigns for real-scale lightning control (in Lugano) and haze/cloud generation (in Geneva). These experiments will constitute the first coherent manipulation of atmospheric process.","2403425","2012-07-01","2017-06-30"
"FireBar-Concept","MULTI-CONCEPTUAL DESIGN OF FIRE BARRIER: A SYSTEMIC APPROACH","Serge Bourbigot","UNIVERSITE DES SCIENCES ET TECHNOLOGIES DE LILLE - LILLE I","The development of science and technology provides the availability of sophisticated products but concurrently, increases the use of combustible materials, in particular organic materials. Those materials are easily flammable and must be flame retarded to make them safer. In case of fire, people must be protected by materials confining and stopping fire. It is one of the goals of the FireBar-Concept project to design materials and assembly of materials exhibiting low flammability, protecting substrates and limiting fire spread.  
The objective of FireBar-Concept is to make a fire barrier formed at the right time, at the right location and reacting accordingly against thermal constraint (fire scenario). This fire barrier can be developed in several ways according to the chemical nature of the material and/or of its formulation:

- Heat barrier formed by inherently flame retarded materials (e.g. mineral fibers, ceramic …) and exhibiting low thermal conductivity (note the assembly of those materials can also provide low thermal conductivity controlling porosity and its distribution)
- Evolution of reactive radicals poisoning the flame and forming a protective ‘umbrella’ avoiding the combustion of the material
- Additives promoting charring of the materials and forming an expanding carbonaceous protective coating or barrier (intumescence)
- Additives forming a physical barrier limiting mass transfer of the degradation products to the flame

The FireBar-Concept project is multidisciplinary and it requires expertise in material science, chemical engineering, chemistry, thermal science and physics. The approach is to make 5 actions linked together by transverse developments (3) according to this scheme: (i) fundamentals of fire barrier, (ii) multi-material and combination of concepts, (iii) modeling and numerical simulation, (iv) design and development of experimental protocols and (v) optimization of the systems.","2429988","2016-01-01","2020-12-31"
"FIRM","Mathematical Methods for Financial Risk Management","Halil Mete Soner","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Since the pioneering works of Black &amp; Scholes, Merton and Markowitch, sophisticated quantitative methods are being used to introduce more complex financial products each year. However, this exciting increase in the complexity forces the industry to engage in proper risk management practices. The recent financial crisis emanating from risky loan practices is a prime example of this acute need. This proposal focuses exactly on this general problem. We will develop mathematical techniques to measure and assess the financial risk of new instruments. In the theoretical direction, we will expand the scope of recent studies on risk measures of Artzner et-al., and the stochastic representation formulae proved by the principal investigator and his collaborators. The core research team consists of mathematicians and the finance faculty. The newly created state-of-the-art finance laboratory at the host institution will have direct access to financial data. Moreover, executive education that is performed in this unit enables the research group to have close contacts with high level executives of the financial industry. The theoretical side of the project focuses on nonlinear partial differential equations (PDE), backward stochastic differential equations (BSDE) and dynamic risk measures. Already a deep connection between BSDEs and dynamic risk measures is developed by Peng, Delbaen and collaborators. Also, the principal investigator and his collaborators developed connections to PDEs. In this project, we further investigate these connections. Chief goals of this project are theoretical results and computational techniques in the general areas of BSDEs, fully nonlinear PDEs, and the development of risk management practices that are acceptable by the industry. The composition of the research team and our expertise in quantitative methods, well position us to effectively formulate and study theoretical problems with financial impact.","880560","2008-12-01","2013-11-30"
"FIRST LIGHT","Early Star-Forming Galaxies and Cosmic Reionisation","Richard Ellis","UNIVERSITY COLLEGE LONDON","Several hundred million years after the Universe was born the first stellar systems began to shine. Energetic photons from early hot stars, free from enrichment by heavy elements, reionised the hydrogen in deep space. Ambitious observational facilities will directly chart this final frontier in cosmic history and any insight we can obtain now will be invaluable in future planning. Key questions include: what is the duration of this reionisation period; was this `cosmic dawn’ a brief or extended process; and what physical processes governed the subsequent evolution of these early galaxies? This proposal aims to trace the history and physics of cosmic reionisation by fully characterising the star-forming galaxy population during and towards the end of the reionisation era. The proposed program has three complementary themes. (i) Tracing the duration of the reionisation process by analysing diagnostic nebular emission lines in the spectra of early galaxies using radiative transfer calculations; the proposed measures can be usefully compared with independent signatures of cold gas during similar epochs determined by the European LOFAR interferometer. (ii) Determining whether star-forming galaxies are the sole agent of reionisation by addressing key uncertainties relating to the number of ionising photons they produce and the fraction that escape; this requires detailed spectroscopy of gravitationally-lensed examples. (iii) Inferring the abundance of the earliest galaxies whose direct detection is beyond reach of current facilities. Stellar masses and ages of galaxies seen at later times will be used to plan surveys in time for the upcoming launch of the James Webb Space Telescope. This research program is observationally challenging but I have demonstrated the relevant techniques are practical through pilot programmes undertaken in California. I am proposing to relocate to University College London and establish a new research effort in Europe to achieve these goals.","2458405","2015-10-01","2021-09-30"
"FirstGalaxies","Finding the most distant galaxies with NIRSpec guaranteed time on the James Webb Space Telescope","Andrew BUNKER","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","""Over the past 15 years, my team have pushed the frontier for the most distant known objects to higher redshifts, exploring galaxies when the Universe was young using the Hubble Space Telescope and large ground-based telescopes. As well as discovering galaxies within the first billion years (90percent of the way back in time to the Big Bang), our knowledge of the composition of the Universe has also grown - dark matter and dark energy dictate the expansion history and initial collapse of structures which ultimately form galaxies. We now know that the gas between the galaxies, initially plasma, became mostly neutral about 300,000 years after the Big Bang, but again became plasma about a billion years later. The first few generations of stars to form, with a contribution from high redshift quasars, might be responsible for this reionization, but we have yet to find the galaxies accounting for the bulk of the ionizing photons and key questions remain: what is the contribution from the faintest dwarf galaxies in the luminosity function at high redshift? what fraction of ionizing photons emitted by stars reach the intergalactic gas? is the first generation of stars forming from primordial hydrogen and helium more efficient in producing ionizing photons?
I am in a privileged position to address these questions, as a member of the ESA Instrument Science Team since 2005 for the near-infrared spectrograph (NIRSpec) on the James Webb Space Telescope (JWST), due to launch in May 2020. Much of our 900 hours of guaranteed time will be spectroscopy of high redshift galaxies, and I am leading the deep tier of our survey to get accurate redshifts (vital for luminosity functions), measure the stellar populations (ages and star formation rates), assess the escape fractions of ionizing photons and determine the metal enrichment (potentially finding the long-sought """"Population III"""", the first stars to form). With this ERC grant I aim to assemble a team to achieve these science goals.
""","2049961","2020-04-01","2025-03-31"
"FIRSTORM","Modeling first-order Mott transitions","Michele FABRIZIO","SCUOLA INTERNAZIONALE SUPERIORE DI STUDI AVANZATI DI TRIESTE","Mott insulators are “unsuccessful metals”, where conduction is impeded by strong Coulomb repulsion. Their use in microelectronics started to be seriously considered in the 1990s, when first reports of field-effect switches appeared. These attempts were motivated by the expectation that the dielectric breakdown in Mott insulators could suddenly release all formerly localized carriers, a significant potential for nanometer scaling. Over the very last years striking experimental data on narrow-gap Mott insulators have finally materialized that expectation disclosing an unprecedented scenario where the metal phase actually stabilized was only metastable at equilibrium, which foreshadows exciting potential applications. These new data call for an urgent theoretical understanding so far missing. In fact, the conventional portrait of Mott insulators has overlooked that Mott transitions are mostly 1st order, implying an extended insulator-metal coexistence. As a result, bias or light may nucleate long-lived metastable metal droplets within the stable insulator, as indeed seen in experiments. The unexpected 1st order nature of dielectric breakdown in Mott insulators and its poorly explored but important conceptual and practical consequences are the scope of my theoretical project. I will model known Mott insulators identifying the variety of mechanisms (Coulomb, lattice distortions) that support and boost the 1st order character of the Mott transition. I will model and study insulator-metal coexistence and associated novel phenomena such as those related to nucleation and wetting at the interface, including possible unexplored role of quantum fluctuations. I will then simulate in model calculations the spatially inhomogeneous dynamics and non-equilibrium pathways across the 1st order Mott transition, relating the results to ongoing experiments in top groups. The outcome of this project is expected to yield immediate conceptual as well as later technological consequences.","1422684","2016-09-01","2021-08-31"
"FIRSTSTEP","Synthesis of 2-D semiconductors with honeycomb nanogeometry, and study of their Dirac-type band structure and opto-electronic properties","Daniel Vanmaekelbergh","UNIVERSITEIT UTRECHT","Graphene redirected the pathways of solid-state physics with a revival of 2-D materials showing Dirac
physics due to their honeycomb geometry. The charge carriers are fundamentally different from those
in conventional electronic systems: the energy vs. wave vector relationship is linear instead of
quadratic, resulting in Dirac bands with massless carriers. A genuinely new class of materials will
emerge provided that classic semiconductor compounds can be molded in the nanoscale honeycomb
geometry: The Dirac-type band structure is then combined with the beneficial properties of
semiconductors, e.g. a band gap, optical and electrical switching, and strong spin-orbit coupling. The
PI recently prepared atomically coherent 2-D PbSe and CdSe semiconductors by nanocrystal assembly
and epitaxial attachment. Moreover, he showed theoretically that these systems combine a
semiconductor gap with Dirac-type valence and conduction bands, while the strong spin-orbit
coupling results in the quantum spin Hall effect. The ERC advanced grant will allow him to develop a
robust bottom-up synthesis platform for 2-D metal-chalcogenide semiconductor compounds with
honeycomb nanoscale geometry. The PI will study their band structure and opto-electronic properties
using several types of scanning tunnelling micro-spectroscopy and optical spectroscopy. The Fermilevel
will be controlled with an electrolyte-gated transistor in order to measure the carrier transport
properties. The results will be compared directly with those obtained on the same 2-D semiconductors
without honeycomb geometry, hence showing the conventional band structure. This should
unambiguously reveal the Dirac features of honeycomb semiconductors: valence band and conduction
band Dirac cones, non-trivial band openings at the K-points that may host the quantum spin Hall
effect, and non-trivial flat bands. 2-D semiconductors with massless holes and electrons open new
opportunities in opto-electronic devices and spintronics.","2500000","2016-12-01","2021-11-30"
"FISH","FaInt Supernovae and Hypernovae: Mechanism and Nucleosynthesis","Friedrich-Karl Wilhelm Thielemann","UNIVERSITAT BASEL","Massive stars of 8-140M⊙ undergo core-collapse at the end of their evolution, leading to a central neutron star or possibly a black hole. Stars in the mass range of 140-260M⊙ have been expected to experience thermonuclear explosions, known in the literature as pair instability supernovae (PISNe). More massive objects will form black holes during their final collapse. If these events lead to ejecta, they will have experienced explosive burning, possibly under the strong influence of interactions with neutrinos. The impact of the most massive objects will enter at the earliest stages of the evolution of galaxies, influencing the abundance pattern visible in the spectra of extremely low metallicity stars, both topics of extremely active research. Apparently one does not observe the abundance yields expected from PISNe, indicating that probably all very massive stars underwent strong mass loss during their evolution and undergo final core collapse. It is still an open issue, whether and how this collapse leads to neutron star formation or black holes (possibly also occurring subsequently), forming as a function of progenitor mass supernova events (SNe), faint supernovae with fallback from the innermost ejected zones (faint SNe), or hypernovae/collapsars/gamma-ray bursts (GRBs) in conjunction with rotation, magnetic fields and highly energetic explosions. The focus of the present proposal is this transition region in stellar progenitor mass and its nucleosynthesis contributions to galactic evolution, linking diverse research fields like nuclear physics far from stability, the equation of state of dense objects, 3D magnetohydrodynamics with neutrino transport, and computational methods. The outcome is of extreme importance in understanding the nucleosynthesis impact of the first stars, the chemical evolution of galaxies and the origin of all elements, including those processes with still highly uncertain origins/sites like the r-process, the nu/p-process or the p-process.","1929075","2013-01-01","2016-12-31"
"FLAMENANOMANUFACTURE","Flame Aerosol Reactors for Manufacturing of Surface-Functionalized Nanoscale Materials and Devices","Sotirios Pratsinis","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Nanotechnology research has been directed mostly to the design and synthesis of (a) materials with passive nanostructures (e.g. coatings, nanoparticles of organics, metals and ceramics) and (b) active devices with nanostructured materials (e.g. transistors, amplifiers, sensors, actuators etc). Little is known, however, about how well the unique properties of nanostructured materials are reproduced during their large scale synthesis, and how such manufacturing can be designed and carried out. A key goal here is to fundamentally understand synthesis of surface-functionalized, nanostructured, multicomponent particles by flame aerosol reactors (a proven scalable technology for simple ceramic oxide nanopowders). That way technology for making such sophisticated materials would be developed systematically for their efficient manufacture so that active devices containing them can be made economically. Our focus is on understanding aerosol formation of layered solid or fractal-like nanostructures by developing quantitative process models and systematic comparison to experimental data. This understanding will be used to guide synthesis of challenging nanoparticle compositions and process scale-up with close attention to safe product handling and health effects. The ultimate goal of this research is to address the next frontier of this field, namely the assembling of high performance active devices made with such functionalized or layered nanoparticles. Here these devices include but not limited to (a) actuators containing layered single superparamagnetic nanoparticles and (b) ultraselective and highly sensitive sensors made with highly conductive but disperse nanoelectrode layers for detection of trace organic vapors in the human breath for early diagnosis of serious illnesses.","2500000","2010-05-01","2015-04-30"
"FLAVOUR","Towards the Construction of the Fundamental Theory of Flavour","Andrzej Jerzy Buras","TECHNISCHE UNIVERSITAET MUENCHEN","Six quarks and six leptons of different kinds, referred to as flavours, form the modern periodic table of the fundamental building blocks of matter. The Standard Model of particle physics successfully describes these elementary particles and the forces between them. A deeper understanding of the flavour structure of quarks and leptons, of their masses and couplings, is however still missing.
Decisive new experiments are about to start in particle physics (LHC, high-intensity flavour facilities). They will test existing theoretical concepts and inspire new ideas. This should allow us to make substantial steps forwards in the construction of the fundamental Theory of Flavour, which is the main goal of this project. Such a theory should allow us to address the following fundamental questions: what is the underlying dynamics differentiating quarks and leptons of different flavour? Is this dynamics related to a new symmetry? How can this new dynamics be tested at low and high energies? These questions are of utmost importance in the context of our search for a new, more fundamental, theory of elementary interactions. They are also key ingredients to understand the strucutre of our Universe. Reaching this goal requires substantial efforts in model building, precision calculations, and phenomenological studies. These different lines of research will be joined in a novel way by the collaboration of the principal investigator with four younger team members. All team members have made, mostly independently, important and often pioneering contributions to the different aspects of this project. The combination of their different expertise in a joint effort is a unique feature of the present proposal.","1578400","2011-05-01","2016-04-30"
"FLEET","Flying Electromagnetic Toroids","Nikolay ZHELUDEV","UNIVERSITY OF SOUTHAMPTON","In this project I will study the generation, detection, and interaction with matter of Flying Toroids, a new type of light pulses never experimentally studied before. This represents an exciting opportunity to advance optics and electromagnetism in a radically new direction since Hertz, Marconi, Popov and Tesla developed technology for generating, detecting, and communicating with transverse electromagnetic waves. 

Conventional transverse electromagnetic waves propagate in free-space with the electric and magnetic field vectors perpendicular to the wave propagation direction, forming the famous triad. Theoretical analysis of recent years has shown that another, very different type of waves exists, which propagate at the speed of light, but only occur as short bursts of electromagnetic energy in the form of Flying Toroids. Flying Toroids are inseparable solutions of Maxwell equations with a unique, doughnut-like configuration of the electric and magnetic fields.  Flying Toroids interact with matter in unique ways, drastically different from that of conventional electromagnetic pulses. 

In a broader context, the electrodynamics of Flying Toroids is an exciting emerging field of optical science linked to intriguing recent developments in physics such as toroidal dipoles and anapoles, and, due to their topology, to Majorana fermions and skyrmions. 

Building on my recent proof-of-principle demonstration of Flying Toroid generation through conversion of few-cycle conventional transverse light pulses in artificial photonic nanostructures, my goal for this project is to experimentally study and understand the fundamental properties of Flying Toroids and their interaction with matter at optical frequencies, and to assess their potential for developing new technologies.  In my vision this project can lead to spectacular new opportunities for spectroscopic and light-enabled applications, and will impact on other branches of science, from astronomy to solid-state physics.","2570198","2018-10-01","2023-09-30"
"FLOODCHANGE","Deciphering River Flood Change","Guenter Bloeschl","TECHNISCHE UNIVERSITAET WIEN","Many major and devastating floods have occurred around the world recently. Their number and magnitude seems to have increased but such changes are not clear. More surprisingly, the exact causes of changes remain a mystery. Although, drivers such as climate and land use change are known to play a critical role, their complex interactions in flood generation have not been disentangled.
The main objectives of this project are to understand how changes in land use and climate translate into changes in river floods, what are the factors controlling this relationship and what are the uncertainties involved. We decipher the relationship between changes in floods and their drivers by analysing the processes separately for different flood types such as flash floods, rain-on-snow floods and large scale synoptic floods. We then use data from catchments in transects across Europe to build a probabilistic flood-change model that explicitly describes the change mechanisms. The model is unconventional as it does not take a reductionist approach but conceptualises the dominant flood change processes at the catchment scale. We test the model on long high-quality flood data series. We use the model as well as the temporal and spatial data variability to quantify the sensitivity of floods to climate and land use change and estimate the uncertainties involved. The data are already available to me or will be made available through my excellent contacts in Europe.
For the first time, it will be possible to systematise the effects of land use and climate on floods which will provide a vital step towards predicting how floods will change in the future.","2263565","2012-04-01","2017-03-31"
"FlowMachines","Flow Machines: Interacting with Style","Francois Pachet","UNIVERSITE PIERRE ET MARIE CURIE - PARIS 6","Content creation is a fundamental activity for developing identities in modern individuals. Yet creativity is hardly addressed by computer science. This project addresses the issue of content creation from the perspective of Flow machines. Flow machines are interactive systems that learn how to generate content, text or music, in the user’s style. Thanks to controlled generation mechanisms, the user can then steer the machine to generate content that fits with their intentions. Flow interactions induce a multiplicative effect that boosts creativity and prompts the user to reflect on their own style. This vision stems from the success stories of several computer-assisted musical systems that showed how interactive dialogs with self-learning interactions provoke flow states.
To enables full control of stylistic generation, the scientific challenge is the reification of style as a flexible texture. This challenge will be addressed by pursuing three original directions in the fields of statistical learning and combinatorial optimization: 1) the formulation of Markov-based generation as a constraint problem, 2) the development of feature generation techniques for feeding machine learning algorithms and 3) the development of techniques to transform descriptors into controllers.
Two large-scale studies will be conducted with well-known creators using these Flow machines, during which the whole creation process will be recorded, stored, and analyzed, providing the first complete chronicles of professional-level artifacts. The artifacts, a music album and a novel, will be published in their respective ecosystems, and the reaction of the audience will be measured and analyzed to further assess the impact of Flow machines on creation. The technologies developed and the pilot studies will serve as pioneering experiments to turn Flow machines into a field of study and explore other domains of creation.","2240120","2012-08-01","2017-07-31"
"FLPCHEM","Development of Frustrated Lewis Pair Chemistry","Gerhard Erker","WESTFAELISCHE WILHELMS-UNIVERSITAET MUENSTER","Frustrated Lewis pair chemistry is an exciting new field of very high current interest. Usually, Lewis acids and Lewis bases quench each other by strong adduct formation when brought together in solution. This is avoided or hindered by the attachment of sufficiently bulky substituents at these components. Non-quenched pairs of bulky Lewis acids and Lewis bases feature an unprecedented potential for cooperative small molecule activation and they induce an amazing manifold of new reactions and reactivities. This project will significantly advance this fascinating new field by the specific design and synthesis of novel advanced frustrated Lewis pairs (FLPs) and by using them for finding and developing new chemical reactions of fundamental chemical building blocks according to the following scheme:
A. Design and Preparation of New Frustrated Lewis Pairs
B. New FLP Reactions
a. New Areas of Metal-free Catalytic Hydrogenation
b. Opening the New Field of FLP-Based Free Radical Chemistry
c. New Oxidation reactions
d. FLP-Based Carbon Dioxide Chemistry
e. FLP Reactions of High Energy Intermediates
With this project and its subdivisions we propose to tackle very timely questions in an innovative and original way by using the enormous potential that the emerging field of frustrated Lewis pairs has to offer.","2100000","2012-04-01","2017-03-31"
"FLUOROCODE","FLUOROCODE: a super-resolution optical map of DNA","Johan M. V. Hofkens","KATHOLIEKE UNIVERSITEIT LEUVEN","""There has been an immense investment of time, effort and resources in the development of the technologies that enable DNA sequencing in the past 10 years. Despite the significant advances made, all of the current genomic sequencing technologies suffer from two important shortcomings. Firstly, sample preparation is time-consuming and expensive, and requiring a full day for sample preparation for next-generation sequencing experiments. Secondly, sequence information is delivered in short fragments, which are then assembled into a complete genome. Assembly is time-consuming and often results in a highly fragmented genomic sequence and the loss of important information on large-scale structural variation within the genome.

We recently developed a super-resolution DNA mapping technology, which allows us to uniquely study genetic-scale features in genomic length DNA molecules. Labelling the DNA with fluorescent molecules at specific sequences and using high-resolution fluorescence microscopy enabled us to produce a map of a genomic DNA sequence with unparalleled resolution, the so called FLUOROCODE. In this project we aim to extend our methodology to map longer DNA molecules and to include a multi-colour version of the FLUOROCODE that will allow us to read genomic DNA molecules like a barcode and probe DNA methylation status. The sample preparation, DNA labelling and deposition for imaging will be integrated to allow rapid mapping of DNA molecules. At the same time nanopores will be explored as a route to high-throughput DNA mapping.

FLUOROCODE will develop technology that aims to complement the information derived from current DNA sequencing platforms. The technology developed by FLUOROCODE will enable DNA mapping at unprecedented speed and for a fraction of the cost of a typical DNA sequencing project. We aniticipate that our method will find applications in the rapid identification of pathogens and in producing genomic scaffolds to improve genome sequence assembly.""","2423160","2012-09-01","2017-08-31"
"FMCoBe","Fluid Mechanics in Collective Behaviour: Multiscale Modelling and Applications","Petros Koumoutsakos","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Fluid mechanics are fundamental to collective behaviour in nature and technology. Fluids pervade complex systems at every scale, ranging from fish schools and flocking birds to bacterial colonies and nanoparticles for drug delivery. Despite its importance, little is known about the role of fluid mechanics in such applications. Is  schooling the result of vortex dynamics synthesised by individual fish wakes or the result of behavioural traits? Is fish schooling energetically favourable? How does blood affect the collective transport of nanoparticles in cancer therapy?

We seek to answer these questions through computational methods that resolve the interaction of fluids with multiple, deforming bodies across scales. Our methods rely on the innovative coupling of multi-scale particles with multi-resolution algorithms and grids. Uncertainty quantification techniques will link computations with experimental data. Learning and optimisation algorithms will investigate the optimality of collective behaviour and its relevance to technological applications.
Novel, scalable software, engineered to facilitate its broad use, will be made available to the scientific and industrial community.
Our group has built strong foundations in computational methods, fluid mechanics, biophysics, nanotechnology and their interfaces and this project gives us the opportunity to reach new frontiers.

Our goal is to provide unprecedented information about vortex dynamics of fish schooling, one of the most intriguing patterns in nature. Increased insight will open new horizons for mechanical understanding of collective behaviour, suggest new experiments and contribute to the rational design of industrial applications ranging from robots to wind farms. We will also shed light on mass transport in tumour induced vasculature to enhance the efficacy of drug delivery by nanoparticles, one of the most promising routes for cancer therapy.","2498800","2014-04-01","2019-03-31"
"FOCUS","Fiber Optic Cable Use for Seafloor studies of earthquake hazard and deformation","Marc-André GUTSCHER","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Two-thirds of the Earth’s surface is covered by water and thus largely inaccessible to modern networks of seismological instruments. The FOCUS project is poised to revolutionize seismic monitoring of the seafloor through a novel use of fiber optic cables to improve hazard assessment and increase early warning capability. Laser reflectometry using BOTDR, commonly used for structural health monitoring of large-scale engineering structures (e.g. - bridges, dams, pipelines, etc.), can measure very small strains (< 1 mm) at very large distances (10 - 200 km). It has never been used to monitor deformation caused by active faults on the seafloor. The objective of the FOCUS project is to demonstrate that this technique can measure small (1 - 2 cm) displacements on a primary test site offshore Sicily where the 28 km long EMSO Catania cable crosses the recently mapped North Alfeo Fault. BOTDR observations must be calibrated by other independent measurements. Therefore, targeted marine geophysical surveys of the seafloor along the trace of the cable and faults are planned, with micro-bathymetry, high-resolution seismics, seafloor seismic stations and use of seafloor geodetic instruments to quantify fault displacement. Once the BOTDR fault-monitoring technique has been tested and calibrated offshore Sicily, the goal is to expand it to other fiber optic cable networks, either existing research networks in earthquake hazard zones (Japan, Cascadia) or to the Mediterranean region through access to retired telecommunication cables, or through the development of dual-use cables with industry partners, (two of the anticipated outcomes of the FOCUS project). The novel secondary use of fiber optic cables as described by FOCUS represents a potentially tremendous breakthrough in seismology, tectonics and natural hazard early warning capability, one that could turn Earth’s future undersea communication infrastructure into a seismological monitoring network of unprecedented scale.","3487911","2018-10-01","2023-09-30"
"FQHE","Statistics of Fractionally Charged Quasi-Particles","Heiblum","WEIZMANN INSTITUTE OF SCIENCE LTD","The discovery of the fractional quantum Hall effect created a revolution in solid state research by introducing a new state of matter resulting from strong electron interactions.  The new state is characterized by excitations (quasi-particles) that carry fractional charge, which are expected to obey fractional statistics.  While odd denominator fractional states are expected to have an abelian statistics, the newly discovered 5/2 even denominator fractional state is expected to have a non-abelian statistics.  Moreover, a large number of emerging proposals predict that the latter state can be employed for topological quantum computing ( Station Q  was founded by Microsoft Corp. in order to pursue this goal).  This proposal aims at studying the abelian and non-abelian fractional charges, and in particular to observe their peculiar statistics.  While charges are preferably determined by measuring quantum shot noise, their statistics must be determined via interference experiments,  where one particle goes around another.  The experiments are very demanding since the even denominator fractions turn to be very fragile and thus can be observed only in the purest possible two dimensional electron gas and at the lowest temperatures.  While until very recently such high quality samples were available only by a single grower (in the USA), we have the capability now to grow extremely pure samples with profound even denominator states.  As will be detailed in the proposal, we have all the necessary tools to study charge and statistics of these fascinating excitations, due to our experience in crystal growth, shot noise and interferometry measurements.","2000000","2009-01-01","2013-12-31"
"FRACTFRICT","Fracture and Friction: Rapid Dynamics of Material Failure","Jay Fineberg","THE HEBREW UNIVERSITY OF JERUSALEM","FractFrict is a comprehensive study of the space-time dynamics that lead to the failure of both bulk materials and frictionally bound interfaces. In these systems, failure is precipitated by rapidly moving singular fields at the tips of propagating cracks or crack-like fronts that cause material damage at microscopic scales. These generate damage that is macroscopically reflected as characteristic large-scale, modes of material failure. Thus, the structure of the fields that microscopically drive failure is critically important for an overall understanding of how macroscopic failure occurs.
The innovative real-time measurements proposed here will provide fundamental understanding of the form of the singular fields, their modes of regularization  and their relation to the resultant macroscopic modes of failure. Encompassing different classes of bulk materials and material interfaces.
We aim to:
[1] To establish a fundamental understanding of the dynamics of the near-tip singular fields, their regularization modes and how they couple to the macroscopic dynamics in both frictional motion and fracture.
[2] To determine the types of singular failure processes in different classes of materials and interfaces (e.g. the brittle to ductile transition in amorphous materials, the role of fast fracture processes in frictional motion).
[3] To establish local (microscopic) laws of friction/failure and how they evolve into their macroscopic counterparts
[4]. To identify the existence and origins of crack instabilities in bulk and interface failure

The insights obtained in this research will enable us to manipulate and/or predict material failure modes. The results of this study will shed considerable new light on fundamental open questions in fields as diverse as material design, tribology and geophysics.","2265399","2010-12-01","2016-11-30"
"FRAPPANT","Formal Reasoning About Probabilistic Programs: Breaking New Ground for Automation","Joost Pieter KATOEN","RHEINISCH-WESTFAELISCHE TECHNISCHE HOCHSCHULE AACHEN","Probabilistic programs describe recipes on how to infer statistical conclusions about data from a complex mixture of uncertain data and real-world observations. They can represent probabilistic graphical models far beyond the capabilities of Bayesian networks and are expected to have a major impact on machine intelligence.

Probabilistic programs are ubiquitous. They steer autonomous robots and self-driving cars, are key to describe security mechanisms, naturally code up randomised algorithms for solving NP-hard problems, and are rapidly encroaching AI. Probabilistic programming aims to make probabilistic modeling and machine learning accessible to the programmer.

Probabilistic programs, though typically relatively small in size, are hard to grasp, let alone automatically checkable. Are they doing the right thing? What’s their precision? These questions are notoriously hard — even the most elementary question “does a program halt with probability one?” is “more undecidable” than the halting problem — and can (if at all) be answered with statistical evidence only. Bugs thus easily occur. Hard guarantees are called for. The objective of this project is to enable predictable probabilistic programming. We do so by developing formal verification techniques.

Whereas program correctness is pivotal in computer science, the formal verification of probabilistic programs is in its infancy. The project aims to fill this barren landscape by developing program analysis techniques, leveraging model checking, deductive verification, and static analysis. Challenging problems such as checking program equivalence, loop-invariant and parameter synthesis, program repair, program robustness and exact inference using weakest precondition reasoning will be tackled. The techniques will be evaluated in the context of probabilistic graphical models, randomised algorithms, and autonomous robots.

FRAPPANT will spearhead formally verifiable probabilistic programming.","2491250","2018-11-01","2023-10-31"
"FRESCO","Efficient, Flexible Synthesis of Molecules with Tailored Shapes: from Photo-switchable Helices to anti-Cancer Compounds","Varinder Aggarwal","UNIVERSITY OF BRISTOL","The creation of new molecular entities and subsequent exploitation of their properties is central to a broad spectrum of research disciplines from medicine to materials but progress has been limited by the difficulties associated with chemical synthesis. We are now proposing a fundamentally new strategy, which has the potential to revolutionise how we conduct complex organic synthesis. The basic C–C bond-forming step involves the reaction of a lithiated carbamate with a boronic ester to give a homologated boronic ester with complete stereocontrol. Furthermore, the reaction shows >98% efficiency in most cases and can be conducted iteratively and in one pot (up to 9 iterations has been demonstrated with full stereocontrol). We will now extend this methodology to more functionalised carbamates as this will enable the rapid synthesis of polypropionates, which are amongst the most important classes of biologically active molecules. The robust methodology is now ripe for transfer to the solid phase as this will enable the preparation of libraries of these molecules. Through applying our assembly-line-synthesis methodology to complex molecules with diverse structures, we will demonstrate its scope, robustness, and full potential. The methodology enables stereochemistry to be ‘dialled in’ to a carbon chain, which in turn controls the conformation and we will exploit this feature in the shape-selective synthesis of molecules. We will explore how the sense of helical chirality of these molecules can be switched (M to P) just with light. We will target helical molecules with specific groups at specific places for optimum binding to disrupt protein–protein interactions involved in cancer. Finally, our methodology provides ready access to a family of building blocks that represent common repeat units found in polyketides. By combining these building blocks iteratively using lithiation-borylation, we should be able to rapidly and reliably prepare complex natural products.","2436379","2015-10-01","2020-09-30"
"FROM-PDE","Frobenius Manifolds and Hamiltonian Partial Differential Equations","Boris Dubrovin","SCUOLA INTERNAZIONALE SUPERIORE DI STUDI AVANZATI DI TRIESTE","The basic idea of the project is to apply methods and results of the theory of integrable systems to non-integrable PDEs. We do not promise to solve any PDE; however, in certain strongly nonlinear regimes, solutions to a conservative non-integrable PDE  exhibit integrable behaviour. The realization of this idea, supported by some preliminary analytical and numerical results, will consist of three main tasks:  1) classify normal forms of quasilinear Hamiltonian PDEs and their perturbations; 2) reduce the lists of asymptotic solutions to an abridged list of universal forms represented via Painlevé transcendents, theta-functions, etc.;  3) establish matching rules between the universal asymptotic expansions. Differential-geometric methods based on the theory of Frobenius manifolds will be crucial in solving the classification problems; analytic and algebro-geometric techniques applied to the Hurwitz spaces of Riemann surfaces will be instrumental in the description of nonlinear oscillatory regimes; selected solutions to Painlevé equations and their generalizations will be needed for the analytic description of transitions from regular to oscillatory behaviour.  The project is aiming at creation of an online library of the main qualitative types of behaviour of solutions to large classes of nonlinear evolutionary PDEs supplied with analytic expressions, numerical codes and visualization tools, as well as with tests of existence of a Hamiltonian structure, integrability or almost integrability. Such a library will both stimulate the research in the field and lead to a high visibility of the project.","864000","2009-01-01","2013-12-31"
"FUBSSY","Functional Biosupramolecular Systems:  Photosystems and Sensors","Stefan Georg Jean-Petit-Matile","UNIVERSITE DE GENEVE","The general objective of this proposal is to discover access to ordered, soft and smart matter for use in materials sciences (e.g. molecular optoelectronics, organic solar cells), biology, medicine and chemistry.

Specific aim 1 focuses on two complementary approaches (zipper assembly; self-organizing surface-initiated polymerization, SOSIP) to build artificial photosystems on solid surfaces, including supramolecular n/p-heterojunctions with oriented multicolor antiparallel redox gradients (“OMARG-SHJs”).

Specific aim 2 is to create sensing systems in lipid bilayers that operate by pattern recognition with polyion/counterion complexes, and to apply the lessons learned to several interconnected topics (diagnostics, fluorescent membrane/nitrate probes, cellular uptake, organocatalysis with anion-À interactions).

To address these challenges, crossfertilization at the interface of synthetic, supramolecular, biological and materials chemistry will be essential.  To produce the broad horizons needed for crossfertilization, projects on different topics are run in parallel.  The proposed approach builds in general on the distinguishing expertise of the (organic) chemist to create new matter, i.e., multistep organic synthesis.  To identify significant, that is responsive or “smart” systems, the invention of functional feedback loops will be emphasized.

Success with aim 1 will provide general solutions to key problems (OMARG-SHJs, SOSIP) and thus lead to broad applications (including high-efficiency organic photovoltaics and dye-sensitized solar cells).  Success with aim 2 will afford synthetic sensing systems that operate, closer than ever, like the membrane-based mammalian olfactory and gustatory systems and open new approaches to crossdisciplinary topics as specified above.","1906200","2011-01-01","2015-12-31"
"FUN-SP","A functional framework for sparse, non-gaussian signal processing and bioimaging","Michael Unser","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","""In recent years, the research focus in signal processing has shifted away from the classical linear paradigm, which is intimately linked with the theory of stationary Gaussian processes. Instead of considering Fourier transforms and performing quadratic optimization, researchers are presently favoring wavelet-like representations and have adopted ”sparsity” as design paradigm.

Our ambition is to develop a unifying operator-based framework for signal processing that would provide the ``sparse"""" counterpart of the classical theory, which is currently missing. To that end, we shall specify and investigate sparse stochastic processes that are continuously-defined and ruled by differential equations, and construct corresponding wavelet-like sparsifying transforms. Our hope is to be able to rigorously connect non-quadratic regularization and sparsity-constrained optimization to well-defined continuous-domain statistical models. We also want to develop a novel Lie-group formalism for the design of steerable, signal-adapted wavelet transforms with improved invariance and sparsifying properties, both in 2-D and 3-D.

We shall use these tools to define new reversible image representations in terms of singular points (contours and keypoints) and to develop novel algorithms for 3-D biomedical image analysis. In close collaboration with imaging scientists, we shall apply our framework to the development of new 3-D reconstruction algorithms for emerging bioimaging modalities such as fluorescence deconvolution microscopy, digital holography microscopy, X-ray phase-contrast microscopy, and advanced MRI.""","2106994","2011-04-01","2016-03-31"
"FUNCA","Functional Nanomaterials via Controlled Block Copolymer Assembly","Ian Manners","UNIVERSITY OF BRISTOL","We outline an ambitious 5 year interdisplinary research programme that introduces a fundamentally new platform to the fabrication of nanoelectronic and liquid crystal devices, current areas of intense scientific and technological interest. The new approach involves the use of block copolymer micelles and block comicelles prepared by Crystallization-Driven Living Polymerization (CDLP) processes. This novel method allows unprecedented access to well-defined micelle architectures (with size control, narrow size distribution, and access to segmented structures that possess heterojunctions). Crosslinking will also be used to optimize micelle mechanical properties where necessary. The new platform offers very promising advantages over competitive methods for realising nanomaterials these include ambient temperature synthesis and solution processing, easy control of dimensions and aspect ratio, electronic properties, and semiconductor/semiconductor or semiconductor/dielectric junction fabrication. In addition, the use of hydrophilic coronas should, in principle, allow the self-assembly processes and subsequent manipulations to be performed in water.","1658544","2010-04-01","2016-03-31"
"FunCapSys","Functional Systems of Capsules","Jonathan NITSCHKE","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The simplest living organism is composed of myriad chemical subsystems, each consisting of structurally complex biomolecules that interact in many ways; the properties of life emerge from these dense connections. Understanding how these interactions take place will help elucidate the foundations of biology, as well as enabling the design and creation of new chemical networks with targeted functions. Deciphering and designing chemical systems requires new tools to be developed, however. In this research programme, we will develop new means for engineering functional chemical systems based upon the use of guest-binding capsules. These hosts will respond to many different signals in predictable ways. Their responses will enable their guests to be transformed in new ways or pumped between phases using light. Chemical signals will enable hosts to be transformed reversibly or irreversibly, changing their guest binding properties so as to favour some guests and disfavour others, and catalysts will be released or taken up, accelerating or impeding catalysed transformations. Ultimately we will design systems where signal transduction occurs in complex cycles and feedback loops, allowing complex behaviour to emerge from abiological systems.","2478236","2017-01-01","2021-12-31"
"FUNCAT","Fundamental Studies in Organometallic Chemistry and Homogeneous Catalysis","Steven Patrick Nolan","THE UNIVERSITY COURT OF THE UNIVERSITY OF ST ANDREWS","The area of catalysis is now at the forefront of the chemical sciences in light of environmental and economic issues. Synthesizing bulk, fine and pharmaceutical compounds using catalysis is now a must in the modern economy. The studies described in the present proposal make use of the full arsenal of techniques available for modern synthetic chemical investigation: fundamental physicochemical studies, experiment-based rational catalyst design and catalytic studies.  The exploratory physicochemical experiments to be performed make use of classical solution reaction calorimetry involving air- and moisture sensitive complexes. This area is very poorly investigated worldwide. The PI is a world leader in the area of organometallic thermochemistry, a technique that is proposed for elaboration in Spain. This section of the proposal will permit an increase of knowledge on fundamental catalytic systems, allowing for determination of enthalpy and kinetics of key reaction steps involved in homogeneous catalysis.  Much needed thermodynamic information will be generated for palladium, gold, iridium and rhenium systems. The development of a rhenium-based system is proposed for investigation in the area of olefin metathesis. Furthermore, ruthenium-systems enabling alkyne metathesis are targeted for synthesis. The development of such olefin modifying system would greatly benefit Europe as catalytic transformations have huge potential in polymer, pharmaceutical and fine chemical industries. The proposed work is both fundamental and applied   high risk and high reward.  It is outside the presently investigated areas of research carried out by our group at the ICIQ. The Principal Investigator (PI) has 25 years of experience in fundamental and applied chemical research and 15 years of experience as an independent researcher in homogeneous catalysis and organometallic chemistry.","2033000","2009-01-01","2014-12-31"
"FUNCOMP","Numerical Computation with Functions Instead of Numbers","Lloyd Nicholas Trefethen","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","""Numerical analysis, scientific computing, and computational science are built on technologies of discretization such as splines, finite differences, and finite elements.  Yet for most applications it is ultimately continuous functions that are of scientific interest, and the discretizations are a means to the end of working with them.  This project will advance mathematics and algorithms to enable scientists and engineers to compute with functions rather than their discretizations.  Such computations will be numerical as opposed to symbolic, for speed and wide applicability, yet will achieve some of the “feel” of symbolic computing.  The goal is to do for computing with functions what floating-point arithmetic has done for computing with numbers: to hide away details of discretization from most users, most of the time.""","1668157","2012-03-01","2017-02-28"
"FUNDMS","Functionalisation of Diluted Magnetic Semiconductors","Tomasz Dietl","INSTYTUT FIZYKI POLSKIEJ AKADEMII NAUK","Low-temperature studies of transition metal doped III-V and II-VI compounds carried out over the last decade have demonstrated the unprecedented opportunity offered by these systems for exploring physical phenomena and device concepts in previously unavailable combinations of quantum structures and ferromagnetism in semiconductors.  The work proposed here aims at combining and at advancing epitaxial methods, spatially-resolved nano-characterisation tools, and theoretical modelling in order to understand the intricate interplay between carrier localisation, magnetism, and magnetic ion distribution in DMS, and to develop functional DMS structures. To accomplish these goals we will take advantage of two recent breakthroughs in materials engineering. First, the attainment of high-k oxides makes now possible to generate interfacial hole densities up to 10^21 cm-3. We will exploit gated thin layers of DMS phosphides, nitrides, and oxides, in which hole delocalization and thus high temperature ferromagnetism is to be expected under gate bias. Furthermore we will systematically investigate how the Curie temperature of (Ga,Mn)As can be risen above 180 K. Second, the progress in nanoscale chemical analysis has allowed demonstrating that high temperature ferromagnetism of semiconductors results from nanoscale crystallographic or chemical phase separations into regions containing a large concentration of the magnetic constituent. We will elaborate experimentally and theoretically epitaxy and co-doping protocols for controlling the self-organised growth of magnetic nanostructures, utilizing broadly synchrotron radiation and nanoscopic characterisation tools. The established methods will allow us to obtain on demand either magnetic nano-dots or magnetic nano-columns embedded in a semiconductor host, for which we predict, and will demonstrate, ground-breaking functionalities. We will also assess reports on the possibility of high-temperature ferromagnetism without magnetic ions.","2440000","2009-01-01","2013-12-31"
"FUNGRAPH","A New Foundation for Computer Graphics with Inherent Uncertainty","George DRETTAKIS","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","The use of Computer Graphics (CG) is constantly expanding, e.g., in Virtual and Augmented Reality, requiring realistic interactive renderings of complex virtual environments at a much wider scale than available today. CG has many limitations we must overcome to satisfy these demands. High-quality accurate rendering needs expensive simulation, while fast approximate rendering algorithms have no guarantee on accuracy; both need manually-designed expensive-to-create content. Capture (e.g., reconstruction from photos) can provide content, but it is uncertain (i.e., inaccurate and incomplete). Image-based rendering (IBR) can display such content, but lacks flexibility to modify the scene. These different rendering algorithms have incompatible but complementary tradeoffs in quality, speed and flexibility; they cannot currently be used together, and only IBR can directly use captured content. To address these problems
FunGraph will revisit the foundations of Computer Graphics, so these disparate methods can be used together, introducing the treatment of uncertainty to achieve this goal.
FunGraph introduces estimation of rendering uncertainty, quantifying the expected error of rendering components, and propagation of input uncertainty of captured content to the renderer. The ultimate goal is to define a unified renderer exploiting the advantages of each approach in a single algorithm. Our methodology builds on the use of extensive synthetic (and captured) “ground truth” data, the domain of Uncertainty Quantification adapted to our problems and recent advances in machine learning – Bayesian Deep Learning in particular.
FunGraph will fundamentally transform computer graphics, and rendering in particular, by proposing a principled methodology based on uncertainty to develop a new generation of algorithms that fully exploit the spectacular (but previously incompatible) advances in rendering, and fully benefit from the wealth offered by constantly improving captured content.","2497161","2018-10-01","2023-09-30"
"FunMagResBeacons","Functionalized Magnetic Resonance Beacons for Enhanced Spectroscopy and Imaging","Malcolm LEVITT","UNIVERSITY OF SOUTHAMPTON","""This project will develop and demonstrate molecular agents called functional magnetic resonance beacons (fMRBs). These will provide a new set of versatile spectroscopic tools for the spatially resolved study of chemistry, biochemistry, diffusion, flow and percolation inside opaque objects. The fMRB agents support hyperpolarized nuclear spin order, which generates enormously enhanced nuclear magnetic resonance (NMR) signals. The agents are designed to maintain such order for long times (between 5 minutes and several hours) in ambient temperature solution, enabling their transport deep inside opaque objects. The molecules are functionalized, so that they “light up"""" in an NMR or magnetic resonance imaging (MRI) experiment, upon triggering by specific chemical signals or physical conditions (sensory functionality), and may also to bind to selected molecular targets (binding functionality). One set of proposed realisations possesses “lock-and-key” functionality, meaning that the hyperpolarized nuclear spin order is “locked” into a form which is invisible in the NMR spectrometer, but which may be “unlocked” at any chosen time by applying a suitable radiofrequency pulse sequence. The following molecular moieties are proposed as storage modules: (1) molecular cages, such as functionalized C60 fullerenes, encapsulating noble gas atoms such as 3He; (2) spin clusters supporting long-lived states, such as pairs of 13C or 15N nuclei, in shielded molecular environments. The sensory moieties include tailored peptide sequences, which may be activated by the presence of particular proteases, while binding modules include moieties such as biotin. The agents are designed to be conveniently transportable in a hyperpolarized state. Potential long-term applications include in vivo molecular imaging by MRI.
""","2762223","2018-10-01","2023-09-30"
"FUNMAT","Self-Organized Nanostructuring in Functional Thin Film Materials","Lars Hultman","LINKOPINGS UNIVERSITET","I aim to achieve a fundamental understanding of the atomistic kinetic pathways responsible for nanostructure formation and to explore the concept of self-organization by thermodynamic segregation in functional ceramics. Model systems are advanced ceramic thin films, which will be studied under two defining cases:   1) deposition of supersaturated solid solutions or nanocomposites by magnetron sputtering (epitaxy) and arc evaporation. 2) post-deposition annealing (ageing) of as-synthesized material.  Thin film ceramics are terra incognita for compositions in the miscibility gap. The field is exciting since both surface and in-depth decomposition can take place in the alloys. The methodology is based on combined growth experiments, characterization, and ab initio calculations to identify and describe systems with a large miscibility gap.  A hot topic is to elucidate the bonding nature of the cubic-SiNx interfacial phase, discovered by us in TiN/Si3N4 with impact for superhard nanocomposites. I have also pioneered studies of self-organization by spinodal decomposition in TiAlN alloy films (age hardening). Here, the details of metastable c-AlN nm domain formation are unknown and the systems HfAlN and ZrAlN are predicted to be even more promising. Other model systems are III-nitrides (band gap engineering), semiconductor/insulator oxides (interface conductivity) and carbides (tribology).   The proposed research is exploratory and has the potential of explaining outstanding phenomena (Gibbs-Thomson effect, strain, and spinodal decomposition) as well as discovering new phases, for which my group has a track-record, backed-up by state-of-the-art in situ techniques. One can envision a new class of super-hard all-crystalline ceramic nanocomposites with relevance for a large number of research areas where elevated temperature is of concern, significant in impact for areas as diverse as microelectronics and cutting tools as well as mechanical and optical components.","2292000","2008-12-01","2013-11-30"
"FUNREN","Functional Renormalization - from quantum gravity and dark energy to ultracold atoms and condensed matter","Wolfgang Christoph Wetterich","RUPRECHT-KARLS-UNIVERSITAET HEIDELBERG","""Functional Renormalization provides a bridge from fundamental
microphysical laws to macroscopic complexity and observations. Acting
like a """"theoretical microscope"""" with variable resolution, it includes in
a stepwise procedure the fluctuation effects which are responsible for
the emergence of complexity. It describes macroscopic phenomena that are not
directly visible on the microscopic level as order, phase transitions and
spontaneous symmetry breaking , and is flexible enough to accommodate the
change of effective degrees of freedom and associated effective laws. The
laws of Nature become dependent on the length scale.

We propose to develop non-perturbative flow equations into a precision tool
for the understanding of many body physics, that can be tested by experiments
with ultracold atoms.
Fundamental questions as a formulation of quantum gravity as a
non-perturbatively renormalizable quantum field theory, the emergence of
fundamental length scales or the origin of dark energy will be tackled
with this method.
We also will address specific applications as the non-linear growth of
structure in cosmology or the phase diagram of models for strongly
correlated electrons.""","1955400","2012-04-01","2017-03-31"
"FURORE","FUndamental studies and innovative appROaches of REsearch on magnetism","Roland Martin Wiesendanger","UNIVERSITAET HAMBURG","Based on our developments of Spin-Polarized Scanning Tunnelling Microscopy (SP-STM) and Magnetic Exchange Force Microscopy (MExFM), both offering spin sensitivity and spatial resolution down to the ultimate limit of single atoms, we will study spin-dependent interactions between individual magnetic atoms on metal surfaces, in diluted magnetic semiconductors, on surfaces of magnetic insulators, as well as between single-atom tips and ultracold quantum gases. Besides the investigation of static spin states and spin interactions, we will manipulate spin states in a controlled manner down to the single atom limit by making use of the spin-transfer torque exerted by spin-currents from an atomically sharp SP-STM tip across a vacuum barrier. Moreover, we will combine spin-current induced magnetization switching experiments on magnetic metallic nanostructures based on SP-STM with pump-probe experiments, thereby studying the fundamentals of magnetization reversal processes both spatially and time-resolved. We will make use of the powerful combination of SP-STM with single-atom manipulation to probe spin-dependent interactions in artificial nanostructures. In the case of magnetic insulators we will probe spin states and spin-dependent interactions based on local measurements of the quantum-mechanical exchange and correlation forces between a single-atom tip with a well-defined spin state and single atoms of the sample. Spin excitations at the level of individual atoms will be probed by a combination of SP-STM with inelastic electron tunnelling spectroscopy, while the combination of MExFM with measurements of the damping of the cantilever oscillation will be employed to reveal local spin excitations in electrically insulating materials. Finally, we will couple an MExFM-type force sensor to the spin state of an optically trapped ultracold quantum gas with the challenging goal to combine scanning probe and quantum optical methods for manipulating quantum states of matter.","2049600","2009-01-01","2013-12-31"
"FUTURE-PRINT","Tuneable 2D Nanosheet Networks for Printed Electronics","Jonathan Nesbitt COLEMAN","THE PROVOST, FELLOWS, FOUNDATION SCHOLARS & THE OTHER MEMBERS OF BOARD OF THE COLLEGE OF THE HOLY & UNDIVIDED TRINITY OF QUEEN ELIZABETH NEAR DUBLIN","In the future, even the most mundane objects will contain electronic circuitry allowing them to gather, process, display and transmit information. The resulting vast network, often called the Internet of Things, will revolutionise society. To realise this will require the ability to produce electronic circuitry extremely cheaply, often on unconventional substrates. This will be achieved through printed electronics, by the assembly of devices from solution (i.e. ink) using methods adapted from printing technology. However, while printed electronics has been advancing rapidly, the development of new, nano-materials-based inks is required for this area to meet its true potential.
We believe recent developments in liquid exfoliation of 2D nanosheets have given us the ideal family of materials to revolutionise electronic ink production. Liquid exfoliation can transform layered crystals into suspensions of nanosheets in very large quantities. In this way we can produce liquid-dispersed nanosheets of a wide range of types including conducting (e.g. graphene, MXenes, TiB2 etc), semiconducting (e.g. MoS2, WSe2, GaS, Black phosphorous etc), insulating (e.g. BN, talc) or electrochemically active (e.g. MoO3, Ni(OH)2, MnO2 etc). These nanosheets can be deposited from liquid to form porous networks of defined electronic type. While these networks have huge applications potential, a large amount of work must be done to translate them into working printed devices.
In this project, we will develop methods to transform large volume suspensions of exfoliated nanosheets into bespoke 2D inks with properties engineered for a range of specific printed device applications. We will learn to use this 2D ink to print patterned or large area 2D nanosheet networks with controlled structure, allowing us to tune the electrical properties of the network during printing. We will combine networks of different nanosheet types into complex heterostructures. This will allow us to print all device components (electrodes, active layers, dielectrics, energy storage layers) from one contiguous, multi-component network. In this way we will produce 2D network transistors, solar cells, displays and energy storage systems. FUTURE-PRINT will revolutionise electronic inks and will offer a new path forward for printed electronics.","2213317","2016-11-01","2021-10-31"
"G-Statistics","Foundations of Geometric Statistics and Their Application in the Life Sciences","Xavier Jean-Louis PENNEC","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","""Invariance under gauge transformation groups provides the natural structure explaining the laws of physics. In life sciences, new mathematical tools are needed to estimate approximate invariance and establish general but approximate laws. Rephrasing Poincaré: a geometry cannot be more true than another, it may just be more convenient, and statisticians must find the most convenient one for their data. At the crossing of geometry and statistics, G-Statistics aims at establishing the mathematical foundations of geometric statistics and exemplifying their impact on selected applications in the life sciences. 

So far, mainly Riemannian manifolds and negatively curved metric spaces have been studied. Other geometric structures like quotient spaces, stratified spaces or affine connection spaces naturally arise in applications. G-Statistics will explore ways to unify statistical estimation theories, explaining how the statistical estimations diverges from the Euclidean case in the presence of curvature, singularities, stratification. Beyond classical manifolds, particular emphasis will be put on flags of subspaces in manifolds as they appear to be natural mathematical object to encode hierarchically embedded approximation spaces. 

In order to establish geometric statistics as an effective discipline, G-Statistics will propose new mathematical structures and theorems to characterize their properties. It will also implement novel generic algorithms and illustrate the impact of some of their efficient specializations on selected applications in life sciences. Surveying the manifolds of anatomical shapes and forecasting their evolution from databases of medical images is a key problem in computational anatomy requiring dimension reduction in non-linear spaces and Lie groups. By inventing radically new principled estimations methods, we aim at illustrating the power of the methodology and strengthening the """"unreasonable effectiveness of mathematics"""" for life sciences.""","2183584","2018-09-01","2023-08-31"
"GAIA-ESO-MW","The Gaia-ESO Milky Way Survey","Gerard Francis Gilmore","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Understanding how galaxies actually form and evolve within our dark matter and dark energy dominated [ɅCDM] universe continues to be an enormous challenge. State of the art simulations of the aggregation of cold dark matter under its own gravitational influence suggest that galaxies grow from very smooth initial conditions through a sequence of merger and accretion events. However, theoretical models of galaxy formation, which necessarily involve modelling star formation and stellar evolution, the creation and dispersal of the chemical elements, the formation and energy output of massive black holes, and the response of gas to radiation and supernova shock waves, among much more, rely more heavily on phenomenological models than on a detailed understanding of physical theory. Thus, these models require calibration with well-studied (nearby) test cases of galaxies which we can study in detail, specifically our own Milky Way Galaxy.
The Gaia-ESO Survey is Europe’s major ground-based project to meet this scientific challenge.
The Gaia-ESO Survey, which began data-taking in January 2012, has been allocated 300nights of telescope time over five years using the European Southern Observatory's Very Large Telescope (VLT-UT2) with its premier multi-object spectrograph, FLAMES.  The project will obtain high-quality spectroscopy of some 100,000 faint stars, systematically covering all the major components of the Milky Way. This will provide the first homogeneous overview of the distributions of kinematics and chemical elemental abundances in the Galaxy. With well-defined samples the Survey will quantify the kinematic+ multi-chemical element abundance distribution functions of the bulge, the thick disk, the thin disc, and the halo stellar components.
This proposal is to provide the core support team for the Co-Principal Investigator of the Gaia-ESO Survey with responsibility for the Milky Way Survey.","2499978","2013-04-01","2019-03-31"
"GALFORMOD","Galaxy formation models for the next generation of evolutionary and cosmological surveys","Simon David Manton White","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Over the next decade, much effort on major astronomical facilities will be dedicated to large-scale surveys of the galaxy population. Their aim is two-fold: understanding the origin and evolution of galaxies and their central supermassive black holes, and clarifying the nature of dark matter, dark energy and the process that produced all cosmic structure. Achieving these goals will require powerful and flexible modelling tools that can simulate galaxy evolution in all viable cosmologies and under a wide variety of assumptions about the governing physical processes. Such capabilities do not currently exist. I propose to develop them through a major expansion of the functionality and scope of the Millennium Simulation archive. New simulations, new theoretical approaches and new web services will allow users to study galaxy formation across the full range of galaxy masses (from dwarf spheroidals to giant cDs). Remote users will be able to change parameters and modelling prescriptions at will, creating virtual surveys of universes with any chosen cosmology and galaxy formation model. Matching to multiwavelength surveys of real galaxies will make it possible to isolate the physical processes driving galaxy evolution, and to characterize the systematic errors that uncertain galaxy formation physics induce in precision estimates of cosmological parameters. Scientific problems where these new capabilities may be decisive in enabling progress include: the role of supermassive black holes in shaping galaxy formation; the origin of diversity in the forms of galaxies and in their nuclear activity; the effects of environment on galaxy structure; the formation history of our own Milky Way; the nature of the first galaxies and their effects on later and more easily observable generations of galaxies; the distribution and nature of dark matter; the origin of all cosmic structure; and the nature of dark energy.","1830000","2010-01-01","2014-12-31"
"GAPWAVE ICS","Waveguide-type semiconductor integrated circuits (ICs) in gaps between conducting surfaces with texture – architecture, electromagnetic modeling and micromachining","Per-Simon Kildal","CHALMERS TEKNISKA HOEGSKOLA AB","In order to explore and exploit the frequency range from 30 GHz up to THz, new types of transmission lines and semiconductor architectures are needed. Conventional microwave technologies that are commonly used below 30 GHz become either too lossy or are too expensive to manufacture, and technologies used in the optical regime are not usable either. The intermediate frequency band is therefore often referred to as the THz gap, indicating the lack of commercialize-able technologies there.

Professor Kildal has invented a fundamentally new regime of transmission line, referred to as gap waveguides. The basis is newly discovered local waves appearing in the gap between two conducting surfaces, controlled by a texture in one or both of the surfaces. The gap waveguide has been verified below 20 GHz, but it will be more advantageous in the THz gap. The texture will for THz applications be of submillimeter or micrometer scale, realizable by micromachining or etching. Also, there is no need for a dielectric substrate, and there is no need for conductive contact between the two surfaces. Therefore, such gap waveguides and circuits for the THz gap can be manufactured with low cost.

The vision is that the topology of this new regime of gap waveguides will facilitate integration of semiconductor devices, and may lay the foundation for new architectures of transistors and other integrated circuits, being located inside the gap encapsulated by the conductive surfaces themselves. In order to reach this vision new and efficient numerical electromagnetic methods and modeling tools need to be developed, taking advantage of the particular gap waveguide geometry, and being able to connect to or replace the charge transport models for the transistors in the doped semiconductors themselves.

The gap waveguide technology can get a tremendous impact on exploring higher frequencies in radio astronomy, communications, and imaging for medical as well as security applications.","1659302","2013-05-01","2017-04-30"
"GAUGE-STRING","Gauge theory - String  theory duality: maximally symmetric case and beyond","Arkadi Alexander Tseitline","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","Quantum field theories with local gauge symmetry  are building blocks of the modern theory of fundamental interactions between  elementary particles. The basic example  is Quantum Chromo Dynamics. There is strong evidence  that QCD is the correct theory of strong interactions, but it has been difficult to  use it to account  for many hadronic phenomena  which is due to large  value of gauge  coupling  at low  energies. Theoretical  understanding of  gauge theory dynamics at large values of coupling  when one cannot use the Feynman diagram perturbation theory is a major problem of physics of strong interactions. Goals include analytic computation of   mass spectrum of  hadrons, etc. The general aim of this proposal is to develop  new theoretical tools to describe strongly coupled  gauge theories. Research in the last decade brought strong evidence that connection of gauge theories  to string theory should be a key to solution of this  problem. Gauge-string duality and,  in particular,  Anti deSitter / conformal field theory (AdS/CFT) correspondence is  one of the most active directions  of current work in theory of fundamental interactions. A remarkable progress  was achieved towards quantitative   understanding of this relation  in the most  symmetric case of   maximally  supersymmetric gauge theory in flat 4 dimensions  dual to  superstring theory in curved 10-dimensional AdS5 x S5 space. We propose a detailed study of this duality from the string theory side using world-sheet methods and hidden integrability of the maximally symmetric theory. The goal is to provide  a first-principles  proof of the duality for the spectrum of states and  also to establish its validity at the level of correlation functions of conformal operators. We also plan to extend  string-theoretic approach to gauge-string duality to less symmetric cases, corresponding, in particular, to certain non-supersymmetric conformal and n=1 supersymmetric non-conformal planar gauge theories.","1679584","2012-02-01","2017-09-30"
"GB-CORRELATE","Correlating the State and Properties of Grain Boundaries","Gerhard Dehm","MAX PLANCK INSTITUT FUR EISENFORSCHUNG GMBH","Phase diagrams revolutionized materials development by predicting the conditions for phase stability and transformations, providing a thermodynamic concept for materials design including synthesis, processing and application. Similarly, surface science has established thermodynamic concepts for surface states and transitions, but the analogon for grain boundaries (GB) is just emerging due to their complexity. GB are among the most prominent microstructure defects separating grains in polycrystalline materials spanning a multidimensional space. Unlocking control of GB phases and their transitions will enable a new level of materials design allowing to tailor functional & structural properties. This proposal targets on (i) predicting and resolving GB phase transitions, (ii) establishing guidelines for GB phase transitions and GB phase diagrams, (iii) correlating GB phase transitions with property changes, (iv) providing compositional-structural design criteria for GB engineering, (v) which will be tested by demonstrators with tailored GB strength and GB mobility. GB-CORRELATE focusses on Cu and Al alloys in form of thin films as this allows to implement a hierarchical strategy expanding from individual special GB to GB networks and a transfer of the GB concepts to thin film applications.
The infinite number of GB requires also statistical approaches; combinatorial thin film deposition will be used to establish Cu and Al alloy films with substitutional (Ag, Al, Cu, Si, Ni) and interstitial (B) solute elements. High throughput grain growth experiments will be employed to detect GB phase transitions by changes in GB mobility. Advanced atomic resolved correlated microscopy and spectroscopy supported by powerful computational approaches will identify GB phases and correlate them with transport properties. Sophisticated in-situ micromechanical studies lay the ground for interlinking GB phases and GB mechanics, finally harvested to create mechanically exceptional materials.","2500000","2018-08-01","2023-07-31"
"GC2.0","Global Change 2.0: Unlocking the past for a clearer future","Sandra Patricia HARRISON","THE UNIVERSITY OF READING","The terrestrial biosphere responds rapidly and sensitively to climate change and is important in mediating physical and biogeochemical feedbacks to climate. There are still enormous uncertainties in our understanding of how the terrestrial biosphere will respond to changes in climate in the 21st century, and large uncertainties in predictions of the climate feedbacks. Many issues that limit our ability to predict the future of the terrestrial biosphere can be addressed by examining what happened in the recent geologic past – where the drivers of climate change are relatively well known and there is abundant globally-distributed, quantitative, well-dated and unambiguous evidence of the biospheric response. The goal of this project is to unleash the power of the palaeo-record to understand the interactions of climate and the terrestrial biosphere, and to explain how terrestrial systems (vegetation, fire, hydrology, biogeochemical cycles including the carbon, trace gas and dust cycles) respond and contribute to long-term (millennial) and rapid (decadal to centennial) climate changes. I will use process-based models with global palaeodata syntheses to address four specific challenges to our understanding of past and future climate and environmental change: 
(1) How does vegetation respond to rapid climate change and what are the consequences of this response for climate?
(2) To what extent does increasing CO2 enhance tree growth or competitive fitness, and how does this translate into changes in ecosystems and ecosystem services?
(3) How does the terrestrial biosphere respond to changes in climate variability and the prevalence of extreme events?
(4) How does the land surface affect regional climates, and why do models persistently fail to predict these effects accurately?
In addressing these challenges, I will deliver public-access data sets, model outputs and comparison tools so the strengths of the palaeorecord can be exploited by the wider global change commun","2497563","2016-09-01","2021-08-31"
"GeCo","Data-Driven Genomic Computing","stefano CERI","POLITECNICO DI MILANO","Next-generation sequencing technology has dramatically reduced the cost and time of reading the DNA. Huge investments are targeted to sequencing the DNA of large populations, and repositories of well-curated sequence data are being collected. Answers to fundamental biomedical problems are hidden in these data, e.g. how cancer arises, how driving mutations occur, how much cancer is dependent on environment. But genomic computing has not comparatively evolved. Bioinformatics has been driven by specific needs and distracted from a foundational approach; hundreds of methods solve individual problems, but miss the broad perspective.

The objective of GeCo is to rethink genomic computing through the lens of basic data management. We will first design the data model, using few general abstractions that guarantee interoperability between existing data formats. Next, we will design a new-generation query language inspired by classic relational algebra and extended with orthogonal, domain-specific abstractions for genomics. Query processing will trace metadata and computation steps, opening doors to the seamless integration of descriptive statistics and high-level data analysis (e.g., DNA region clustering and extraction of regulatory networks).

Genomic computing is a “big data” problem, therefore we will also achieve computational efficiency by using parallel computing on both clusters and public clouds; the choice of a suitable data model and of computational abstractions will boost performance in a principled way. The resulting technology will be applicable to individual and federated repositories, and will be exploited for providing integrated access to curated data, made available by large consortia, through user-friendly search services.  Our most far-fetching vision is to move towards an Internet of Genomes exploiting data indexing and crawling. The PI’s background in distributed data, data modelling, query processing and search will drive a radical paradigm shift.","2500000","2016-09-01","2021-08-31"
"GEM-TRAIT","GEM-TRAIT: The Global Ecosystems Monitoring and Trait Study: a novel approach to quantifying the role of biodiversity in the functioning and future of tropical forests","Yadvinder Singh Malhi","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","""This proposal directly addresses one of the great challenges in Earth system science: how will the terrestrial biosphere respond to global atmospheric change and, more specifically, how does the biodiversity of the biosphere moderate or affect that response? This proposal focuses on tropical forests. We are currently unable to understand how tropical forests will respond to climate change because there is (i) a data-deficit: we simply do not have the data to understand the relationship between tropical forest diversity and ecosystem science; and (ii) a theory-deficit: we have not developed an adequate and quantitative theoretical framework to relate functional biodiversity to ecosystem function. This proposal will directly address both these deficits.
Firstly, I will build a unique global tropical ecosystems monitoring network (GEM), that will measure in comprehensive detail the structure, productivity and metabolism of 47 tropical forest sites over a globally synchronous 2.5 year period. In addition, I will develop a large dataset of functional diversity by collecting functional traits of leaves and wood.
Secondly, the theory deficit will be addressed by drawing on the recent development of a novel mathematical formalism that links biodiversity to ecosystem function. This formalism focuses on the distribution of traits within an ecosystem, links this distribution to ecosystem function, and develops predictions of how the shape of the distribution is controlled by environment, biological interactions and previous states of the ecosystem. I will further develop this theory, test its predictions against my unique field data, and ultimately use it to develop a new biodiversity-focussed way of representing tropical forests in ecosystem and Earth system models. This new approach used to answer questions such as: how does the functional diversity of tropical forests affect their resilience to climate change, and how will this diversity respond to atmospheric change?""","2500000","2013-05-01","2018-04-30"
"GEMETHNES","Geometric Measure Theory in non-Euclidean spaces","Luigi Ambrosio","SCUOLA NORMALE SUPERIORE","Geometric Measure Theory and, in particular, the theory of currents, is one of the most basic tools in problems in Geometric Analysis, providing a parametric-free description of geometric objects which is very efficient in the study of convergence, analysis of concentration and cancellation effects, chenges of topology, existence of solutions to Plateu&apos;s problem, etc. In the last years the PI and collaborators obtained ground-breaking results on the theory of currents in metric spaces and on the theory of surface measures in Carnot-Caratheodory spaces. The goal of the project is a wide range analysis of Geometric Measure Theory in spaces with a non-Euclidean structure, including infinite-dimensional spaces.","749800","2010-06-01","2015-05-31"
"GEMIS","Generalized Homological Mirror Symmetry and Applications","Ludmil Katzarkov","UNIVERSITAT WIEN","Mirror symmetry arose originally in physics, as a duality between $N = 2$ superconformal field theories. Witten formulated a more mathematically accessible version, in terms of topological field theories. Both conformal and topological field theories can be defined axiomatically, but more interestingly, there are several geometric ways of constructing them.  A priori, the mirror correspondence is not unique, and it does not necessarily remain within a single class of geometric models. The classical case relates $\sigma$-models, but in a more modern formulation, one has mirror dualities between different Landau-Ginzburg models, as well as between such models and $\sigma$-models; orbifolds should also be included in this. The simplest example would be the function $W: \C \rightarrow \C$, $W(x) = x^{n+1}$, which is self-mirror (up to dividing by the $\bZ/n+1$ symmetry group, in an orbifold sense). While the mathematics of the $\sigma$-model mirror correspondence is familiar by now, generalizations to Landau-Ginzburg theories are only beginning to be understood.  Today it is clear that Homologcal Mirror Symmetry (HMS)  as a categorical correspondence works and it is  time for developing direct geometric applications to classical problems - rationality of algebraic varieties and Hodge conjecture. This the main goal of the proposal. But in order to attack the above problems we need to generalize  HMS and explore its connection to new developments in modern Hodge theory.  In order to carry the above program we plan to further already working team Vienna, Paris, Moscow,  MIT.","1060800","2009-01-01","2013-12-31"
"GemX","Towards a ton-scale Ge-76 observatory for neutrinoless double beta decay","Stefan SCHÖNERT","TECHNISCHE UNIVERSITAET MUENCHEN","The observation that matter dominates over anti-matter in the Universe is one of the most critical open questions in physics. A natural explanation of this asymmetry postulates neutrinos as their own anti-particles, usually referred to as Majorana particles. The only practical way to establish the Majorana character of neutrinos is the experimental search for neutrinoless double-beta decay (NDBD). This decay violates lepton-number conservation and would establish new physics beyond the Standard Model of particle physics. The Germanium eXploration (GemX) project will focus on cutting-edge research towards a ton-scale NDBD decay experiment based on germanium detectors enriched in 76Ge, and thereby sustain a European leadership also in the next-generation worldwide experimental competition. With its superior energy resolution and lowest background, a one-ton 76Ge experiment has potentially the highest sensitivity for discovering NDBD decay amongst the next-generation experiments. A discovery would be groundbreaking in the fields of particle physics, astrophysics and cosmology. The goal of GemX is to develop and evaluate novel HPGe detectors enriched in 76Ge, test their performance in LEGEND-200 and inform the design decisions of the future flagship 1000-kg experiment LEGEND-1000, which the PI leads as elected European spokesperson. GemX will (1) investigate new Ge detector designs with increased mass and improved pulse shape discrimination to enhance background reduction; (2) develop a crystal growth process from germanium material enriched in 76Ge for large high-purity Ge crystals with suitable net-impurity concentrations in Europe; (3) develop the production of large Ge detectors enriched in 76Ge with minimal activation by cosmic radiation and with full control of surface contaminations from alpha contaminations; (4) deploy, test and operate the novel detectors in the TUM underground liquid argon test stand and in LEGEND-200 at the LNGS, Italy.","3355460","2018-10-01","2023-09-30"
"GENESIS","GEnerating extreme NEutrons for achieving controlled r-process nucleosyntheSIS","julien FUCHS","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The project aim is to perform the first direct measurements of neutron capture and beta-decay rates related to the “r-process” of nucleosynthesis. This process, based on squeezing at once multiple neutrons in a nucleus, is presently thought to be the main mechanism that forms the heaviest elements in our Solar System and in stars. 
At present, there are large discrepancies between the observed element abundances in stars and those found from simulations. It is speculated that this problem stems from the uncertainties in nuclear parameters, particularly in the plasma environment. These nuclear parameters have not been experimentally verified due to the too-low flux of current neutron facilities and the lack of means to create on-site hot and dense plasmas.
Lasers are not the first thing that comes to mind as a neutron source, but with the upcoming ultra high-power laser facilities (Apollon in 2018 and ELI-NP in 2019), high-density and high-energy protons can be generated. Through spallation, these can then produce neutrons with the needed flux, a flux comparable to that found in Supernovae. To further emulate the astrophysical scenario, auxiliary lasers can be used to turn the target material into a plasma.
In practice, this project will aim to measure neutron capture and beta-decay rates, as well as yields and abundances of the products of nucleosynthesis obtained by exposing heavy-ion targets to laser-produced extreme neutron fluxes. These targets will be either in a plasma or a solid state. In plasmas, we will investigate the effect of excited nuclear states, created by the plasma photons and electrons, on neutron capture. In solid targets, we will take advantage of the unique possibility of generating on-site unstable nuclei, and then re-expose them to the neutron beam in order to measure double neutron capture.","3494784","2019-01-01","2023-12-31"
"GeoBrown","Brownian geometry: at the interface between probability theory, combinatorics and mathematical physics.","Jean-François LE GALL","UNIVERSITE PARIS-SUD","The main purpose of this proposal is to explore the canonical models of planar random geometry that have been introduced in the recent years. We call this theory Brownian geometry because one of the central objects, the Brownian map, arises as the universal scaling limit of many  discrete models of large random graphs embedded in the plane, in a way very similar to Brownian motion, which is the continuous limit of many different classes of random paths. The preceding scaling limit holds for the Gromov-Hausdorff distance on compact metric spaces. Furthermore, recent developments show that, in addition to its metric structure, the Brownian map can be equipped with a conformal structure.

Our objectives will be to combine the different approaches to develop a systematic study of the Brownian map and its variants called the Brownian disk and the Brownian plane, as well as of the associated discrete models, which are finite graphs embedded in the plane or infinite random lattices such as the uniform infinite planar triangulation. We will also study random phenomena in random geometry, starting with random walks on infinite random lattices, with the ultimate goal of constructing Brownian motion on our continuous models. A question of importance in mathematical physics is to understand the behavior of statistical physics models in random geometry. Another fundamental question is to connect the conformal structure of the Brownian map with  the conformal embeddings that are known to exist for discrete planar maps.

The field of random geometry gives rise to exceptionally fruitful interactions between specialists of probability theory, theoretical physicists and mathematicians coming from other areas, in particular from combinatorics. To ensure the best chances of success for the proposed research, we will rely on the expertise of several members of the Laboratoire de Mathématiques d'Orsay, and on the unique environment of Université Paris-Sud and neighboring institutions.","1263607","2017-05-01","2022-04-30"
"GeoLocLang","GeoLocLang","Laurent FARGUES","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""I formulated recently a conjecture that should allow to geometrize the local Langlands correspondence over a non-archimedean local field. This mixes p-adic Hodge theory, the geometric Langlands program and the classical local Langlands correspondence. This conjecture says that given a discrete local Langlands parameter of a reductive group over a local field of equal or unequal characteristic, one should be able to construct a perverse Hecke eigensheaf on the stack of G-bundles on the ""curve"" I defined and studied in my joint work with Fontaine. 
I propose to construct, study and establish the basic properties of the geometric objects involved in this conjecture, this stack of G-bundles being a ""perfectoid stacks"" in the framework of Scholze theory of perfectoid spaces. At the same time I propose to establish the first steps in the proof of this conjecture,  study particular cases in more details and explore consequences of this conjecture.""","1301863","2017-10-01","2022-09-30"
"GETOM","Geometry and Topology of Open Manifolds","Gérard Robert Besson","UNIVERSITE GRENOBLE ALPES","The purpose of this project is to study the interactions between Riemannian Geometry and the topology of open manifolds. A general question is to find the best Riemannian metric on a given manifold; a related question is to understand the topological consequences of the existence of a metric with given properties. This programme has already been highly successful in the compact case whereas basic questions are not answered in the open case. The key tool which ought to be used in the three items is the Ricci flow with bubbling off,  in dimension 3, described by myself and my collaborators. This opened the way to applying Ricci flow to non-compact manifolds and it is a breakthrough which gives a very optimistic approach to some deep conjectures. The study of Whitehead manifolds will open a wide realm of research  since results are scarce. This will provide work for quite a few graduate students and for several years. My goal is to make such a significant progress that the subject will become proeminent in Riemannian Geometry. Again, for this the Ricci flow will be an unavoidable tool. Constructing explicit Riemannian metrics with various properties is another goal, which pertains to the same circle of ideas, and will lead to a systematic study of these spaces. This is a new and groundbreaking direction of research. I have already obtained some results in these direction and I intend to go much further and to enhance my national and international collaborations using the grant if accepted.","998277","2013-02-01","2018-01-31"
"GLASSDEF","Driven Glasses: from statistical physics to materials properties","Jean-Louis Barrat","UNIVERSITE GRENOBLE ALPES","Amorphous systems form a large fraction of the solid materials that surround us, from polymer glasses to mineral or metallic glasses, from toothpaste (a colloidal paste) to granular materials. Still, a theoretical framework for describing the mechanical properties of such materials, comparable to the dislocation theory that describes crystalline systems, is still missing. Our understanding of prominent experimental feature such as the heterogeneous character of deformation, or the temperature and rate dependence of the mechanical response, is very limited.
These materials indeed combine several difficulties. In contrast to liquids or crystals, they are intrinsically out of equilibrium, and their microstructure presents a large statistical distribution of mechanically distinct local environments.  The importance of the notion of heterogeneity in the mechanical behaviour of amorphous systems is being increasingly recognized, still there is no numerical or theoretical model that incorporates this microscopic feature into a macroscopic description of deformation and flow.
The aim of the proposed research program is to build such models, within a multiscale approach seeking inspiration from dislocation dynamics, from the statistical physics of glasses and from the physics of dynamical critical phenomena. The proposed approach is based on a combination of intensive numerical simulations at the atomic scale and at a coarse grained scale, which will necessitate the development of efficient numerical schemes. The statistical analysis will allow us to understand the universal and non universal features of material behaviour in terms of the interactions between the atomic constituents, and to establish the validity and importance of new concepts such as mechanical activation or dynamical heterogeneities.","1763858","2012-07-01","2017-06-30"
"GLC","Langlands correspondence and its variants","David Kazhdan","THE HEBREW UNIVERSITY OF JERUSALEM","Sometimes in the sciences there are different yet complementary descriptions for the same object. This extends to the particle-wave duality of quantum mechanics; one mathematical analog of this duality is the Fourier transform. Questions that are difficult when formulated in one language of science may become simple when interpreted in another. The Langlands conjecture posits the existence of a correspondence between problems in arithmetic and in Representation Theory. The Langlands conjecture has only been proven for a limited number of cases, but even this has solved problems such as the famous Fermat conjecture. The aim of this project is to continue study of the &quot;classical&quot; aspects of the Langlands conjecture and to extend the conjecture to the quantum geometric Langlands correspondence, higher-dimensional fields, Kac-Moody groups (with D.Gaitsgory: quantum Langlands correspondence; D.Gaitsgory and E. Hrushevsi: groups over higher-dimensional fields; A. Braverman: Kac-Moody groups; R. Bezrukavnikov, S.Debacker, Y.Varshavsky: classical aspects of the correspondence; A. Berenstein: geometric crystals and crystal bases). The quantum case is much more symmetric than the classical case and can lead in the limit q-&gt;0 to new insights into the classical case. The quantum case is also related to the multiple Dirichlet series. New results in the quantum case would lead to progress in understanding important Number Theoretic questions. Extending the Langlands correspondence to groups over higher-dimensional fields could substantially enlarge its applicability. Studying Kac-Moody groups would provide tools for the new important class of L-functions. This progress could lead to a proof of the existence of the analytic continuation of classical L-functions. The geometric Langlands correspondence is closely related to T-symmetry in 4-dimensional gauge theory and the understanding of this relation is important for both Mathematics and Physics.","1277060","2010-01-01","2014-12-31"
"GlobalBioIm","Global integrative framework for Computational Bio-Imaging","Michael Unser","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","A powerful strategy for increasing the quality and resolution of medical and biological images is to acquire larger quantities of data (Fourier samples for MRI, projections for X-ray imaging) and to jointly reconstruct the complete signal by correctly reallocating the measurements in 3D space/time and integrating all the information available. The underlying image sequence is reconstructed globally as the result of a very large-scale optimization that exploits the redundancy of the signal (spatio-temporal correlation, sparsity) to improve the solution. Due to recent advances in the field, we are arguing that such a “bigger data” integration is now within reach and that our team is ideally qualified to lead the way. A successful outcome will profoundly impact the design of future bioimaging systems.
We are proposing a unifying framework for the development of such next-generation reconstruction algorithms with a clear separation between the physical (forward model) and signal-related (regularization, incorporation of prior constraints) aspects of the problem. The pillars of our formulation are: an operator algebra with a corresponding set of fast linear solvers; an advanced statistical framework for the principled derivation of reconstruction methods; and learning schemes for parameter optimization and self-tuning. These core technologies will be incorporated into a modular software library featuring the key components for the implementation and testing of iterative reconstruction algorithms. We shall apply our framework to improve upon the state of the art in the following modalities: 1) phase-contrast X-ray tomography in full 3D; 2) structured illumination microscopy; 3) single-particle analysis in cryo-electron tomography; 4) a novel multipose fluorescence microscopy; 5) real-time MRI, and 6) a new multimodal digital microscope. In all instances, we shall work in close collaboration with the imaging scientists who are in charge of the instrumentation.","2499515","2016-10-01","2021-09-30"
"GlobalMass","Global land ice, hydrology and ocean mass trends","Jonathan Louis BAMBER","UNIVERSITY OF BRISTOL","Sea level rise will be one of the most serious and costly consequences of future climate change. Constraining the sources and sinks of sea level change is essential for understanding the drivers of past variations and for improving predictions of future behaviour. Matching estimates of sea level rise with the components that affect it is a long standing problem in geosciences spanning multiple disciplines: oceanography, glaciology, hydrology and solid Earth physics. Traditionally, each part of the problem has been tackled separately using different data, techniques and physical understanding. This is because of the challenge in determining just one component but also because of the different expertise and understanding within the various communities.  The proposed research will, for the first time, tackle all components simultaneously. I will combine all the relevant observations (both from satellites and in situ) with physical principles of the coupled system to solve for all components of the sea level budget. First, I will produce a data-driven estimate for glacio-isostatic adjustment that is independent of any assumptions about Earth structure or ice loading history. Second, I will partition the sea level budget into its steric, hydrological, cryospheric and solid earth components for 1981-2020. Third, I will apply the methods and datasets to re-evaluate the 20th Century sea level record. These advances will also result in the determination of regional mass trends of land ice and hydrology over a ~30 year period. In the process of attaining these goals in geosciences, I will also develop state of the art techniques for statistical inference of Big Data.  I have developed and tested the approach, using a subset of the data, for the Antarctic ice sheet. The approach is unique, global in scale and will address fundamental problems across four different disciplines in geosciences, as well as advancing techniques in statistical inference and computer science.","2397430","2016-08-01","2021-07-31"
"GLOBALSEIS","NEW GOALS AND DIRECTIONS FOR OBSERVATIONAL GLOBAL SEISMOLOGY","Augustinus Nolet","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","One of the major paradoxes in the geosciences is the contrast between the geochemical evidence for limited mass-exchange between lower and upper mantle, and the geophysical arguments for significant mass exchange, needed to prevent the mantle from melting in the geological past. Seismic tomography, when ultimately combined with geodynamical modeling, needs to provide estimates of present-day flux. Indeed, tomography has shown evidence for slabs penetrating into the lower mantle; but no quantitative information on the degree of mass exchange and heat flux can, as yet, reliably be obtained from tomographic images.  It is crucial that the boundary between upper- and lower mantle be imaged at greater precision, certainly in the plume-rich southern hemisphere. This requires a combined effort of improvements both experimentally and theoretically. Much progress has recently been obtained by my group in Princeton before I returned to Europe. I propose to build upon those accomplishments, and to (1) Expand the data acquisition to the oceans by developing hydrophone-equipped floats, with the goal to improve data coverage in regions that are important to investigate heat flux: the plume-rich southern hemisphere in particular, (2) Combine different seismological data sets spanning a wide range of frequencies, with the goal to obtain tomographic images that allow for a quantitative estimate of heat flux (both upwards through plumes and downwards through the sinking of slab fragments), with emphasis on the boundary between upper- and lower mantle, (3) Exploit the extra resolution offered by the frequency-dependent sensitivity of body waves (multifrequency tomography), (4) Incorporate wavelet expansions into the tomographic inversion, with the aim to resolve more detail in the model where the data allow a higher resolution, (5) Obtain a multidisciplinary interpretation of new  tomographic results through interaction with geodynamicists and geochemists.","2500000","2009-02-01","2015-01-31"
"GLOSTAR","A Global View of Star Formation in the Milky Way","Karl M. Menten","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Stars with more than about ten solar masses dominate galactic ecosystems and understanding the circumstances of their formation is one of the great challenges of modern astronomy. The spectacular HII regions they excite delineate the spiral arms of galaxies such as our own when seen face on making it clear that star formation and Galactic structure are intimately related. We propose to attain a Global View of Star Formation in the Milky Way in a powerful multi-pronged approach. Using VLBI observations of maser sources associated with young protostars, we will measure distances by trigonometric parallax to most of the dominant star forming regions in the Galaxy, which will reveal its spiral structure as well as faithfully represent the luminosity and masses of its constituents. A survey for submillimeter emission from dust, which we are presently pursuing, will deliver the locations of unseen deeply embedded protostars and protoclusters. We plan to combine this data with a comprehensive program to study the gaseous content of the protostellar regions and a very sensitive survey of the Galactic plane with the newly Expanded Very Large Array to find masers and hypercompact HII regions, pinpointing the very centers of the earliest star-forming activity. We also propose to study the infrared emission from more developed massive star clusters, deriving distance with the classic spectro-photometric method, properly calibrated with trigonometric parallaxes, and for the first time adapted to an extensive IR dataset. Our synoptic approach will utilize Europe s premier telescopes including ESO s VLT, the European VLBI Network, the APEX telescope, and ALMA to create a coherent, unique dataset with true legacy value for a global perspective on star formation in our Galaxy.","2355079","2010-05-01","2015-04-30"
"GLYCANAL","High-Throughput Cryogenic Spectroscopy for Glycan Analysis","Thomas RIZZO","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Glycans, or oligosaccharides, are ubiquitous in biological systems. Because they decorate the surface of cells, they play a key role in virtually all cellular recognition processes and are implicated in almost every major disease.  Despite their importance, the characterization of glycan primary structure lags far behind that of proteins and DNA because of their intrinsic isomeric complexity. The isomeric nature of the monosaccharide building blocks, the stereochemistry of the glycosidic bond, the possibility of multiple attachment points, and the occurrence of isomeric branched structures all make glycans difficult to analyze.

Although mass spectrometry (MS) is one of the most sensitive approaches for glycan analysis, it has difficulty to distinguish all these various types of isomerisms. Ion mobility spectrometry (IMS) combined with MS has demonstrated some ability to identify glycan anomers and regioisomers, but cannot easily distinguish isomeric disaccharides, for example.

We have recently demonstrated that cryogenic infrared spectroscopy provides unique vibrational fingerprints of glycans that distinguishes all the various types of isomerism. When combined with simultaneous measurements of mass and ion mobility, these fingerprints can be tabulated in a database and used to identify a given glycan from a mixture. However, adding a spectroscopic dimension to ion mobility and mass measurements requires additional time, which hampers it use as an analytical tool. To use spectroscopic data for real-world glycan analysis, one must multiplex the measurement process and record the vibrational spectrum of many species simultaneously.

This project involves designing and constructing an instrument that combines state-of-the-art ion mobility separation, cryogenic ion spectroscopy, and time-of-flight mass spectrometry to perform high throughput analysis of glycan primary structure.  The success of this project would represent a tremendous breakthrough for glycoscience.","2499801","2018-09-01","2023-08-31"
"GMODGAMMADYNAMICS","Dynamics on homogeneous spaces, spectra and arithmetic","Elon Lindenstrauss","THE HEBREW UNIVERSITY OF JERUSALEM","We consider the dynamics of actions on homogeneous spaces of algebraic groups,
We propose to tackle the central open problems in the area, including understanding actions of diagonal groups on homogeneous spaces without an entropy assumption, a related conjecture of Furstenberg about measures on R / Z invariant under multiplication by 2 and 3, and obtaining a quantitative understanding of equidistribution properties of unipotent flows and groups generated by unipotents.

This has applications in arithmetic, Diophantine approximations, the spectral theory of homogeneous spaces, mathematical physics, and other fields. Connections to arithmetic combinatorics will be pursued.","1229714","2011-01-01","2016-12-31"
"GOCAT","Gold(III) Chemistry: Structures, Bonding, Reactivity and Catalysis","Manfred Bochmann","UNIVERSITY OF EAST ANGLIA","Maximising energy efficiency and competitiveness while minimising waste and environmental impact of chemical processes, from large scale commodities to fine chemicals and pharmaceuticals, depends crucially on catalysis, and in particular on our ability to tailor catalysts to specific needs. Gold catalysts have seen a meteoric rise in recent years (nearly 40,000 WOS citations in 2011). However, since gold was so long considered inert, major compound classes are unknown. For example, we recently synthesized the first thermally stable gold(III) hydrides, gold peroxides (the first for gold in any oxidation state) and gold(III) alkene complexes (while platinum analogues of the latter have been known since 1827). Based on >20 years of pioneering research into the identification of catalytically active species and homogeneous catalytic reaction mechanisms, our ground-breaking results will form the basis of an ambitious programme on gold chemistry to delineate the structures and reactivities of these major classes of complexes. Ligand design will be of crucial importance to achieve the stability required for catalytic and synthetic applications. We have also found unprecedented reactivity that links these complexes to the water splitting cycle and hydrogen generation. Structure-reactivity relationships and mechanisms will be established that provide the knowledge base for general applicability.  The outcomes will underpin synthetic methodology for fine chemicals and pharmaceuticals and impact on the materials and medicinal applications of gold complexes.","2439223","2014-02-01","2019-01-31"
"GRACE","Resource Bounded Graph Query Answering","Wenfei Fan","THE UNIVERSITY OF EDINBURGH","When we search for a product, can we find, using a single query, top choices ranked by Google and at the same time, recommended by our friends connected on Facebook? Is such a query tractable on the social graph of Facebook, which has over 1.31 billion nodes and 170 billion links? Is it feasible to evaluate such a query if we have bounded resources such as time and computing facilities? These questions are challenging: they demand a departure from the traditional query evaluation paradigm and from the classical computational complexity theory, and call for new resource-constrained methodologies to query big graphs.

This project aims to tackle precisely these challenges, from fundamental problems to practical techniques, using radically new approaches. We will develop a graph pattern query language that allows us to, e.g., unify Web search (via keywords) and social search (via graph patterns), and express graph pattern association rules for social media marketing. We will revise the conventional complexity theory to characterize the tractability of queries on big data, and formalize parallel scalability with the increase of processors. We will also develop algorithmic foundations and resource-constrained techniques for querying big graphs, by ""making big data small"". When exact answers are beyond reach in big graphs, we will  develop data-driven and query-driven approximation schemes to strike a balance between the accuracy and cost. As a proof of the theory, we will develop GRACE, a system to answer graph pattern queries on big GRAphs within bounded resourCEs, based on the techniques developed. We envisage that the project will deliver methodological foundations and practical techniques for querying big graphs in general, and for improving search engines and social media marketing in particular. A breakthrough in this subject will advance several fields, including databases, theory of computation, parallel computation and social data analysis.","2171524","2015-11-01","2020-10-31"
"GRACOL","Graph Theory: Colourings, flows, and decompositions","Carsten Thomassen","DANMARKS TEKNISKE UNIVERSITET","Graph theory is a relatively new branch of mathematics. Early sources of inspiration are Kirchhoff’s theory of electrical networks and the 4-color problem, both from the 19th century. In the 20th century graph theory was one of the most rapidly growing branches of mathematics with applications to theoretical computer science (design and analysis of algorithms), operations research (combinatorial optimization) and models in engineering and economics. The internet may be thought as a graph. There are also strong ties to geometry, topology, probability theory and logic.

The main subjects in the project are graphs in the plane and on higher surfaces, graph decomposition, the Tutte polynomial and the graph flow conjectures, and also combinatorial problems arising from differential geometry. The project is centered around applying new approaches to some classical problems in graph theory, in particular problems in chromatic graph theory and flow theory. In some sense these problems have an algebraic unification in the Tutte polynomial of two variables. The Tutte polynomial has as special valuations (fixing one of the variables) the chromatic polynomial (introduced in 1912 by Birkhoff) and the flow polynomial. More recently, the Tutte polynomial has also become of interest in statistical mechanics.

Among the specific problems to be investigated is Tutte’s 3-flow conjecture from the early 1970es, the problem if the flow polynomial can have arbitrarily large roots (motivated by Tutte’s 5-flow conjecture), the Merino-Welsh conjecture on the numbers of spanning trees, acyclic orientations and totally cyclic orientations, and Wegner’s conjecture from 1977 about squares of planar cubic graphs. We expect to get significant new insight (but not complete solutions) to the two notoriously hard flow conjectures of Tutte (both of which are also described in Wikipedia). We expect to almost solve the Merino-Welsh conjecture. We expect to completely solve the Wegner conjecture.","1518471","2013-02-01","2018-01-31"
"GRAPHALGAPP","Challenges in Graph Algorithms with Applications","Monika Hildegard Henzinger","UNIVERSITAT WIEN","This project has two thrusts of equal importance. Firstly, it aims to develop new graph algorithmic techniques, specifically in the areas of dynamic graph algorithms, online algorithms and approximation algorithms for graph-based optimization problems. Thus, it proposes to solve long-standing, fundamental problems that are central to the field of algorithms. Secondly, it plans to apply these techniques to graph algorithmic problems in different fields of application, specifically in computer-aided verification, computational biology, and web-based advertisement with the goal of significantly advancing the state-of-the-art in these fields. This includes theoretical work as well as experimental evaluation on real-life data sets.

Thus, the goal of this project is a comprehensive approach to algorithms research which involves both excellent fundamental algorithms research as well as solving concrete applications.","2428258","2014-03-01","2019-08-31"
"Graphene and Beyond","Theory of Two-Dimensional Materials: Graphene and Beyond","Vladimir Falko","LANCASTER UNIVERSITY","This projects aims to explore physics of the new class of materials: atomically thin films of layered crystals. Graphene, because of its extraordinary electronic properties, will be a major focus of this project. In view of application of graphene in electronics, we shall model electronic transport and dynamical properties of devices based upon epitaxial graphene (monolayer and bilayer), graphene deposited on atomically flat substrates, and chemically modified graphene. In the family of graphene, the bilayer allotrope is the less understood, though it has already been discovered to have quite unique electronic properties, and we shall develop theories of the electron-electron correlation effects, quantum transport and quantum Hall effect in bilayer graphene. But beyond graphene, we shall also investigate electronic properties of ultrathin films of hexagonal boron nitride on account of its insulating and optical properties, and on account of their use in hybrid devices such as graphene/h-BN based transistors. In parallel, we shall search for new opportunities in the world of two-dimensional materials beyond graphene. For this, we shall model theoretically electronic properties, correlations effects, optical properties, and electronic transport properties of single layers and bilayers of hexagonal transition-metal dichalcogenides with a broad range of composition and ultrathin films of bismuth-based trichalcogenides.","430271","2012-10-01","2013-10-31"
"GRAPHENOCHEM","Large Scale Production, Cloning, Chemical Functionalization and Materials Applications of Graphene","Andreas Hirsch","FRIEDRICH-ALEXANDER-UNIVERSITAET ERLANGEN NUERNBERG","We propose the development of modern wet chemical concepts for the mass production and chemical modification of graphene - a rapidly rising star on the horizon of materials science - opening the door for superior but still elusive applications such as transparent electrodes, field effect transistors, solar cells, gas sensors and polymer enforcement. Owing to its spectacular electronic properties graphene is expected to be the most promising candidate to replace classical Si-technology and no longer requires any further proof of its importance in terms of fundamental physics. However, fully exploiting the proposed applications requires the availability of processable graphene in large quantities, which generally has been considered to be an insurmountable challenge. This is where the GRAPHENOCHEM project sets in. Our laboratory has been pioneering and is at the forefront of carbon allotrope chemistry. After having investigated basic principles for the functionalization of the 0-dimensional fullerenes and the 1-dimensional carbon nanotubes, which lead to synthesis of numerous examples of derivatives with tailor made properties, we recently started successfully with the investigation of wet chemical approaches for the efficient production of graphene sheets using graphite as an inexpensive starting material. The strategy of GRAPHENOCHEM is to combine chemistry, nanotechnology and materials science to establish highly efficient protocols for the mass production of soluble graphene and the subsequent processing to a whole variety of thins films, composites and devices with outstanding properties. To our knowledge we are the first synthetic organic chemists facing this challenge. We propose to go through the following sequential key objectives, namely: Development of efficient protocols for the mass production of soluble single layer graphene, cloning of graphene, chemical functionalization and doping of graphene, and engineering of graphene based materials and devices.","1436400","2010-02-01","2015-01-31"
"GravBHs","A New Strategy for Gravity and Black Holes","ROBERTO ALEJANDRO EMPARAN GARCIA DE SALAZAR","UNIVERSITAT DE BARCELONA","General Relativity (GR) encompasses a huge variety of physical phenomena, from the collision of astrophysical black holes, to the dynamics (via holography) of strongly-coupled plasmas and the spontaneous symmetry-breaking in superconductors. Black holes play a central role in all this. However, their equations are exceedingly hard to solve. The apparent lack of a generic tunable parameter that allows to solve the theory perturbatively (like the electric coupling constant in electrodynamics, or the rank of the gauge group in large-N Yang-Mills theory) is arguably the single most important obstacle for generic efficient approaches to the physics of strong gravity and black holes. I argue that one natural parameter suggests itself: GR can be defined in an arbitrary number of dimensions D. Recently I have demonstrated that the limit of large D is optimally tailored for the investigation of black holes, classical and potentially also quantum. Explicit preliminary studies have proved that the concept is sound, powerful, and applicable even in four dimensions.
This encourages the pursuit of a full-scale program with two major goals:
(A) Reformulating GR and Black Hole physics around the large-D limit in terms of an effective membrane theory of black holes, coupled (non-perturbatively in 1/D) to an effective theory for gravitational radiation.
(B) Resolution of outstanding problems in gravitational physics, in particular of problems of direct relevance to cosmic censorship (critical collapse, endpoint of black brane instabilities), and of the quantum theory of black holes.
With the new tools of (A), a large number of additional problems in black hole physics and in holographic duality can be solved, which guarantee very substantial fallback objectives. These include black hole collisions, black hole phase diagrams, instabilities, holographic dynamics of finite-temperature systems, and potentially any problem that can be formulated in an arbitrary number of dimensions.","2138825","2016-10-01","2021-09-30"
"GravityLS","Large Scale Structure Constraints of General Relativity","Pedro Tonnies Gil Ferreira","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The past decade has witnessed a phenomenal success in relativistic cosmology. It is now possible to constrain the structure, content and evolution of the Universe with an accuracy of a few percent. The current focus of cosmology is to understand some of the fundamental problems in physics such as: the accelerated expansion of the universe, the nature of dark energy and dark matter and the initial conditions of large scale structure. Key to addressing these problems is ascertaining the role general relativity plays in cosmology. While we now have remarkable astrophysical constraints on general relativity on milliparsec scales, we have no direct cosmological constraints on gigaparsec scales. Given the quality of the up and coming cosmological data, it is now time to tackle the challenge of constraining general relativity on large scales.

I propose a coordinated program to study, model and test general relativity with the aim of obtaining definitive constraints from cosmology. My programme will have three themes. A theoretical first theme will focus on developing a unified formalism for describing deviations from general relativity on cosmological scales.  A phenomenological second theme will develop the tools necessary to model the three main regimes of large scale structure: horizon size (or ultra-large) scales, the quasi-static scales (which most current surveys are targeting) and the non-linear scales. In the final, observational theme I will develop the tools needed for the data analysis of current and future cosmological surveys (such as redshift and weak lensing surveys) and a complete pipeline for the analysis of the Euclid, LSST and SKA next generation surveys. As a result I will determine the state of the art constraints on general relativity on cosmological scales and in doing so, definitively establish the role of gravity in the accelerated expansion of the universe.","2863366","2016-09-01","2021-08-31"
"GravQuantMat","Gravity, Black Holes and Strongly Coupled Quantum Matter","Jerome Gauntlett","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","States of matter in which the interactions between the microscopic constituents are both strong and quantum mechanical lie at the frontier of our understanding of nature. Such states appear in a wide variety of settings including high temperature superconductors, gases of cold atoms and the quark- gluon plasma created in the high-energy collisions of nuclei. Understanding the properties of such strongly coupled quantum matter poses huge conceptual challenges because standard perturbative techniques break down at strong coupling. In a remarkable development, the mathematical framework of string theory has provided a fundamentally new way to study strongly coupled quantum field theories using a dual, weakly coupled gravitational description. Furthermore, this duality states that the phase structure of the quantum field at finite temperature is precisely described by black hole geometries. The principal thrust of the proposal is to develop our understanding of these gravitational techniques in order to make contact with real world systems, particularly in condensed matter physics.

The proposal focuses on four main topics in this emerging, rapidly developing and interdisciplinary field. The first is to extend our understanding of known strongly coupled quantum critical ground states using gravitational solutions and also to search for new ones. The second is to map out the phase structure of strongly coupled quantum field theories at finite temperature by constructing a wide variety of new black hole solutions. Superconducting and spatially modulated phases will be a particular focus. Thirdly, fermion spectral functions will be calculated to extend our understanding of non-Fermi liquids, which are known to arise in many materials. The fourth topic is to explore the behaviour of strongly coupled systems in situations far from thermal equilibrium by studying the dynamical process of black hole formation.","1963542","2014-03-01","2019-02-28"
"GRBS","Gamma Ray Bursts as a Focal Point of High Energy Astrophysics","Tsvi Piran","THE HEBREW UNIVERSITY OF JERUSALEM","Gamma-Ray Bursts (GRBs), short and intense bursts of gamma-rays originating from random directions in the sky, are the brightest explosions in our Universe. They involve ultra-relativistic motion, huge magnetic fields, the strongest gravitational fields, acceleration of photons, neutrinos and cosmic rays to ultra high energies, the collapse of massive stars, mergers of neutron star binaries and formation of newborn black holes. They are at the focal point of relativistic high energy astrophysics and they serve as the best laboratory for extreme physics.    The internal-external shocks model was formulated to explain their inner working. This model had impressive successes in interpreting and predicting GRB properties. Still it had left many fundamental questions unanswered. Furthermore, recently it has been confronted with puzzling Swift observations of the early afterglow and it is not clear if it needs minor revisions or a drastic overhaul.  I describe here an extensive research program that deals with practically all aspects of GRB. From a technical point of view this program involves sophisticated state of the art computations on one hand, fundamental theory and phenomenological analysis of observations and data analysis on the other one.  My goal is to address both old and new open question, considering, among other options the possibility that the current model has to be drastically revised.  My long term goal, beyond understanding the inner working of GRBs, is to create a unified theory of accretion acceleration and collimation and of emission of high energy gamma-rays and relativistic particles that will synergize our understanding of GRBs, AGNs, Microquasars, galactic binary black holes SNRs and other high energy astrophysics phenomena. A second hope is to find ways to utilize GRBs to reveal new  physics that cannot be explored otherwise.","1933460","2009-01-01","2014-12-31"
"GREENEST","Gas turbine combustion with Reduced EmissioNs Employing extreme STeam injection","Christian Oliver Rudolf Martin Paschereit","TECHNISCHE UNIVERSITAT BERLIN","Global energy consumption is continuously increasing, leading to an increased world wide demand for new power generation installations in the near future. In order to protect the earth s climate, energy conversion efficiency and the use of sustainable resources have to be improved significantly to reduce the emission of the greenhouse gas CO2. To maintain our high standard of living and to enhance it for developing countries, the improved technologies have to be cost-neutral. Gas turbines play today a major role in energy generation. In the future, gas turbines will become even more important, when old coal-fired steam cycle power plants are replaced by integrated gasification plants. However, current gas turbine technology experiences a flattening technology curve and further increase in total efficiency at low NOx emissions is only achieved in incremental small steps. Additionally, current technology is not prepared to operate on hydrogen-rich fuels from biological resources or coal gasification. A new approach was developed that promises a significant improvement in efficiency and emissions and provides the ability to burn hydrogen-rich fuels. For operation on carbon-containing fuels, it enables CO2 capture at low cost. The concept is based on a high pressure air-steam gas turbine cycle using extremely high amounts of steam. The goal of the proposed project is to investigate the fundamentals of ultra wet combustion to develop the technology for a prototype combustor which is capable of burning natural gas, hydrogen and fuels from coal or biowaste gasification at low NOx emissions. Research will include the combustion process, the aerodynamic design, acoustics and control, combining the main disciplines of the Chair of Experimental Fluid Dynamics.","3137648","2010-07-01","2016-06-30"
"GRIFFIN","General compliant aerial Robotic manipulation system Integrating Fixed and Flapping wings to INcrease range and safety","Anibal OLLERO","UNIVERSIDAD DE SEVILLA","The goal of GRIFFIN is the derivation of a unified framework with methods, tools and technologies for the development of flying robots with dexterous manipulation capabilities. The robots will be able to fly minimizing energy consumption, to perch on curved surfaces and to perform dexterous manipulation. Flying will be based on foldable wings with flapping capabilities. They will be able to safely operate in sites where rotorcrafts cannot do it and physically interact with people.  Dexterous manipulation will be performed maintaining fixed contact with a surface, such as a pole or a pipe, by means of one or more limbs and manipulating with others overcoming the limitations of dexterous manipulation in free flying of existing aerial manipulators. Compliance will play an important role in these robots and in their flight and manipulation control methods. The control systems will be based on appropriate kinematic, dynamic and aerodynamic models. The GRIFFIN robots will have autonomous perception, reactivity and planning based on these models. They will be also able to associate with others to perform cooperative manipulation tasks. New software tools will be developed to facilitate the design and implementation of these complex robotic systems. Thus, configurations with different complexity could be derived depending on the requirements of flight endurance and manipulation tasks from simple grasping to more complex dexterous manipulation. The implementation will be based on additive and shape deposition manufacturing to fabricate multi-material parts and parts with embedded electronics and sensors. In GRIFFIN we will develop a small flapping wings proof of concept prototype which will be able to land autonomously on a small surface by using computer vision, a manipulation system with the body attached to a pole, and finally full size prototypes which will demonstrate flying, landing and manipulation, including cooperative manipulation, by maintaining the equilibrium.","2499750","2018-11-01","2023-10-31"
"GRINDOOR","Green Nanotechnology for the Indoor Environment","Claes-Göran Sture Granqvist","UPPSALA UNIVERSITET","The GRINDOOR project aims at developing and implementing new materials that enable huge energy savings in buildings and improve the quality of the indoor environment. About 40% of the primary energy, and 70% of the electricity, is used in buildings, and therefore the outcome of this project can have an impact on the long-term energy demand in the EU and the World. It is a highly focused study on new nanomaterials based on some transition metal oxides, which are used for four interrelated applications related to indoor lighting and indoor air: (i) electrochromic coatings are integrated in devices and used in “smart windows” to regulate the inflow of visible light and solar energy in order to minimize air condition and create indoor comfort, (ii) thermochromic nanoparticulate coatings are used on windows to provide large temperature-dependent control of the inflow of infrared solar radiation (in stand-alone cases as well as in conjunction with electrochromics), (iii) oxide-based gas sensors are used to measure indoor air quality especially with regard to formaldehyde, and (iv) photocatalytic coatings are used for indoor air cleaning. The investigated materials have many things in common and a joint and focused study, such as the one proposed here, will generate important new knowledge that can be transferred between the various sub-projects. The new oxide materials are prepared by advanced reactive gas deposition—using unique equipment—and high-pressure reactive dc magnetron sputtering. The materials are characterized and investigated by a wide range of state-of-the-art techniques.","2328726","2011-06-01","2016-05-31"
"GROGandGIN","Growth in Groups and Graph Isomorphism Now","Laszlo Pyber","MAGYAR TUDOMANYOS AKADEMIA RENYI ALFRED MATEMATIKAI KUTATOINTEZET","""In recent years there has been spectacular progress in studying growth in groups. A central result in this new area, obtained by Pyber-Szabo' (with a similar result proved by Breuillard-Green-Tao), shows that powers of generating subsets of finite simple groups of """"bounded dimension"""" grow fast. Extending this Product Theorem Szabo' and the PI also proved a weaker version of a conjecture of Helfgott-Lindenstrauss. The Product Theorem has deep consequences in the study of groups, number theory and random walks. A central open question of the area is to remove the dependence on dimension in our Product Theorem. The PI formulated a new Conjecture, as a step forward. The way to further progress is via combining techniques from asymptotic group theory and probability theory. It is from this perspective that the current GROGandGIN proposal addresses issues concerning random walks. We examine how recent probabilistic arguments for random walks in the symmetric group may be transferred to matrix groups. While the first results in the subject of growth concern matrix groups we see an evolving theory of growth in permutation groups. This relies on earlier work of Babai and the PI which aims at finding proofs which do not use the Classification of Finite Simple Groups (CFSG). Similarly, Babai's famous Quasipolynomial Graph Isomorphism Algorithm builds on ideas from CFSG-free proofs due to him. The PI has recently removed CFSG from the analysis of Babai's algorithm. Our method goes """"halfway"""" towards removing CFSG from proofs of growth results for permutation groups, currently a major open problem. The GROGandGIN initiative plans to improve various other parts of Babai's paper, working with several people who look at it from different angles, with an eye towards obtaining a Polynomial Graph Isomorphism algorithm. The GROGandGIN team will also study growth in Lie groups since the theory of random walks in Lie groups has been revitalised using analogues of our Product Theorem.""","1965340","2017-08-01","2022-07-31"
"GUDHI","Algorithmic Foundations of Geometry Understanding in Higher Dimensions","Jean-Daniel Boissonnat","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","""The central goal of this proposal is to settle the algorithmic
foundations of geometry understanding in dimensions higher than 3.  We
coin the term geometry understanding to encompass a collection
of tasks including the computer representation and the approximation
of geometric structures, and the inference of geometric or topological
properties of sampled shapes.

The need to understand geometric structures is ubiquitous in science
and has become an essential part of scientific computing and
data analysis.
Geometry understanding is by no means limited to
three dimensions. Many applications in physics, biology,
and
engineering require a keen understanding of the geometry of a variety
of higher dimensional spaces to
capture concise information from the underlying often highly
nonlinear structure of
data. Our approach is complementary to manifold learning
techniques and aims at  developing an effective theory for geometric and
topological data analysis.

To reach these objectives,  the guiding principle will be to foster a
symbiotic relationship between theory and practice, and to address
fundamental research issues along three parallel advancing
fronts. We will simultaneously develop mathematical approaches
providing theoretical guarantees, effective algorithms that are
amenable to theoretical analysis and rigorous experimental validation,
and perennial software development.  We will undertake the
development of a high-quality open source software platform to
implement the most important geometric data structures and algorithms
at the heart of geometry understanding in higher dimensions. The
platform will be a unique vehicle towards researchers from other
fields and will serve as a basis for groundbreaking advances in
scientific computing and data analysis.""","2497597","2014-02-01","2019-01-31"
"GUIDEDNW","Guided Nanowires: From Growth Mechanism to 
Self-Integrating Nanosystems","Pablo Ernesto Joselevich Fingermann","WEIZMANN INSTITUTE OF SCIENCE LTD","The large-scale assembly of nanowires (NWs) with controlled orientation on surfaces remains one challenge toward their integration into practical devices. A recent paper in Science from the PI’s group reported the guided growth of millimeter-long horizontal NWs with controlled orientations on crystal surfaces. The growth directions and crystallographic orientation of GaN NWs are controlled by their epitaxial relationship with different planes of sapphire, as well as by a graphoepitaxial effect that guides their growth along surface steps and grooves. Despite their interaction with the surface, these horizontally grown NWs have surprisingly few defects, exhibiting optical and electronic properties superior to those of vertically grown NWs. We observed that whereas in a 2D film stress accumulates in two directions, in a NW stress accumulates along its axis, but can relax in the transversal direction, making the 1D system much more tolerant to mismatch than a 2D film. This new 1D nanoscale effect, along with the graphoepitaxial effect, subverts the paradigm not only in the young field of NWs, but also in the established field of epitaxy. This paves the way to highly controlled semiconductor structures with potential applications not available by other means.

The aim of this project is to investigate the guided growth of NWs and unleash its vast possibilities toward the realization of self-integrating nanosystems.

First, we will generalize the guided growth of NWs to a variety of semiconductors and substrates, and produce ordered arrays of NWs with coherently modulated composition and doping.

Second, we will conduct fundamental studies to investigate the correlated structure, growth mechanism, optical and electronic properties of guided NWs.

Third, we will exploit the guided growth of NWs for the production of various functional self-integrating systems, including nanocircuits, LEDs, lasers, photovoltaic cells, photodetectors, photonic and nonlinear optical devices.","2063872","2014-02-01","2019-01-31"
"H2-SMS-CAT","Engineering of Supported Molten Salt Catalysts for Dehydrogenation Reactions and Hydrogen Production Technologies","Peter Wasserscheid","FRIEDRICH-ALEXANDER-UNIVERSITAET ERLANGEN NUERNBERG","The ultimate goal in the development of more efficient catalytic technologies is to combine selectivity, productivity, robustness and ease of processing on the highest possible level. For this purpose, new approaches to integrate molecular catalysis into heterogeneous systems are required. This H2-SMS-CAT project aims to establish homogeneous catalysis in heterogeneous systems in the temperature range of 200°C to 500°C. The project will focus on the engineering of Supported Molten Salt catalysts, i.e. materials that contain as the catalytic active film a eutectic molten salt mixture which is immobilized on the high internal surface of an inorganic support. Within the project, the H2-SMS-CAT technology will be exemplified for selected dehydrogenation reactions and hydrogen production technologies. The proposed demonstrator applications are of great technical relevance in the context of hydrogen storage and transportation technologies and for catalytic alkane activation.
Our team is ideally suited to undertake this venture. We are excited by the idea to combine our top-level expertise in ionic liquid/molten salt chemistry, organometallics and reaction engineering to unlock high temperature applications for molecular defined, homogeneous, high temperature dehydrogenation catalysis. The project will cover aspects of support material engineering, catalytic eutectics development, catalyst and reactor design as well as mechanistic and spectroscopic investigations. In case of success, the outcome of this project will be of fundamental relevance for the whole field of catalysis. New insight into the nature of operating catalytic materials can be expected from a detailed comparison of classical metal-on-support catalysts with our new H2-SMS-CAT systems.","1862400","2011-01-01","2015-12-31"
"HADE","Harmonic Analysis and Differential Equations: New Challenges","Luis Vega","UNIVERSIDAD DEL PAIS VASCO/ EUSKAL HERRIKO UNIBERTSITATEA","This project sets forth cutting-edge challenges in the field of Mathematical Physics that will be solved within a common framework by making novel use of classical tools of Harmonic Analysis such as Oscillatory Integrals and Trigonometric Sums, the Cauchy operator, and the so-called Carleman estimates. Three aspects will be covered:
1. Vortex Filament Equation (VFE).
2. Relativistic and Non-relativistic Critical Electromagnetic Hamiltonians.
3. Uncertainty Principles (UPs) and Applications.
The interaction of vortex filaments is considered a key issue in order to understand turbulence which is seen by many as the most relevant unsolved problem of classical physics. VFE first appeared as an approximation of the dynamics of isolated vortex filaments. I want to understand what happens when at time zero the filament is a regular polygon. Preliminary theoretical arguments together with some numerical experiments suggest that the different corners behave like different vortex filaments that interact with each other in such a way that the dynamics seem chaotic. I will prove the so-called Frisch-Parisi conjecture, showing that behind this chaotic behavior there is an underlying algebraic structure that controls the dynamics.
The Dirac equation, despite being one of the basic equations of Mathematical Physics, is very poorly understood from an analytical point of view. I will use the classical Cauchy operator in a modern way to explain some key Hamiltonian systems such as the MIT bag model for quark confinement.
UPs are at the heart of different fields like Quantum Mechanics, Harmonic Analysis, and Information Theory. We want to use a new approach to analyze modern versions of UPs that are not well understood. In order to do this, I will look at the problem from the point of view of partial differential equations making novel use of the Carleman estimates. This analysis will also be extended to the discrete setting where even classical UPs such the one by Hardy are not solved yet","1672103","2015-12-01","2020-11-30"
"HADES","Benthic diagenesis and microbiology of hadal trenches","Ronnie N Glud","SYDDANSK UNIVERSITET","With this project, called HADES, we aim to provide the first detailed, combined analysis of benthic diagenesis and microbial ecology of some of the deepest oceanic trenches on Earth. We argue that deep trenches, some of the most remote, extreme, and scantly explored habitats on Earth, are hotspots of deposition and mineralization of organic material. With the development of novel autonomous in situ instrumentation to overcome large sampling artifacts from decompression, we will i) determine rates of benthic metabolism and the importance of the deep trenches for the marine carbon and nitrogen cycles, ii) explore the unique benthic microbial communities driving these processes, and iii) investigate the proposed great role of virus in regulating microbial performance and carbon cycling in hadal sediments. By comparing trenches from contrasting oceanic settings the project provides a completely novel general analysis of hadal biogeochemistry and the role of deep trenches in the oceans, as well as fundamental new insights into the composition and functioning of microbial communities at extreme pressure.","3185000","2016-01-01","2020-12-31"
"HARG","Harmonic analysis on reductive groups","Eric Marcus Opdam","UNIVERSITEIT VAN AMSTERDAM","We propose to attack a variety of fundamental open problems in
harmonic analysis on $p$-adic and real reductive groups.

Specifically we seek solutions to the local Langlands conjectures
and various normalization problems of discrete series representations.
For $p$-adic groups, affine Hecke algebras are a major technical tool.
Our understanding of these algebras with unequal parameters has
advanced recently and allows us to address these problems.
We will compute the Plancherel measure on the Bernstein components
explicitly. Using a new transfer principle of Plancherel measures
between Hecke algebras we will combine Bernstein components to form
$L$-packets, following earlier work of Reeder in small rank.
We start with the tamely ramified case, building on work of
Reeder-Debacker. We will also explore these methods for $L$-packets
of positive depth, using recent progress due to Yu and others.
Furthermore we intend to study non-tempered
unitary representations via affine Hecke algebras, extending the
work of Barbasch-Moy on the Iwahori spherical unitary dual.

As for real reductive groups we intend to address essential
questions on the convergence of the Fourier-transform. This theory
is widely developed for functions which transform finitely under a
maximal compact subgroup. We wish to drop this condition in order
to obtain global final statements for various classes of rapidly
decreasing functions. We intend to extend our results to certain types of
homogeneous spaces, e.g symmetric and multiplicity one spaces. For doing
so we will embark to develop a suitable spherical character theory for
discrete series representations and solve the corresponding normalization
problems.

The analytic nature of the Plancherel measure and the correct interpretation
thereof is the underlying theme which connects the various parts of
this proposal.","1769000","2011-03-01","2016-02-29"
"HARMONY","""Harmonic identification, mitigation and control in power electronics based power systems""","Frede Blåbjerg","AALBORG UNIVERSITET","""Global electrical energy consumption is still increasing which demands that power capacity and power transmission capabilities must be doubled within 20 years. Today 40 % of the global energy consumption is processed by electricity in 2040 this may be up to 70 %. Electrical power production is changing from conventional, fossil based sources to renewable power resources. Highly efficient and sustainable power electronics in power generation, power transmission/distribution and end-user applications are introduced to ensure more efficient use of electricity. Traditional centralized electricity production with unidirectional power flows in transmission and distribution system will be replaced by the operation and control of intelligent distribution systems which are much more based on power electronics systems and having bidirectional power flow. Such large scale expansion of power electronics usage will change the characteristic of the power system by introducing more harmonics from generation, from the efficient load systems all resulting in a larger risk of instability and more losses in the future power system. The projects goal is to obtain “Harmony” between the renewable energy sources, the future power system and the loads in order to keep stability at all levels seen from a harmonic point of view. The project establishes the necessary theories, models and methods to identify harmonic problems in a power electronic based power system, a theoretical and hardware platform to enable control of harmonics and mitigate them, and develops on-line methods to monitor the harmonic state of the power system. The outcomes are new tools for identifying stability problems in power electronics based power systems and new control methods for reducing the harmonic presence and reduce the overall instability risks. Further, new design methods for active and passive filters in renewable energy systems, in the power system and in the power electronics based loads will be developed""","2500000","2013-03-01","2018-02-28"
"HAS","Harmonic Analysis and l-adic sheaves","David Kazhdan","THE HEBREW UNIVERSITY OF JERUSALEM","""In  recent years there has been impressive development of the higher category theory and in particular development of the categorical counterpart of the Langlands conjecture over fields of finite characteristic. But until now, this development has had little bearing on the classical problems which deal with spaces of functions. The main goal of this proposal is to build the technique to apply the category theory  to classical problems. Of course on the way I will have to deal with problems in the categorical realm.

The first part of the proposal deals with construction of characters of irreducible representations of reductive groups over local nonarchimedian fields F in terms of traces of the Frobenious endomorphisms which should lead to the proof of the """"Stable center conjecture"""" at least for representations of depth zero.

The second part is on the extension of the definition of L-functions of representations of reductive F-groups  corresponding to an arbitrary representation of the dual groups. As it is now, the definition is known only for very special representations of the dual group and only in the case of classical groups.

The third part is on the extension of the classical theory to representations of Kac-Moody groups over local fields.""","1569488","2015-10-01","2020-09-30"
"HBAR-HFS","Hyperfine structure of antihydrogen","Eberhard Widmann","OESTERREICHISCHE AKADEMIE DER WISSENSCHAFTEN","Antihydrogen is the simplest atom consisting entirely of antimatter. Since its counterpart hydrogen is one of the best studied atoms in physics, a comparison of antihydrogen and hydrogen offers one of the most sensitive tests of CPT symmetry. CPT, the successive application of charge conjugation, parity and time reversal transformation is a fundamental symmetry conserved in the standard model (SM) of particle physics as a consequence of a mathematical theorem. These conditions for this theorem to be fulfilled are not valid any more in extensions of the SM like string theory or quantum gravity. Furthermore, even a tiny violation of CPT symmetry at the time of the big bang could be a cause of the observed antimatter absence in the universe. Thus the observation of CPT violation might offer a first indication for the validity of string theory, and would have important cosmological consequences.
This project proposes to measure the ground state hyperfine (HFS) splitting of antihydrogen (HBAR), which is known in hydrogen with relative precision of 10^–12. The experimental method pursued within the ASACUSA collaboration at CERN-AD consists in the formation of an antihydrogen beam and a measurement using a spin-flip cavity and a sextupole magnet as spin analyser like it was done initially for hydrogen. A major milestone was achieved in 2010 when antihydrogen was first synthesized by ASACUSA. In the first phase of this proposal, an antihydrogen beam will be produced and the HBAR-HFS will be measured to a precision of around 10^–7 using a single microwave cavity. In a second phase, the Ramsey method of separated oscillatory fields will be used to increase the precision further. In parallel methods will be developed towards trapping and laser cooling the antihydrogen atoms. Letting the cooled antihydrogen escape in a field free region and perform microwave spectroscopy offers the ultimate precision achievable to measure the HBAR-HFS and one of the most sensitive tests of CPT.","2599900","2012-03-01","2017-02-28"
"HBAR12","Spectroscopy of Trapped Antihydrogen","Jeffrey Scott Hangst","AARHUS UNIVERSITET","Antihydrogen is the only stable, neutral antimatter system available for laboratory study.  Recently, the ALPHA Collaboration at CERN has succeeded in synthesizing and trapping antihydrogen atoms, storing them for up to 1000 s, and performing the first resonant spectroscopy, using microwaves, on trapped antihydrogen.  This last, historic result paves the way for precision microwave and laser spectroscopic measurements using small numbers of trapped antihydrogen atoms.  Because of the breakthroughs made in our collaboration, it is now possible, for the first time, to design antimatter spectroscopic experiments that have achievable milestones of precision. These measurements require a next-generation apparatus, known as ALPHA-2, which is the subject of this proposal.  The items sought are hardware components and radiation sources to help us to test CPT (charge conjugation, parity, time reversal) symmetry invariance by comparing the spectrum of antihydrogen to that of hydrogen.  More generally, we will address the very fundamental question: do matter and antimatter obey the same laws of physics?  The Standard Model says that they must, but mystery continues to cloud our understanding of antimatter - as evidenced by the unexplained  baryon asymmetry in the universe.  ALPHA's experiments offer a unique, high precision, model-independent view into the internal workings of antimatter.","2136888","2013-05-01","2018-12-31"
"HBEAM","Probing chemical dynamics at surfaces with ultrafast atom pulses","Alec MICHAEL WODTKE","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Ultra-short light pulses have become invaluable in time-resolved studies in chemistry and physics. But many important processes are initiated by collisions. While lasers have revolutionized experiments using light pulses, experimentally proven concepts for producing ultra-short pulses of neutral matter are still in their infancy. Hence, our ability to control when a collision occurs is still extremely limited. Recently, we have reported bunch-compression photolysis, the first demonstrated method for producing ultra-short pulses of neutral matter. Here, photolysis of jet-cooled hydrogen iodide is carried out with femto-second laser pulses whose frequency bandwidth has been spatially ordered. Thus, fast H-atom photoproducts overtake slow ones, producing an ultra-short pulse.The central objective of this project is to develop bunch-compression photolysis as a tool for ultrafast timing experiments involving collisions of ultrashort pulses of H-atoms at synchronously photo-excited solid surfaces. Bunch-compression photolysis allows collisions at a surface to be synchronized with photoexcitation on the ps time scale, opening up new ways to study the dynamics of collisions at selectively photo-excited surfaces that have not yet relaxed. Studies on collision dynamics involving excitons produced in 2D semiconductors is one exciting direction for this work. Experiments on synchronized H atom collisions with vibrationally excited surfaces prepared by infrared photoexcitation is another - this enables kinetics experiments with surface site-specificity as well as the direct observation of reaction intermediates. The work and ideas presented here show how to overcome the most challenging barrier to a new class of time-resolved dynamics experiments, opening new frontiers in the study of surface chemistry, where we will begin to understand how selected degrees of freedom of the solid influence collision dynamics and reaction rates.","2499356","2017-10-01","2022-09-30"
"HD-Tomo","High-Definition Tomography","Per Christian Hansen","DANMARKS TEKNISKE UNIVERSITET","In computed tomography we mimic the brain’s ability to synthesize an object’s 3D structure from many projections by solving thousands of equations. Many efficient methods have been developed to do that, and the results can be impressive when the object is illuminated from many angles and the noise is negligible. However, one decisive factor behind the human brain's unrivalled success with 3D reconstruction remains to be incorporated into computed tomography: The ability to use prior information – an organized accumulation of experience with other objects in the world. The goal of the project is to accomplish this.

The time is ripe to use the power of state-of-the-art mathematics and scientific computing to develop the enabling mathematical technology for next-generation tomographic reconstruction algorithms that are flexible enough to incorporate a variety of available prior information, and thus achieve major improvements in the details and reliability of high-definition reconstructions – sharper images with reliable details. In contrast to existing approaches our goal is to make it possible to incorporate all available prior information in various forms, by replacing ad-hoc assumptions in the current tomography algorithms with prior-driven data representation models and reconstruction methods.

We will look outside the world of classical tomography and incorporate elements and techniques from related areas, tuned to the particular problems that arise in tomography. While research in tomography is often performed either in the application areas or in specialized mathematical communities, we will create a unique research environment with tight collaborations between all the necessary activities as well as scientific and industrial users of tomography. For the first time we will be able to compute reliable high-definition 3D / 4D reconstructions based on the total amount of prior information, without the reconstructions being deteriorated by algorithmic limitations.","2159602","2012-08-01","2017-07-31"
"HECATE","Hydrogen at Extreme Conditions: Applying Theory to Experiment for creation, verification and understanding","Graeme John ACKLAND","THE UNIVERSITY OF EDINBURGH","Hydrogen is the simplest and most abundant element in the universe. It exists under extreme conditions in stars and planets.  Nuclear fusion, requires creating such extreme temperature and pressure on earth.  Lightweight storage of hydrogen in condensed form would unleash its potential as a fuel. The behaviour of a collection of protons and electrons presents an iconic challenge in fundamental physics.

Diamond anvil cells (DAC) recently generated pressures above 400GPa, accessing conditions where the mechanical work of compression equals the chemical bonding energy.  Most elements undergo dramatic structural changes in this regime, and rival predictions for hydrogen include molecular and atomic metals, superfluidity, superconductivity and one-dimensional melting.  Yet when the new phase IV was discovered in 2011, it was none of these things: it was a totally unexpected complex molecular insulator. At these conditions experimental data is sparse: we must exploit it to the fullest extent, yet previous theoretical work has concentrated on routine density functional theory (DFT) simulation producing unmeasurable predictions.  I will conduct a programme combining neutron scattering and Raman spectroscopy with theory and simulation focused on measurable quantities.  This will require developing and implementing heuristic theories which do not currently exist.

I will develop methods to find free energy, theory to extract Raman frequencies and linewidths from simulation, and techniques to determine the signature from entanglement of quantum rotors.  This requires a thorough re-examination of the quantum scattering processes in the framework of DFT, including the interaction timescale and in metals, and a full quantum treatment of indistinguishable nuclei.

Thus HECATE will be uniquely placed not only to produce new phases of hydrogen, but to reliably identify what has been found.","2277206","2016-10-01","2021-09-30"
"HECTOR","MICROWAVE-ASSISTED MICROREACTORS: DEVELOPMENT OF A HIGHLY EFFICIENT GAS PHASE CONTACTOR WITH DIRECT CATALYST HEATING","Jesús Marcos Santamaría Ramiro","UNIVERSIDAD DE ZARAGOZA","While heterogeneous catalysis is often considered a mature science, the so-called enabling technologies are often able to produce significant enhancements in the rate of reaction or in the selectivity towards a given product. Two of these enabling technologies constitute the focal point of this project, where nonclassical energy input by microwave iradiation and alternative reaction engineering (microreactors operating under a stable solid-gas temperature gap) will be used to obtain substantial improvements in the yield or in the energy efficiency of chemical processes.
This project aims for a breakthrough in reactor engineering by developing a new type of heterogeneous catalytic reactor, capable of operating under a controlled solid-gas temperature difference.
To implement this innovative technology, we will deploy different materials that are sensitive to microwave radiation (zeolite films with/without deposition of metallic particles, metallic films and nanoparticles) on the channels of microreactors made of materials that are transparent to microwaves. A basic study of adsorption and heating processes under microwave irradiation will lead to the selection of materials and conditions that enable operation under a significant temperature difference between the catalyst and the gas phase. The advantages obtained from this novel concept will be exploited in specific, industrially important, reaction processes (CO oxidation in H2 streams; VOC combustion in lean mixtures; ethylene epoxidation), where significant improvements in reaction yield and/or operating costs are expected. At the same time, new scientific and technological insight will be gained in the area of catalyst heating by microwaves.","1851179","2011-03-01","2017-02-28"
"HELIOS","Heavy Element Laser Ionization Spectroscopy","Pieter Van Duppen","KATHOLIEKE UNIVERSITEIT LEUVEN","The aim of this proposal is to develop a novel laser-spectroscopy method and to study nuclear and atomic properties of heaviests elements in order to address the following key questions:
- Is the existence of the heaviest isotopes determined by the interplay between single-particle and collective nucleon degrees of freedom in the atomic nucleus?
- How do relativistic effects and isotopic composition influence the valence atomic structure of the heaviest elements?

The new approach is based on in-gas jet, high-repetition, high-resolution laser resonance ionization spectroscopy of short-lived nuclear-reaction products stopped in a buffer gas cell. The final goal is to couple the new system to the strongest production facility under construction at the ESFRI-listed SPIRAL-2 facility at GANIL (France) and to study isotopes from actinium to nobelium and heavier elements.
An increase of the primary intensity, efficiency, selectivity and spectral resolution by one order of magnitude compared to present-day techniques is envisaged, which is essential to obtain the required data .

The challenges are:
- decoupling the high-intensity heavy ion production beam (> 10^14 particles per second) from the low-intensity reaction products (few atoms per second)
- cooling of the reaction products from MeV/u to meV/u within less then hundred milliseconds
- separating the wanted from the, by orders of magnitude overwhelming, unwanted isotopes
- performing high-resolution laser spectroscopy on a minute amount of atoms in an efficient way.

Nuclear properties (charge radii, nuclear moments and spins) as well as atomic properties (transition energies and ionization potentials) will be deduced in regions of the nuclear chart where they are not known: the neutron-deficient isotopes of the actinide elements, up to nobelium (Z = 102) and beyond. The data will validate state-of-the-art calculations, identify critical weaknesses and guide further theoretical developments.","2458397","2012-03-01","2017-02-28"
"HELIOS","Towards Total Scene Understanding using Structured Models","Philip Hilaire Sean Torr","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","""This project is at the interface between computer vision and linguistics: the aim is to have an algorithm generate relevant sentences that describe a scene given one or more images.

Scene understanding has been one of the central goals in computer vision for many decades. It involves various individual tasks, such as object recognition, action understanding and 3D scene recovery. One simple definition of this task is to say scene understanding is equivalent to being able to generate meaningful natural language descriptions of a scene, an important problem in computational linguistics. Whilst even a child can do this with ease, the solution of this fundamental problem has remained elusive. This is because there has been a large amount of research in computer vision that is very deep, but not broad, leading to an in depth understanding of edge and feature detectors, tracking, camera calibration, projective geometry, segmentation, denoising, stereo methods, object detection etc. However, there has been only a limited amount of research on a framework for integrating these functional elements into a method for scene understanding.

Within this proposal I advocate a complete view of computer vision, in which the scene is dealt with as a whole, in which problems which are normally considered distinct by most researchers are unified into a common cost function or energy. I will discuss the form the energy should take and efficient algorithms for learning and inference. Our preliminary experiments indicate that such a unified treatment will lead to a paradigm shift in computer vision with a quantum leap in performance. We intend to build embodied demonstrators including a prosthetic vision aid to the visually impaired. The World Health Organization gives a figure of over 300 million such people world wide, which means that in addition to being transformative in the areas of linguistics, HCI, robotics, and computer vision, this work will have a massive impact world wide""","2493495","2014-01-01","2018-12-31"
"HEPGAME","Solving High Energy Physics Equations using Monte Carlo Gaming Techniques","Jozef Antoon Maria Vermaseren","STICHTING NEDERLANDSE WETENSCHAPPELIJK ONDERZOEK INSTITUTEN","The main objective of this proposal is to perform (hitherto unsolved) calculations in Quantum Field Theory (QFT) most of which are urgently needed to make optimal use of upcoming experimental data from the Large Hadron Collider. These specific calculations have been intractable thus far due to their enormous demand of man and computer power.

We will make use of the brand new technique of Monte Carlo Tree Search (MCTS) from the fields of Artificial Intelligence (AI) and gaming to resolve this issue and automatize the derivation of formulas and the construction of computer programs. To do so, we will first develop MCTS into a viable QFT tool. Calculations and derivation of the formulas will be done by the (open source) symbolic system FORM developed by the PI.

A spinoff of the proposal will be the adaptation and extension of FORM to allow the physics and the AI to work well together. We will make the new technology available for other researchers, enabling a wide range of calculations at a new level of  precision.","1739000","2013-07-01","2018-06-30"
"Herifuel","Heterometallic Rings for Future Electronics","Richard WINPENNY","THE UNIVERSITY OF MANCHESTER","The proposal is to use our great synthetic control to examine the use of heterometallic cyclic coordination compounds (heterometallic rings, HRs) in two distinct application areas. One is of immediate impact: the use of the HRs as resist materials for lithography. This work has already been patented and is being developed as a means to fabricate devices that will be needed at the 7 nm node and smaller. The synthetic control also means we can make resists for extreme UV lithography (13 nm wavelength) which meet the tight specifications needed for industrial application. The second application is more long term, which is the proposal that such rings could be used as qubits in quantum information processing. Here we will build on recent work that has established a diamagnetic matrix in which complex polymetallic assembly could be incorporated. This gives us the opportunity of performing algorithms during the project and hence laying the ground-work for future developments.","2477003","2018-09-01","2023-08-31"
"HERMES","HERMES – High Exponential Rise in Miniaturized cantilever-like Sensing","Anja Boisen","DANMARKS TEKNISKE UNIVERSITET","Miniaturized cantilever–like sensors have evolved rapidly.  However, when it comes to major breakthroughs in both fundamental studies as well as commercial applications these sensors face severe challenges: i) reliability – often only one or two measurements are performed for the same conditions due to very slow data generation and the results are rarely confirmed by orthogonal sensing technologies, ii) sensitivity – in many applications the need is now for ultra-low sensitivities, iii) reproducibility – very few results have been reported on reproducibility of these sensors iv)throughput –extremely slow and tedious read-out technologies. In order to take a great leap forward in cantilever-like sensing I suggest a new generation of simplified and optimized cantilever-like sensing structures implemented in a DVD based platform which will specifically address these issues.

My overall hypothesis is that the true potential of these exciting sensors can only be released when using a simple and reliable read-out system that allows us to focus on the mechanical performance of the sensors. Thus we will keep the sensors as simple as possible. The DVD readout makes it possible to generate large amount of data and to focus on mechanics and the interplay between mechanics, optics and electrochemistry. It will be a technological challenge to realize a robust and reliable DVD  platform, that facilitates optical read-out as well as actuation. The DVD platform will enable a fast and iterative development of hybrid cantilever-like systems which draw upon our more than 10 years experience in the field. These sensors will be realised using Si and polymer based cleanroom fabrication. Focus is on design, fabrication, characterization and applications of cantilever-like sensors and on DVD inspired system integration.  By the end of HERMES we will have a unique platform which will be the onset of many new types of specific high –throughput applications  and sensor development projects.","2499466","2013-02-01","2018-01-31"
"HEXTREME","Hexahedral mesh generation in real time","Jean-François REMACLE","UNIVERSITE CATHOLIQUE DE LOUVAIN","Over one million finite element analyses are preformed in engineering offices every day and finite elements come with the price of mesh generation. This proposal aims at creating two breakthroughs in the art of mesh generation that will be directly beneficial to the finite element community at large. The first challenge of HEXTREME is to take advantage of the massively multi-threaded nature of modern computers and to parallelize all the aspects of the mesh generation process at a fine grain level. Reducing the meshing time by more than one order of magnitude is an ambitious  objective: if minutes can become seconds, then success in this research would definitively radically change the way in which engineers deal with mesh generation. This project then proposes an innovative approach to overcoming the major difficulty associated with mesh generation: it aims at providing a fast and reliable solution to the problem of conforming hexahedral mesh generation. Quadrilateral meshes in 2D and hexahedral meshes in 3D are usually considered to be superior to triangular/tetrahedral meshes. Even though direct tetrahedral meshing techniques have reached a level of robustness that allow us to treat general 3D domains, there may never exist a direct algorithm for building unstructured hex-meshes in general 3D domains.  In HEXTREME, an indirect approach is envisaged that relies on recent developments in various domains of applied mathematics and computer science such as graph theory, combinatorial optimization or computational geometry. The methodology that is proposed for hex meshing is finally extended to the difficult problem of boundary layer meshing. Mesh generation is one important step of the engineering analysis process. Yet, a mesh is a tool and not an aim. A specific task of the project is dedicated to the interaction with research partners that are committed to beta-test the results of HEXTREME. All the results of HEXTREME will be provided as an open source in Gmsh.","2244238","2016-10-01","2021-09-30"
"HFAKT","Homogeneous Flows and their Application in Kinetic Theory","Jens Marklof","UNIVERSITY OF BRISTOL","""Since the pioneering work of Maxwell and Boltzmann in the 1860s and 1870s, a major challenge in mathematical physics has been the derivation of macroscopic evolution equations from the fundamental microscopic laws of classical or quantum mechanics. The key idea of the present proposal is to introduce a renormalization technique that will provide a new route to attack some of the most important questions in the kinetic theory of gases. This technique uses the ergodic theory of flows on homogeneous spaces (homogeneous flows for short), and builds on my recent breakthrough in the case of the Lorentz gas (in joint with Andreas Strömbergsson, Uppsala). The key feature of the proposed approach is measure rigidity, a deep and powerful technical tool that has so far mainly seen success in solving long-standing problems in number theory and quantum chaos. The arguments developed in this proposal are not only interesting from a rigorous mathematical viewpoint, but also yield a heuristic mechanism for finding previously unknown kinetic transport equations that incorporate the effects of long-range order in microscopic particle distributions. This project joins together two distinct research fields, kinetic theory and the ergodic theory of flows on homogeneous spaces. If successful, the proposed research will constitute a significant breakthrough in the subject.""","1339620","2012-04-01","2017-03-31"
"HI-DIM COMBINATORICS","High-dimensional combinatorics","Nathan Linial","THE HEBREW UNIVERSITY OF JERUSALEM","This research program originates from a pressing practical need and from a purely new geometric perspective of discrete mathematics..
Graphs play a key role in many application areas of mathematics, providing the perfect mathematical description of all systems that are governed by pairwise interactions, in computer science, economics, biology and more. But graphs cannot fully capture scenarios in which interactions involve more than two agents. Since the theory of hypergraphs is still too under-developed, we resort to geometry and topology, which view a graph as a one-dimensional simplicial complex. I want to develop a combinatorial/geometric/probabilistic theory of higher-dimensional simplicial complexes. Inspired by the great success of random graph theory and its impact on discrete mathematics both theoretical and applied, I intend to develop a theory of random simplicial complexes.
This combinatorial/geometric point of view and the novel high-dimensional perspective, shed new light on many fundamental combinatorial objects such as permutations, cycles and trees. We show that they all have high-dimensional analogs whose study leads to new deep mathematical problems. This holds a great promise for real-world applications, in view of the prevalence of such objects in  application domains.
Even basic aspects of graphs, permutations etc. are much more sophisticated and subtle in high dimensions. E.g., it is a key result that randomly evolving graphs undergo a phase transition and a sudden emergence of a giant component. Computer simulations of the evolution of higher-dimensional simplicial complexes, reveal an even more dramatic phase transition. Yet, we still do not even know what is a higher-dimensional giant component.
I also show how to use simplicial complexes (deterministic and random) to construct better error-correcting codes. I suggest a new conceptual approach to the search for high-dimensional expanders, a goal sought by many renowned mathematicians.","1754600","2013-10-01","2018-09-30"
"Hi-SENS","Surface Enhanced NMR Spectroscopy","David Lyndon Emsley","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The ability to determine molecular structures from single crystals by diffraction methods has transformed science. However, if the system under investigation is located at a surface, the problem of structure elucidation is largely unsolved. Due to the increasing frequency with which such samples are encountered, particularly in the area of new materials for energy and catalysis, there is a critical need for the development of new methods for structure characterization of surfaces.
Nuclear magnetic resonance (NMR) spectroscopy would be the method of choice for characterizing surfaces were it not that the detection limit is far too low to allow many modern materials to be examined. The sensitivity of NMR thus poses the major limitation to surface characterization.
We recently introduced a new approach using Dynamic Nuclear Polarization (DNP) to enhance surface NMR signals. The project will capitalize on this new concept and develop DNP surface enhanced NMR spectroscopy (DNP SENS) through a series of new concepts to address the following challenges: (i) to characterize materials with surface areas three orders of magnitude lower than currently, specifically to detect surface NMR signals from materials with surface areas of ~1 m2/g, rather than ~1000 m2/g today; and (ii) to determine structure-activity relationships in advanced functional materials, specifically by developing NMR correlation methods capable of determining structure and dynamics of surface species in conjunction with DNP SENS.
These objectives require a gain in DNP SENS sensitivity of three orders of magnitude, and we propose to do this through innovative NMR experiments, better DNP enhancements, isotopic labeling, and high magnetic fields. The approaches go well beyond the frontier of current research.
The project will yield a broadly applicable method for structural characterization of complex surfaces not previously available by any other approach, resulting in new chemistry and chemical processes.","3449400","2013-01-01","2017-12-31"
"HIDDeN","HIDDeN - Exploring the Hidden Dusty Nuclei of Galaxies","Eva Susanne AALTO","CHALMERS TEKNISKA HOEGSKOLA AB","Luminous infrared galaxies (LIRGs) emit most of their bolometric luminosity in the far-infrared. They are mainly powered by extreme bursts of star formation and/or Active Galactic Nuclei (AGNs; accreting supermassive black holes (SMBHs)) in their centres. LIRGs are the closest examples of rapid evolution in galaxies and a detailed study of LIRGs is critical for our understanding of the cosmic evolution of galaxies and SMBHs. Centres of some LIRGs are deeply obscured and unreachable at optical, IR and even X-ray wavelengths. These hidden nuclei therefore represent a largely unexplored phase of the growth of central regions with their SMBHs. Large growth spurts are suspected to occur when the SMBHs are deeply embedded. Obscured AGNs thus can provide new constraints on the AGN duty cycle, give the full range of environments and astrophysical processes that drive the growth of SMBHs, and help to complete the picture of connections between the host galaxy and SMBH. Many dust embedded AGNs are still to be discovered as studies suggest that a significant fraction of SMBHs may be obscured in the local and more distant Universe. In the HIDDeN project we use mm and submm observational methods to reach behind the curtain of dust in the most embedded centres of LIRGs, allowing us to undertake ground-breaking studies of heretofore hidden rapid evolutionary phases of nearby galaxy nuclei. HIDDeN takes advantage of emerging opportunities to address the nature of near-field, and redshift z=1-2, obscured AGNs/starbursts and their associated molecular inflows and outflows in the context of their evolution and the starburst-AGN connection. In particular we use the ALMA and NOEMA telescopes, supported by JVLA, LOFAR, HST and future JWST observations, to address four interconnected goals: A. Probing the Dusty Interiors of Compact Obscured Nuclei (CONs), B. The cold winds of change - Molecular Outflows from LIRGs and AGNs, C. The Co-Evolution of Starbursts and AGNs and D. Are there hidden CONs at z=1-2","2496319","2018-10-01","2023-09-30"
"HIERARSACOL","Hierarchical Self Assembly of Colloids: Control and Manipulation from Nano-Granular","Alfons Van Blaaderen","UNIVERSITEIT UTRECHT","Goal: to significantly extend our ability to manipulate the Self Assembly (SA) of colloidal nanoparticles (NPs) into complex 1D/2D/3D architectures (regular clusters, (composite)strings/rods, sheets, submicron colloidal crystals/liquid crystal phases of the NPs) over multiple length scales going from nano to that of granular matter. In the nano-regime quantum size effects cause materials properties to become strongly size dependent and thus highly tunable. Moreover, the synthesis of many NPs (metals, semiconductors, magnetic materials) is advanced enough that they can be made to crystallize into regular 3D lattices with new exciting functionality caused by collective effects. By performing SA in several independent stages, materials properties can be further tailored in new ways because of both access to different length scales and different NP combinations. In order to make systematic progress we will determine inter-NP potentials using 3D imaging. Both using subdiffractive confocal microscopy and cryogenic tomographic transmission electron microscopy. We will also use external fields (optical tweezers, electric/magnetic fields, shear) both to realize the complex architectures, but also to change particle properties dynamically. E.g., in monodisperse droplets of nematic phases of luminescent rodlike NPs an electric field can dramatically affect the scattering and emission of individual droplets. The droplets can subsequently be ordered in strings, sheets or crystals. Repeating the SA again delivers supra structures on the granular scale to tune e.g. heat or reagent flows. These projects combined will not only deliver new fundamental knowledge on SA, but the results are also expected to be directly useful for realizing applications based on the new meta-materials realized such as in displays, lighting, (optical) storage, (bio)sensing, catalysis, spintronics, photonic crystals, and the opto-electronics field in general.","2494334","2012-06-01","2017-05-31"
"HIEXP","High Dimensional Expanders, Ramanujan Complexes and Codes","Alex LUBOTZKY","THE HEBREW UNIVERSITY OF JERUSALEM","""Expander graphs have been playing a fundamental role in many areas of computer science. During the last 15 years they have also found important and unexpected applications in pure mathematics. The goal of the current research is to develop systematically high-dimensional (HD) theory of expanders, i.e., simplicial complexes and hypergraphs which resemble in dimension d, the role of expander graphs for d = 1. There are several motivations for developing such a theory, some from pure mathematics and some from computer science. For example, Ramanujan complexes (the HD versions of the """"optimal"""" expanders, the Ramanujan graphs) have already been useful for extremal hypergraph theory. One of the main goals of this research is to use them to solve other problems, such as Gromov's problem: are there bounded degree simplicial complexes with the topological overlapping property (""""topological expanders""""). Other directions of HD expanders have applications in property testing, a very important subject in theoretical computer science. Moreover they can be a tool for the construction of locally testable codes, an important question of theoretical and practical importance in the theory of error correcting codes. In addition, the study of these simplicial complexes suggests new quantum error correcting codes (QECC). It is hoped that it will lead to such codes which are also low density parity check (LDPC). The huge success and impact of the theory of expander graphs suggests that the high dimensional theory will also bring additional unexpected applications beside those which can be foreseen as of now.""","1592500","2016-08-01","2021-07-31"
"HIGGS@LHC","Search and study of the Higgs bosons at the LHC","Abdelhak Djouadi","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The major issue and the forefront research activity in particle physics today
is the exploration of the mechanism that generates the elementary particle
masses.  In the Standard Model that describes three of the four basic  forces in
nature - the electromagnetic, weak and strong  interactions - this fundamental
mechanism leads to the existence of a new type of  particle, the Higgs boson,
which has escaped detection so far. The discovery of this  particle, which will
have a paramount importance and far-reaching implications, is the major goal of
the CERN Large Hadron Collider which  recently started operation after 20 years
of preparation. The observation of the Higgs boson at the LHC and the
determination of its fundamental properties will  be the  essential issue
addressed by the present research project.  A comprehensive investigation of the
various Higgs boson detection channels at the LHC, production mechanisms and
decay  modes,  as well as the major sources of backgrounds will be performed in
a way that is as close as possible to the experimental conditions.  Precise
theoretical  predictions, including higher order quantum effects, will be
provided and the associated  uncertainties will be assessed. The implications of
observing the Higgs particle for the Standard Model and  for new physics beyond
it, such as supersymmetric theories and models with extra space-time
dimensions, will  be investigated in detail. Besides the  Principal Investigator
who will devote 80% of his time on the project, the research team will be
formed by theoretical physicists from three  laboratories in the Paris area, LPT
Orsay, LPTHE Jussieu and IPhT Saclay, as well as a staff member of the CERN
Theory Unit. This group will be completed by the six postdoctoral fellows and
two PhD students that will be appointed. The duration of the project, five
years, will crucially coincide with the period in which the LHC is expected to
make major breakthroughs in the field under investigation.","1160005","2013-03-01","2018-02-28"
"HiggsSelfCoupling","Uncovering the Origins of Mass: Discovery of the di-Higgs Process and Constraints on the Higgs Self-Coupling","Cigdem ISSEVER","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The Standard Model of particle physics describes the elementary constituents of matter and their interactions. In 2012, its last ingredient, the Higgs boson, was discovered at the Large Hadron Collider (LHC). The exploration of the Higgs boson is now one of the most exciting avenues to explore for New Physics beyond the Standard Model and allows some of the most pressing problems in theoretical physics to be addressed, such as the origins of the electroweak symmetry breaking mechanism. This important mechanism gives elementary particles their masses but the nature of this mechanism remains a mystery. 

A particularly crucial measurement is the production cross-section of Higgs boson pairs, which provides unique information on the Higgs self-coupling and on the underlying nature of the electroweak symmetry breaking mechanism. Most feasibility studies of the Higgs self-coupling conclude that there will be insufficient data for this measurement in the coming decade. However, my recent feasibility studies indicate that by using the Higgs pair production process with four bottom quarks in the final state, the discovery of the di-Higgs process and its cross section measurement can be made much earlier. This project aims to develop and complete the first measurement of the di-Higgs cross section and most stringent bounds on the Higgs self-coupling before 2023.
To achieve this goal I will develop new experimental techniques to improve the background reduction rates and enhance the signal. The objectives are the development of novel bottom quark energy reconstruction algorithms, new bottom quark and Higgs identification techniques, and neural network analysis tools. Analysis of ATLAS data will then enable searches for New Physics and ultimately the di-Higgs cross section measurement to constrain the Higgs self-coupling. This landmark measurement will lead to the confirmation of how particles acquire mass and open new avenues to understand what lies beyond the Standard Model.","2262897","2019-06-01","2024-05-31"
"High-Spin-Grav","Higher Spin Gravity and Generalized Spacetime Geometry","Marc HENNEAUX","UNIVERSITE LIBRE DE BRUXELLES","Extensions of Einstein’s gravity containing higher spin gauge fields (massless fields with spin greater than two) constitute a very active and challenging field of research, raising many fascinating issues and questions in different areas of physics. However, in spite of the impressive achievements already in store, it is fair to say that higher spin gravity has not delivered its full potential yet and still faces a rich number of challenges, both conceptual and technical.  The objective of this proposal is to deepen our understanding of higher spin gravity, following five interconnected central themes that will constitute the backbone of the project: (i) how to construct an action principle; (ii) how to understand the generalized space-time geometry invariant under the higher-spin gauge symmetry – a key fundamental issue in the project; (iii) what is the precise asymptotic structure of the theory at infinity; (iv) what is the connection of the higher spin algebras with the hidden symmetries of gravitational theories; (v) what are the implications of hypersymmetry, which is the higher-spin version of supersymmetry. Holography in three and higher dimensions will constitute an essential tool.

One of the motivations of the project is the connection of higher spin gravity with tensionless string theory and consistent theories of quantum gravity.","1841868","2016-10-01","2021-09-30"
"HIGHTEICH","Higher Teichmüller-Thurston Theory: Representations of Surface Groups in PSL(n,R)","François Pierre Calixte Labourie","UNIVERSITE DE NICE SOPHIA ANTIPOLIS","Higher Teichmüller-Thurston theory is the study of a specific component of representations of a surface group of genus g in PSL(n,R). Teichmüller theory depends on a parameter: the genus g of the surface. Higher Teichmüller-Thurston introduces a new paramater n so that classical theory corresponds to n=2. Teichmüller theory is a crossroad between dynamics, complex analysis, spectral theory, geometry and integrable systems. It has started with the study of Kleinian groups and have received strong impulses from many fields throughout last century. To quote but a few: arithmetic (through the study of automorphic forms), geometry (Thurston&apos;s theory of hyperbolic structures), dynamics (the ergodic properties of the geodesic flow) and physics (conformal field theory and representations of the Virasoro algebra). The main objective of the proposal is to develop new connections between dynamics, complex analysis, integrable systems beyond classical Teichmüller Theory in the context of higher Teichmüller-Thurston theory. Among the very concrete and challenging goals of this proposal, we have: A Riemann uniformisation theorem for the Hitchin component, the construction and quantisation of a universal algebra for all Hitchin components, computations of volumes and characteristic numbers of (Higher) Riemann moduli spaces, Higher Laminations. The resources will be essentially used for the hiring of post-doc, graduate students, pre-doc students, visiting scientists, international conferences and summer schools. It will take place at University Paris Sud XI.","1549200","2010-05-01","2015-08-31"
"HIGHZ","HIGHZ: Elucidating galaxy formation and evolution from very deep Near-IR imaging","Marijn Franx","UNIVERSITEIT LEIDEN","""Studies of high redshift galaxies require very deep Near-IR imaging. This allows the study of z=2-4 galaxies redward of the Balmer/4000 Angstrom break, and the detection of UV-bright galaxies at z>7. Two new facilities wil revolutionize these studies: the VISTA telescope built for ESO, and the Near-IR channel on WF3 for HST. They will become available at the start of the grant period. We propose to build a group to analyze the imaging data from these facilities. We will make use of the fact that I am Co-PI on the ultra-deep """"ULTRA-VISTA"""" survey on the VISTA telescope, and we will analyze public and privately proposed data from WF3. The following science questions will be addressed: (1) what is the origin and evolution of the Hubble sequence out to z=3, (2) what is the evolution of the Luminosity Function of UV bright galaxies between z=6 to z=11, and what galaxies cause re-ionization, (3) how does the mass function of quiescent and star forming galaxies evolve to z=4, and how do the correlation functions of subpopulations evolve as a function of redshift. A crucial component of this proposal is the request for support for a junior faculty position. This person will take on the lead for the highly specialized data processing, and will supervise the analysis of the selection effects, and other crucial components needed for a proper analysis.""","1471200","2009-09-01","2014-08-31"
"HIPERCAM","HiPERCAM: A high-speed camera for the study of rapid variability in the Universe","Vikram Dhillon","THE UNIVERSITY OF SHEFFIELD","When stars die, they form white dwarfs, neutron stars or black holes. These are key objects in astrophysics as their extreme gravities, densities and pressures allow us to test our theories of fundamental physics at the limits of their predictive powers. One of the best ways of studying white dwarfs, neutron stars and black holes is via their variations in brightness. Unfortunately, due to the small physical sizes of these objects, they are extremely faint and their variability occurs on timescales of milliseconds to seconds, too fast to be recorded by the current generation of astronomical instruments on the world's largest telescopes.

What is required is a new type of astronomical instrument with the capability to take high-speed exposures with no noise from either the detector or the atmosphere. Such an instrument would enable us to answer some of the most important questions in astrophysics, such as: What are the progenitors of type Ia supernovae? What is the equation of state of the degenerate matter found in white dwarfs and neutron stars? What is the nature of the flow of matter close to the event horizon of black holes? What gravitational wave signals are likely to be detected by the next generation of space and ground-based detectors?

My aim is to answer the above questions by building and exploiting a new astronomical instrument: HiPERCAM. HiPERCAM will be by far the best high-speed camera in the world, giving an order of magnitude improvement in performance over what has come before. It will be mounted on the world's best telescopes, including the 8.2-m VLT in Chile and the 10.4-m GTC on La Palma. This revolutionary new camera will incorporate a novel scintillation-correction mechanism and the latest in low-noise, high-speed detector technology. Although challenging, my track record with previous related instruments (e.g. ULTRACAM) and my preceding technology demonstration projects shows that my proposed research programme is feasible.","3491336","2014-01-01","2019-12-31"
"HIPOCAT","High Performance Lewis Acid Organocatalysis","Benjamin List","MAX PLANCK INSTITUT FUER KOHLENFORSCHUNG","Although the demand for chiral enantiomerically pure molecules as pharmaceuticals, agrochemicals, and liquid crystals is growing strongly, the use of asymmetric catalysis for their production is rare. The most common industrial method to produce enantiopure compounds is still chiral resolution, which wastes half of the material. This surprises considering that catalytic methods are potentially cost-, energy, and resource-saving, have a lesser impact on the environment, and are in line with the general concepts of green chemistry and sustainability. Recently organocatalysis has grown into one of three fundamental classes of asymmetric catalysts complementing metal- and biocatalysis.  In principle, organocatalysts have many beneficial features such as air and moisture stability, non-toxicity, and easy accessibility, making them attractive for industrial applications. However, most organocatalysts are insufficiently active and require high catalysts loadings, counterbalancing these positive features. Remarkably, of the four types of organocatalysts, Brønsted bases and acids, and Lewis bases, and acids, organic Lewis acid catalysts have been almost entirely ignored. Very recently though, within the group of the applicant, the finding was made that such catalysts can be extremely active and enantioselective, suggesting the possibility for truly high performance organocatalysis. This proposal therefore aims at the design of novel organic Lewis acid catalysts, their exploration in asymmetric catalysis, and their mechanistic understanding. The program is expected to lead the way towards the next generation organocatalysts, which will rival the efficiency of the most active metal- and biocatalysts, and have the potential to profoundly change the way chiral molecules are made.","2490000","2011-02-01","2016-01-31"
"HISTORYNU","The HI Story of Galaxy Evolution in the Nearby Universe","Jan Mathijs Van Der Hulst","RIJKSUNIVERSITEIT GRONINGEN","Current radio telescopes have imaged the HI in and around a few hundred galaxies in the nearby universe, but to reach out into the universe to the time when the decline in star formation started will require new facilities. An essential ingredient in the process of galaxy formation and evolution is the balance between the acquisition of gas through merging and accretion and the depletion of gas through stripping and outflows induced by star formation and in some cases active galactic nuclei (AGN). This balance will depend on the environment. The life long gas supply and regulation of this balance will determine what kind of galaxy emerges in the end. The strong decline in star formation density in the universe over the last 7-8 Gyr indicates strong evolution. To be able to understand this evolution it is necessary to assess the balance between the acquisition and depletion of gas in many individual galaxies and many different environments over this period of cosmic time.
In less than two years the Westerbork Synthesis Radio Telescope (WSRT) will be upgraded with a wide field capability using the novel technology of phased array feeds (PAFs) developed in the Netherlands. This system, named APERTIF, will make it possible to survey large areas of sky rapidly to depths covering the last 3 Gyr of cosmic time.
This proposal requests funding for harvesting the huge potential of the APERTIF system. It focuses on analyzing the data of a blind, medium deep survey of HI out to a distance of 1250 Mpc (a redshift of z = 0.25) covering an area of ~500 deg2 (or a volume of over 30 million Mpc3) to explore the balance between gas acquisition and gas removal processes in galaxies in different environments. This balance is fundamental to the way galaxies evolve. Primary goal is to obtain a full inventory of this balance and show which processes dominate in which environments.","2500000","2012-09-01","2018-08-31"
"HITSUPERJU","Higher-dimensional topological solids realized with multiterminal superconducting junctions","Iouli Vyacheslavovitch Nazarov","TECHNISCHE UNIVERSITEIT DELFT","""Recently I revealed a deep operational analogy between an exotic material and an electronic device, i.e. between a 3-dimensional topological solid and a 4-terminal superconducting junction. Specifically, the 3d Weyl singularities revealed in the energy spectrum of this quantum device give rise to quantized trans-conductance in two leads that is typical for 2-dimensional topological Quantum Hall materials. The quantized value can be  tuned with the third control phase. 

I propose to capitalize on this breakthrough by realizing artificial n-dimensional (topological) solid materials by (n+1)-terminal superconducting junctions. This seemed to be fundamentally forbidden so far. In particular, in the framework of one research direction I will address the realization of higher Chern numbers. The edges and interfaces are important in topological solids, they need to be structured. For the artificial topological materials made with multi-terminal superconducting junctions such structuring is impossible in geometric coordinate space. However, the fact that the charge and superconducting phase are quantum-conjugated quantities provide the unique possibility for  the structuring in multi-dimensional charge space that I will access in the framework of another direction.  These two research directions will be supplemented by a more technical effort devoted to computational (quantum) dynamics of multi-terminal superconducting junctions.

The proposed way to """"conquer"""" higher dimensions for condensed matter physics is of clear fundamental importance. Exciting applications are at the horizon, too. The exotic quantum states under consideration can be topologically protected and thus useful for quantum information processing. Quantized trans-resistance as well as other topological invariants may be important  in metrology. More generally, the research proposed will boost the whole field of electronic devices wherever topology guarantees the discrete stability of device characteristics""","1522810","2016-08-01","2021-07-31"
"HoldCancerBack","What Holds Cancer Cells Back?","Josef KÄS","UNIVERSITAET LEIPZIG","Since decades the bulk of cancer research focusses on the genetic and molecular level. To complement this knowledge, I will focus on the collective behaviour of cancer cells in cell clusters and in the extracellular matrix (ECM).
Conventional cancer research tackles issues like genetic changes, signalling pathways or intracellular mechanisms, I want to answer the question: When is a cancer cell jammed or when can it overcome the yield stress to actively “flow” in a dense microenvironment (ME)? I have brought forward the basic idea within the concept of Physics of Cancer that changes in a cancer cell’s material properties determine its metastatic potential. As follows I propose the next breakthrough by determining a predictive phase diagram for unjamming transitions of cancer cells. 
Cancer cell jamming is quantified by cell speed as a measure of the motile forces and by cellular shape to account for the interplay between cell contractility and adhesion. Our self-propelled Voronoi model (SPV) will explain whether a cell is jammed by its neighbours or the ECM, overcoming the limitations of existing theories which only apply to specific environments.
Building on my leadership in cell biomechanics and the exclusive access to two types of carcinomas (mamma, cervix), I will introduce highly innovative bionic modulators of intracellular mechanics and develop live cancer cell tracking in biopsies as a ground-breaking alternative to vital imaging. While these approaches are perfect to prove that unjamming transitions are key to tumour progression, I will investigate to what extent fluid, i.e. unjammed, tissue behaviour can be detected by magnetic resonance imaging  elastography (MRE) as an individual predictive marker for metastasis. Moreover the results may guide surgeons when concerning the local spreading of cancer and thus greatly empower surgery in tumour therapies.","2379250","2017-08-01","2022-07-31"
"HOLMES","The Electron Capture Decay of 163Ho to Measure the Electron Neutrino Mass with sub-eV sensitivity","Stefano Ragazzi","ISTITUTO NAZIONALE DI FISICA NUCLEARE","""HOLMES is aimed at directly measuring the electron neutrino mass using the electron capture (EC) decay of 163Ho.
The measurement of the absolute neutrino mass represents a major breakthrough in particle physics and cosmology. Due to their abundance as big-bang relics, massive neutrinos strongly affect the large-scale structure and dynamics of the universe. In addition, the knowledge of the scale of neutrino masses, together with their hierarchy pattern, is invaluable to clarify the origin of fermion masses beyond the Higgs mechanism.
The innovative approach of HOLMES consists in the calorimetric measurement of the energy released in the decay of 163Ho. In this way, all the atomic de-excitation energy is measured, except that carried away by the neutrino. A finite neutrino mass m causes a deformation of the energy spectrum which is truncated at Q-m, where Q is the EC transition energy. The sensitivity depends on Q - the lower the Q, the higher the sensitivity - and 163Ho is an ideal isotope with a Q around 2.5keV. The direct measurement exploits only energy and momentum conservation, and it is therefore completely model-independent. At the same time, the calorimetric measurement eliminates systematic uncertainties arising from the use of external beta sources, as in neutrino mass measurements with beta spectrometers, and minimizes the effect of the atomic de-excitation process uncertainties.
HOLMES will deploy a large array of low temperature microcalorimeters with implanted 163Ho nuclei. The resulting mass sensitivity will be as low as 0.4eV. HOLMES will be an important step forward in the direct neutrino mass measurement with a calorimetric approach as an alternative to spectrometry. It will also establish the potential of this approach to extend the sensitivity down to 0.1eV.
The detection techniques developed for HOLMES will have an impact in many frontier fields as astrophysics, material analysis, nuclear safety, archeometry, quantum communication.""","3057067","2014-02-01","2019-01-31"
"HOLOGRAM","Holomorphic Dynamics connecting Geometry, Root-Finding, Algebra, and the Mandelbrot set","Dierk Sebastian SCHLEICHER","TECHNISCHE UNIVERSITAT BERLIN","Dynamical systems play an important role all over science, from celestial mechanics, evolution biology and
economics to mathematics. Specifically holomorphic dynamics has been credited as “straddling the
traditional borders between pure and applied mathematics”. Activities of numerous top-level
mathematicians, including Fields medalists and Abel laureates, demonstrate the attractivity of holomorphic
dynamics as an active and challenging research field.

We propose to work on a research project based in holomorphic dynamics that actively connects to adjacent
mathematical fields. We work on four closely connected Themes:

A. we develop a classification of holomorphic dynamical systems and a Rigidity Principle, proposing
the view that many of the additional challenges of non-polynomial rational maps are encoded in the simpler
polynomial setting;

B. we advance Thurston’s fundamental characterization theorem of rational maps and his lamination
theory to the world of transcendental maps, developing a novel way of understanding of spaces of iterated
polynomials and transcendental maps;

C. we develop an extremely efficient polynomial root finder based on Newton’s method that turns the
perceived problem of “chaotic dynamics” into an advantage, factorizing polynomials of degree several
million in a matter of minutes rather than months – and providing a family of rational maps that are highly
susceptible to combinatorial analysis, leading the way for an understanding of more general maps;

D. and we connect this to geometric group theory via “Iterated Monodromy Groups”, an innovative
concept that helps solve dynamical questions in terms of their group structure, and that contributes to
geometric group theory by providing natural classes of groups with properties that used to be thought of as
“exotic”.","2312481","2016-10-01","2021-09-30"
"HOLOMAN","Holographic acoustic assembly and manipulation","Peer Fischer","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Acoustic waves exert forces when they interact with matter. Sound, and in particular ultrasound, which has a wavelength of a few hundred microns in water, is a benign and versatile tool, that has been successfully used to manipulate, trap and levitate microparticles and cells. The acoustic contrast between the material and the medium, and the spatial variation of the ultrasound field determine the interaction. Resonators and arrays of a few hundred transducers have thus far been used to generate the sound fields, but the former only yields highly symmetrical pressure patterns, and the latter cannot be scaled to achieve complex fields. 

Our radically new approach uses a finely contoured 3D printed acoustic hologram to generate pressure fields with orders of magnitude higher complexity than what has been possible to date. The acoustic hologram technology is a route towards truly sophisticated and 3D sound fields. This project will research the necessary computational and experimental tools to generate designed 3D ultrasound fields. We will investigate ways to use acoustic holograms for rapid manufacturing, the controlled manipulation of microrobots, and the assembly of cells. The 3D pressure fields promise the assembly and fabrication of an entire 3D object in “one shot”, something that has not been realized to date. We will also study the formation of 3D cellular assemblies, and more realistic 3D tumour models. This project will develop the technology, materials, processes, and understanding needed for the generation and use of sophisticated 3D ultrasound fields, which opens up entirely new possibilities in physical acoustics and the manipulation of matter with sound.","2420125","2019-02-01","2024-01-31"
"HOME","Habitability of Martian Environments: Exploring the Physiological and Environmental Limits of Life","Dirk Schulze-Makuch","TECHNISCHE UNIVERSITAT BERLIN","The low average temperature and low water activity of the Martian near-surface environment makes it challenging for living organisms to persist and propagate.  Nonetheless, recent mission results indicate that environmental conditions exceed locally and temporarily the lower thresholds for life to exist. Furthermore, specific soil minerals, or combinations thereof, appear to provide a suitable habitat for microbial life, especially if associated with low-temperature brines or hygroscopic salts. Thus, a quantitative understanding of the habitability potential of the Martian near-surface environment, past and present, is very much needed and the focus of this proposal. To achieve this objective, we will test different types of soils and some of Earth’s hardiest organisms, using them as models (‘Mars-analogues’), to see if they can survive and perhaps even grow under the various environmental stresses known to exist on Mars. A major tool of our laboratory investigations will be the experimentally proven state-of-the-art Mars Simulation Chamber at the German AeroSpace Center, to which various soils materials and microorganisms will be exposed. The planned experimental investigations and models will be concurrently updated by analyzed mission data, particularly from landers and rovers (e.g., Curiosity Rover), to adjust our work to the newest Martian geochemical and environmental data available. Results from our proposed work will timely provide critical scientific knowledge to interpret incoming data from ESA’s ExoMars mission, which is scheduled for launch in 2016/2018. As one important deliverable of our work we will also construct a Mars Soil Analyzer, an instrument which will be designed for a future mission to Mars with the objective to achieve Technology Readiness Level 6 at the completion of the proposed study.","2494215","2014-08-01","2019-07-31"
"HOPE","Humans On Planet Earth - Long-term impacts on biosphere dynamics","Harry John Betteley BIRKS","UNIVERSITETET I BERGEN","A critical question in Earth system science is what was the impact of prehistoric people on the biosphere and climate? There is much information about human impact through clearance, agriculture, erosion, and modifying water and nutrient budgets. Humans have greatly changed the Earth in the last 8000 years, but did humans modify the major ecological processes (e.g. assembly rules) that shape community assembly and dynamics? Did inter-relationships between processes change in response to human impact? Lyons et al. & Dietl (2016 Nature) suggest that human activities in the last 6000 years had such impacts. Dietl proposes that using past ‘natural experiments’ to predict future changes is “flawed” and “out is the use of uniformitarianism”. As using natural experiments is a common strategy and uniformitarianism is the major working concept in Earth sciences, it is imperative to test whether prehistoric human activity changed major ecological processes determining community development. To test this hypothesis, patterns in pollen-stratigraphical data for the past 11,500 years from over 2000 sites across the globe will be explored consistently using numerical techniques to discern changes in 25 ecosystem properties (richness, evenness, and diversity; turnover; rates of change; taxon co-occurrences, etc.). Patterns in these properties will be compared statistically at sites within biomes, between biomes, within continents, and between continents to test the hypotheses that prehistoric human activities changed the basic ecological processes of community assembly and that their inter-relationships changed through time. These areas provide major contrasts in human prehistory and biomes. HOPE is interdisciplinary: pollen analysis, databases, multivariate analysis, ecology, new statistical methods, numerical simulations, statistical modelling. HOPE’s impact goes beyond human effects on the biosphere and extends to the very core of Earth science’s basic conceptual framework.","2278884","2018-01-01","2022-12-31"
"HORIZOMS","New Horizons for Mass Spectrometry","Detlef Schroeder","USTAV ORGANICKE CHEMIE A BIOCHEMIE, AV CR, V.V.I.","This project of basic research in chemistry tackles the challenge to close the wide gap between chemical and physical processes occurring in liquid solution and sophisticated studies of model systems in the idealized gas phase. By such the project aims to reach a convergence between the microscopic and macroscopic world. To this end, the planned research will span the range from isolated atoms in the high vacuum to the real species present in solution. Specifically, the proposal is focused on:

- Ion solvation in dipolar media (e.g. aqueous salt solutions)
- Influence of sequential solvation on redox processes (relevant in corrosion, for example)
- Mechanisms by which metal catalysts facilitate chemical reactions (e.g. polymerizations)

In order to address these tasks, the PI will combine his profound expertise in gas-phase methods  with well-established techniques from solution chemistry for the development of new and innovative coupling techniques which will allow to derive direct correlations between micro- and macroscopic properties. A particular highlight is the planned online-coupling of electron paramagnetic resonance (EPR) with electrospray ionization (ESI) mass spectrometry, which is unique worldwide and offers the exploration of new dimensions for both, EPR methods and ESI mass spectrometry.
While the project is located in chemical sciences, physical-organic chemistry in particular, the prospects of the proposal range not only far beyond the PI's specific field of specialization into other areas of chemistry, but due to the enormous relevance of ion solvation, redox processes, and catalysis also into biological, physical and geological sciences with implications up to our daily life.
The PI has excellent publication records with >330 scientific papers, >7800 citations, and a Hirsch-Index of 44. Despite several risks along the way, the expertise of the PI thus warrants a successful realization of this challenging project.","764999","2009-07-01","2013-03-31"
"HORIZONCF","New horizons in organo-fluorine chemistry","David O'hagan","THE UNIVERSITY COURT OF THE UNIVERSITY OF ST ANDREWS","The project aims to take new thinking and concepts in organofluorine chemistry and apply this thinking to the design of novel performance molecules to explore properties and function in a predictable manner. The focus is on two areas. The first involves organic materials/polymers and the second focuses on selected topics in biochemical and medicinal chemistry. Both areas exploit the stereoelectronic influence of the C-F bond, and its interaction with nearby functional groups. In particular the polar nature of the C-F bond is now used as a design feature to manipulate molecular conformation across a range of case studies, judged to be of contemporary interest. One aspect of the programme will prepare a series of compounds containing multiple fluoromethylene groups. Care will be taken to prepare individual stereoisomers for comparitive studies. The aim is to develop new structural motifs for liquid crystals and polar polymers. The study e will extend to the design and synthesis of small, but highly polar, monomers for polymerisation. There is a particular focus on preparing a new generation of polar organic polymers, as potential piezo- and ferro- electric materials to meet the current challenge to prepare novel self-poling materials. The research programme emerges from an increaing recognition that the C-F bond responds to the stereo-electronic influence of neighbouring functional groups. Some functional appear frequently in biochemistry. The programme will utilise the stereogenic placement of the C-F bond in the design of neurotransmitter analogues, to influence and explore their binding conformation to receptors. The central methodology will involve advanced methods in organic synthesis, and in particular the construction of molecules with C-F at stereogenic centres. The programme will also involve advanced tecniques for conformational analysis (NMR, X-ray, computational), polymer analysis and biochemical assays.","1418575","2010-01-01","2014-12-31"
"HotMol","Hot Molecules in Exoplanets and Inner Disks","Svetlana Berdyugina","","Understanding the nature and distribution of habitable environments in the Universe is one of the fundamental goals of modern astrophysics. For the life we know, liquid water on the planetary surface is a prerequisite. However, a direct detection of liquid water on exoplanets, and especially on a potentially habitable Earth-size planet, is not yet possible. The existence of water almost certainly implies the presence of atmospheric water vapour which must evaporate under stellar irradiation from a cloud deck or from the surface, together with other related molecules. Therefore, devising sensitive methods to detect hot molecules on exoplanets is of high importance.  This proposal develops several exploratory theoretical and observational aspects of precision spectropolarimetry for detecting water vapour and other volatiles on exoplanets and in the inner part of protoplanetary disks. These are new tools for making progress in our understanding which fraction of planets acquires water and how planet formation influences their habitability. As a “double differential” technique, spectropolarimetry has enormous advantages for dynamic range problems, like detection of weak line signals against a large stellar background and exploration at scales beyond the angular resolution of telescopes, which are crucial for both exoplanets and inner disks. Direct detection of polarized spectral lines enables recovering precise orbits of exoplanets (including non-transiting systems) and evaluating their masses as well as potentially their magnetic fields. First applied to hot Jupiters the developed tools will create a firm foundation for future exploration of Earth-like planets with larger telescopes. The same technique applied to planetesimals in the inner disks of young stars yields their orbits, temperature, and chemical composition. These will provide constraints on the formation of a planetary atmosphere in the vicinity of the star and its habitable zone.","2436000","2012-04-01","2017-03-31"
"HOWTOCONTROLGRAPHENE","Search for mechanisms to control massless electrons in graphene","Carlo Beenakker","UNIVERSITEIT LEIDEN","Conduction electrons in the carbon monolayer known as graphene have zero effective mass. This property offers unique opportunities for fast electronics, if we can somehow learn to control the dynamics of particles which have a charge but no mass. Fresh ideas are needed for this purpose, since an electric field is incapable of stopping a massless electron (its velocity being energy independent).

The applicant and his group at the Lorentz Institute for Theoretical Physics in Leiden University have started exploring the new physics of graphene soon after the announcement two years ago of the discovery of massless electrons in this material. We have identified several promising control mechanisms, and are now ready to embark on a systematic search. Our objective is to discover ways to manipulate in a controlled manner three independent electronic degrees of freedom: charge, spin, and valley.

The charge is the primary carrier of classical information, being strongly coupled to the environment, while the spin is the primary carrier of quantum information, in view of its weak coupling to the environment. The valley degree of freedom (which defines the chirality of the massless particles) is intermediate between charge and spin with regard to the coupling to the environment, and provides some unique opportunities for control. In particular, we have the idea that by acting on the valley rather than on the charge it would be possible to fully block the electronic current (something which an electric field by itself is incapable of). To study these effects we will need to develop new methodologies, since the established methods to model quantum transport in nanostructures are unsuitable for massless carriers.","1563800","2009-06-01","2013-10-31"
"HUMAN TEXTILES","Human, Woven, Tissue-Engineered Blood Vessels (TEBV) Exclusively from Cell-Assembled Extracellular Matrix (CAM).","Nicolas L'HEUREUX","INSTITUT NATIONAL DE LA SANTE ET DE LA RECHERCHE MEDICALE","""Synthetic vascular grafts perform very poorly in small diameter applications (coronary/peripheral bypass) and for dialysis access. Better vascular conduits for these applications would be life and limb-saving for a very large patient population. A biological, human, tissue-engineered blood vessel (TEBV) may be such a device. We have developed a method to produce robust sheets of cell-assembled extracellular matrix (CAM) from normal, adult, human fibroblasts in vitro. These have been rolled into TEBV and shown promising clinical results. However, this initial rolling approach is very costly, time consuming and has limited mechanical design potential. Here, we propose a new textile-based assembly method that can lift all these limitations.

Task#1 will aim at processing CAM sheets into various types of yarns (human and large animal) and characterizing composition, organization, and mechanical properties. Task#2 will aim at quantifying the in vivo remodeling of the various yarns in nude rats (human yarn) and in an allogeneic recipient (large animal) as subcutaneous implants. This screening process will identify yarns with the best biological response and mechanical profiles. Task#3 will aim at weaving human and animal, non-living, TEBVs with clinically relevant biological and mechanical properties. Task#4 will evaluate the long-term (1 year) performance of the animal TEBV in an allogeneic setting.

This study will provide:
1) in-depth understanding of the immune reactivity of this CAM, both from the innate and specific immune system.
2) long-term performance data of a woven, CAM-based, TEBV in an allogeneic setting (animal).
3) a human woven TEBV with clinically relevant mechanical properties ready for in vivo testing.
This “next generation” assembly method will reduce TEBV production time/cost 3-fold and represents a more versatile, reliable and highly tunable approach. HUMAN TEXTILES will provide a COMPLETELY NEW TYPE OF SCAFFOLD for engineering a variety of organs.""","2491543","2018-11-01","2023-10-31"
"hybridFRET","hybridFRET - deciphering biomolecular structure and dynamics","Claus Seidel","HEINRICH-HEINE-UNIVERSITAET DUESSELDORF","To understand and modulate biological processes, we need their spatiotemporal molecular models. In this project we propose to build these models by a holistic approach. The recent methodological and technical advances in fluorescence spectroscopy and microscopy as well as in multi-scale modelling of complex biochemical systems set the stage to tackle cross-fertilizing challenges in biophysics, biochemistry and cell biology. The applicant proposes to develop a novel integrative platform for a Molecular Fluorescence Microscope (MFM) to achieve ultimate resolution in space (sub-nanometer) and time (picoseconds) for characterizing structure and dynamics of proteins. MFM will combine Multi-parameter Fluorescence Detection with Computational Microscopy (molecular dynamics and coarse grained simulations) in a hybrid approach, first, to derive a complete molecular description of all fluorescence properties of the tailored dyes in proteins (objectives 1 and 2) and, second, to utilize this information in simulations to report on the protein properties (objective 3). In this hybrid approach high precision FRET measurements are the core experimental technique (hybridFRET). The MFM will allow us to tackle the central biophysical question of how intra- and intermolecular domain interactions modulate proteins' overall structure, dynamics, and thus ultimately function (objective 4). In this proposal we will apply MFM to two prototypic proteins of significant medical relevance. The combination with Multi-parameter Fluorescence Image Spectroscopy will exploit the ultimate resolution of the MFM for molecular protein imaging in live cells. To follow and ultimately understand biological processes, we need their spatiotemporal models of the integrative fluorescence spectroscopy platform. Until now, no holistic use of fluorescence spectroscopy for structural modelling of proteins has been reported.","2499458","2015-12-01","2020-11-30"
"HYDRA-CHEM","Hydrothermal and Ionothermal Chemistry For Sustainable Materials (HYDRA-CHEM)","Markus Antonietti","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","This proposal aims to develop a novel type of chemistry by using hydrothermal or ionothermal reaction conditions to generate novel materials and polymers in a more sustainable fashion. Hydrothermal and ionothermal carbonization of sugars or crude biomass can lead directly to large scale carbon materials (and therefore to carbon-negative products and measures), and improvement by processing and hybridization with petrochemistry is expected to yield carbon structures with controlled surface chemistry and nano-morphology and therefore an extended application spectrum, useful for transfer to society. Model experiments indicate that using other simply available substances as  monomers  under HYDRA-conditions can  result in high performance engineering plastics, ultrahigh surface materials for gas-storage, or novel fragments which are otherwise non accessible in comparable qualities under similarly sustainable conditions. That way, it is expected that HYDRA-Chem can expand synthetic chemistry in the direction of high temperatures and rather unusual solvent conditions.","2461600","2008-11-01","2014-06-30"
"HYDRATIONLUBE","Hydration lubrication: exploring a new paradigm","Jacob Klein","WEIZMANN INSTITUTE OF SCIENCE LTD","In recent years, as first established in some 6 papers in Science and Nature from the PI s group, a new paradigm has emerged. This reveals the remarkable and unsuspected - role of hydration layers in modulating frictional forces between sliding surfaces or molecular layers in aqueous media, termed hydration lubrication, in which the lubricating mode is completely different from the classic one of oils or surfactants. In this project we address the substantial challenges that have now arisen: what are the underlying mechanisms controlling this effect? what are the potential breakthroughs that it may lead to? We will answer these questions through several interrelated objectives designed to address both fundamental aspects, as well as limits of applicability. We will use surface force balance (SFB) experiments, for which we will develop new methodologies, to characterize normal and frictional forces between atomically smooth surfaces where the nature of the surfaces (hydrophilic, hydrophobic, metallic, polymeric), as well as their electric potential, may be independently varied. We will examine mono- and multivalent ions to establish the role of relaxation rates and hydration energies in controlling the hydration lubrication, will probe hydration interactions at both hydrophobic/hydrophilic surfaces and will monitor slip of hydrated ions past surfaces. We will also characterize the hydration lubrication properties of a wide range of novel surface systems, including surfactants, liposomes, polymer brushes and, importantly, liposomes, using also synchrotron X-ray reflectometry for structural information. Attainment of these objectives should lead to conceptual breakthroughs both in our understanding of this new paradigm, and for its practical implications.","2304180","2010-05-01","2015-04-30"
"HyDream","Selective Hydrogenation of Arenes - A Dream Reaction","Frank GLORIUS","WESTFAELISCHE WILHELMS-UNIVERSITAET MUENSTER","The hydrogenation of ketones and olefins is one of the oldest synthetically used transformations. The reaction is highly sustainable and its value has been acknowledged by two Nobel Prizes. In contrast, the hydrogenation of arenes is still underexplored due to the high kinetic barrier caused by aromaticity. However, the selective arene hydrogenation constitutes a dream reaction for use in an ideal synthesis: The transformation is straightforward, uses readily available substrates, and is able to build-up an astonishing amount of complexity, with the potential to form multiple defined sterocentres, in a single step. With our first paper on selective arene hydrogenation published in 2004, we became pioneers in this field and have continuously made important contributions using metal–carbene complexes. As a world-leader in this area and with expertise in several relevant fields of catalysis, we are perfectly suited to convert arene hydrogenation into a reliable and general transformation within the frame of this project. We will provide rapid access to sought-after motifs and consequently will enable breakthroughs in material and life sciences. 

Key to our success will be the design of strongly electron-donating carbene ligands and deep mechanistic understanding. Specifically, we will develop solutions for the problematic hydrogenation of heteroatom-substituted arenes, and heteroarenes. Utilising the soluble nature of a homogenous catalyst, we also envision applications in the hydrogenation of polymers, offering direct access to new materials. Furthermore, the use of syngas is expected to allow for the development of a merged hydrogenation-hydroformylation reaction to yield highly functionalised cyclohexanes in a single step from minimally functionalised arenes. Finally, we aim to develop chiral versions of our highly reactive metal–carbene catalyst to enable the previously unknown but highly desirable enantioselective hydrogenation of benzene derivatives.","2495000","2018-10-01","2023-09-30"
"HYMAGINE","Hybrid CMOS/Magnetic components and systems for energy efficient, non-volatile, reprogrammable integrated electronics","Bernard Dieny","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","Spinelectronics merges magnetism and electronics (Nobel Prize 2007). Besides its fundamental interest, it has found applications in hard disk drives (1998) and in non-volatile standalone memories (MRAM, on market since 2006). MRAMs integrate CMOS components with magnetic tunnel junctions (MTJ). The PI and his team are convinced that besides MRAMs, this hybrid CMOS/MTJ technology can yield a totally new approach in the way electronic devices are designed. Most CMOS devices such as microprocessors are based on Von Neumann architecture in which logic and memories are separate components. The unique set of characteristics combined within MTJs: cyclability, switching speed, scalability, makes it possible to conceive novel electronic systems in which logic and memory are intimately combined in non-volatile logic components (non-volatile CPU). Such systems would have outstanding advantages in terms of energy savings, logic-memory communication speed, ultrafast reprogrammability, compactness, design simplicity. The objective of this project is to lay the fundation of this novel approach, which requires addressing both fundamental and more applied issues. The basic issues concern the improvement and reliability of spintronic materials, mastering the speed and coherence of magnetization switching, developing tools for the quantitative interpretation of MTJ properties and for designing hybrid CMOS/MTJ devices. The applied goals are the conception, building and testing of a few illustrative devices demonstrating the outstanding advantages of this technology. A further one is to establish an internationally recognized roadmap for this non-volatile logic. If successful, its impact on European microelectronics and magnetism industry could be huge.","2500000","2010-07-01","2015-06-30"
"HYMEM","Hybrid Nanosystems in phospholipid membranes","Jochen Feldmann","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","The cell membrane is the biological platform for many vital processes such as photosynthesis, cell-cell contact, biomolecular recognition, endocytosis and ion transport. Artificial phospholipid membranes play an important role in unravelling the physical and chemical characteristics of membranes and their microscopic role in membrane function. On the other hand a manifold of novel hybrid nanosystems consisting of different nano-objects and exhibiting specific physical and chemical functions have been developed. This proposal aims at incorporating hybrid nanoparticle systems with specific optical functions into artificial and cell membranes in order to build up a nanophotonic toolbox for nanoscopic optical manipulators, local spectroscopic studies and the combination of both. This nanophotonic toolbox for membranes will open up novel strategies for the optical control of membrane function, the controlled optothermal release and lateral guiding as well as the local spectroscopy and analysis of biomolecules. The planned toolbox also contains new optical concepts for transfection and drug delivery.","2368320","2011-03-01","2016-02-29"
"HYPERSINGLET","Hyperpolarized Singlet NMR","Malcolm Levitt","UNIVERSITY OF SOUTHAMPTON","Nuclear magnetic resonance (NMR) is the most widely used spectroscopic tool in the physical sciences. Techniques are now available that provide experimental access to hyperpolarized molecules, in which NMR signals are enhanced by up to 5 orders of magnitude, with potentially revolutionary implications. However, the lifetime of the hyperpolarized state is usually limited by the nuclear spin-lattice relaxation time, called T1, and which is typically in the range of a few seconds to about 1 minute. The range of applications accessible to hyperpolarized NMR is restricted by the need to use the hyperpolarized substance within this short timescale. In this proposal, we aim to extend the lifetime of hyperpolarized substances by exploiting a phenomenon first described in our laboratory - namely the exceptional lifetime of nuclear singlet states. These are quantum superposition states of nuclear spin pairs which are protected against many common relaxation mechanisms, with experimentally demonstrated lifetimes of up to 25 minutes. We will (i) identify, design and synthesize substances that support nuclear spin states with especially long lifetimes; (ii) design and demonstrate methodology for hyperpolarizing long-lived nuclear singlet states; (iii) perform test-of-principle experiments showing enhanced NMR imaging of flow and diffusion using hyperpolarized nuclear singlet states, in contexts emulating those found in clinical magnetic resonance imaging (MRI); (iv) design and demonstrate experiments and molecular systems that allow the hyperpolarized singlet order to be transformed into magnetization of strongly magnetic nuclei such as protons, with benefits to the signal strength and to the spatial resolution. In summary we will bridge the gap between the high promise of long-lived nuclear singlet states and the world of real applications, with an emphasis on demonstrating the feasibility of real-world in vivo NMR and MRI applications.","2877583","2012-02-01","2016-07-31"
"IASI-FT","IASI - Flux and temperature","Cathy CLERBAUX","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","IASI - Flux and temperature

July 2016 was Earth's warmest month on record. The first six months of 2016 were also the warmest six-month period since modern meteorology observations began. This, along with the recent so-called “hiatus” in the warming trend, and the Paris climate agreement, all attracted scientific and public attention as to how reliable the historical temperature record is, and to the level of confidence in future model climate projections. Although the role of satellites in observing the variability and change of the Earth system has increased in recent decades, remotely-sensed observations are still underexploited to accurately assess climate change fingerprints. The IASI - Flux and Temperature (IASI-FT) project aims at providing new benchmarks for top-of-atmosphere radiative flux and temperature observations using the calibrated radiances measured twice a day at any location by the IASI instrument on the suite of MetOp satellites.  

The main challenge is to achieve the stringent accuracy and stability necessary for climate studies, particularly for climate trends. Building upon the expertise accumulated by my group during the last 10 years, I propose the development of innovative algorithms and statistical tools to generate climate data records at the global scale, of (1) spectrally resolved outgoing radiances, (2) land and sea skin surface temperatures, and (3) temperatures at selected altitudes. Time series of these quantities will be compared with in situ and other satellite observations if available, atmospheric reanalyses, and climate model simulations. The observed trends will be analyzed at seasonal and regional scales in order to disentangle natural (weather/dynamical) variability and human-induced climate forcings. This project, while clearly research-oriented, will lead towards an operational integrated observational strategy for the Earth climate system, given that the IASI program started in 2006 and will last until 2040 at least.","2200000","2017-10-01","2022-09-30"
"IAXOplus","Towards the detection of the axion with the International Axion Observatory","Igor GARCIA IRASTORZA","UNIVERSIDAD DE ZARAGOZA","The nature of the Dark Universe is an outstanding question in modern science, and is connected with our understanding of the reality at the most fundamental level. Despite the enormous success of the Standard Model (SM) of particle physics, a number of shortcomings of the theory and the fact that it does not account for the Dark Matter and Energy, prompt theorists to propose possible hypothetical extensions. 
Some of these extensions predict the existence of very-light and very-weakly-coupled axions (or axion-like particles, ALPs). Recent theoretical and phenomenological work is sharpening the physics case of these particles. They are now considered as very motivated portals for physics beyond the SM, and in particular as very plausible Dark Matter candidates. In addition, some intriguing astrophysical observations might be interpreted as hints for their existence. 
The International Axion Observatory IAXO is one of the most ambitious proposals to find the axion. Its baseline configuration relies on the search for solar axions, but could also host relic axion detectors. IAXO will go well beyond current experiments' sensitivity and will probe a large fraction of the -still unexplored- parameter space of the axion and ALPs. The scope of the present proposal encompasses the realization of a first complete intermediate experimental stage, BabyIAXO,  including prototypes of the IAXO magnet and detection systems. It will already provide relevant physics outcome in the time-frame of the current grant, while preparing the ground for, and extending the physics reach of, the full IAXO. In particular, BabyIAXO will already be able to test a number of axion and ALP models that are invoked by the aforementioned astrophysical hints and therefore already at this stage there is potential for discovery. The detection of a new fundamental pseudoscalar -potentially solving the DM problem- would lead to a breakthrough in Particle Physics, Cosmology and Astrophysics.","3106875","2018-10-01","2023-09-30"
"ICD","Intermolecular Coulombic decay and control of photoinduced processes in physics, chemistry, and biology","Lorenz S. Cederbaum","RUPRECHT-KARLS-UNIVERSITAET HEIDELBERG","When embedded in a suitable environment, excited or ionized atoms and molecules can hand over their excess energy to their neighbors extremely efficiently via the interatomic (intermolecular) Coulombic decay (ICD) mechanisms. The ICD has been predicted theoretically by the applicant and co-workers and has recently found full confirmation in a series of spectacular experiments. The theoretical and experimental work on ICD performed until now and the progress achieved established the generality of the phenomenon and open new horizons for the ICD research. There is an enormous potential inherent in the ICD and the present proposal is aimed at exploring and exploiting it in systems of physical, chemical, and biological interest. In particular, the high efficiency of the ICD compared to various photoinduced processes like photon emission, isomerization, and charge transfer, makes the ICD extremely attractive for quenching in a controllable fashion such fundamental processes in biophysically relevant systems. Furthermore, the ICD phenomenon produces low-energy electrons and can be expected to be a relevant source of such electrons in nature. It has been proven that low-energy electrons induce serious damages in DNA  and it is natural to investigate the importance of the ICD in the production of such electrons after irradiation. We are convinced of the fundamental and practical relevance of ICD and our vision is to be able to exploit this basic process in systems of interest. To achieve this breakthrough requires an enormous investment in advancing methodologies. This, in turn, can only be reached by a highly motivated strong team of scientists closely collaborating over a long period of time. The support by the ERC can substantially contribute to the realization of this vision.","1950000","2009-02-01","2015-01-31"
"ICE&LASERS","Innovative Concepts for Extracting climate and atmospheric composition records from polar ice cores using new LASER Sensors","Jerome Chappellaz","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","ICE&LASERS propose to make a breakthrough in two challenges of paleoclimate science:

(1) Extending the Antarctic ice core records to 1.5 million years ago is critical to understand the unexplained climate shift from 40,000-year periodicities to 100,000-year ones, calling for a different climate sensitivity to orbital forcing. We propose to revolutionize ice core science by building an innovative probe making its own way into the ice sheet within a single field season, to measure in situ the depth profile of H2O isotopes in ice as well as greenhouse gas concentration in trapped gases, down to bedrock. This high gain/high risk project will allow us to rapidly qualify different “oldest ice” sites, and to immediately obtain the main climatic signals of interest;

(2) Why the atmospheric CO2 and CH4 concentrations varied by up to 40 and 100%, respectively, during glacial-interglacial cycles is still highly debated. We will combine revolutionary detectors with new extraction techniques to measure with unsurpassed accuracy and resolution the concentrations of CH4, CO2 and CO (a tracer related to the CH4 cycle), and the isotopic ratios of CO2 and CO in polar ice. We will constrain theories of past changes in the carbon cycle and of climate feedbacks, and will provide more insight into possible natural feedbacks under a warming future.

ICE&LASERS tackles both scientific challenges, thanks to an analytical revolution for measuring trace gases and their stable isotopes: Optical-Feedback Cavity-Enhanced Absorption Spectroscopy (OFCEAS), recently patented by one of the four CNRS research units involved in the project. ICE&LASERS will contribute to maintain European ice core science at its current leading position, and to optimize the transfer of innovative laser physics to important environmental problems.","2986718","2012-03-01","2018-02-28"
"ICEMASS","Global Glacier Mass Continuity","Hans Andreas Max Kääb","UNIVERSITETET I OSLO","For the first time in history satellite data and respective archive holdings are now sufficient in terms of their spatial and temporal resolution, and their accuracy, to measure volume changes, velocities and changes in these velocities over time for glaciers and ice caps other than ice sheets on a global scale.
The ICEMASS project will derive and analyse glacier thickness changes using satellite laser and radar altimetry, and satellite-derived and other digital elevation models, and convert these to a global glacier mass budget. Such data set will enable major steps forward in glacier and Earth science, in particular: constrain current sea-level contribution from glaciers; complete climate change patterns as reflected in glacier mass changes; quantify the contribution of glacier imbalance to river run-off; allow to separate glacier mass loss from other components of gravity changes as detected through satellite gravimetry; and allow improved modelling of the isostatic uplift component due to current changes in glacier load.
These results will be connected to global-scale glacier dynamics, for which a global set of repeat optical and radar satellite images will be processed to measure displacements due to glacier flow and their annual to decadal-scale changes. The analysis of these data will enable several major steps forward in glacier and Earth science, in particular: progress the understanding of glacier response to climate and its changes; provide new insights in processes underlying spatio-temporal variability and instability of glacier flow on decadal scales; improve understanding of dynamic thickness change effects; allow estimating global calving fluxes; progress understanding of transport in glaciers and their role in landscape development; and help to better assess potentially hazardous glacier lakes.","2395320","2013-03-01","2019-02-28"
"iCOMM","New Frontiers in Nanophotonics: Integrating Complex Beams and Active Metasurface Devices","Anatoly ZAYATS","KING'S COLLEGE LONDON","Complex, structured optical beams have unique properties offering new degrees of freedom for achieving unusual wavefront, polarisation and optical angular momentum demanded in microscopy, optical trapping and manipulation of nano-objects, information encoding in optical communications, holography, quantum technologies and laser micromachining. Metasurfaces, a subwavelength-thin nanostructured films, which were initially developed for controlling the phase of light and its reflection and transmission beyond the Snell’s law, provide a rich playground for generation and manipulation of structured beams. iCOMM will establish a metasurface platform for generating and controlling complex vector beams in space and time and develop its applications in sensing and identification of chiral molecules and nonlinear optical trapping. Using unique optical properties of designer-metasurfaces capable of controlling both phase and amplitude of light, nonlinear interactions of pulsed vector beams will be optimised and explored. We will aim to develop a series of active metamaterial chips for nonlinear control of CVBs, linear and nonlinear sensing of chiral molecules and optical trapping applications, opening new application areas in information processing and biochemical technologies. This will be a transformative development for the applications of complex vector beams and metasurfaces in optical communications, displays, security and bio- and chemical sensing and optical trapping. The success of the project will unlock the potential of metasurfaces in providing tuneability for the improvement of the real-world photonic devices and provide insight into physical phenomena which are vital for various areas of photonics and sensing, demonstrating commercially-viable application of metasurfaces and complex beams. It will transform the areas of both complex beams and metasurfaces by introducing real-time active control and consolidate and enhance the European leadership in this field.","2737327","2018-09-01","2023-08-31"
"ICON","Integrated Real-time Feedback Control and post-processing for image Restoration","Michel Herman G Verhaegen","TECHNISCHE UNIVERSITEIT DELFT","My goal is to develop new computational tools for image restoration by real-time feedback control with full images recorded by a CCD camera. iCON will enable to breakaway from the existing quasi-static Adaptive Optics (AO) or off-line phase diversity approaches. The improvements over these existing image restoration methods are a consequence of three innovative steps taken in this project. The first is the modelling through system identification of the coupled dynamics between the temporal and spatial varying dynamics of the wavefront aberrations that blur the images. New multidimensional distributed Subspace Identification methods will be developed to derive mathematical models that predict the coupled dynamics of the total imaging plant. The use of subspace identification will enable to extract accurate prediction models since no a priori model parameterization is needed, since no use is made of nonlinear parameter optimization and since use can be made of closed-loop data. The accurate predictions are used in the real-time feedback controller to correct the aberrations when they actually occur. The second is the enabled use of the CCD image recording for both identification and real-time control. This sensor provides much more detailed information on the wavefront aberration and the object compared to classically used AO pupil wavefront sensors, e.g. a Shack-Hartmann. The third is the coupling between real-time image restoration and post-processing whereby the real-time feedback provides accurate prior information for the complicated nonlinear optimization in post-processing. The new iCON methodology will enable to consider spatio-temporal feedback on the total imaging plant from the onset of the instrument design cycle. This will lead to finding a better balance between imaging resolution on one hand and size, cost and complexity on the other. Therefore iCON will be a key enabling technology for developing low cost high resolution imaging instruments.","2499358","2014-02-01","2019-01-31"
"iCONNECT","Intracranial COnnection with Neural Networks for Enabling Communication in Total paralysis","Nicolas Franciscus Ramsey","UNIVERSITAIR MEDISCH CENTRUM UTRECHT","iCONNECT aims to give severely paralyzed people the means to communicate by merely imagining to talk or make hand gestures. Imagining specific movements generates spatiotemporal patterns of neuronal activity in the brain which I intend to record and decode with an intracranial Brain-Computer Interface (BCI) system. Many people suffer from partial or full loss of control over their body due to stroke, disease or trauma, and this will increase with population ageing. With both duration and quality of life beyond 60 increasing in the western world, more and more people will suffer from the consequences of function loss (mostly stroke) with the prospect of living for decades with the handicap, and will stand to benefit from restorative technology that has yet to be developed. I believe that functionality can be restored with brain implants. My goal is to develop a BCI that can interpret activity patterns on the surface of the brain in real-time. For this we need to discover how the brain codes for (imagined) actions, how codes can be captured and decoded and how an intracranial BCI system impacts on a user. I will use state of the art techniques (7 Tesla MRI and electrocorticography, ECoG) to explore brain codes and develop decoding strategies. Interactions between user and implanted device will be studied in paralyzed people. I will directly link decoded movements to animated visual feedback of the same body part, expecting to induce a feeling of ownership of the animation, and thereby a sense of actual movement. This research is only possible because of the latest developments in imaging of human brain activity, machine learning techniques, and micro systems technology. My lab is unique in bringing together all these techniques. Success of the project will lead to deeper understanding of how sensorimotor functions are represented in the human brain. The ability to ‘read’ the brain will add a new dimension to the field of neural prosthetics.","2498829","2013-08-01","2018-07-31"
"IDEA HEUSLER!","Inverse Design on an Atomic scale: Multifunctional Heusler compounds!","Claudia Anna Maria Felser","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Heusler compounds are a remarkable material, a vast collection of more than 1000 members with a large number of functionalities. The prototype Cu2MnAl is a ferromagnetic compound, even though none of its elemental constituents are magnetic. This is typical for this material class; the properties of many of the Heuslers can be forecast simply by counting the number of their valence electrons. Most of the Heusler compounds with 8, 18 or 24 valence electrons are semiconductors. The band gap can be tuned from more than 4 eV to zero by changing systematically the composition and simultaneously the lattice constant. Co2-Heusler compounds with more than 24 valence electrons are half-metallic ferromagnetic. Such compounds display nearly full spin polarized conduction electrons making them very useful for spintronics. Mn2-Heusler compounds show excellent performance for spin torque transfer applications. Another class of Heusler compounds has been predicted to be topological insulators, a new quantum state of matter. The precondition for my ambitious proposal was our systematic investigation of the Heusler family, which has led to detailed insight into the structure- electronic structure to property relation. The short term vision of this proposal is the design, synthesis and investigation of building blocks based on the Heusler structure with combined functionalities; such as superconducting topological insulators. With a virtual lab approach the materials with a desired property will be designed. The large number, the tuneability and controllability of the properties makes this material class the ideal system for the design of artificial multifunctional material on an atomic scale which allows contactless switchable functionalities via external control by fields, current, temperature, or other physical quantities. The long term vision is the synthesis of such an artificial material made by thin film or chemical methods for future electronics and energy technologies.","2360240","2012-04-01","2017-03-31"
"iGEO","Integrated geodynamics: Reconciling geophysics and geochemistry","Jeannot Alphonse Trampert","UNIVERSITEIT UTRECHT","How are deep mantle processes related to the mapped geological record? How can we reconcile geochemical observations with geophysical inferences? These are first order unanswered questions despite our steady progress in imaging the Earth's internal structure and understanding the high temperature and pressure properties of minerals. To make a breakthrough, we have to understand solid-state convection in the Earth's mantle in much greater detail. Much is known about the physical processes, such as melting and the delicate interaction between thermal and chemical buoyancy, but the parameters that enter their mathematical description are not very well known. Once these parameters are determined, the thermo-chemical evolution of our planet can self-consistently be modelled. The state-of-the-art is to roughly estimate these parameters and qualitatively compare the modelling to some relevant geophysical, geochemical or geological observations. This comparison is not comprehensive and never explains all observables. We propose a radically new approach, where all observables are used together to infer these parameters directly, using a fully non-linear Bayesian inference technique based on neural networks. This will determine for the first time the initial conditions at the Earth's formation, the Earth-like flow parameters essential to model the thermo-chemical evolution of our planet and produce models that are simultaneously consistent with the main different geophysical and geochemical datasets.","3480600","2013-05-01","2018-04-30"
"iHEART","An Integrated Heart Model for the simulation of the cardiac function","Alfio Maria QUARTERONI","POLITECNICO DI MILANO","The goal of this project is to construct, mathematically analyze, numerically approximate, computationally solve, and validate on clinically relevant cases a mathematically-based integrated heart model (IHM) for the human cardiac function. The IHM comprises several core cardiac models – electrophysiology, solid and fluid mechanics, microscopic cellular force generation, and valve dynamics – which are then coupled and finally embedded into the systemic and pulmonary blood circulations. It is a multiscale system of Partial Differential Equations (PDEs) and Ordinary Differential Equations (ODEs) featuring multiphysics interactions among the core models.

The physical and mathematical properties of each core model and those of the even more complex integrated heart model (IHM) will be analyzed. The numerical approximation of IHM develops along several steps: introduce new high order methods for the core models, carry out their stability and convergence analysis, devise new paradigms for their numerical coupling, and construct optimal, scalable, and adaptive preconditioners for the efficient solution of the resulting large-scale discrete problems. To address data variability in clinically relevant cases, new reduced order models and efficient computational techniques will be developed also for forward and inverse uncertainty quantification problems. Two software libraries, LifeHEART and RedHEART, will be built and made available to the scientific community.

The project is original, very ambitious, mathematically inspired and rigorous, tremendously challenging, and groundbreaking. If successful, it will provide researchers from applied mathematics and life sciences, cardiologists, and cardiac surgeons with a powerful tool for both the qualitative and quantitative study of cardiac function and dysfunction. iHEART has the potential to drive improvements in diagnosis and treatment for cardiovascular pathologies that are responsible for more than 45% of deaths in Europe.","2351544","2017-12-01","2022-11-30"
"ILID","Ionic Liquid Interface Dynamcis","Hans-Peter Steinrueck","FRIEDRICH-ALEXANDER-UNIVERSITAET ERLANGEN NUERNBERG","The aim of the project is to take the ground-breaking step from our present knowledge of static properties to the understanding and control of dynamical processes at ionic liquid interfaces. Ionic liquids (ILs) are chosen as model systems for liquids in general for two reasons: First, their structural diversity allows their properties to be tailored over a wide range, and second, they can be studied using the extremely powerful methods of surface science in ultra-high vacuum due to their low vapor pressure. Such studies cannot be performed for conventional liquids, since they evaporate. 
ILs are not only relevant from a fundamental point of view, but also for a variety of applications. In catalysis, two new concepts have been put forward: Supported Ionic Liquid Phase (SILP) and Solid Catalyst with Ionic Liquid Layer (SCILL). In both, a high surface area solid substrate is covered with a thin IL film, which contains a dissolved metal complex for SILP, or which modifies active sites at the support for SCILL. For these and other applications, a fundamental understanding of the dynamical processes at the gas/IL and/or IL/support interfaces is strongly needed, but does not exist. Equally important, but even more challenging, is the investigation of the dynamics of chemical reactions in ILs, also under electrochemical conditions.
Therefore, the applicant proposes a multi-method approach with new and unique setups to follow these dynamical processes in real time, that is, while they occur. Towards this goal, four key topics will be addressed: (A) How do gases pass through the gas/liquid interface? (B) How does the liquid/solid interface form? (C) Real-time studies of reactions in ILs, and (D) Real-time studies of electrochemical processes in ILs. The achieved insight will then enable to control the processes at the molecular level by tailoring the properties of the ILs. This promises a breakthrough not only for ILs, but for liquid interfaces in general.","2498125","2016-10-01","2021-09-30"
"IMAGINE","Imaging Magnetism in Nanostructures using Electron Holography","Rafal Edward Dunin-Borkowski","FORSCHUNGSZENTRUM JULICH GMBH","""Future developments in the control, functionalization and manipulation of magnetic nanoparticles and nanoscale magnetic devices require an understanding of collinear and non-collinear spin configurations and correlated changes of electronic structure on the sub-nanometer scale.

In this project, an experimental methodology will be developed to allow magnetic spin structures in differently shaped nanomagnets to be visualized quantitatively and correlated with their crystallographic, compositional and defect structures. The project is based on the development of electron holography in the transmission electron microscope. It aims to provide quantitative measurements of internal magnetic fields in nanoparticles and devices that have characteristic dimensions of between 2 and 20 nm with a spatial resolution of better than 1 nm, both in projection and in three dimensions. Developments in instrumentation will allow the measurements to be acquired in situ in the electron microscope at elevated and reduced specimen temperatures and in the presence of oxidizing and reducing gases.

The project is highly interdisciplinary, requiring close collaboration between scientists working on nanoparticle synthesis, device fabrication, magnetic modeling, computational mathematics and characterization techniques. It will provide a powerful new analytical tool at the frontiers of the highest spatial resolution analysis of spin and electronic structures that will have far-reaching impact beyond a specific research domain, not only in fundamental magnetism but also for applications that include magnetic recording, spintronics, catalysis and biomedical applications of magnetic nanoparticles.

The methodology will also benefit European industry by providing a new leading edge in the fast growing international market of in situ transmission electron microscopy.""","2499140","2013-04-01","2018-03-31"
"IMPaCT","Implementing Multi-Party Computation Technology","Nigel Paul Smart","KATHOLIEKE UNIVERSITEIT LEUVEN","The goal of IMPaCT is to turn Multi-Party Computation (MPC) from a stage in which we are beginning to obtain practical feasibility results, to a stage in which we have fully practical systems. It has long been acknowledged that MPC has the potential to provide a transformative change in the way security solutions are enabled. As it presently stands this is currently only possible in limited applications; deployments in restricted scenarios are beginning to emerge. However, in turning MPC into a fully practical technology a number of key scientific challenges need to be solved; many of which have not yet even been considered in the theoretical literature. The IMPaCT project aims to address this scientific gap, bridge it, and so provide the tools for a future road-map in which MPC can be deployed as a widespread tool, as ubiquitous as encryption and digital signatures are today.

Our scientific approach will be to investigate new MPC protocols and techniques which take into account practical constraints and issues which would arise in future application scenarios. Our work, despite being scientifically rigorous and driven from deep theoretical insight, will be grounded in practical considerations. All systems and protocols proposed will be prototyped so as to ensure that practical real world issues are taken into account. In addition we will use our extensive industrial linkages to ensure a two way dialogue between potential users and the developers of MPC technology; thus helping to embed future impact of the work in IMPaCT.

Our workplan is focused around key scientific challenges which we have identified on the road to fully practical MPC applications. These include the design of methodologies to cope with the asynchronicity of networks, how to realistically measure and model MPC protocols performance, how to utilize low round complexity protocols in practice, how to deal with problems with large input sizes (e.g. streaming data), and many more.","2499938","2016-10-01","2021-09-30"
"IMPUNEP","Innovative Materials Processing Using Non-Equilibrium Plasmas","Allan Matthews","THE UNIVERSITY OF MANCHESTER","Current bulk materials processing methods are nearing their limit in terms of ability to produce innovative materials with compositional and structural consistency.
The aim of this ambitious project is to remove barriers to materials development, by researching novel methods for the processing of engineering materials, using advanced non-equilibrium plasma systems, to achieve a paradigm shift in the field of materials synthesis. These new processes have the potential to overcome the constraints of existing methods and also be environmentally friendly and produce novel materials with enhanced properties (mechanical, chemical and physical).
The research will utilise plasmas in ways not used before (in bulk materials synthesis rather than thin film formation) and it will investigate different types of plasmas (vacuum, atmospheric and electrolytic), to ensure optimisation of the processing routes across the whole range of material types (including metals, ceramics and composites).
The materials synthesised will have benefits for products across key applications sectors, including energy, healthcare and aerospace. The processes will avoid harmful chemicals and will make optimum use of scarce material resources.
This interdisciplinary project (involving engineers, physicists, chemists and modellers) has fundamental “blue skies” and transformative aspects. It is also high-risk due to the aim to produce “bulk” materials at adequate rates and with consistent uniform structures, compositions and phases (and therefore properties) throughout the material. There are many challenges to overcome, relating to the study of the plasma systems and materials produced; these aspects will be pursued using empirical and modelling approaches. The research will pursue new lines of enquiry using an unconventional synthesis approach whilst operating at the interface with more established discipline areas of plasma physics, materials chemistry, process diagnostics, modelling and control.","2499283","2013-02-01","2018-09-30"
"INCOVID","Inpainting-based Compression of Visual Data","Joachim WEICKERT","UNIVERSITAT DES SAARLANDES","Generating huge amounts of visual data, be it images or videos, has never been easier than today. This creates a growing demand for lossy codecs (coders and decoders) that produce visually convincing results also for very high compression rates. Popular transform-based codecs such as JPEG and JPEG 2000 have reached a state where one cannot expect significant improvements anymore. To go beyond their limitations, fundamentally different ideas are needed.

Inpainting-based codecs can change this situation. They store only a small, carefully optimised part of the data. In the decoding step, the missing information is filled in with a suitable inpainting mechanism. A successful realisation of inpainting-based codecs can offer decisive advantages over transform-based codecs: The stored information is more intuitive and closer to the mechanisms of human perception. Moreover, the concept is very flexible: It allows to integrate a number of different features and can be tailored towards dedicated applications. Most importantly, the higher the compression rate, the larger are the qualitative advantages over transform-based codecs. 

However, the potential of these codecs is widely unexplored so far, since difficult fundamental problems must be solved first. This includes optimisation of the data and the inpainting process, sophisticated data coding, and the design of real-time capable sequential and parallel numerical algorithms. We are committed to addressing all these challenges in an integrated approach: 
We cover the entire spectrum from its theoretical foundations over benchmarking and highly efficient numerical algorithms to codecs  for specific applications, and a real-time 4K video player as demonstrator. 

This will lift inpainting methods from a visually pleasant image editing tool to a fundamental paradigm in coding. Research results that enter forthcoming coding standards will also have an impact on everybody's daily life.","2460000","2017-10-01","2022-09-30"
"INFANT EARTH","The Making of the Earth – Reading the Geochemical Code from Meteorites and the Earth’s Oldest Rocks","Carsten Muenker","UNIVERSITAET ZU KOELN","It is still an open question how Earth became the rocky habitable planet as we know it today. This is because there is a significant time gap of several 100 million years between Earth’s oldest rock archives (ca. 4 billion years old) and most extraterrestrial samples like meteorites that archive the birth of our solar system ca. 4.5 billion years ago. Within this time gap, three key processes that shaped our planet took place, i.e., Earth’s growth via asteroidal collisions, formation of the metal core and a first solid crust, and the delivery of volatiles such as water. Because rock samples are lacking, these fundamental processes have to be traced indirectly, by using highly sophisticated geochemical tools like isotope or trace element compositions of younger rocks or meteorites. 

With this proposal, I plan to better unravel Earth’s earliest history and better identify its building blocks, by combining the geochemical record locked in Earth`s oldest rocks and extraterrestrial samples. The ground breaking nature of this work is the development of new geochemical techniques that are way beyond the current state of art, and many of them will be applied at an unprecedented level of sensitivity and precision. I will cover three linked approaches, namely high precision analyses of (i) nucleosynthetic isotope anomalies, (ii) radiogenic isotopes and (iii) trace elements. To better constrain the history of volatile delivery to the nascent Earth, a focus will be on comparing the geochemical record provided by refractory and volatile elements. In their synergy, the results will provide a major advance in unravelling Earth’s earliest history.

INFANTEARTH builds on high profile research strengths of our group, where a unique pool of collaborating scientists and analytical equipment are available. We have also acquired a nearly unique collection of Earth’s oldest rock samples and of extraterrestrial samples supplied from institutions such as NASA or collaborating museums.","2499735","2015-09-01","2021-08-31"
"InfiniteBayesian","Bayesian Statistics in Infinite Dimensions: Targeting
Priors by Mathematical Analysis","Adrianus Willem Van Der Vaart","UNIVERSITEIT LEIDEN","I propose novel methods for understanding key aspects that are essential
to the future of Bayesian inference for high- or infinite-dimensional
models and data.  By combining my expertise on empirical processes and
likelihood theory with my recent work on posterior contraction I shall
foremost lay a mathematical foundation for the Bayesian solution to
uncertainty quantification in high dimensions.

Decades of doubt that Bayesian methods can work for high-dimensional
models or data have in the last decade been replaced by a belief that
these methods are actually especially appropriate in this
setting. They are thought to possess greater capacity for
incorporating prior knowledge and to be better able to combine data
from related measurements. My premise is that for high- or
infinite-dimensional models and data this belief is not well founded,
and needs to be challenged and shaped by mathematical analysis.

My central focus is the accuracy of the posterior distribution as
quantification of uncertainty. This is unclear and has hardly been
studied, notwithstanding that it is at the core of the Bayesian
method. In fact the scarce available evidence on Bayesian credible
sets in high dimensions (sets of prescribed posterior probability)
casts doubt on their ability to capture a given truth. I shall discover
how this depends strongly on the prior distribution, empirical or
hierarchical Bayesian tuning, and posterior marginalizaton, and therewith
generate guidelines for good practice.

I shall study these issues in novel statistical settings (sparsity and
large scale inference, inverse problems, state space models,
hierarchical modelling), and connect to the most recent, exciting
developments in general statistics.

I work against a background of data-analysis in genetics, genomics,
finance, and imaging, and employ stochastic process theory,
mathematical analysis and information theory.","2190000","2013-05-01","2018-04-30"
"INNOSTOCH","INNOVATIONS IN STOCHASTIC ANALYSIS AND APPLICATIONS with emphasis on STOCHASTIC CONTROL AND INFORMATION","Bernt Karsten Øksendal","UNIVERSITETET I OSLO","""For almost all kinds of dynamic systems modeling real processes in nature or society, most of the mathematical models we can formulate are - at best - inaccurate, and subject to random fluctuations and other types of """"noise"""". Therefore it is important to be able to deal with such noisy models in a mathematically rigorous way. This rigorous theory is stochastic analysis. Theoretical progress in stochastic analysis will lead to new and improved applications in a wide range of fields.

The main purpose of this proposal is to establish a research environment which enhances the creation of new ideas and methods in the research of stochastic analysis and its applications. The emphasis is more on innovation, new models and challenges in the research frontiers, rather than small variations and minor improvements of already established theories and results. We will concentrate on applications in finance and biology, but the theoretical results may as well apply to several other areas.

Utilizing recent results and achievements by PI and a large group of distinguished coworkers, the natural extensions from the present knowledge is to concentrate on the mathematical theory of the interplay between stochastic analysis, stochastic control and information. More precisely, we have ambitions to make fundamental progress in the general theory of stochastic control of random systems and applications in finance and biology, and the explicit relation between the optimal performance and the amount of information available to the controller. Explicit examples of special interest include optimal control under partial or delayed information, and optimal control under inside or advanced information. A success of the present proposal will represent a substantial breakthrough, and in turn bring us a significant step forward in our attempts to understand various aspects of the world better, and it will help us to find optimal, sustainable ways to influence it.""","1864800","2009-09-01","2014-08-31"
"INPAINTING","Inpainting Tools for Video Post-production: Variational theory and fast algorithms","Vicent Caselles Costa","UNIVERSIDAD POMPEU FABRA","The goal of this project is the mathematical investigation of smoothness and self-similarity principles in generating natural images, the mathematical formulation and unification of both ideas in a variational form, and its application to develop models and algorithms for image processing tasks.
The proposed research will lead to the formulation and mathematical analysis of new variational principles for image and movie processing, the analysis of their underlying geometric measure theory and partial differential equations, unifying local and nonlocal approaches as respective mathematical expressions of the ideas of regularity and self-similarity.  Our research will be guided by a thorough investigation of the inpainting problem (including images, video and stereo video inpainting), as a very suitable model for testing the proposed ideas.
The first practical impact will be the development of models and algorithms for 2D and 3D image and video editing and manipulation, enabling the deletion and insertion of objects. As a second impact we will provide the theoretical background and implementation of a set of algorithms for 2D to 3D conversion of video data enabling the generation of 3D content for 3D TV from existing 2D video. Due to its fundamental nature, the proposed models may impact other image and video processing areas such as  denoising, restoration, optical flow computation, or stereo, that share similar challenges. Although their study is not in the scope of this project, it will be fostered by the dissemination of our results and the public release of our algorithms.
The PI has a long experience in the variational formulation of image processing problems, with key contributions in the variational formulations of edge detection and image inpainting, mathematical morphology, and the analysis of Total Variation based models. On the practical side, he has been contributing to the development of video post-production tools in several projects led by industry.","515055","2013-04-01","2014-09-30"
"InPairs","In Silico Pair Plasmas: from ultra intense lasers to relativistic astrophysics in the laboratory","Luís Miguel DE OLIVEIRA E SILVA","INSTITUTO SUPERIOR TECNICO","How do extreme electromagnetic fields modify the dynamics of matter? Will quantum electrodynamics effects be important at the focus of an ultra intense laser? How are the magnetospheres of compact stellar remnants formed, and can we capture the physics of these environments in the laboratory? These are all longstanding questions with an overarching connection to extreme plasma physics.
Electron-positron pair plasmas are pervasive in all these scenarios. Highly nonlinear phenomena such as QED processes, magnetogenesis, radiation, field dynamics in complex geometries, and particle acceleration, are all linked with the collective dynamics of pair plasmas through mechanisms that remain poorly understood.
Building on our state-of-the-art models, on the availability of enormous computational power, and on our recent transformative discoveries on ab initio modelling of plasmas under extreme conditions, the time is ripe to answer these questions in silico. InPairs aims to understand the multidimensional dynamics of electron-positron plasmas under extreme laboratory and astrophysical fields, to determine the signatures of the radiative processes on pair plasmas, and to identify the physics of the magnetospheres of compact stellar remnants, focusing on the electrodynamics of pulsars, that can be mimicked in laboratory experiments using ultra high intensity lasers and charged particle beams.
This proposal relies on massively parallel simulations to bridge the gap, for the first time, between the pair plasma creation mechanisms, the collective multidimensional microphysics, and their global dynamics in complex geometries associated with laboratory and astrophysical systems. Emphasis will be given to detectable signatures e.g. radiation and accelerated particles, with the ultimate goal of solving some of the central questions in extreme plasma physics, thus opening new connections between computational studies, laboratory experiments, and relativistic plasma astrophysics.","1951124","2016-09-01","2021-08-31"
"INPEC","Interacting Photon Bose-Einstein Condensates in Variable Potentials","Ernst Martin Weitz","RHEINISCHE FRIEDRICH-WILHELMS-UNIVERSITAT BONN","""Bose-Einstein condensation, the macroscopic ground state occupation of a system of bosonic particles below a critical temperature, has in the last two decades been observed in cold atomic gases and in solid-state physics quasiparticles. The perhaps most widely known example of a bosonic gas, photons in blackbody radiation, however exhibits no Bose-Einstein condensation, because the particle number is not conserved and at low temperatures the photons disappear in the system’s walls instead of massively occupying the cavity ground mode. This is not the case in a small optical cavity, with a low-frequency cutoff imprinting a spectrum of photon energies restricted to well above the thermal energy. Using a microscopic cavity filled with dye solution at room temperature, my group has recently observed the first Bose-Einstein condensate of photons.

Building upon this work, the grant applicant here proposes to study the physics of interacting photon Bose-Einstein condensates in variable potentials. We will study the flow of the light condensate around external perturbations, and exploit signatures for superfluidity of the two-dimensional photon gas. Moreover, the condensate will be loaded into variable potentials induced by optical index changes, forming a periodic array of nanocavities. We plan to investigate the Mott insulating regime, and study thermal equilibrium population of more complex entangled manybody states for the photon gas. Other than in an ultracold atomic gas system, loading and cooling can proceed throughout the lattice manipulation time in our system. We expect to be able to directly condense into a macroscopic occupation of highly entangled quantum states. This is an issue not achievable in present atomic physics Bose-Einstein condensation experiments. In the course of the project, quantum manybody states, when constituting the system ground state, will be macroscopically populated in a thermal equilibrium process.""","2133560","2013-03-01","2018-02-28"
"INSEETO","In-situ second harmonic generation for emergent electronics in transition-metal oxides","Manfred FIEBIG","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Since transition-metal oxides heterostructures can be grown by pulsed laser deposition (PLD) with semiconductor-like accuracy, fascinating phases and functionalities derived from their spin-charge correlations have been discovered. So far, reflection high-energy electron diffraction is the only widely established technique for monitoring the structure and homogeneity of multilayers in-situ, while they are growing, and provide direct feedback information on how to optimise the growth process. With our proposal we will introduce second harmonic generation (SHG) as new in-situ technique that allows us to track spin-and charge-related phenomena such as ferroelectricity, (anti-) ferromagnetism, insulator-metal transitions, domain coupling effects or interface states in a non-invasive way throughout the deposition process. With this we are pursuing two goals: first, to establish SHG as new in-situ characterization technique in PLD which monitors strong spin-charge correlation effects while they emerge during growth; second, to apply in-situ SHG for tailoring novel functionalities in exemplary chosen types of transition-metal-oxide heterostructures of great current interest. These model systems are (i) proper ferroelectrics tuned to high-k dielectric response and improper ferroelectrics whose behaviour is determined by the unusual nature of the polar state; (ii) compounds in which the interplay of strain and defects leads to novel and reversibly tuneable states of matter; (iii) heterostructures with functionalities originating from the interaction across interfaces. In-situ SHG as new, property-monitoring tool in PLD has an immense potential to uncover new states of matter and functionalities. We are convinced that this will play an essential role in the leap towards the next generation of functional oxide heterostructures.","2498714","2017-01-01","2021-12-31"
"INSPECTRA","Silicon-photonics-based laser spectroscopy platform: towards a paradigm shift in environmental monitoring and health care","Roeland Baets","UNIVERSITEIT GENT","The Principal Investigator and his team will open up new horizons in the field of laser spectroscopy through basic research on silicon-photonics-based Spectroscopic Systems-On-Chip (SpecSOC’s). The key question being addressed is: how can the powerful concepts of high-index-contrast nanophotonics be combined with the extreme accuracy of silicon technology and with the performance of hybrid silicon/III-V integration in order to create system-on-chip functionalities for advanced (bio-)spectroscopy.
We will first focus research on integrated lasers or Laser Systems-on-Chip (LaSOC’s) capable of providing very wide wavelength tuning in the infrared, mid-infrared or visible. These lasers will have an unprecedented combination of properties. They will differ from existing semiconductor lasers in the sense that they combine the best of III-V semiconductor technology and silicon technology in unique cavity structures exploiting high index contrast in three dimensions.
In the second phase of the project we will shift the focus from laser-oriented novelty to spectroscopy-oriented novelty and investigate SpecSOC’s with an unprecedented system performance that matches the requirements of mainstream real-life spectroscopy. We will explore coherent optical detection techniques for sensitivity enhancement, microporous coatings for on-chip gas sensing and implant-oriented tissue spectroscopy.
Our research will lead to a paradigm shift in laser spectroscopy, in the sense that it will turn an advanced spectroscopy system into a small form-factor commodity system. This will have an enormous impact on applications such as point-of-care medical diagnosis and medical implants, monitoring of air, water and food quality. Furthermore the on-chip spectroscopy systems will be highly valuable for fundamental research.","2183000","2011-04-01","2017-03-31"
"INSTABILITIES","Instabilities and nonlocal multiscale modelling of materials","Davide Bigoni","UNIVERSITA DEGLI STUDI DI TRENTO","""Failure in ductile materials results from a multiscale interaction of discrete microstructures hierarchically emerging through subsequent material instabilities and self-organizing into regular patterns (shear band clusters, for instance). The targets of the project are: (i.) to disclose the failure mechanisms of materials through analysis of material instabilities and (ii.) to develop innovative microstructures to be embedded in solids, in order to open new possibilities in the design of ultra-resistant materials and structures.
The link between the two targets is that micromechanisms developing during failure inspire the way of enhancing the mechanical properties of materials by embedding microstructures. The aim is to provide design tools to obtain groundbreaking and unchallenged mechanical properties employing discrete microstructures, for instance to design a microstructure defining a material working under flutter condition.
The design of these microstructures will permit the achievement of innovative dynamical properties, defining elastic metamaterials, for instance, permitting the fabrication of flat lenses for elastic waves, evidencing negative refraction and superlensing effects. The objective is the discovery of these effects in mechanics, thus disclosing new horizons in the dynamics of materials.
Microstructures introduce length scales and nonlocal effects in the mechanical modelling, which involve the use of higher-order theories.
The analysis of these effects, usually developed within a phenomenological approach, will be attacked from the fundamental and almost unexplored point of view: the explicit evaluation of nonlocality, related to the microstructure via homogenisation theory.""","2379359","2014-03-01","2019-02-28"
"INSULATRONICS","Controlling Electric Signals with Insulating Antiferromagnets and Insulating Ferromagnets","Arne Brataas","NORGES TEKNISK-NATURVITENSKAPELIGE UNIVERSITET NTNU","The proposal aims to facilitate a revolution of information and communication technologies by controlling electric signals with antiferromagnetic insulators and ferromagnetic insulators. We recently discovered that antiferromagnets can be active components in spintronics devices despite their lack of a macroscopic magnetic moment, and even when they are insulating.

Conventional electronics- and spintronics-based logic and memory devices, interconnects, and microwave oscillators are based on (spin-polarized) charge transport, which inherently dissipates power due to ohmic losses. The research proposed seeks to determine the extents to which “Insulatronics” has the potential to control the electric and thermal signal generation, transmission, and detection in more power-efficient ways.

Insulatronics is profoundly different because there are no moving charges involved so the power reduction is significant. We hope to establish the extents to which spin-waves and coherent magnons in antiferromagnetic insulators and ferromagnetic insulators can be strongly coupled to electric and thermal currents in adjacent conductors and utilize this coupling to control electric signals. The coupling will be facilitated by spin-transfer torques and spin-pumping – a technique we pioneered – as well as spin-orbit torques and its reciprocal process of charge-pumping.

The core of this project focuses on the theoretical and fundamental challenges facing Insulatronics. Beyond the duration of the project, if we are successful, the use of spin signals in insulators with extremely low power dissipation may enable superior low-power technologies such as oscillators, logic devices, interconnects, non-volatile random access memories, and perhaps even quantum information processing.","2140503","2015-12-01","2020-11-30"
"INSYSBIO","Industrial Systems Biology of Yeast and A. oryzae","Jens Nielsen","CHALMERS TEKNISKA HOEGSKOLA AB","Metabolic engineering is the development of new cell factories or improving existing ones, and it is the enabling science that allows for sustainable production of fuels and chemicals through biotechnology. With the development in genomics and functional genomics, it has become interesting to evaluate how advanced high-throughput experimental techniques (transcriptome, proteome, metabolome and fluxome) can be applied for improving the process of metabolic engineering. These techniques have mainly found applications in life sciences and studies of human health, and it is necessary to develop novel bioinformatics techniques and modelling concepts before they can provide physiological information that can be used to guide metabolic engineering strategies. In particular it is challenging how these techniques can be used to advance the use of mathematical modelling for description of the operation of complex metabolic networks. The availability of robust mathematical models will allow a wider use of mathematical models to drive metabolic engineering, in analogy with other fields of engineering where mathematical modelling is central in the design phase. In this project the advancement of novel concepts, models and technologies for enhancing metabolic engineering will be done in connection with the development of novel cell factories for high-level production of different classes of products. The chemicals considered will involve both commodity type chemicals like 3-hydroxypropionic acid and malic acid, that can be used for sustainable production of polymers, an industrial enzyme and pharmaceutical proteins like human insulin.","2499590","2010-01-01","2014-12-31"
"INTECOCIS","Introducing Exascale Computing in combustion instabilities Simulations (INTECOCIS)","Thierry Poinsot","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""INTECOCIS is a project on energy production by combustion built by IMFT (experiments, theory and instabilities) and CERFACS (numerical simulation). Combustion produces 90 percent of the earth energy and will remain our first energy source for a long time. Optimizing combustors is a key issue to burn fossil and renewable fuels more efficiently but also to replace wind or solar energy production on days without sun or wind. This optimization cannot take place without numerical simulation (‘virtual combustors’) that allows to test designs without building them. These virtual combustors cannot account for combustion instabilities (CI) which are a major risk in combustors where they induce vibration, loss of control and destruction. CIs cannot be predicted reliably today.  INTECOCIS aims at introducing recent progress in High Performance Computing (HPC) into studies of CIs, to build simulation tools running on massively parallel computers that can predict CIs in future combustors and assess methods to control them. To achieve this goal, the simulations used today for CIs will be revolutionized to integrate recent HPC capacities and have the capabilities and brute power required to compute and control CI phenomena. A second objective of INTECOCIS is to distribute these HPC-based tools in Europe. These tools will integrate UQ (uncertainty quantification) methodologies to quantify the uncertainties associated with the simulations because CIs are sensitive to small changes in geometry, fuel composition or boundary conditions. Moreover, simulation tools also contain uncertain parameters (numerical methods, space and time discretization, impedances, physical sub models) that will have to be investigated as well. Most of the work will be theoretical and numerical but INTECOCIS will also include validation on laboratory burners (at IMFT and other laboratories in Europe) as well as applications on real combustors for European companies collaborating with IMFT and CERFACS.""","2488656","2013-02-01","2018-01-31"
"INTEGRAL","Integrable Systems in Gauge and String Theory","Konstantin Zarembo","STOCKHOLMS UNIVERSITET","The project is aimed at uncovering new links between integrable systems, string theory and quantum field theory. The goal is to study non-perturbative phenomena in strongly-coupled field theories, and to understand relationship between gauge fields and strings at a deeper level.","1693692","2014-03-01","2019-02-28"
"INTELHYB","Next generation of complex metallic materials with intelligent hybrid structures","Jürgen Eckert","OESTERREICHISCHE AKADEMIE DER WISSENSCHAFTEN","In a modern society, metallic materials are crucially important (e.g. energy, safety, infrastructure, transportation, health, medicine, life sciences, IT). Contemporary examples with inherent challenges to be overcome are the design of ultrahigh specific strength materials. There is a critical need for successful developments in this area in particular for reduced energy consumption, reduction of pollutant emissions and passenger safety. Alternative approaches include improved thermal stability and creep resistance of high-temperature alloys for energy conversion, which are generally used in power plants and turbine engines, high temperature process technology, and fossil-fuel driven engines. The ageing European society makes biomedical materials for implant and stent design also crucially important. A drawback of nearly all current high strength metallic materials is that they lack ductility (i.e. are brittle and hard to form)- or on the opposite side, they may be highly ductile but lack strength. The key concept behind INTELHYB is to define new routes for creation of tailored metallic materials based on scale-bridging intelligent hybrid structures enabling property as well as function optimization. The novelty of this proposal as compared to conventional ideas is that they apply to monolithic amorphous materials or bulk microcrystalline. The basis will be founded on innovative strategies for the design, synthesis and characterization of intrinsic length-scale modulation and phase transformation under highly non-equilibrium conditions. This will include the incorporation of dispersed phases which are close to or beyond their thermodynamic and mechanical stability limit thus forming a hierarchically structured hybrid and ductile/tough alloys. Alternatively, the material itself will be designed in a manner such that it is at the verge of its thermodynamic/mechanical stability.","2499920","2014-02-01","2019-01-31"
"IntelliAQ","Artificial Intelligence for Air Quality","Martin SCHULTZ","FORSCHUNGSZENTRUM JULICH GMBH","The IntelliAQ project will develop novel approaches for the analysis and synthesis of global air quality data based on deep neural networks. The foundation of this project is the world’s largest collection of surface air quality measurements, which was recently assembled by the principal investigator and plays a pivotal role in the ongoing first comprehensive Tropospheric Ozone Assessment Report (TOAR). This database will be complemented with data from the world’s leading effort to collect global air pollutant measurements in near realtime and combined with high-resolution geodata, weather information, and satellite retrievals of atmospheric composition in order to characterize individual measurement locations and regional air pollution patterns. State-of-the-art deep learning methods will be applied to this unprecedented dataset in order to 1) fill observation gaps in space and time, 2) provide short-term forecasts of air quality, and 3) assess the quality of air pollutant information from diverse measurements. The combination of diverse data sources is unique, and the project will be the first to apply the full potential of deep neural networks on global air quality data. The achievement of the three IntelliAQ objectives will shift the analysis of global air pollutant observations to a new level and provide a basis for the future development of innovative air quality services with robust scientific underpinning. Due to the heterogeneity of the multivariate data, lack of structure, and generally unknown uncertainty of the input data, the project also poses challenges for existing deep learning methods, and will thus lead to new developments in this field. Direct outcomes of the project will be a substantial improvement of global air quality information including methods to assess the quality of air pollution measurements, and a new data-driven method for forecasting air quality at local scales.","2498761","2018-10-01","2023-09-30"
"INTELLICORR","Intelligent corrosion management underpinned by advanced engineering science","Anne NEVILLE","UNIVERSITY OF LEEDS","Our planet’s population will continue to grow rapidly; between 2010 and 2025 the population will grow by 1.1bn. Urbanisation and growth of the consumer class in developing countries will lead to unprecedented demands on energy. There is arguably no bigger challenge to society than ensuring the security of affordable and environmentally-sustainable energy.  
Hydrocarbons will provide a large proportion of the world’s energy for the foreseeable future.  There is no escape from the critically low oil price worldwide. Innovation becomes important in this price environment.  “Easy” oil has already been found; future supply will come from complex reservoirs requiring enhanced oil recovery (EOR).  There is a massive growth in renewables technology; the EU is making steady progress towards its 2020 target. The EU renewables energy share increased from 8% to 15% in the decade to 2013.  Energy supply and consumption brings with it the global issue of climate change as emissions from industry and transport increase.  Inextricably linked to energy is the reduction of the global carbon footprint and Carbon Capture and Storage (CCS) offers the only real technology that can handle the already produced carbon dioxide.
Corrosion in energy and environmental control linking to energy supply provide the underpinning rationale for this proposal. Corrosion is one of the major life-limiting factors for energy supply (oil and gas, renewables, EOR) and in environmental pollution control (CCS) and is estimated to cost 3% GDP.  This proposal brings some of the most exciting experimental and modelling engineering science to create a framework for the intelligent management of corrosion. INTELLICORR will use synchrotron techniques, advanced microscopy, numerical methods and environmental/cost analysis to bring about unprecedented advances in (a) prediction and management of localised pitting corrosion and (b) novel methods for green corrosion protection using the natural corrosion product layer","2271830","2017-07-01","2022-06-30"
"INTENT","Structured Reactors with INTensified ENergy Transfer for Breakthrough Catalytic Technologies","Enrico TRONCONI","POLITECNICO DI MILANO","Critically important heterogeneous catalytic reactions for energy conversion and chemicals production have been run for several decades in fixed bed reactors randomly packed with catalyst pellets, whose operation is intrinsically limited by slow heat removal/supply. There is urgent need for a new generation of process equipment and chemical reactors to address the current quest for process intensification. I propose that a game-changing alternative is provided by structured reactors wherein the catalyst is washcoated onto or packed into structured substrates, like honeycomb monoliths, open-cell foams or other cellular materials, fabricated with highly conductive metallic (Al, Cu) materials. The goal of this project is to fully elucidate fundamental and engineering properties of such novel conductive structured catalysts, investigate new concepts for their design, manufacturing, catalytic activation and operation (e.g. 3D printing, packed foams, energy supply by solar irradiation), and demonstrate their potential for a quantum leap in the intensification of three crucial catalytic processes for the production of energy vectors: i) distributed H2 generation via efficient small-size reformers; ii) conversion of syngas to clean synthetic fuels in compact (e.g. skid-mounted) reactors; iii) production of solar H2. To this purpose I will combine advanced CFD modelling with lab-scale experimentation in order to identify the optimal structure-performance relation of existing and novel substrates, use such new knowledge to design optimized prototypes, apply unconventional additive manufacturing technologies for their production, and construct a semi-industrial tubular pilot reactor to test them at a representative scale. The project results will enable novel reactor designs based on tuning geometry, materials and configurations of the conductive internals to match the activity - selectivity demands of specific process applications, while impacting also other research areas.","2484648","2016-11-01","2021-10-31"
"INTERCOCOS","Interdisciplinary research: Connecting complex plasmas with colloidal dispersions","Gregor Eugen Morfill","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","This proposal represents the first concerted effort to study classical strongly coupled systems at the most fundamental individual-particle level, by using complementary approaches from different physics domains - complex plasmas and colloidal dispersions. These are complementary in many ways, the most important being that complex plasmas are virtually undamped at the particle timescales, whereas colloidal dispersions are overdamped and thus can be brought into equilibrium in a controlled way. Otherwise, both fields have similar advantages: Fully resolved 3D particle trajectories can easily be visualized, the pair interactions are tunable, and particles can be manipulated individually or collectively.
The principal scientific aim is to study generic dynamical and self-organization processes at a detail not possible in the past. Scientific objectives include 1) particle dynamics of liquids, with the emphasis on mesoscopic processes in the supercooled state, e.g. dynamical heterogeneity, 2) phase transitions in solids, with particular attention on the evolutionary paths of crystal structure development and defect dynamics, 3) non-equilibrium phase transitions, with the focus on lane formation in driven binary systems and the dynamics at “atomistic” timescales, 4) phase separation in binary fluids, with the emphasis on the role of pair interactions in the demixing kinetics, as well as in the transition between spinodal decomposition and nucleation and growth regimes, 5) hydrodynamics at the discreteness limit, especially the onset and development of hydrodynamic instabilities at the particle scale, and 6) critical phenomena in particle systems with “designed” pair interactions, where the range and strength of attractive/repulsive parts can be tuned by external fields.","2119800","2011-06-01","2016-05-31"
"INTERCOM","The Influence of Interfaces, Confinement and Compartmentalization on Chemical Reactions","Wilhelm Huck","STICHTING KATHOLIEKE UNIVERSITEIT","Water is essential for life on our planet and is the solvent of choice for Nature to carry out her syntheses. In contrast, our methods of making complex organic molecules have taken us far away from the watery milieu of biosynthesis. Indeed, it is fair to say that most organic reactions commonly used both in academic laboratories and in industry fail in the presence of water or oxygen. At the same time of course, chemical reactors are very different from the cellular environment where Nature s synthesis is carried out. This research proposal aims to incorporate some of the key characteristic of cellular reactors, i.e. confinement, compartmentalization and interfaces, into model droplet-based reactors. The envisioned reactors will comprise of monodisperse aqueous droplets in oil carrier phases with volumes ranging from pL to nL, produced in microfluidics devices or in tubing, in very large numbers. These droplets will have precisely determined interfacial areas, which can be used for the study of so-called on water reactions, a new area of synthetic chemistry rapidly gaining in interest. Furthermore, the interfaces can be functionalized with catalytically active surfactants and by confining the droplets into ever decreasing volumes, the effect of nanoconfinement on enzymatic and other reactions can be studied. Finally, individual droplets provide a completely compartmentalized environment, suitable for the study of single enzymes in a crowded environment, but also for systematic studies into communication between compartmentalized, mutually incompatible, reaction systems. This proposal presents a radically new approach to increasing our understanding of chemical reactions in confined spaces and at interfaces and provides a technological platform for the creation of chemically linked networks with emerging complexity.","2147726","2010-05-01","2015-04-30"
"Interface","Quantum Optical Interfaces for Atoms and Nano-electro-mechanical Systems","Eugene Polzik","KOBENHAVNS UNIVERSITET","Quantum interfaces capable of transferring quantum states and generating entanglement between fields and matter are set to play a growing role in the development of science and technology. Development of such interfaces has been a crucial component in quantum information processing and communication. In the past decade quantum interfaces between atoms and optical photons have been extensively explored by a number of leading groups. Quantum state transfer between light and atoms, such as quantum memory and quantum teleportation, entanglement of massive objects, as well as measurements and sensing beyond standard quantum limits have been demonstrated by the group of the PI.

We propose to develop a robust, integrated and scalable atom-light interface and to incorporate it into a hybrid multi-facet quantum network with other relevant quantum systems, such as nano-mechanical oscillators and electronic circuits.

Towards this ambitious goal we will develop room temperature atomic quantum memories in spin protecting micro-cells (mu-cells) and opto-mechanical and electromechanical strongly coupled systems. Interfacing atoms, electronic circuits and nano-mechanical oscillators we will perform ultrasensitive quantum limited field and force measurements and quantum teleportation of states across the range of these systems.

In the fundamental sense, this research program will further broaden the horizons of quantum physics and quantum information processing by expanding it into new and unexplored macroscopic domains.","2493000","2012-07-01","2017-06-30"
"Interfaces","Manipulating Acoustic wavefronts using metamaterials for novel user interfaces","Sriram SUBRAMANIAN","THE UNIVERSITY OF SUSSEX","In this project we will leverage developments in acoustic meta-materials to build interactive systems that manipulate sound to create experiences with the same ease and fidelity as we are so accustomed to doing with light. This involves designing and evaluating new acoustic meta-materials AND building interactive systems that create novel interaction experiences that were hitherto impossible to achieve.

We will use acoustic metamaterials technology to build a Spatial Sound Modulator (SSM) that aims to be a software controlled device that transforms an input acoustic wave into a time-variable, user-defined acoustic field. SSM comprises of a surface made of electronically adjustable acoustic metamaterial bricks. Each brick in the surface can individually vary the phase of an incident acoustic field, to shape the complex output field. 

Our objectives are: 
1.	Design, implement and evaluate dynamically reconfigurable metamaterial unit-cells and surfaces using transmissive modes of operation. We will explore narrow-band devices for air-borne operation at low ultrasonic frequencies (e.g. 40 kHz).  
2.	Design SSMs from a spatial distribution of metamaterial unit cells. Specifically, we will identify discretization strategies, digital control mechanisms and develop concepts that are efficient and reduce field reconstruction errors while at the same time constructing the SSM from a small set of reconfigurable metamaterial unit-cells. 
3. 	Create multiple application-specific prototypes of the SUM and identify context specific design constraints and trade-offs.","2457718","2018-05-01","2023-04-30"
"INTERSOLAR","Rectifying interfaces for solar driven fuel synthesis","James Robert Durrant","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","There is rapidly growing interest in the science required to enable the conversion of solar energy into molecular fuels, motivated both by the need to develop a renewable, globally scalable transportation fuel strategy and the need to address the intermittency limitations of solar electrical power generation. Rapid progress is being made in the fabrication of inorganic, low cost, nanostructured photoelectrodes which utilise visible irradiation for such fuel syntheses, including water photolysis and CO2 photoreduction. However the efficiency of low cost photoelectrodes remains modest, due significantly to electron / hole recombination in the photoelectrode competing effectively with interfacial photochemistry. I propose to address this limitation by the use of multilayer interfaces designed to achieve enhanced uni-directional (i.e.: rectifying) charge separation, building directly from the extensive lessons I have learnt from my studies addressing an analogous challenge in dye sensitized solar cells. A key focus will be on the functionalisation of photoelectrodes with molecular and/or inorganic multi-electron catalysts to enhance the specificity and efficiency of the photoelectrode for fuel synthesis, exploiting recent, rapid advances in the syntheses of such catalysts. The use of rectifying interfaces is essential for the encorporation of such catalysts onto photoelectrodes, enabling the accumulation of multiple oxidations on the catalyst without this accumulation resulting in enhanced recombination losses. The proposal will undertake the assembly of such multilayer photoelectrodes, utlilising state of the art photoelectrode and catalyst materials, and the functional characterisation of these photoelectrodes, including measurement of interfacial electron transfer dynamics, with the aim of developing materials design rules which will enable systematic optimisation of photoelectrode function for efficient solar driven fuel synthesis.","1800000","2012-04-01","2018-03-31"
"INTERSTELLAR","The Interstellar Medium of High Redshift Galaxies","Andrea FERRARA","SCUOLA NORMALE SUPERIORE","When and how did the first galaxies form across cosmic history? Were they different from present-day ones? This is only a small subset of key cosmological questions that the combination of deep galaxy observations, theoretical modeling, and powerful simulations envisaged here will allow us to answer for the first time. 

Deep galaxy surveys have provided a first valuable characterization of early galaxies in the Epoch of Reionization (redshift z > 6), mostly in terms of their stellar content. However, almost nothing is known about their internal structure and Interstellar Medium (ISM). This is in striking contrast with galaxies at z < 2, for which ISM observations have enabled a much more complete physical description. Hence, a substantial progress in the study of early galaxies must be based on techniques able to probe their ISM. Conversely, ISM studies will help completing the “stellar” picture.  

Interstellar will bridge this gap. Its main aim is to understand the internal structure and interstellar medium of galaxies in the Epoch of Reionization by performing theoretical modeling and high fidelity simulations. By post-processing the simulations and calibrating them with local analogs, we will produce mock images/spectra used to (i) interpret available high-redshift observations, and (ii) plan breakthrough experiments with ALMA, JWST and E-ELT.   

The advent of ALMA, JWST, E-ELT and advances in computational cosmology make the study of high-z ISM one of the most promising areas of development in cosmology. 

The aim will be achieved through 5 objectives distributed among 3 Work Packages (WPs). WP1 is concerned with theoretical work, a preparatory phase for the cosmological simulations performed in WP2. WP2 represents the production phase of the project and will deliver cutting-edge zoom simulations of a sample of high-z galaxies and their ISM. Finally, WP3 is concerned with the exploitation of the numerical results and their integration with observations.","2151875","2017-10-01","2022-09-30"
"INTEXseas","An integrated weather-system perspective on the characteristics, dynamics and impacts of extreme seasons","Johann Heinrich WERNLI","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Single extreme weather events can be hazardous, but for certain socioeconomic sectors the seasonal aggregation of weather is particularly harmful. Extremes on timescales up to two weeks are typically related to specific weather systems, but no such link exists for extreme seasons. Therefore, they are very difficult to meteorologically understand, despite their utmost societal relevance. This project aims at filling this gap, providing a multi-faceted analysis of different types of extreme seasons in a changing climate. Very large ensembles of climate simulations serve to investigate the characteristics and dynamics of the, e.g., hottest and coldest, and wettest and driest, season in regions worldwide. The extreme season characteristics include their spatial scale and their extremeness given the entire distribution of seasonal values in this region. Their dynamics is related to the fundamental understanding of the sequence of weather events that makes a season extreme: is it a single, highly unusual weather event that renders a season the most extreme (e.g., an unprecedented heat wave) or rather an unusual frequency of well-known weather systems (e.g., a series of strongly precipitating cyclones). These paradigms, referred to as “something new” vs. “more of the same”, are particularly relevant when considering extreme seasons in a warming climate. This project will combine state-of-the-art climate modelling, a unique set of weather-system diagnostics informed by profound dynamical understanding, and novel impact assessment pathways to address three main hypotheses: 1) different types of extreme seasons differ in terms of their spatial scale and relation to weather systems; 2) for specific types of extreme seasons, future climate simulations indicate a marked increase of extremeness; and 3) for certain socioeconomic sectors, the consequences of the future modulation of extreme seasons is more severe than inferred from climate change trend considerations alone.","2500000","2018-11-01","2023-10-31"
"INTICE","Pathways to Intrinsically Icephobic Surfaces","Dimosthenis Poulikakos","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Icing of surfaces is common in nature and technology, affecting everyday life and often causing catastrophic events. Despite progress in recent years in the area of hydrophobicity, engineered surfaces that can be employed in applications based on their intrinsic icephobicity, going beyond classical additional chemical coatings or heating treatments, are not a reality. Understanding and counteracting surface icing brings with it significant scientific challenges, which form an intersection of nucleation thermodynamics, interfacial thermofluidics and surface nanoengineering/science. This project will investigate important mesoscale phenomena (a term used here to summarily describe phenomena manifesting themselves in the spatial range from the order of a nanometer to a hundred microns), which affect the behavior of water on surfaces with respect to icing. With the resulting, unifying knowledge base, the aim is to identify pathways for the design and fabrication of a new class of intrinsically icephobic surfaces, based on their a-priori engineered composition and texture. Our aim is to identify anti-nucleation and anti-wetting phenomena, leading to surfaces having long ice nucleation time scales, low water contact and retention, and low ice adhesion. The effects of surface texture curvature on ice nucleation, local liquid confinement on freezing point depression, and mesoscale texture features on interfacial thermofluidics, have intertwined and sometimes counteracting impacts on surface icing behavior, which we aim at unraveling, to determine pathways to high performance surfaces. Connected to all this is the employment of advanced surface texture fabrication and methods to perform the necessary experiments, lending consideration to the development of such surfaces for future applications. Beyond icephobicity, this research has clear implications to the characterization of mesoscale phase change phenomena, for multiphase heat and mass transfer processes and devices.","2498043","2015-11-01","2020-10-31"
"INTIF","Inorganic nanotubes and fullerene-like materials: new synthetic strategies lead to new materials","Reshef Tenne","WEIZMANN INSTITUTE OF SCIENCE LTD","Inorganic nanotubes (INT) and particularly inorganic fullerene-like materials (IF) from 2-D layered compounds, which were discovered in the PI laboratory 16 years ago, are now in commercial use as solid lubricants (www.apnano.com) with prospects for numerous applications, also as part of nanocomposites, optical coatings, etc. The present research proposal capitalizes on the leadership role of the PI and recent developments in his laboratory, much of them not yet published. New synthetic approaches will be developed, in particular using the WS2 nanotubes as a template for the growth of new nanotubes. This include, for example PbI2@WS2 or WS2@NbSe2 core-shell nanotubes, which could not be hitherto synthesized. Other physical synthetic approaches like ablation with solar-light, or pulsed laser ablation will be used as well. Nanooctahedra of MoS2 (NbS2), which are probably the smallest IF (hollow cage) structures, will be synthesized, isolated and studied. Extensive ab-initio calculations will be used to predict the structure and properties of the new INT and IF nanoparticles. Cs-corrected transmission electron microscopy will be used to characterize the nanoparticles. In particular, atomic resolution bright field electron tomography will be developed during this study and applied to the characterization of the INT and IF nanoparticles. The optical, electrical and mechanical properties of the newly sythesized INT and IF materials will be investigated in great detail. Devices based on individual nanotubes will be (nano)fabricated and studied for variety of applications, including mechanical and gas sensors, radiation detectors, etc. Low temperature measurements of the transport properties of individual INT and IF will be performed.","1618238","2008-12-01","2014-02-28"
"IntRanSt","Integrable Random Structures","Neil O'Connell","UNIVERSITY COLLEGE DUBLIN, NATIONAL UNIVERSITY OF IRELAND, DUBLIN","The last few years have seen significant advances in the discovery and development of integrable models in probability, especially in the context of random polymers and the Kardar-Parisi-Zhang (KPZ) equation. Among these are the semi-discrete (O'Connell-Yor) and log-gamma (Seppalainen) random polymer models. Both of these models can be understood via a remarkable connection between the geometric RSK correspondence (a geometric lifting, or de-tropicalization, of the classical RSK correspondence) and the quantum Toda lattice, the eigenfunctions of which are known as Whittaker functions.  This connection was discovered by the PI and further developed in collaboration with Corwin, Seppalainen and Zygouras.  In particular, we have recently introduced a powerful combinatorial framework which underpins this connection.  I have also explored this connection from an integrable systems point of view, revealing a very precise relation between classical, quantum and stochastic integrability in the context of the Toda lattice and some other integrable systems.  The main objectives of this proposal are (1) to further develop the combinatorial framework in several directions which, in particular, will yield a wider family of integrable models, (2) to clarify and extend the relation between classical, quantum and stochastic integrability to a wider setting, and (3) to study thermodynamic and KPZ scaling limits of Whittaker functions (and associated measures) and their applications.  The proposed research, which lies at the interface of probability, integrable systems, random matrices, statistical physics, automorphic forms, algebraic combinatorics and representation theory, will make novel contributions in all of these areas.","1579299","2015-10-01","2021-09-30"
"INVARIANTCLASS","Invariant Representations for High-Dimensional Signal Classifications","Stéphane Mallat","ECOLE NORMALE SUPERIEURE","Considerable amounts of high-dimensional signals are continuously being acquired, whether audio, images, videos, or specialized signals for example in geophysics or medicine. Automatic classification and retrieval is strongly needed to analyze and access these massive data sets, but current algorithms often produce too many errors. For high-dimensional signals, supervised classification algorithms are typically applied to reduced ``feature vectors''. These feature representations are specialized for each signal modality, for example speech, music, images, videos or seismic signals.  This proposal aims at unifying these approaches to improve classification performances, by developing a general mathematical and algorithmic framework to optimize representations for classification. Classification errors result from representations which are not sufficiently informative or which maintain too much variability. The central challenge is to understand how to construct stable, informative invariants, while facing progressively more complex sources of variability. The first task concentrates on invariants to the action of finite groups including translations, rotations and scalings, while preserving stability to deformations. The second task addresses unsupervised representation learning from training data. The third task explores stable representations of invariant geometric signal structures, which is an outstanding problem.These challenges involve building new mathematical tools in harmonic and wavelet analysis, geometry and statistics, in close interaction with numerical algorithms. Classification applications to audio, images, video signals or geophysical signals are expected to serve as a basis for groundbreaking technological advances.","2316000","2013-03-01","2019-02-28"
"INVISIBLE","Advanced Amorphous Multicomponent Oxides for Transparent Electronics","Elvira Fortunato","FACULDADE DE CIENCIAS E TECNOLOGIADA UNIVERSIDADE NOVA DE LISBOA","Imagine having a fully transparent and flexible, foldable, low cost, displays or at the glass window of your home/office, a transparent electronic circuit, do you believe on that? Maybe you are asking me if I am writing science fiction. No I am not. In fact this is a very ambitious objective but is tangible in the framework of this project due to the already acquired experience in the development of transparent thin film transistors using novel multifunctional and multicomponent oxides that can behave as active or passive semiconductor materials. This is an interdisciplinary research project aiming to develop a new class of transparent electronic components, based on multicomponent passive and active oxide semiconductors (n and p-types), to fabricate the novel generation of full transparent electronic devices and circuits, either using rigid or flexible substrates. The emphasis will be put on developing thin film transistors (n and p-TFTs) and integrated circuits for a broad range of applications (from inverters, C-MOS like devices, ring oscillators, CCDs backplanes for active matrices, biossensor arrays for DNA/RNA/proteins detection), boosting to its maximum their electronic performances for next generation of invisible circuits. By doing so, we are contributing for generating a free real state electronics that is able to add new electronic functionalities onto surfaces, which currently are not used in this manner and that silicon cannot contribute. The multicomponent metal oxide materials to be developed will exhibit (mainly) an amorphous or a nanocomposite structure and will be processed by PVD techniques like rf magnetron sputtering at room temperature, compatible with the use of low cost and flexible substrates (polymers, cellulose paper, among others). These will facilitate a migration away from tradition  silicon like  fab based batch processing to large area, roll to roll manufacturing technology which will offer significant advantages","2250000","2009-01-01","2014-12-31"
"INVPROB","Inverse Problems","Lassi Juhani Päivärinta","TALLINNA TEHNIKAULIKOOL","Inverse problems constitute an interdisciplinary field of science concentrating on the mathematical theory and practical interpretation of indirect measurements. Their applications include medical imaging, atmospheric remote sensing, industrial process monitoring, and astronomical imaging. The common feature is extreme sensitivity to measurement noise. Computerized tomography, MRI, and exploration of the interior of earth by using earthquake data are typical inverse problems where mathematics has played an important role. By using the methods of inverse problems it is possible to bring modern mathematics to a vast number of applied fields. Genuine scientific innovations that are found in mathematical research, say in geometry, stochastics, or analysis, can be brought to real life applications through modelling. The solutions are often found by combining recent theoretical and computational advances. The study of inverse problems is one of the most active and fastest growing areas of modern applied mathematics, and the most interdisciplinary field of mathematics or even science in general.
The exciting but high risk problems in the research plan of the PI include mathematics of invisibility cloaking, invisible patterns, practical algorithms for imaging, and random quantum systems. Progress in these problems could have a considerable impact in applications such as construction of metamaterials for invisible optic fibre cables, scopes for MRI devices, and early screening for breast cancer. The progress here necessitates international collaboration. This will be realized in upcoming programs on inverse problems. The PI is involved in organizing semester programs in inverse problems at MSRI in 2010, Isaac Newton Institute in 2011, and Mittag-Leffler -institute in 2012.","1800000","2011-03-01","2016-02-29"
"IONACES","Understanding ion transport in nanoporous carbons; application to energy storage and sustainable development","Patrice Simon","UNIVERSITE PAUL SABATIER TOULOUSE III","Electrochemical Double-Layer Capacitors Electrochemical Capacitors (EDLC) are promising devices for clean energy storage applications. In EDLCs, the charges are stored electrostatically at the electrolyte / electrode interface, which confers them high power and cycling capabilities. Until recently, it was believed that charge storage in porous carbon EDLC electrodes could be achieved only if the pore size of the carbon was larger than the electrolyte ions with their solvation shells. Using Carbides Derived Carbons (CDCs) which have controlled pore sizes between 0.6 nm and 1.1 nm, we recently demonstrated that high capacitive performances could be obtained when the pore size is smaller than the solvated ion size.
The origin of this capacitance increase is still unclear despite important modelling efforts achieved by many research groups. Using our fine-tuned, controlled pore size CDCs carbons with narrow pore size distribution, we propose here an integrated approach combining the use of experimental electrochemical methods (EQCM, EIS, CV…) and in-situ analytical techniques (NMR, XRD), to computational modelling (Molecular Dynamics, Monte Carlo and Reverse Monte Carlo methods) to elucidate the ion transport and adsorption mechanisms inside nanopores.
A direct application of this fundamental approach concerns the energy storage with supercapacitors. Thanks to the unique features offered by the CDCs, we propose to develop the next generation of high-energy density micro-supercapacitors from bulk CDC films.
The evidence of the increase of the capacitive ion adsorption associated with ion partial desolvation in micropores is also of great interest in different areas such as water desalination. CDCs, which have demonstrated volumetric capacitance improvement of 100% compared to activated carbon for supercapacitor application, are appealing materials for water desalination applications, which will be the last part of the project.","1494050","2012-04-01","2017-03-31"
"iPLASMM","Frontiers in nanophotonics: integrated plasmonic metamaterials devices","Anatoly Zayats","KING'S COLLEGE LONDON","Photonic methamaterials have unique optical properties not available in natural materials. The key question is how to integrate metamaterials within nanophotonics circuitry to harness all the advantages they offer in controlling light on the nanoscale. iPLANET will develop a plasmonic nanorod metamaterial platform for applications covering the entire spectral range from mid-IR through telecom to visible and UV, be CMOS compatible and monolithically integratable in photonic circuitry. Using the unique optical properties of nanorod-based metamaterials, a very high density of photonic states can be achieved, essential for controlling light emission, scattering and nonlinearity in the nanophotonic environment. The project will challenge the frontiers of nanophotonics through the use of these specific to metamaterial properties to achieve integrated nonlinear photonic components with reduced size and energy consumption and integrated bio- and chemical sensors with increased sensitivity, multi-parameter sensing in a broad spectral range, all on the same metamaterial platform. This will be a transformative development for the applications of nanophotonics in optical information processing in integrated photonic circuits and for the realization of integrated sensors for point-of-care devices, security and environmental monitoring.  The success of the project will unlock the potential of metamaterials for the improvement of the real-world photonic devices and provide insight into physical phenomena which are vital for various areas of optical physics and sensing. This will probably be the first demonstration of commercially-viable application of metamaterials. It will transform the areas of both nanophotonics and metamaterials and consolidate and enhance the European leadership in this field.","2257459","2013-04-01","2018-03-31"
"IRMIDYN","Iron mineral dynamics in redox-affected soils and sediments: Pushing the frontier toward in-situ studies","Ruben KRETZSCHMAR","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","IRMIDYN will study the dynamics of redox-driven iron mineral transformation processes in soils and sediments and impacts on nutrient and trace element behavior using a novel approach based on enriched stable isotopes (e.g., 57Fe, 33S, 67Zn, 113Cd, 198Hg) in combination with innovative experiments and cutting-edge analytical techniques, most importantly 57Fe Mössbauer and Raman micro-spectroscopy and imaging. The thermodynamic stability and occurrence of iron minerals in sufficiently stable Earth surface environments is fairly well understood and supported by field observations. However, the kinetics of iron mineral recrystallization and transformation processes under rapidly changing redox conditions is far less understood, and has to date mostly been studied in in mixed reactors with pure minerals or sediment slurries, but rarely in-situ in complex soils and sediments. Thus, we do not know if and how fast certain iron mineral recrystallization and transformation processes observed in the laboratory actually occur in soils and sediments, and which environmental factors control the transformation rates and products. Redox-driven iron mineral recrystallization and transformation processes are key to understanding the biogeochemical cycles of C, N, P, S, and many trace elements (e.g., As, Zn, Cd, Hg, U). In face of current global challenges caused by massive anthropogenic changes in biogeochemical cycles of nutrients and toxic elements, it is paramount that we begin to understand and quantify the dynamics of these processes in-situ and learn how we can apply our mechanistic (but often reductionist) knowledge to the natural environment. This project will take a large step toward a better understanding of iron mineral dynamics in redox-affected Earth surface environments, with wide implications in biogeochemistry and other fields including environmental engineering, corrosion sciences, archaeology and cultural heritage sciences, and planetary sciences.","3154658","2018-11-01","2023-10-31"
"IRQUAT","Information and Randomness in Quantum Theory","Andreas Winter","UNIVERSITAT AUTONOMA DE BARCELONA","Quantum information science is one of the most dynamic and exciting areas of science today, its significance ranging from the ultimate physical limits of information processing, to fundamental issues of quantum mechanics, to new mathematics and prospects of realising novel, enhanced computation and communication
technologies.
Drawing on ideas from physics, mathematics and computer science, one of the core programmes of this highly interdisciplinary field is to understand and harness quantum mechanics in terms of information theory.
Past steps in this programme include Shor's quantum factoring algorithm, quantum cryptography, and the creation of quantum Shannon and entanglement theory. Dramatic recent developments, while opening up exciting new avenues, at the same time pose major challenges: the proof that quantum capacities can be non-additive showed that the landscape of quantum Shannon theory is actually much richer than previously envisaged; the information theory of generalized non-locality pointed out a completely new take on the foundations of quantum mechanics; and mathematical tools such as the probabilistic method have had a major impact across the board from entanglement theory to quantum Shannon theory and statistical mechanics.
In the proposed research I will address key current open questions in quantum Shannon theory, including long-standing coding problems, and a deeper understanding of the pervasive non-additivity of quantum capacities; develop probabilistic tools directly related to quantum information; find new entropy inequalities and entropic uncertainty relations; characterise entanglement and non-local correlations in information theoretic terms; and realise an ambitious programme of founding statistical mechanics on generic quantum dynamics.","1440119","2011-05-01","2017-04-30"
"ISONEB","Isotopic records of solar nebula evolution and controls on planetary compositions","Timothy Elliott","UNIVERSITY OF BRISTOL","This project has three linked strands that will combine to constrain the birth environment of the solar system and the nebular processes that shape bulk planetary compositions.   Firstly, I will use ultra-high precision isotope ratio measurements in bulk meteorites to determine the stellar origin of the pre-solar of material that controls the gross compositional differences between planetary bodies. Secondly I will identify the mineralogical hosts of this isotopic variability, using in situ laser ablation analyses with a unique collision-cell multi-collector inductively coupled plasma mass-spectrometer, developed in close collaboration with an industrial partner (Thermo Fisher) as part of the project.  Thirdly, I will establish a chronology for the mixing of the pre-solar material within the nebula, by dating individual meteorite components (chondrules) using ‘absolute’ Pb and relative 26Al-26Mg approaches and analysing the same aliquots for their mass-independent isotopic compositions.  These observations will be quantitatively interpreted using novel numerical models of particle dynamics in the protoplanetary disk, in collaboration with Fred Cieala.  This is an ambitious project that builds on the analytical prowess of the laboratory I have developed at Bristol and couples this with challenging technical developments and inter-disciplinary, modelling calculations.  This work will radically improve our understanding of the history of the early solar system and the fundamental processes that shape its evolution.","3425271","2013-07-01","2019-06-30"
"ITHACA","An Information Theoretic Approach to Improving the Reliability of Weather and Climate Simulations","Timothy PALMER","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The aim of this project is to develop a new synergy between climate and computer science to increase the accuracy and hence reliability of comprehensive weather and climate models. The scientific basis for this project lies in the PI’s pioneering research on stochastic sub-grid parametrisations for climate models. These parametrisations provide estimates of irreducible uncertainty in weather and climate models, and will be used to determine where numerical precision for model variables can be reduced without degradation. By identifying those bits that carry negligible information – typically in high-wavenumber components of the dynamical core and within parametrisation and Earth-System modules – computational resources can be reinvested into areas (resolution, process representation, ensemble size) where they are sorely needed. This project will determine scale-dependent estimates of information content as rigorously as possible based on a variety of new tools, which include information-theoretic diagnostics and emulators of imprecision, and in a variety of models, from idealised to comprehensive. The project will contribute significantly to the development of next-generation weather and climate models and is well timed for the advent of exascale supercomputing where energy efficiency is paramount and where movement of bits, being the single biggest determinant of power consumption, must be minimised. The ideas will be tested on emerging hardware capable of exploiting the benefits of mixed-precision arithmetic. A testable scientific hypothesis is presented: a proposed increase in forecast reliability arising from an increase in the forecast model’s vertical resolution, the cost being paid for by a reduction in precision of small-scale variables. This project can be expected to provide new scientific understanding of how different scales interact in the nonlinear climate system, for example in maintaining persistent atmospheric flow regimes.","2494117","2017-10-01","2022-09-30"
"Jellyclock","Light Actuated Self-Pulsing Mircogels","Martin MOELLER","DWI LEIBNIZ-INSTITUT FUR INTERAKTIVE MATERIALIEN EV","Living organisms teach us how to design material structures that can move autonomously. Such motility is not restricted to animated organisms but can also originate from local differences t expansion coefficients in ligneous compounds. This challenges the design of micro-objects that can perform mechanical work and undergo locomotion. Irrespective of the specific material, three fundamental tasks must be solved: (i) to fuel the material for the actuation; (ii) to control the morphing of the object in time and space; and (iii) to establish a feed-back mechanism that enables timing of a sequence of steps. The later refers to an integrated clock function in order to pulse the energy input for distinct mechanical strokes. 
Within JELLYCLOCK, we address all three questions at the example of light driven hydrogel micro-objects. We have developed light sensitive microgels that change their shape within milliseconds. IR-irradiation of gold nanorods, entrapped in a thermosensitive hydrogel, is used to heat the gel from inside and enable a gradated spatial and temporal control of its swelling and shrinking. The water-based actuation will be directed to generate a non-reciprocal deformation as required for locomotion at low Reynolds numbers. So far, a directed cyclic deformation action relies on the outside modulation of the irradiation. We will extent this concept by introducing self-oscillating absorption efficiency, so that a stepwise body deformation becomes feasible under continuous irradiation. The project comprises (1) the advanced design of hydrogel based actuators driven by modulated light, (2) achievement of a precise control of the deformation in time and space , and as the actual disruptive step, (3) the realization of a self-sustaining pulsation under continuous near IR irradiation. 
Such soft micro engines strike a new path to micro-robotics for biomedical or biomechanical applications, or to create micro devices that could mix, sort and circulate fluid.","2280000","2016-08-01","2020-07-31"
"KETENCYCLS","Biomimetic Late Stage Aromatisation Reactions: from Cancer Chemotherapy to Novel Polymers","Anthony Gerard Martin Barrett","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","The applicant is seeking funding to support research on late stage biomimetic syntheses of resorcylates from readily available non-aromatic precursors. A series of diketo-dioxinones will be synthesised using mild Claisen condensation reactions and converted into the reactive intermediates triacyl-ketenes by retro-Diels Alder reactions. These will be trapped with alcohols to directly provide the corresponding resorcylate esters and related macrocyclic lactones. These compounds are of considerable importance in that they occur in structurally complex natural products, which are active as anticancer agents, antibiotics, immunosuppressants or analgesics. Examples include aigialomycin D, radicicol, cruentaren A and mycophenolic acid. Late aromatisation avoids the severe problems associated with existing syntheses based on aromatic precursors. Our studies will allow for the development of very concise total syntheses of the biologically potent Hsp90 inhibitor radicicol and mitochondrial F-ATPase inhibitor cruentaren A. The methods will be amenable for the synthesis of libraries of synthetic natural product analogues in the quest for superior drugs to treat cancer. The novel ring opening, ring closing and crossed metathesis of cyclooctyne derivatives will be introduced and will simplify the route to cruentaren A. Resorcylate methodology will be extended to terpenoid-resorcylates, using a new decarboxylative allyl transfer process, which will greatly simplify routes to angelicoin A, cristatic acid, mycophenolic acid and hongoquercin B. Finally, both the aromatisation strategy and ring opening metathesis polymerisation of cyclooctyne derivatives will be applied in the synthesis of novel oligomers and polymers including polyfunctional polyesters and poly-ynes.","1960938","2011-03-01","2016-02-29"
"KL2MG-interactions","K-theory, L^2-invariants, manifolds, groups and their interactions","Wolfgang Lueck","RHEINISCHE FRIEDRICH-WILHELMS-UNIVERSITAT BONN","Many  milestone results in mathematics emerge from interactions and transfer of techniques and methods between different areas. I want to attack outstanding problems concerning K-theory, L^2-invariants, manifolds and group theory. The time is ripe to use the exciting and profound progress that has been made during the last years in the individual areas to build new bridges, gain new insights, open the door to new applications, and to trigger new innovative activities worldwide lasting beyond the proposed funding period.

The starting point are the prominent  conjectures of Farrell-Jones on the algebraic K- and L-theory of group rings, of  Baum-Connes on the topological K-theory of reduced group C^*-algebras, and of Atiyah on the integrality of L^-Betti numbers.

I intend to analyze and establish the Farrell-Jones Conjecture in other settings such as topological cyclic homology of ""group rings"" over the sphere spectrum, algebraic K-theory of Hecke algebras of totally disconnected groups, the topological K-theory of Fr'echet group algebras, and Waldhausen's A-theory of classifying spaces of groups.  This has new and far-reaching consequences for automorphism groups of closed aspherical manifolds, the structure of group rings,  and representation theory.  Recent proofs by the PI  of the Farrell-Jones Conjecture for certain classes of groups require input from homotopy theory, geometric group theory, controlled topology and flows on metric spaces, and will be transferred to the new situations. There is also a program towards a proof of the Atiyah Conjecture based on the Farrell-Jones Conjecture and ring theory. Furthermore, I want to attack open problems such as  the approximation of L^2-torsion for towers of finite coverings, and the relation of the first L^2-Betti number, the cost and the rank gradient of a finitely generated  group. I see a high potential for new striking applications of the Farrell-Jones Conjecture and L^2-techniques to manifolds and groups.","1719583","2015-11-01","2020-10-31"
"LASER-IMS","Coupling laser spectroscopy to mass spectrometry and ion mobility: from fundamentals to analytical sciences","Philippe Dugourd","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""The motivations for studying the gaseous structures of peptides and proteins range from developing efficient methodologies for protein identification and quantification to fundamental understanding of folding processes and biomolecular architectures. Mass spectrometry allows analyzing biomolecular ions in a variety of different forms, for example, as a function of charge state, bound to their targets or in multimers following aggregation. Ion mobility-mass spectrometry is a revolutionary technique regarding the separation of complex mixtures and structural isomers and can now play an important role in the study of protein shapes and their complexes. In parallel, optical spectroscopy is also emerging as a key element in gas phase methods available for investigating biomolecular ions. In this proposal, we want to combine spectroscopic approaches with high resolution mass spectrometry and ion mobility to widen and deepen the scope of mass spectrometry in its various fields of applications including structural biology, protein identification and quantification. The general idea behind our approach is to simultaneously obtain “orthogonal” information on gas phase molecular conformation from ion mobility and optical spectroscopy, with in addition the possibility of using light to induce conformational changes or the formation of new species. A goal of this cross-disciplinary proposal is to study the structural reorganization of proteins occurring in relation to aggregation, radical formation and oxidation, focusing on misfolding and the earliest stages of tau protein assembly. Moreover, fundamental understanding of structure and reactivity of peptides and proteins will serve as a lever for the development of applications in analytical sciences using photon based dissociation. Major advances are expected from pioneering instrument development and from crossing boundaries between physics, chemistry and biology.""","2489100","2013-04-01","2018-03-31"
"LASSO","Learning, Analysis, SynthesiS and Optimization of Cyber-Physical Systems","Kim Guldstrand Larsen","AALBORG UNIVERSITET","Cyber-physical systems (CPS) are emerging throughout society, e.g. traffic systems, smart grids, smart cities, and medical devices, and brings the promise to society of better solutions in terms of performance, efficiency and usability. However, CPS are often highly safety critical, e.g. cars or medical devices, thus the utmost care must be taken that optimization of performance does not hamper crucial safety conditions. Given the constant growth in complexity of CPS, this task is becoming increasingly demanding for companies to handle with existing methods. The principle barrier for mastering the engineering of complex CPS being both trustworthy and efficient is the lack of a theoretical well-founded framework for CPS engineering supported by powerful tools, that will enable companies to timely meet increasing market demands.

With his extensive contributions to model-driven methodologies, and as provider of one of the “foremost” tools for embedded systems verification, the PI is well prepared to provide the missing framework. The LASSO framework will support the quantitative modeling of both cyber- and physical components, their efficient analysis, the learning of models for unknown components, as well as automatic synthesis and optimization of missing cyber-components from specifications. LASSO will provide the new generation of scalable tools for CPS, allowing trade-offs between safety constraints and performance measure to be made.

LASSO will achieve its objective by ground-breaking and extensive combinations of two different research areas: model checking and machine learning. The framework will develop a complete metric approximation theory for quantitative models, allowing properties to be inferred from reduced or learned models with metric guarantees of their validity in the original system. Further, the applicability of the framework will be validated through a number of CPS case studies, and the tools developed will be made generally available.","2493750","2015-11-01","2021-10-31"
"LAYERENG-HYBMAT","Molecular-Layer-Engineered Inorganic-Organic Hybrid Materials","Maarit Johanna Karppinen","AALTO KORKEAKOULUSAATIO SR","""On-demand-designed and precision-synthesized multicomponent or hybrid materials with unorthodox combinations of properties are potential keys to fascinating next-generation devices. At the same time there is a strong scientific desire to create a comprehensive repertory of basic understanding, design strategies and experimental tools to construct such outstanding smart materials from different building blocks and to shape them into sophisticated hierarchical architectures.
In LAYERENG-HYBMAT I propose a fundamentally new category of nanocomposite materials, that is, layer-by-layer grown coherent inorganic-organic hybrid materials where the cohesion between the layers is based on covalent bonding. Such materials are – once carefully designed and fabricated – able to display in a single material a tailored combination of properties of conventional inorganics and organics, and even beyond. The core hypothesis is that such intimately fused outstanding hybrids are materialized in a simple but extremely elegant manner by mimicking the state-of-the-art thin-film technology, i.e. ALD (atomic layer deposition), originally developed for purely inorganic thin films. The proposed method combines ALD and MLD (molecular layer deposition) cycles and enables the layer-by-layer deposition of coherent inorganic-organic thin films and coatings through sequential self-limiting gas-surface reactions with high precision for the composition and polymer-chain dispersity. With additional nanostructuring capacity these materials have the potential to open up new horizons in electronics, photonics, thermoelectrics, diagnostics, packaging, etc.
The project builds on my long experience in frontier new-material research on other types of multilayered materials and successful proof-of-the-concept ALD/MLD experiments, and addresses all the fundamental aspects of new-material design, modelling, precision synthesis, property tailoring and function characterization.""","2358102","2014-02-01","2019-01-31"
"LCC","Coupled Cluster Calculations on Large Molecular Systems","Poul Jørgensen","AARHUS UNIVERSITET","Quantum mechanics provides the key to the understanding of the molecular world. Many years of theoretical research have made coupled cluster calculations the state-of-the-art method for small molecules, where calculations have reached an accuracy often challenging experimental results. To describe large molecular systems with coupled cluster methods, the computational scaling with the system size of existing methods represents a roadblock to progress. The ultimate goal is to obtain coupled cluster methods that scale linearly with system size and where the calculations are embarrassingly parallel, such that calculations for small and large molecular systems require the same computational wall time. This proposal describes how this goal may be accomplished. The key is to express the coupled cluster wave function in a basis of local Hartree-Fock (HF) orbitals. We have recently shown how such a local HF basis may be obtained and described how linear-scaling, embarrassingly parallel coupled cluster energies may be obtained. Here we present proof-of-concept calculations for the energy and the molecular gradient for the simple model MP2 (second order Møller-Plesset perturbation theory) and propose to use the same technology for higher level coupled cluster methods to yield not only the energy of a large molecule, but also molecular properties as the equilibrium geometry, harmonic frequencies, excitation energies and transition moments, nuclear shieldings, polarizabilities and electronic and vibrational circular dichroism. This proposal will open a new era of accurate quantum calculations on large molecular systems such as nanoparticles and proteins. The presented developments will accelerate research, not only in chemistry and physics, but in molecular science and engineering in general.","1738432","2012-05-01","2017-04-30"
"LEAP","Large European Array for Pulsars","Michael Kramer","THE UNIVERSITY OF MANCHESTER","In general relativity and other relativistic theories of gravity, space and time are combined to form ``space-time'' which is curved in the presence of mass. As masses move, for instance like the two components in a binary system, ripples in space-time are created that propagate through the Universe, very much like waves caused by a stone falling into a pond. These ``gravitational waves'' (GWs) are known to exist from the effect that they have on a system of two orbiting stars. After inferring their existence indirectly, the next great challenge is the {\em direct} detection of GWs. While this is the aim of a number of gravitational wave detectors around the world, a detection has not been made.  Fortunately, a method exists that allows us today to detect GWs directly, in a frequency range that is much lower but complementary to those covered by ground-based detectors. This method utilises the radio astronomical observations of a special type of star known as radio pulsars.  We propose an experiment to achieve the ground-breaking goal of GW detection with the help of an innovative approach.  At the heart of this approach, named LEAP, lies the goal to combine the collective power of Europe's biggest radio-telescopes to form the biggest fully-steerable telescope on Earth, providing a ``leap'' in our sensitivity to go beyond the threshold that delivers the first direct detection of GWs. While the rewards for a successful detection of GWs are immense, we demonstrate that this is possible by harvesting the experience and resources uniquely available in Europe.","2455285","2009-01-01","2014-09-30"
"LEAPS","Light effected autonomous molecular pumps: Towards active transporters and actuating materials","Alberto Credi","ALMA MATER STUDIORUM - UNIVERSITA DI BOLOGNA","The crucial role played by molecular motors in major biological processes gives a clue on the potential of these nanoscale devices for technology. Their exploitation depends on our ability to build working and robust artificial systems, and to interface them with their environment or other molecular constructs for using the motion to carry out tasks.

The goal of this project is to develop the first synthetic photochemical supramolecular pumps and to apply them for performing nanoscale transport functions and macroscopic actuation. The motor modules, which rely on a functioning and affordable minimalist design based on first principles and threaded topologies, operate autonomously away from equilibrium by using light as a clean energy source, can be switched on/off chemically, and are easy to make and functionalize. Appropriately designed motors will be embedded in the bilayer of vesicles to pump molecules across physically separated places, thereby photogenerating concentration gradients. In parallel we plan to arrange the pump modules in oligomeric tracks and investigate the autonomous, directional and processive displacement of a molecule over a few nm. These linear motors will be equipped with a cargo that can be loaded/unloaded with control, yielding the first man-made molecular transporters. Finally, we will integrate the pump components in polymeric scaffolds such that the photoinduced operation of the motors produces a non-equilibrium entanglement of the polymer chains, that can be eventually unravelled by chemical stimulation. Such materials may be used to convert, store, and reuse the energy of (sun)light upon demand.

All the above functionalities are unprecedented for wholly synthetic chemical structures. Their demonstration would be a landmark result in supramolecular chemistry and nanoscience, and open up radically new directions for nanotechnology, nanomedicine, and energy conversion.","2362950","2016-10-01","2021-09-30"
"LEARN","Limitations, Estimation, Adaptivity, Reinforcement and Networks in System Identification","Lennart Ljung","LINKOPINGS UNIVERSITET","The objective with this proposal is to provide design tools and algorithms for model management in robust, adaptive and  autonomous engineering systems. The increasing demands on reliable models for systems of ever greater complexity have pointed to several insufficiencies in today's techniques for model construction. The proposal addresses key areas where new ideas are required. Modeling a central issue in many scientific fields. System Identification is the term used in the Automatic Control Community for the area of building mathematical models of dynamical systems from observed input and output signals, but several other research communities work with the same problem under different names, such as (data-driven) learning.

We have identified   five specific themes where progress is both acutely needed and feasible:
1.	Encounters with Convex Programming Techniques: How to capitalize on the remarkable recent progress in convex and semidefinite programming to obtain efficient, robust and reliable algorithmic solutions.
2.	Fundamental Limitations:  To develop and elucidate what are the limits of model accuracy, regardless of the modeling method. This can be seen as a theory rooted in the Cramer-Rao inequality in the spirit of invariance results and lower bounds characterizing, e.g., Information Theory.
3.	Experiment Design and Reinforcement Techniques: Study how well tailored and ``cheap'' experiments can extract essential information about isolated model properties. Also study how such methods may relate to general reinforcement techniques.
4.	Potentials of Non-parametric Models: How to incorporate and adjust techniques from adjacent research communities, e.g. concerning manifold learning and Gaussian Processes in machine learning.
5.	Managing Structural Constraints: To develop structure preserving identification methods for networked and decentralized systems.

We have ideas how to approach each of these themes, and initial attempts are promising.","2500000","2011-01-01","2015-12-31"
"LEGOTOP","From Local Elements to Globally Ordered TOPological states of matter","Yuval OREG","WEIZMANN INSTITUTE OF SCIENCE LTD","We present a novel constructive approach for realizations of topological states of matter. Our approach starts with well-understood building blocks, and proceeds towards coupling them to create the desired states. This approach promises both a guide for a tunable experimental realization of states which have not been observed so far, and a theoretical tool for deeper understanding of different topological states, their dualities and inter-relations.

We will apply the constructive approach in two different directions. In the first direction our goal will be the construction of topological superconductors. Our tool will be Josephson junctions in which superconductors are coupled by two- and three-dimensional electronic non-superconducting systems. Two dimensional examples include transition metal dichalcogenides, Quantum Hall states, Quantum Anomalous Hall states, and the (111) bi-layer state, which may be viewed as a fractionalized electron-hole condensate. Three dimensional examples include Weyl semi-metals and weak topological insulators.

In the second direction our goal is the construction of fractionalized spin liquid states. Our building block will be a Majorana-Cooper box, which is a superconducting quantum dot coupled to semi-conducting wires that host Majorana zero modes. We will consider arrays of such boxes. The ratio of the box's charging energy to inter-box tunnel-coupling determines whether the array is superconducting or insulating. We will aim to use insulating arrays for realizing fractionalized and non-abelian spin liquids, study the transition to the superconducting state, and search for possible relations between the topological properties on both sides of the transition.

A deeper comprehension and a feasible path for realization of these states would have a profound effect on the field of topological matter and will open novel avenues for universal topological quantum computers.","1532163","2018-10-01","2023-09-30"
"LEMAP","Laboratory Experiments on Magnetic Phenomena in Geo- and Astrophysics","Roberto Frank STEFANI","HELMHOLTZ-ZENTRUM DRESDEN-ROSSENDORF EV","Cosmic magnetic fields, including those of planets, stars, and galaxies, are being generated by the homogenous dynamo effect in flowing electrically conducting fluids. Once produced, these fields may play an active role in cosmic structure formation by fostering angular momentum transport and mass accretion onto central objects, like protostars or black holes, by means of the magnetorotational instability (MRI). Complementary to the decades-long theoretical research into both effects, the last years have seen great progress in respective experimental investigations. The dynamo effect had been verified in three liquid sodium experiments in Riga, Karlsruhe and Cadarache. The helical and the azimuthal versions of the MRI, as well as the current-driven Tayler instability (TI), were demonstrated at Helmholtz-Zentrum Dresden - Rossendorf (HZDR). Here, I propose to make three further breakthroughs in this research field. First, I plan to demonstrate dynamo action based on a precession driven flow of liquid sodium in a cylindrical vessel. Besides thermal and compositional buoyancy, precession has been discussed as a complementary power source of the dynamos of the Earth, the ancient Moon, and other cosmic bodies. A second experiment will deal with magnetically triggered flow instabilities of astrophysical importance, with the main focus on attaining standard MRI, and various combinations of MRI and TI. Both experiments will be carried out at the DRESDYN facility at HZDR which has been conceived by me and which will enter into operation in 2019. In contrast to these well-advanced experimental concepts, my third liquid sodium experiment, which aims at showing the magnetic destabilization of rotating flows with radially increasing angular velocity, still requires more numerical simulations and design engineering. Given the comparatively less demanding technical parameters of this set-up, I expect first experimental results within the funding period, too.","2493250","2018-11-01","2023-10-31"
"LHCDMTOP","Novel Dark Matter Searches with Top Quarks at the Large Hadron Collider","Daniel TOVEY","THE UNIVERSITY OF SHEFFIELD","This project will address directly the two most important unanswered questions in particle physics: the Standard Model (SM) hierarchy problem and the nature of dark matter (DM). The SM was recently completed with the discovery of the Higgs boson at the Large Hadron Collider (LHC) in 2012. We know, however, that the SM cannot be the end of the story for fundamental physics, because it suffers from two major flaws: a lack of stability for the mass of the Higgs boson (the hierarchy problem), and a lack of a candidate for the invisible DM particles known to make up most of the matter in the universe. I will address both of these key problems of modern physics by searching at the LHC for new beyond the SM (BSM) partner states for the SM top quark decaying to new DM particles. The greatly increased quantities of data and world-record collision energies generated by the LHC in the next three years will provide an unprecedented opportunity to find such top partners.  Confirmation of their existence would solve the hierarchy problem by providing a mechanism for stabilising the mass of the Higgs boson, while first observation of DM at the LHC would revolutionise our understanding of cosmology and provide a key pointer to the physics of the very early universe. Many leading BSM physics models predict the existence of both top partners and DM, and so this interdisciplinary project provides a unique opportunity to take the next major step forward in developing a unified theory of nature. I will focus on top partners which decay to a top quark and a DM particle, with the former decaying purely to jets and the latter escaping the detector unseen. I will use novel kinematic techniques developed by me to identify and characterise this signal in LHC data, and also accurately measure for the first time the dominant SM background process of associated production of top quarks and a Z boson, which is of great theoretical interest in its own right.","1584650","2016-05-01","2020-04-30"
"LHCTHEORY","Theoretical predictions and analyses of LHC physics: advancing the precision frontier","Michelangelo Mangano","EUROPEAN ORGANIZATION FOR NUCLEAR RESEARCH","The primary goal of this research proposal is to push to new levels of precision the predictive power of theoretical analyses of the phenomena observed at the Large Hadron Collider (LHC) at CERN. The start-up of the LHC has opened a new era in the exploration of the fundamental laws of Nature. This is expected to lead, among other results, to the clarification of the mechanism breaking the electroweak symmetry of fundamental interactions, to the discovery of new elementary particles, possibly accounting for the Dark Matter seen in the cosmos, and to the observation of new interactions, acting differently on matter and antimatter, to explain the observed baryon asymmetry of the universe.
The crucial ingredient in the success of this ambitious programme is the ability to interpret the signals extracted by the experiments.  To decode their properties and match them to the dynamics of possible new physics models relies on the numerical simulation of such dynamics, and on the ability to distinguish it from that of the known Standard Model (SM) processes. The past two decades have witnessed a continuous progress in this field, driven by the exploitation of the data from previous colliders, such as LEP, HERA and the Tevatron. The complexity of the LHC final states, the large rates of processes with many jets and their role in mimicking the production and decay of possible new particles, call for an aggressive effort to radically improve the current quality and accuracy of the theoretical modelling, to match the unprecedented discovery potential and measurement precision of the LHC experiments.
Capitalizing on recent theoretical advances, driven in significant part by the work of the PI and the team members, this proposal outlines a challenging and ambitious programme to advance to new levels the precision, generality and scope of the analysis tools used by both experimentalists and theorists engaged in LHC physics.","2049600","2012-04-01","2017-03-31"
"LIBNMR","Structure and Function: The Development and Application of Novel Ex- and In-situ NMR Approaches to Study Lithium Ion Batteries and Fuel Cell Membranes","Clare Philomena Grey","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Two new research programs will be established at Cambridge University: 1. Lithium ion batteries (LIBs). New positive and negative electrode materials are required for a range of LIB applications, which are lighter, have higher capacities, and can be operated at higher rates. To this end, I will establish a joint synthesis and characterization program, aimed at understanding how LIB-materials function and sometimes fail, in order to provide the fundamental insight required to design the next generation of LIBs. In particular we will use NMR spectroscopy, with other relevant characterization tools, including pair distribution function analysis, to investigate structure and Li dynamics. The specific objectives are (i) to develop novel in situ NMR techniques to investigate LIBs under realistic operating conditions, including the very high rates required for batteries for transportation. (ii) Utilize these methodologies to investigate a wide range of electrode systems, including conversion reactions, doped phosphates and composite electrodes. 2. Electrolytes for Solid Oxide Fuel Cells. This smaller program will investigate both oxygen and proton transport in ceramic materials, focusing on doped perovskites. Identification of the differences between the local structures of the ions that contribute to the ionic conductivity, and those that remain trapped in the lattice, represents a challenge for many experimental structural probes. Our objectives are to use NMR techniques to determine local structure and motion, in order to identify (i) how doping controls structure and (ii) the conduction mechanisms responsible for ionic conductivity. For the proton conductors, we will determine mechanisms for proton incorporation and investigate proton mobility in the bulk/grain boundaries.","1918270","2010-04-01","2015-03-31"
"LIBPR","Liberating Programming","David Harel","WEIZMANN INSTITUTE OF SCIENCE LTD","We propose to provide the theoretical, algorithmic and methodological foundations, and build the supporting tools, to bring about a major, paradigmatic, revolutionary change in the way software and systems are programmed and executed, based on the idea of liberated programming, a sweeping extension of the scenario-based play-in/play-out approach to program design and execution that I and my group have done around the language of live sequence charts (LSCs). Play-in is a new way of software programming, combining the ideas of showing and teaching, instead of telling, relying on friendly advanced user interfaces, and using intuitive yet formal and expressive visual languages.  Play-out is a general name for the technologies of executing played-in programs using powerful tools such as model-checking and synthesis.   Our proposed work is divided into four main threads:  (1) play-in, the development of new languages and interaction techniques; (2) play-out, the development of new execution technologies; (3) domain specific adaptations and applications; and (4) integration and tools.  The play-in techniques proposed include the translation of systems requirements given in natural language into an executable artifact, the use of novel and dynamic human machine interaction techniques, relying on visual languages as target languages. The play-out execution methods proposed include the use of model-checking and synthesis algorithms, compilation, and execution environments that learn.  Domain specific applications proposed include web services, tactical simulators, embedded systems, and biological modeling.    Finally, we propose to build prototype tools that will allow the evaluation of the new technologies and their dissemination into the academic community and industry.","2102958","2009-01-01","2013-12-31"
"LIDA","""Light on the Dark: Probing Dark Matter, Dark Energy and Dark Ages""","Jean-Paul Kneib","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","""Despite impressive progress in cosmology over the last decade, our understanding of the universe is still limited particularly in some Dark Areas: Dark Matter, Dark Energy & Dark Ages, where progress is particularly difficult. I propose to build a focused research group in a unique environment at the crossroad of astrophysics, cosmology and fundamental physics. The goal is to shade new “Light on the DArk” (LIDA) using new analysis techniques and new observations coming from two complementary surveys.
The first survey will target the fifty most massive strong lensing galaxy clusters and will shade new lights: i) on the nature of the Dark Matter through constraints coming from: the detailed mass distribution, the importance of substructures, and possible measurement of Dark Matter particle decay or self-annihilation, ii) on the most distant galaxies and the identification of the nature of the sources participating in the cosmic-reionization and ending the Dark Ages, iii) on the nature of the Dark Energy, through the accurate modeling of numerous multiple images in the best lensing clusters.
Second, with wide field imaging and spectroscopic surveys of the galaxy distribution we will probe new cosmological tests: i) the combination of galaxy-galaxy weak lensing and galaxy clustering in the framework of the halo model, ii) the measurement of the angle distribution of galaxy pairs, and thanks to the huge volume probed by the BOSS survey, iii) the test of the isotropy principle at various location of the surveyed volume. Finally we will prepare for the next generation of cosmological surveys by conducting an emission line galaxies redshift survey (e-BOSS) similar as those planned by the future facilities such as BigBOSS and EUCLID.
The LIDA project can started now, and will benefit from: new data collected by both space and ground-based telescopes over the next five years, as well as an active and vibrant work location in one of the best ranked university in Europe.""","2499935","2012-09-01","2017-08-31"
"LIFENAV","Reliable Lifelong Navigation for Mobile Robots","Thomas Wolfram Burgard","ALBERT-LUDWIGS-UNIVERSITAET FREIBURG","Our goal is to develop the fundamental approaches required to design mobile robot systems that can reliably operate over extended periods of time in dynamically changing environments.  To achieve this, robots need the ability to learn and update appropriate models of their environment including the dynamic aspects and to effectively incorporate all the information into their decision-making processes.  The time is ripe for the next step in navigation: the algorithms for state estimation and navigation in static environments have reached a high level of sophistication and the underlying models and learning algorithms are well-understood.  Our goal is to take these approaches further and to develop effective and object-oriented three-dimensional representations, that cover all aspects of the dynamic environment required for reliable and long-term mobile robot navigation.  The outcome of this research will be relevant for all applications that are based on autonomous navigation in real-world scenarios including autonomous robots, mobile manipulation, transportation systems, or autonomous cars.","2499228","2011-08-01","2016-07-31"
"LIFETimeS","Light-Induced Function: from Excitation to Signal through Time and Space","Benedetta MENNUCCI","UNIVERSITA DI PISA","Organisms of all domains of life are capable of sensing, using and responding to light. The molecular mechanisms used are diverse, but most commonly the starting event is an electronic excitation localized on a chromophoric unit bound to a protein matrix. The initial excitation rapidly “travels” across space to be converted in other forms of energy and finally used to complete the biological function. The whole machinery spans many different space and time scales: from the ultrafast electronic process localized at the subnanoscale of the chromophoric units, through conformational processes which involve large parts of the protein and are completed within micro- to milli-seconds, up to the activation of new protein-protein interactions requiring seconds or more. Theoretically addressing this cascade of processes calls for new models and computational strategies able to reproduce the dynamics across multiple space and time scales. Such a goal is formidably challenging as the interactions and the dynamics involved at each scale follow completely different laws, from those of the quantum world to those of classical particles. Only a strategy based upon the integration of quantum chemistry, classical atomistic and coarse-grained models up to continuum approximations, can achieve the required completeness of description. This project aims at developing such integration and transforming it into high-performance computing codes. The completeness and accuracy reached by the simulations will represent a breakthrough in our understanding of the mechanisms, which govern the light-driven bioactivity. Through this novel point of observation of the action from the “inside”, it will be possible not only to reveal the ‘design principles’ used by Nature but also to give a “practical” instrument to test “in silico” new techniques for the control of cellular processes by manipulating protein functions through light.","2400000","2018-09-01","2023-08-31"
"LightNet","LightNet - Tracking the Coherent Light Path in Photosynthetic Networks","Niek van Hulst","FUNDACIO INSTITUT DE CIENCIES FOTONIQUES","ature has developed photosynthesis to power life. Networks of light harvesting antennas capture the sunlight to funnel the photonic energy towards reaction centres. Surprisingly, quantum coherences are observed in the energy transfer of photosynthetic complexes, even at room temperature.
Does nature exploit quantum concepts? Does the coherence help to find an optimal path for robust or efficient transfer? How are the coherences sustained? What is their spatial extent in a real light-harvesting network? So far only solutions of complexes were studied, far from the natural network operation, putting on hold conclusions as to a biological role of the coherences.
My group recently succeeded in the first detection of coherent oscillations of a single photo-synthetic complex at physiological conditions, and non-classical photon emission of individual complexes. These pioneering results, together with our expertise in nanophotonics, pave the way to address photosynthetic networks in real nano-space and on femtosecond timescale. Specific objectives are:

--Ultrafast single protein detection: tracing the fs coherent energy transfer path of an individual complex; addressing the very nature of the persistent coherences.
-Beyond fluorescence: light harvesting complex are designed for light transport, not emission. I will explore innovative alternatives: optical antennas to enhance quantum efficiency; detection of stimulated emission; and electrical read-out on graphene.
-Nanoscale light transport: using local excitation and detection by nanoholes, nanoslits and scanning antenna probes I will spatially map the extent of the inter-complex transfer.
-The network: combining both coherent fs excitation and localized nanoscale excitation/detection I will track the extent of coherences throughout the network.

The impact of this first exploration of light transport in a nanoscale bionetwork ranges to solar energy management, molecular biology, polymer chemistry and material science.","2856250","2016-01-01","2020-12-31"
"LILO","Light-In, Light-Out: Chemistry for sustainable energy technologies","Edwin Charles Constable","UNIVERSITAT BASEL","The project is concerned with a coordinated approach to the development of of novel chemical strategies for light harvesting by photovoltaic cells and light generation using light emitting electrochemical cells. Both technologies have proof of principle results from the PIs own laboratory and others world-wide. The bulk of efficient dye sensitized solar cells rely on transition metal complexes as the photoactive component as the majority of traditional organic dyes do not possess long term stability under the operating conditions of the devices. LECs based upon transition metal complexes have been shown to possess lifetimes sufficiently long and efficiencies sufficiently high to become a viable alternative technology to OLEDs in the near future. The disadvantages of state of the art devices for both technologies is that they are based upon second or third row transition metal complexes. Although these elements are expensive, the principle difficulties arise from their low abundance, which creates significant issues of sustainability if the technology is to be adopted. The aim of this project is three-fold. Firstly, to further optimise the individual technologies using conventional transition metal complexes, with increases in efficiency leading to lower metal requirements. Secondly, to explore the periodic table for metal-containing luminophores based on first row transition metals, with an emphasis upon copper and zinc containing species. The final aspect is related to the utilization of solar derived electrons for water splitting reactions, allowing the generation of hydrogen and/or reaction products of hydrogen with organic species. This latter aspect is related to the mid-term requirement for liquid fuels, regardless of the primary fuel sources utilized. The program will involve design and synthesis of new materials, device construction and evaluation (in-house and with existing academic and industrial partners) and iterative refinement of structures","2399440","2011-01-01","2015-12-31"
"LiNAss","Light-induced NanoAssembly","Jeremy John Baumberg","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","NanoMaterials have huge promise in a wide range of applications of societal importance. Intricate combinations of metals, semiconductors, dielectrics, and molecular components in three-dimensional configurations, have new and unusual properties. Such advanced functions are at the heart of photovoltaics, magnetic and quantum information technologies, photosynthesis, water splitting, electronics, batteries, fuel cells, catalysis and many more crucial areas. Despite much research, we simply cannot yet make such nanomaterials at will. This problem is thus a major challenge for the future decades that we need to solve. The proposal here uses bottom-up assembly of nano-components combined with the application of controlling beams of light, as a new approach to sub-nm precision capable of scale-up.
The exact arrangement of nano-sized components can drastically change the optical response of a nanostructure. We directly exploit this optical sensitivity to structure. Irradiation by specific wavelengths of laser light builds up strong optical fields only in parts of the structure which transiently have the right configuration. These regions of high field can be spatially localised to 1nm, far smaller than the wavelength of light. If this induces enhanced binding then optical selection preferentially selects specific morphologies. The principal goal of this proposal is to demonstrate the new strategies for reliable nano-constructs at the 1nm scale, which can be produced in large numbers with essentially identical architecture. Several approaches will be explored in parallel, using the light to either glue together nano building blocks, or to deposit the energy needed to grow nanostructures directly. In addition developing ways for light to flex structures can result in significant changes to the optical spectra, thus providing exquisitely-sensitive feedback on the nanoscale. Light is a crucial observational tool, requiring development of real-time sub-ms spectroscopies.","2050000","2013-04-01","2018-03-31"
"LIQAD","Long-range interacting quantum systems and devices","Tilman Pfau","UNIVERSITAET STUTTGART","Controlled correlations in a quantum network are at the heart of emerging quantum technologies for communication, information processing and computation. The scaling to a large number of interconnected nodes has so far remained an open challenge. Here mesoscopic ensembles of atoms which can be well controlled in their geometry and which provide rapidly switchable long range interactions promise an alternative approach with a significant simplification for quantum devices and networks. Finite temperatures up to even above room temperature operation of the resulting quantum devices might be possible and the upscaling to quantum networks with millions of nodes seems within reach.
Therefore I propose to study Rydberg interacting mesoscopic ensembles at low and high temperatures. In the first part fundamental building blocks for quantum devices and networks based on the so called Rydberg blockade in mesoscopic ensembles will be studied in an ultracold environment. In the second part I will investigate how to transfer these ideas to scalable ensembles in thermal micro-vapor cells. As the range of interaction can be on the order of micrometers, standard techniques in lithography can be used to produce mesoscopic ensembles confined in glass cells. Display fabrication technologies used for the production of TFT LC (thin-film transistor liquid crystal) displays can be used to scale the number of connected mesoscopic ensembles up dramatically. I will investigate to what extend the interdisciplinary combination of micro- and display technology and atomic physics enables the parallel operation of many scalable single photon sources for example to feed a large linear optical quantum network. This resulting ground-breaking perspective for the applicability of quantum devices and networks justifies the risk to explore fundamentally and technologically unexplored territory.","2407200","2011-03-01","2016-02-29"
"LLPS-NMR","Nuclear magnetic resonance spectroscopy of liquid-liquid phase separation","Markus Heinz Georg Sebastian ZWECKSTETTER","DEUTSCHES ZENTRUM FUR NEURODEGENERATIVE ERKRANKUNGEN EV","Liquid-liquid phase separation (LLPS) is a phenomenon inherent to the thermodynamics of liquids, is critical for the development of technologically useful fluids and underlies some of the biggest health changes in our society. LLPS is based on transitions between two different forms of liquid that have the same chemical composition, but distinct energy, entropy and density. Despite the importance of LLPS for technology and health, however, only a very low-resolution view primarily through light microscopy is currently available for LLPS states formed by peptides and proteins. Because of this bottleneck, the interactions, which stabilize liquid droplets, and regulate their biogenesis, as well as a rationale for the biochemical function of LLPS, have remained mysterious. 

To tackle this massive unmet need, I propose to develop powerful methods of NMR spectroscopy that go far beyond the state-of-the-art and team them up with mechanobiology/force microscopy to break the resolution barrier of polypeptide LLPS and push the description of the internal organization of liquid droplets from micrometer to sub-nanometer. Although highly challenging, the novel methods when successful will (i) disentangle the structure and kinetics of intrinsically disordered proteins within LLPS reaction chambers in space and time, (ii) unravel the nature of chemical reactions in liquid droplets, and (iii) decipher LLPS regulation by posttranslational modifications, nucleic acids and critical changes in cellular environment at atomic resolution. The innovative nature of the proposal is designed to unravel the innermost forces in liquid droplets and to transform our knowledge about the chemistry of liquid phase-separated protein states. Findings from this proposal will provide critical guidance in the development of systems to encapsulate bioactive molecules and to develop better treatments for human diseases.","2460278","2018-09-01","2023-08-31"
"Load Slice Core","Load Slice Core: A Power and Cost-Efficient Microarchitecture for the Future","Lieven Eeckhout","UNIVERSITEIT GENT","The ideal processor building block is a power and cost-efficient core that can maximize the extraction of memory hierarchy parallelism, a combination that neither traditional in-order nor out-of-order cores provide. We propose the Load Slice Core microarchitecture, a restricted out-of-order engine aimed squarely at extracting memory hierarchy parallelism, which, according to preliminary results, delivers a nearly 8 times higher performance per Watt per euro compared to an out-of-order core. 

The overarching objective of this project to fully determine the potential of the Load Slice Core as a key building block for a novel multi-core processor architecture needed in light of both current and future challenges in software and hardware, including variable thread-level parallelism, managed language workloads, the importance of sequential performance, and the quest for significantly improved power and cost efficiency. 

We anticipate significant improvement in multi-core performance within the available power budget and cost by combining chip-level dynamism to cope with variable thread-level parallelism along with the inherent power- and cost-efficient Load Slice Core design. If we are able to demonstrate the true value and potential of the Load Slice Core to address future hardware and software challenges, this project will have a long-lasting impact on the microprocessor industry moving forward.","2499500","2018-01-01","2022-12-31"
"LocalStructure","Local Structure of Sets, Measures and Currents","David Preiss","THE UNIVERSITY OF WARWICK","The objective of this research proposal is to develop new methods to answer a number of fundamental questions generated by the recent development of modern analysis. The questions we are interested in are specifically related to the study of local structure of sets and functions in the classical Euclidean setting, in infinite dimensional Banach spaces and in the modern setting of analysis on metric spaces. The main areas of study will be:

(a) Structure of null sets and representation of (singular) measures, one of the key motivations being the differentiability of Lipschitz functions in finite dimensional spaces.

(b) Nonlinear geometric functional analysis, with particular attention to the differentiability of Lipschitz functions in infinite dimensional Hilbert spaces and Banach spaces with separable dual.

(c) Foundations of analysis on metric spaces, the key problems here being representation results for Lipschitz differentiability spaces and spaces satisfying the Poincar\'e inequality.

(d) Uniqueness of tangent structure in various settings, where the ultimate goal is to contribute to the fundamental problem whether minimal surfaces (in their geometric measure theoretic model as area minimizing integral currents) have a unique behaviour close to any point.","2068147","2012-05-01","2017-09-30"
"LOFAR-AUGER","From Black Holes to Ultra-High Energy Cosmic Rays: Exploring the Extremes of the Universe with Low-Frequency Radio Interferometry","Heino Falcke","STICHTING KATHOLIEKE UNIVERSITEIT","Black holes (BHs) and ultra-high energy cosmic rays (UHECRs) are two extremes of the universe that link particle physics and astrophysics. BHs are the most efficient power generators in the universe while UHECRs are the most energetic particles ever detected. As we showed previously, a major fraction of the power of BHs is channeled into radio-emitting plasma jets, which are also efficient particle accelerators. Are BHs also responsible for UHECRs? This long-standing question could be answered soon, through the dawn of cosmic ray astronomy. The giant Auger observatory has now shown for the first time that the arrival directions of UHECRs are non-isotropic, potentially pointing back to their sources of origin. BHs turned out to be major suspects, but other sources could still also be responsible. To address this conclusively and to establish cosmic ray astronomy as a productive new field in the coming years, we need to increase statistics, expand current observatories, and have complementary all-sky radio surveys available to identify sources, since radio emission traces particle acceleration sites. Here, techniques pioneered by the Low-Frequency Array (LOFAR) promise major advances. First of all, working on LOFAR we uncovered a new technique to detect UHECRs with radio antennas and verified it experimentally. The technique promises to increase the number of high-quality events by almost an order of magnitude and provides much improved energy and direction resolution. We now want to implement this technique in Auger, combining LOFAR and AUGER know-how. Secondly, LOFAR and soon other SKA pathfinders will significantly improve all-sky radio surveys with high sensitivity, resolution, and image quality. Hence, we will use LOFAR to understand the astrophysics of UHECR source candidates and compile a radio-based catalog thereof. We start with jets from BHs and move later to other sources. Together this will allow us to identify UHECR sources and study them in detail.","3460000","2009-01-01","2013-12-31"
"LOFARCORE","Unravelling the Cosmic Reionization History","Antonius Gerardus De Bruyn","STICHTING ASTRON, NETHERLANDS INSTITUTE FOR RADIO ASTRONOMY","""One of the frontier cosmological research themes of the past decade has been the Epoch of Reionization (EoR). This era marked the start of the formation of visible baryonic structures: stars, (dwarf-)galaxies, clusters and the cosmic web. Some top level questions surrounding COsmic REionization are: """"When did it start ?”, “How long did it take ?”, “What were the main ionizing sources ?”, and “What did the Universe look like during this crucial evolutionary phase ?""""  Simulations of reionization suggest how it may have proceeded, but hard observational facts on the EoR are few and far between.
At the start of the EoR all hydrogen was cold and neutral. At redshift z=6 the transition to an ionized Universe was completed. The 21cm line, redshifted to much longer wavelength, is a key diagnostic during this phase. LOFAR, the European LOw Frequency Array, will be the premier instrument to explore this epoch, simultaneously over the full redshift range from z=6.5-11.5 ! The required angular resolution is also well matched to the 2.5 km CORE of LOFAR.  Detecting redshifted HI from the EoR is generally acknowledged as the most difficult radio astronomical project ever attempted. As PI of the LOFAR EoR Key Science Project and as LOFAR Calibration Project scientist, I have worked hard for 10 years to prepare LOFAR and myself for this endeavour.
In November 2012 LOFAR commenced a long (> 3-5 years) observing campaign, generating several PB of data per year.  A new phase, with extraordinary challenges, still lies ahead. ERC funding for the LOFARCORE team will permit me to ramp up the execution of this program, and  in a timely manner (we have competition from US, Australian and Indian groups), solve problems along the way, and analyze and publish what will be ground-breaking results on the COsmic REionization history. The LOFAR EoR project will be the pathfinder for even more daunting future explorations, using SKA, of the phases preceeding the EoR, i.e. Cosmic Dawn.""","2952628","2014-04-01","2018-06-30"
"LogCorrelatedFields","Extremes in logarithmically correlated fields","Ofer Zeitouni","WEIZMANN INSTITUTE OF SCIENCE LTD","The proposed research deals with the extremes of logarithmically correlated fields, in both the Gaussian and non-Gaussian setups. Examples of such fields are branching random walks, the (discrete) two dimensional Gaussian free field, the set of points left uncovered by a random walk on the two dimensional torus at times close to the cover time of the torus, the (absolute) values of the characteristic polynomial of random matrices, Ginzburg-Landau models, and more. The proposal builds on recent progress in the study of the maximum and of the extremal process of the two dimensional Gaussian free field, which was made possible by Gaussian comparisons and the introduction of a refined version of the second moment method.   The proposed research will develop the tools needed for building a general and flexible theory applicable to general logarithmically correlated fields. Applications to the multiplicative chaos will also be considered.","1292500","2016-06-01","2021-12-31"
"LogCorRM","Log Correlations and Random Matrices","Jon KEATING","UNIVERSITY OF BRISTOL","Random Matrix Theory has been of central importance in Mathematical Physics for over 50 years. It has deep connections with many other areas of Mathematics and a remarkably wide range of applications.  In 2012, a new avenue of research was initiated linking Random Matrix Theory to the highly active area of Probability Theory concerned with the extreme values of logarithmically correlated Gaussian fields, such as the branching random walk and the two-dimensional Gaussian Free Field. This connects the extreme value statistics of the characteristic polynomials of random matrices asymptotically to those of the Gaussian fields in question, allowing some important and long-standing open questions to be addressed for the first time. It has led to a flurry of activity and significant progress towards proving some of the main conjectures. A remarkable discovery has been that the characteristic polynomials of random matrices exhibit, asymptotically, a hierarchical branching/tree structure like that of the branching random walk.  However, many of the most important questions remain open. My aim is to attack some of these problems using ideas and techniques that have so far not been applied to them: I believe it is possible to compute some important statistical quantities relating to the extreme values of characteristic polynomials exactly, for the first time, by establishing connections with integrable systems, representation theory, and enumerative combinatorics.  Such connections have not previously been explored.  I anticipate that this will have a significant impact on an area that is currently in a rapid phase of development and that it will settle some of the principal unresolved conjectures. I further believe that ideas exploiting the hierarchical branching structure may have new and unexpected implications for areas connected with Random Matrix Theory, including, in particular, Number Theory, and I plan to explore these too.","1778516","2017-09-01","2022-08-31"
"LoTGlasSy","Low Temperature Glassy Systems","Giorgio Parisi","UNIVERSITA DEGLI STUDI DI ROMA LA SAPIENZA","Jamming of hard spheres is a new critical phenomenon whose exponents are different from those of the other known transitions. These exponents have been recently computed in a mean field approximation whose limits of validity are not known. Even if their values are in very good agreement with the ones obtained by accurate numerical simulations, the deep reasons for this success are not understood.

Trampolining from these results I plan to develop a theory of the large scale properties of the free energy landscape of glasses at low temperature. I will use techniques of statistical field theory and of renormalization group to identify and compute universal features. This proposal has the following goals.

• We will develop a complete analytic theory of the infinite pressure limit (jamming) of hard spheres in dimensions greater than the upper critical dimensions. We will first compute analytically the upper critical dimension. Numerical simulations suggest that the upper critical dimensions is equal to or smaller than 2: this result is puzzling and it would be very interesting to find out if this indication is supported by the theory. We will also investigate in detail the scaling properties and the conformal invariance of the correlation functions.
• Starting from these results we will derive universal properties of glassy materials in the low temperature regions in the classical and in the quantum regime. The properties of multiple equilibrium configurations are crucial; we will study the structure of small (localized or extended) oscillations around them, the classical and quantum tunneling barriers.
• We will analyze both equilibrium features and off-equilibrium features (like plasticity and the time dependence of the specific heat). The subject has been widely discussed and phenomenological laws have been derived. I aim to obtain these laws from first principles using the properties of the free energy landscape in glasses that will be derived analytically.","1760000","2016-06-01","2021-05-31"
"LTDBud","Low Dimensional Topology in Budapest","Andras Istvan Stipsicz","MAGYAR TUDOMANYOS AKADEMIA RENYI ALFRED MATEMATIKAI KUTATOINTEZET","""Heegaard Floer theory.  In this project (in collaboration with P. Ozsváth and Z. Szabó) we plan to extend our earlier results computing various versions of Heegaard Floer homologies purely combinatorially.  We also plan to find combinatorial definitions of these invariants (as graded groups). Such results will potentially lead to a combinatorial description of 4-dimensional Heegaard Floer (mixed) invariants, conjecturally equivalent to Seiberg-Witten invariants of smooth 4-manifolds. In particular, we hope to find a combinatorial proof of Donaldson’s diagonalizability theorem, and find relations between the Heegaard Floer and the fundamental groups of a 3-manifold.

Contact topology. Using Heegaard Floer theory and contact surgery, a systematic study of existence of tight contact structures on 3-manifolds is planned. Similar techniques also apply in studying Legendrian and transverse knots in contact 3-manifolds. In particular, the verification of the existence of tight structures on 3-manifolds given by surgery on a knot (with high enough framing) in the 3-sphere is proposed. Using the Legendrian invariant of knots, Legendrian and transverse simplicity can be conveniently studied. The ideas detailed in this part are planned to be carried out partly in collaboration with Paolo Lisca, Vera Vértesi and Hansjörg Geiges.

Exotic 4-manifolds. Extending our previous results, we plan to investigate the existence of exotic smooth structures on 4-manifolds with small Euler characteristics, such as the complex projective plane CP2, its blow-up CP2#CP2-bar, the  product of two complex projective lines CP1×CP1 and ultimately the 4-dimensional sphere S4. We plan to investigate the effect of the Gluck transformation. Possible extensions of the rational blow down procedure (successful in producing exotic structures) will be also studied. We plan collaborations with Zoltán Szabó, Daniel Nash and Mohan Bhupal in these questions.""","1208980","2012-04-01","2017-03-31"
"LUCIFER","Low-background Underground Cryogenic Installation For Elusive Rates","Fernando Ferroni","ISTITUTO NAZIONALE DI FISICA NUCLEARE","In the field of fundamental particle physics the neutrino has become more and more important in the last few years, since the discovery of its mass. In particular, the ultimate nature of the neutrino (if it is a Dirac or a Majorana particle) plays a crucial role not only in neutrino physics, but in the overall framework of fundamental particle interactions and in cosmology. The only way to disentangle its ultimate nature is to search for the so-called Neutrinoless Double Beta Decay (0½DBD). The goal of LUCIFER is to build a background-free 0½DBD experiment with a discovery potential better than the future, already approved, funded experiments. Although aiming at a discover, in the case of insufficient sensitivity the LUCIFER technique will be the demonstrator for a higher mass experiment able to probe the entire inverted hierarchy region of the neutrino mass and to start approaching the direct one. The idea of LUCIFER is to join the bolometric technique proposed for the CUORE experiment (one of the few 0½DBD experiments in construction world-wide) with the bolometric light detection technique used in cryogenic dark matter experiments. The bolometric technique allows an extremely good energy resolution while its combination with the scintillation detection offers an ultimate tool for background rejection. Preliminary tests on several 0½DBD detectors have clearly demonstrated the excellent background rejection capabilities that arise from the simultaneous, independent, double readout (heat + scintillation).","3294400","2010-03-01","2016-02-29"
"LUCKY STAR","Exploring the outer solar system beyond Neptune using stellar occultations","Bruno SICARDY","SORBONNE UNIVERSITE","The solar system beyond Neptune’s contains largely unaltered material from the primordial circum-solar disk. It also kept the memory of the early planetary migrations, and thus contains essential information on the origin and evolution of our planetary system.

Here I propose to study the Trans-Neptunian Objects (TNOs) using the stellar occultation technique. It consists in observing the passage of remote TNOs in front of those “Lucky Stars”, that reveal shapes, atmosphere and rings of bodies from sub-km to thousand-km in size. Very few teams in the world master this method. The European-led network that I coordinate is now leader in predictions, instrumentation, observations and analysis related to stellar occultations, with innovative approaches and unprecedented results.

In the last decade, our group led the field by discovering rings around the asteroid-like object Chariklo, detecting sub-km TNOs and drastic variations of Pluto’s atmospheric pressure. Based on those noteworthy discoveries and unique skills of ours, I will coordinate the following work packages:

(1) Rings around small bodies - Understand the newly found Chariklo’s rings, tackle the theory of rings’ origins and evolutions around small bodies, discover new ring systems around other bodies. 

(2) Very small, sub-km TNOs and Oort Cloud objects - Constrain the collisional history of our early outer solar system, and possibly detect Oort Cloud objects.

(3) Pluto’s atmosphere – Explore Pluto’s atmosphere and its atypical seasonal cycle, search for atmospheres around other TNOs.

(4) Explore specific, large TNOs – Provide their sizes, shapes, albedos and densities.

These programs are timely in view of NASA/New Horizons Pluto flyby in July 2015, and the ESA/GAIA mission expected to provide a greatly improved astrometric catalog release in 2016.

Most of the budget will be dedicated to human power to conduct observations and their analysis, plus the associated travel and telescope time expenses.","2423750","2015-11-01","2020-10-31"
"LUCRETIUS","Foundations for Software Evolution","John Mylopoulos","UNIVERSITA DEGLI STUDI DI TRENTO","Software evolution refers to changes made to a software system after its deployment. These changes may be caused by changing requirements, domain assumptions, or computing infrastructure, and may affect the system’s implementation, architecture and/or requirements. Evolution may be automatic (aka self-adaptation), or manual, or something in-between. This project aims to develop principles that underlie, and concepts, tools and techniques that support evolution. The project will focus on software-intensive systems. Such systems consist of software, human and organizational elements that work together to fulfill organizational and human objectives. Our proposed research is founded on ideas and research results from Requirements Engineering (RE). Evolution is to be supported by design-time models that are made available at run-time. These models capture system requirements and domain assumptions, augmented with design and implementation details. When evolution is automatic, supported by monitor-diagnose-compensate-execute feedback loops, these models determine (i) what is to be monitored, (ii) whether the system is operating according to its intended purpose, (iii) what are possible compensations for deviations from intended behaviour, (iv) how to evolve the system. When evolution is manual, these models support evolution activities, carried out by humans, by offering a comprehensive picture of the requirements and assumptions under which the system operates, along with traceability links between elements of these models and code. This means that design-time models need to capture stakeholder intentions and commitments, social interactions, business processes, and organizational goals that ultimately determine system requirements. Expected results from the project include the development of novel concepts, tools and techniques for designing evolution-enabled software-intensive systems.","2462095","2011-04-01","2016-03-31"
"M2C","FOLLOWING THE MOST MASSIVE GALAXY CLUSTERS ACROSS COSMIC TIME","Monique Arnaud","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","""Our project aims at testing the standard LCDM scenario for the formation of collapsed structures. Taking advantage of the advent of cluster detection via the SZ effect, we will use the most massive clusters of galaxies and their evolution as laboratory. We build on the Planck SZ survey, the first All Sky survey since the RASS X-ray survey. We will substantially extend the nominal Planck cluster catalogue by developing novel detection techniques based on a simultaneous search of objects in Planck and RASS maps, reaching lower masses and higher redshifts, while keeping a high catalogue purity.
For the first time, we will have the sample size, redshift leverage, and completeness, for a decisive test of the standard LCDM model of the dark matter gravitational collapse on cluster scale. The test will be provided by a full statistical analysis of the dark matter profiles and their evolution. The X-ray technique to derive mass profile will be extended to the full cluster population. This will be made possible by an integrated approach involving systematic confrontation of observations with tailor-made numerical simulations.
Using multi-wavelength data and simulations, we will also assess the dynamical behaviour of the baryons as they collect in dark matter potential. We will i) provide the first complete census of the dark matter, hot and cold baryons and its evolution ii) quantify the thermo- dynamical state up to z~1 as a probe of hierarchical structure formation and gravitational heating iii) probe the link between non-thermal and thermal components.
The project will either cement our current understanding of the dark matter collapse, a prerequisite to any assessment of the specific baryon physics, or points towards the need for revision of the current paradigm, with important cosmological implications. The new detection techniques will be applicable to future surveys. Lastly, we will provide a ‘gold sample’ of galaxy clusters, ideal for cosmological parameter estimate.""","2456035","2014-04-01","2019-03-31"
"M4D","Metal Microstructures in Four Dimensions","Dorte JUUL JENSEN","DANMARKS TEKNISKE UNIVERSITET","The goals are:
1) to develop a universal laboratory-based 4D X-ray microscope with potentials in the broad field of materials science and beyond;
2) to advance metal research by quantifying local microstructural variations using the new 4D tool and by including the effects hereof in the understanding and modelling of industrially relevant metals.

Today, high resolution 4D (x,y,z,time) crystallographic characterization of materials is possible only at large international facilities. This is a serious limitation preventing the common use. The new technique will allow such 4D characterization to be carried out at home laboratories, thereby wide spreading this powerful tool.

Whereas current metal research mainly focuses on average properties, local microstructural variations are present in all metals on several length scales, and are often of critical importance for the properties and performance of the metal. In this project, the new technique will be the cornerstone in studies of such variations in three types of metallic materials: 3D printed, multilayered and micrometre-scale metals. Effects of local variations on the subsequent microstructural evolution will be followed during deformation and annealing, i.e. during processes typical for manufacturing, and occurring during in-service operation.

Current models largely ignore the presence of local microstructural variations and lack predictive power. Based on the new experimental data, three models operating on different length scales will be improved and combined, namely crystal plasticity finite element, phase field and molecular dynamics models. The main novelty here relates to the full 4D validation of the models, which has not been possible hitherto because of lack of sufficient experimental data.

The resulting fundamental understanding of the inherent microstructural variations and the new models are foreseen to be an integral part of the future design of metallic materials for high performance applications.","2496519","2018-10-01","2023-09-30"
"M5CGS","From Mutations to Metastases: Multiscale Mathematical Modelling of Cancer Growth and Spread","Mark Andrew Joseph Chaplain","UNIVERSITY OF DUNDEE","Cancer is one of the major causes of death in the world (particularly the developed world), with around 11 million people diagnosed and around 7 million people dying each year. The World Health Organisation predicts that current trends show around 9 million will die in 2015, with the number rising to 11.5 million in 2030.    Cancer growth (viz. solid tumour growth) is a complicated phenomenon involving many inter-related processes across a wide range of spatial and temporal scales, and as such presents the mathematical modeller with a correspondingly complex set of problems to solve.  This proposal will develop multi-scale mathematical models for the growth and spread of cancer and will focus on three main scales of interest: the sub-cellular, cellular and macroscopic.  -- The sub-cellular scale refers to activities that take place within the cell or at the cell membrane, e.g. DNA synthesis, gene expression, cell cycle mechanisms, absorption of vital nutrients, activation or inactivation of receptors, transduction of chemical signals.  -- The cellular scale refers to the main activities of the cells, e.g. statistical description of the progression and activation state of the cells, interactions among tumour cells and the other types of cells present in the body (such as endothelial cells, macrophages, lymphocytes), proliferative and destructive interactions, aggregation and disaggregation properties.  -- The macroscopic scale refers to those phenomena which are typical of continuum systems, e.g. cell migration, diffusion and transport of nutrients and chemical factors, mechanical responses, interactions between different tissues, tissue remodelling.   The proposal is multi-disciplinary in nature and aims to develop quantitative, predictive mathematical models of solid tumour growth which can ultimately be used in planning patient-specific treatment protocols such as chemotherapy, surgery and radiotherapy.","1680974","2009-09-01","2014-08-31"
"MACI","Moduli, Algebraic Cycles, and Invariants","Rahul Vijay PANDHARIPANDE","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Algebraic geometry is the study of varieties -- the zero sets of polynomial equations in several variables. The subject has a central role in mathematics with connections to number theory, representation theory, and topology. Moduli questions in algebraic geometry concern the behavior of varieties as the coefficients of the defining polynomials vary. At the end of the 20th century, several basic links between the algebraic geometry of moduli spaces and path integrals in quantum field theory were made. The virtual fundamental class plays an essential role in these connections. I propose to study the algebraic cycle theory of basic moduli spaces. The guiding questions are: What are the most important cycles? What is the structure of the algebra of cycles? How can the classes of geometric loci be expressed? The virtual fundamental class and the associated invariants often control the answers. A combination of virtual localization, degeneration, and R-matrix methods together with new ideas from log geometry will be used in the study.

Most of the basic moduli spaces in algebraic geometry related to varieties of dimension at most 3 -- including the moduli of curves, the moduli of maps, the moduli of surfaces, and the moduli of sheaves on 3-folds -- will be considered. The current state of the study of the algebraic cycle theory in these cases varies from rather advanced (for the moduli of curves) to much less so (for the moduli of surfaces). There is a range of rich open questions which I will attack: Pixton's conjectures for the moduli of curves, the structure of the ring of Noether-Lefschetz loci for the moduli of K3 surfaces, the holomorphic anomaly equation in Gromov-Witten theory, and conjectures governing descendents for the moduli of sheaves. The dimension 3 restriction is often necessary for a good deformation theory and the existence of a virtual fundamental class.","2496055","2018-09-01","2023-08-31"
"MADPII","Multiscale Analysis and Design for Process Intensification and Innovation","Guy B.M.M. Marin","UNIVERSITEIT GENT","The current pressures on the major industrial players have necessitated a more urgent push for increased productivity, process efficiency, and waste reduction; i.e. process intensification.  Future sizable improvements in these entrenched industrial processes will require either completely novel production technologies, fundamental analysis/modeling methods, or a combination of both.  This proposal aims to approach this challenge by using multiscale modeling and experimentation on three fronts: (1) detailed analysis of industrial processes to generate new fundamental chemical understanding, (2) multiscale modeling and evaluation of high-volume chemical processes using a multiscale approach and fundamental chemical understanding, and (3) show the practical applicability of the multiscale approach and use it to critically examine novel technologies in the context of industrial processes. The novel technology portion of this proposal will be focused around a class known as rotating bed reactors in a static geometry (RBR-SG).  We will investigate three processes that could benefit from RBR-SG technology: (1) fast pyrolysis of biomass, (2) gasification of biomass, and (3) short contact time catalytic partial oxidation of light hydrocarbons.  Experimental reactor and kinetic work and validated computational fluid dynamics (CFD) modeling of the process mentioned above will be used.  We will construct two RBR-SG units; heat transfer, adsorption, and pyrolysis gas/solid experiments will be performed in one, while non-reacting flow tests will be performed in the other with other phase combinations.  Detailed kinetic models will provide novel insights into the reaction dynamics and impact other research and technologies. The combination of kinetic and CFD models will clearly demonstrate the benefits of a multiscale approach, will definitively identify the process(es) benefitting most from RBR-SG technology, and will enable a first design of the RBR-SG based on our results.","2494700","2012-05-01","2017-04-30"
"MAGIC","(Nano)-Materials for cell Growth, Imaging and Communication","Luisa De Cola","CENTRE INTERNATIONAL DE RECHERCHE AUX FRONTIERES DE LA CHIMIE FONDATION","MaGIC intends to explore the use of nano/micro objects, in particular zeolite L, as materials for imaging, and, when the zeolites are used as substrates, for analyzing and manipulating cells. In particular in vivo and in vitro imaging, cell growth on nano/micro patterned zeolite monolayers, and understanding some of the processes of cell-to-cell communication are the ambitious goals of this proposal. We intend to achieve these goals through 5 objectives:
1. Synthesis and characterization of zeolites and loading and trapping of dye molecules.
2. Patterned zeolite monolayers and microcontact printing for asymmetric functionalization and cells transfer.
3. Molecular imaging using nanoporous materials as multiresponsive probes.
4. Cell growth, proliferation and stimulation of processes in spatially confined areas.
5. Communication between cells and cell differentiation.
The project is extremely challenging and if successful will open new horizons in the use of nanomaterials in combination with living systems and will develop new technologies for handling delicate substrates and assemblies. The numerous ideas and problems that MaGIC addresses are of fundamental importance and collectively represent an interesting approach to simply mimicking nature, connecting biological components to abiotic materials in order to understand the mechanisms of the biological systems or to take advantage of the unique properties of the ‘non-biological’ components in a natural setting (in vivo and in vitro). The stepwise approach, starting from the use of the nanomaterials for observing the surrounding environment (cell imaging), and proceeding to their assembly in functional architectures, culminates in the realization of special interfaces with the ambition to realize and study cell-to-cell communication.","1800270","2010-07-01","2015-06-30"
"MaGic","The Materials Genome in Action","Berend Smit","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","It is now possible to make an enormous spectrum of different, novel nanoporous materials simply by changing the building blocks in the synthesis of Metal Organic Frameworks (MOF) or related materials. This unique chemical tunability allows us to tailor-make materials that are optimal for a given application. The promise of finding just the right material seems remote however: because of practical limitations we can only ever synthesize, characterize, and test a tiny fraction of all possible materials. To take full advantage of this development, therefore, we need to develop alternative techniques, collectively referred to as Materials Genomics, to rapidly screen large numbers of materials and obtain fundamental insights into the chemical nature of the ideal material for a given application. The PI will tackle the challenge and promise posed by this unprecedented chemical tunability through the development of a multi-scale computational approach, which aims to reliably predict the performance of novel materials before synthesis. We will develop methodologies to generate libraries of representative sets of synthesizable hypothetical materials and perform large-scale screening of these libraries. These studies should give us fundamental insights into the common molecular features of the top-performing materials. The methods developed will be combined into an open access infrastructure in which our hypothetical materials are publicly accessible for data mining and big-data analysis. The project is organized in three Work Packages, each centered around finding better materials for carbon capture: (1) screen materials for gas separations and develop the tools to predict the best materials for carbon capture; (2) gain insights into and develop a computational methodology for screening the mechanical properties of nanoporous materials; (3) achieve an understanding of the amine-CO2 chemistry in diamine-appended MOFs and use this to predict their performance.","2486720","2015-11-01","2020-10-31"
"MAGICAL","CMOS/magnetoelectronic Integrated Circuits wil Multifunctional Capabilities","Bernard Dieny","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","Spin Transfer Torque Magnetic memories (STT-MRAM) are receiving a growing R&D effort within the microelectronic industry aiming at the replacement of DRAM or SRAM at sub-20nm nodes. 
MAGICAL seeks to significantly innovate through groundbreaking advances in ultra-low power multifunctional systems based on hybrid CMOS/magnetic technology. With the development of portable electronics and of the Internet of Things (IOT), more and more functions must be embedded on chips: logic/memory, sensing, communication, etc. The current hurdles with today's technology are power consumption, communication bandwidth, processing/ packaging costs. MAGICAL will demonstrate that these limitations can be largely overcome through hybrid CMOS/magnetic technology.
The project will follow three main goals:
- Firstly, we will strengthen the STT-MRAM technology by investigating two novel ideas aiming at solving two remaining difficulties in sub-20nm STT-MRAM development: the nanostructuration of magnetic tunnel junctions and the long-term data retention. This will open the path to high density (>Gbit) STT-MRAM.
-Secondly, we will demonstrate that Digital, analog (3D magnetic field sensing for orientation sensor), RF communication functions can be realized with the same baseline technology as the one developed for STT-MRAM. As a result, these three types of functions can be homogeneously integrated in a single chip, a major improvement compared to conventional heterogeneous integration. The prime benefits expected from MAGICAL are: ultralow power thanks to MRAM non volatility and on-chip computation capability, greatly improved communication functionalities (cloud as well as intrachip communication), reduced process/packaging costs. 
-Thirdly, through various actions, MAGICAL will aim at narrowing the cultural gap that still exists between magnetism and microelectronics communities.
The project could definitely help the European microelectronic systems industry improve its leadership position.","2500000","2015-11-01","2020-10-31"
"MAGREPS","High-resolution tweezers for DNA replication and sequence identification","Vincent,jean,marie,christian Croquette","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","We propose to investigate the enzymes responsible for DNA replication and repair in micromanipulation experiments with a resolution of a single base. The detailed mechanism by which DNA is synthesized base after base and the coordination of the enzymes involved in this process are not fully understood. We shall develop new magnetic tweezers using lithographic techniques associated with evanescent field detection to address these issues. We shall build arrays of these devices working in parallel, each one on a single DNA molecule and where the measurement of its extension reveals enzymatic activity. The DNA molecule in these devices will form a hairpin the opening of which can be detected with a single base resolution.

We will study the different enzymes involved in DNA replication. Firstly, we wish to follow in real time the incorporation of bases one by one by a DNA-polymerase and investigate the proof-reading mechanism of this enzyme. We shall also investigate the translocation mechanisms of different helicases involved in DNA replication and repair. Finally, we plan to study the cooperative action between different enzymes involved in the replication machinery with the help of parallelized micro-tweezers: the coupling between helicase and primase in the lagging strand synthesis, the coupling between the helicase and polymerase during leading strand synthesis and the coordination between leading and lagging strand synthesis.

Moreover observing a DNA-polymerase at the single base level is the first step of a DNA sequencing method. Preliminary experiments demonstrate that the unzipping assay is a new way to determine the position of a small DNA sequence with single base resolution. We shall investigate different experimental schemes to achieve this goal.","2193566","2011-09-01","2016-08-31"
"MAINCHEM","Non-Classical Main Group Chemistry; Supramolecular Chemistry and Catalysis","Dominic Wright","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The classical view of the periodic table suggests, in particular, that there are distinct boundaries between the chemistries of s-block, p-block and d-block elements stemming from the availability and type of valence orbitals present and fundamental properties such as electronegativity. Yet this long-excepted, text-book view can provide a barrier to progress in a number of key chemical areas because it prevents us thinking about the true picture, that there is more commonly a continuum of structural and reactivity properties which overlap large segments of the periodic table. This proposal seeks to move through these barriers by establishing fundamental and practical applications of p-block element chemistry in supramolecular chemistry (the classical domain of carbon chemistry) and catalysis (the classical domain of transition metals).The broad theme of the proposed project is to study non-classical aspects of main group chemistry. This takes the form of two major components which span the non-metallic and metallic areas of the p-block, (i) the development of systematic approaches for the building of macromolecular inorganic systems and their application in host-guest, gas storage and separation, and (ii) the applications of p-block metals in a broad spectrum of stoichiometric and catalytic bond-forming reactions.","1485429","2012-04-01","2017-03-31"
"MajoranaTopIn","Majorana Fermions in Topological Insulator Platforms","Yoichi ANDO","UNIVERSITAET ZU KOELN","Majorana fermions were recently discovered in topological superconductors as exotic quasiparticles having the curious property of being their own antiparticles. They are not only interesting as novel relativistic quasiparticles, but are also useful for realizing fault-tolerant quantum computers. However, currently available platforms to materialize Majorana fermions are limited, and the existing platforms have respective drawbacks for actually building qubits for a scalable quantum computer. Also, various unusual properties are predicted for Majorana fermions, but few have been experimentally addressed. To make a leap in the Majorana-fermion research which is technically highly demanding, one needs to grow state-of-the-art materials and tightly combine them with mesoscopic device research. By performing such an integrated research efforts in the same laboratory, this project aims to explore new platforms for Majorana qubits and to establish new methodologies to address peculiar physics of Majorana fermions. As new platforms, we pursue (i) three-dimensional topological-insulator nanoribbons and (ii) ferromagnetic topological-insulator thin films, both of which will be proximity-coupled to an s-wave superconductor. Each of them allows for conceiving Majorana qubits based on different principles, which will be tested in this project. Also, by developing new methodologies, we will elucidate (i) non-Abelian statistics probed by interferometry and (ii) quantized/universal heat transport phenomena probed by thermal conductance. These works will be complemented by materials growth efforts involving molecular beam epitaxy and detailed characterizations of the local electronic states using scanning tunnelling spectroscopy. If successful, this project will not only contribute to the realization of scalable quantum computers, but also elucidate the non-Abelian statistics, which is a fundamentally new property of particles and is ground breaking in physics.","2406250","2017-05-01","2022-04-30"
"MALADY","MACROSCOPIC LAWS AND DYNAMICAL SYSTEMS","Carlangelo Liverani","UNIVERSITA DEGLI STUDI DI ROMA TOR VERGATA","Physics provides descriptions of the world at many different scales, yet the relations between such descriptions are poorly understood. In particular, since Boltzmann and Einstein, we interpret the world we see as the product of the microscopic dynamics of a large number of atoms. In spite of this, no satisfactory rigorous derivation of a macroscopic equation (e.g. the heat equation) from such a microscopic physical model exists. This sorry state of affairs is extremely unsatisfactory both from the theoretical point of view and for applications. Indeed, as the technology is entering the mesoscopic scale (nanotechnology), the need for a rigorous understanding of how the phenomenological macroscopic laws emerge and of their limits of validity becomes paramount. We believe that recent advances in the theory of Dynamical Systems and Probability, to which the members of our team have contributed, allow key progresses in the understanding of the above problem. The ultimate goal of this proposal is the derivation of macroscopic evolution laws from a microscopic Hamiltonian evolution. To this end we will consider a series of intermediate models: a) inspired to an anharmonic chain with some noise (of a fixed strength and not itself responsible for the changes in the local energy); b) inspired to hard spheres interacting via elastic collisions and confined by fixed periodic obstacles (gas of geometrically constrained hard spheres). The above project entails the solution of major problems in the fields of Dynamical Systems and Probability. In addition, it would contribute to substantiate Boltzmann&apos;s theoretical picture by providing a conclusive rigorous example of non-equilibrium macroscopic behavior arising from an (interacting) microscopic mechanical model.","1372720","2010-04-01","2015-07-31"
"MAMBA","Molecular mechanism of amyloid β aggregation","Sara Elisabet Snogerup Linse","LUNDS UNIVERSITET","Generation of toxic oligomers during aggregation of amyloid beta peptide (Abeta42) into amyloid fibrils is a central event in Alzheimer disease. Understanding the aggregation process is therefore one important step towards therapy and diagnosis of the disease. We propose a physical chemistry approach with the goal of finding the molecular mechanisms behind the process in terms of the underlying microscopic steps and the molecular driving forces governing each step. We will use methodology developed recently in our laboratory yielding unprecedented reproducibility in the kinetic data. The methodology relies on optimization of every step from production and purification to isolation of highly pure monomeric peptide, and inertness and minimized area of all surfaces. We will use cell viability studies to detect toxic oligomeric species, and selective radio-labeling experiments to pinpoint the origin of those species. In order to obtain insight into the molecular determinants and the relative role of different kinds of intermolecular interactions for each microscopic step, we will study the concentration dependent aggregation kinetics as a function of extrinsic and intrinsic parameters. Extrinsic parameters include temperature, salt, pH, biological membranes, other proteins, and low and high Mw inhibitors. Intrinsic parameters include point mutations and sequence extension/truncation. We will perform detailed kinetic studies for each inhibitor to learn which step in the process is inhibited coupled to cell toxicity assays to learn whether the generation of toxic oligomers is limited. We will use spectroscopic techniques, dynamic light scattering, cryogenic transmission electron microscopy and mass spectrometry coupled to HD exchange to learn about structural transitions as a function of process progression under different conditions to favor different microscopic steps. The results may lead to improved diagnostics and therapeutics of Alzheimer disease.","2499920","2014-02-01","2019-01-31"
"MAMSIE","Mixing and Angular Momentum tranSport of massIvE stars","Conny Aerts","KATHOLIEKE UNIVERSITEIT LEUVEN","With the CoRoT & Kepler data analysed, the time is optimal to move from observational asteroseismology to innovative stellar modelling of the steal factories of the Universe. With MAMSIE, we follow the footsteps of helioseismologists some 30 years after them, but this time we shall be developing inversion methods for stellar structure based on gravity-mode oscillations that probe the deep stellar interior. MAMSIE will lead to new models for a variety of single and binary stars with masses between 3 and 30 M⊙ whose space photometry and high-resolution spectroscopy reveal sufficient seismic information on their gravity modes to invert the frequencies and compute the stars’ structure. In contrast to the conventional theoretical approach to stellar evolution, the data-driven approach of MAMSIE will allow us to include angular momentum transport due to internal gravity waves, as well as mixing prescriptions for turbulent entrainment, from coupling of the output of 3D hydrodynamical simulations of these phenomena to specialised seismic observables of relevance for massive stars. Our sample includes slow and fast rotators, with and without a magnetic field, with and without a stellar wind. The new models will be placed in an evolutionary context for optimal assessment of the evolution of internal rotation, angular momentum, and chemical mixing throughout stellar life of massive stars. The output of the stellar modelling will provide fundamentals for all topics in modern astrophysics that rely on massive star models. MAMSIE is overarching and will require a multidisciplinary team led by an expert in gravity-mode oscillations working in close collaboration with a 3D hydrodynamics expert; it will offer a highly competitive environment for PhD and postdoctoral research on the astrophysics of massive stars.","2498941","2016-01-01","2020-12-31"
"MASC","MASC: Materials that Impose Architecture within Stem Cell Populations","Kevin Morris Shakesheff","THE UNIVERSITY OF NOTTINGHAM","This proposal aims to harness breakthroughs in polymer science, nanotechnology and materials processing to create new classes of materials that mimic the architecture of the human body. The materials will be exploited to tackle grand challenges in  stem cell science and in the development of new biomaterials that promote regeneration.  The human body uses materials to impose architecture on populations of cells within developing or regenerating tissues. Architectural components of these tissues include three-dimensional spatial and temporal patterns of growth factors, spatial arrangements of multiple cell types and modulation of local elasticity. Orchestration of these architectural features is essential in the precise control of stem cell differentiation and tissue morphogenesis in vivo. This ERC Grant will create new classes of biomaterials that bridge the gap between the exquisite control of architecture in the developing human body and the crude structure imposed on cell populations in vitro during cell culture and biomaterials-assisted tissue repair. The research programme is organised into 2 major strands: TOOLS and DEMONSTRATORS.  Within TOOLS, new materials and techniques will be invented that represent a step-change in our ability to impose architecture on stem cell populations in vitro. Within DEMONSTRATORS, 3 grand challenges in healthcare and stem cell science will be addressed through demonstrations that synthetic materials can be designed to match the architecture of our developing bodies. This interdisciplinary project will be undertaken by a team of interdisciplinary scientists within the Wolfson Centre for Stem Cells Tissue Engineering and Modelling (STEM). To undertake this research project help from collaborators across Europe is required. Existing and new collaborations will ensure that the most advanced materials science and stem cell biology is exploited to create world leading tools that radically change regenerative medicine.","2290857","2010-01-01","2014-12-31"
"MassQ","Massive-Object Quantum Physics","Roman Schnabel","UNIVERSITAET HAMBURG","The world of quantum physics is usually associated with the microscopic cosmos of atoms and photons. In principle, but so far without any demonstration, even heavy objects can exhibit the distinguished properties of quantum world particles. In 1935, Einstein, Podolsky and Rosen (EPR) challenged a particular prediction of quantum theory saying that two particles can exist in a so-called entangled state in which the two particles do not have individually defined (‘local’) positions and momenta. Most interestingly, the existence of entangled states was subsequently fully confirmed in experiments with photons and atoms.
The new project MassQ aims to test and to confirm quantum theory in the macroscopic world of massive, human-world sized objects by realizing an EPR entanglement experiment with heavy mirrors. Two kg-sized mirrors will be cooled to low temperature and their centre of mass motion driven by radiation pressure of intense laser light in such a way that the mirrors will lose their individually defined positions and momenta. As a result, their joint motion will form a unified massive quantum object.
This project will realize a fundamental test of quantum theory in the so far unexplored regime of human-world sized objects. Recent advances in gravitational wave detector research and in opto-mechanics make this project feasible. The vision of this project points even further into the future. This project aims to lay the basis for a completely new class of physics experiments. Mirrors with kilogram masses have a proper gravitational field and cause a space-time curvature in their vicinity. This way, in principle, the dynamics of two heavy entangled mirrors need to be described not only by quantum theory but also by general relativity. Today it is completely unclear what the results of such a new class of physics experiments will be. Undoubtedly, they are important to illuminate the deep connection between the two most successful theories in physics.","1566210","2014-02-01","2019-01-31"
"MASSTEV","Mass hierarchy and particle physics at the TeV scale","Ignatios Antoniadis","EUROPEAN ORGANIZATION FOR NUCLEAR RESEARCH","The research goal of this proposal is the investigation of the most fundamental aspects of particle physics models and gravity at high energies, and establishing the connection between these findings and experiments. The main fundamental questions that will be addressed are: What is the origin of mass for the mediators of the weak interactions and its connection with the masses of quarks and leptons? Why this mass is hierarchically different from the Planck scale which makes gravity so weak compared to the other three known fundamental interactions described by the current Standard Model of particle physics? Why this enormous mass hierarchy is quantum mechanically stable? What is the theory that describes physical laws at TeV energies which will be explored in the near future by the Large Hadron Collider at CERN? These questions are at the very frontier of knowledge of theoretical particle physics and phenomenology and their intersection with gravity and string theory.   All members of the proposed research team have made breakthrough contributions in putting forward and developing new ideas that dominated such a research during the past 10 years. Although there is a certain overlap in the interests, each member brings a different unique expertise to the research, which will strongly resonate with the other members activity. Obviously, this project is strongly correlated with LHC physics confronting theoretical predictions with observations and using experimental data for building new theories and correcting existing models. In such an intense dynamical process, participation of doctoral students and postdoctoral researchers will be absolutely crucial and their active involvement is an essential component of the project. The main funding required by the project from the EU is for hiring of 14 person-years of PhD students and 14 person-years of postdocs.","1999992","2008-12-01","2014-08-31"
"MASTER","Mastering the Computational Challenges in Numerical Modeling and Optimum Design of CNT Reinforced Composites","Papadrakakis","NATIONAL TECHNICAL UNIVERSITY OF ATHENS - NTUA","The innovative and challenging objective of the MASTER project is the numerical modeling and optimum design of complex carbon nanotube (CNT)-reinforced composite morphologies, via a novel and computationally efficient molecular mechanics-based, multiscale stochastic numerical simulation approach, in conjunction with a robust optimization methodology. The rationale of the project is to propose a generic approach for an accurate numerical modeling, efficient analysis and robust design considering uncertainties, of high performance CNT-reinforced composites, in terms of mechanical and damping properties, which could have far reaching implications in the design of current as well as future nano-scale reinforced composites. The above undertaking is confronted with the excessive computational effort required to achieve the proposed objective. This computational effort will be mastered with highly efficient multiscale simulation approaches, innovative numerical solution methods, metaheuristic optimization algorithms, soft computing tools and the exploitation of the recent advances in high performance computing technology. The project has a multidisciplinary dimension by combining various scientific fields such as: molecular mechanics; continuum mechanics; stochastic mechanics; optimization; numerical analysis; soft computing; nanotechnology; material science and computer technology. The achievements of this project are expected to significantly enhance our knowledge on the analysis and design of nanocomposites beyond the current state of the art.","2496000","2012-03-01","2018-02-28"
"MASTRUMAT","The Mathematics of the Structure of Matter","Jan Philip Solovej","KOBENHAVNS UNIVERSITET","""The objective of the proposed research in mathematical physics is to study the quantum theory of matter from a mathematical perspective.  We will consider systems ranging from the tiny scale of atoms over macroscopic everyday matter to the gigantic scale of stars. Despite the range in scale, these systems may all be described by many-body quantum physics. Our aim is to rigorously understand their stability and structure, in particular, exotic phenomena such as Bose-Einstein condensation, superconductivity, superfluidity, and special low-dimensional behavior. The ultimate goal is to understand the full many-body theory but we will also investigate simpler approximate models such as Bogolubov’s model for superfluidity, Bardeen-Cooper-Schrieffer (BCS) and Ginzburg-Landau (GL) models of superconductivity, and the Hartree-Fock model of atoms and molecules. The role of such models is two-fold. On one hand they are of independent interest. On the other hand and more importantly they may give information about the many-body theory. This is true to the extent we can estimate the degree to which they approximate in appropriate limits.
From a mathematical point of view our approach is variational. All the theories we consider are formulated in terms of energy functionals. The full many-body theories are linear but due to the essentially uncontrolled number of variables they are extremely complicated. The approximate models are non-linear. Surprisingly they are nevertheless significantly simpler due to the reduction in degrees of freedom. Often the limits correspond to semiclassical limits for spectral problems.
Examples of specific goals that we will pursue are
*Establish the thermodynamic properties of matter coupled to electromagnetic fields
*Derive GL theory as a limit of BCS theory
*Estimate ionization energies of large atoms and molecules
*Derive spectral estimates of Lieb-Thirring type for 2D anyons
*Derive semiclassical expansions for problems with low regularity""","1465016","2013-04-01","2018-03-31"
"MATFUN","Functions of Matrices: Theory and Computation","Nicholas John Higham","THE UNIVERSITY OF MANCHESTER","Functions of matrices are widely used in science, engineering and the social sciences, due to the succinct and insightful way they allow problems to be formulated and solutions to be expressed. New applications involving matrix functions are regularly being found, ranging from small but difficult problems in medicine to huge, sparse systems arising in the solution of partial differential equations. The objective of this research is to make breakthroughs in theory and algorithms that will have a major impact on applications that employ matrix functions. Productive lines of enquiry and novel methodological approaches have been identified across the spectrum of the subject. In the theory, significant advances on nonprimary functions, structured matrices, and nonnormality will be obtained. New and improved algorithms for evaluating of a variety of functions as well as their Fréchet derivatives and condition numbers will be developed. For the key problem of computing the action of a matrix function on a vector a highly promising, novel approach for the matrix exponential will be developed and applied within exponential integrators. Massively parallel machines will be targeted by developing asynchronous algorithms for matrix functions, which promise a step change in scalability to very large numbers of processors and thereby significant impact on large scale applications in computational science and engineering.

This research programme is ground-breaking in going beyond the state of the art across the whole range of matrix functions research from theory to software. The expected impact is high both within the field and in the many applications areas that will benefit from the new and improved algorithms.","2069120","2011-03-01","2016-02-29"
"MATHCARD","Mathematical Modelling and Simulation of the Cardiovascular System","Alfio Quarteroni","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","This research project aims at the development, analysis and computer implementation of mathematical models of the cardiovascular system. Our goal is to describe and simulate the anatomic structure and the physiological response of the human cardiovascular system in healthy or diseased states.  This demands to address many fundamental issues. Blood flow interacts both mechanically and chemically with the vessel walls and tissue, giving rise to complex fluid-structure interaction problems. The mathematical analysis of these problems is complicated and the related numerical analysis difficult.  We propose to extend the recently achieved results on blood flow simulations by directing our analysis in several new directions. Our goal is to encompass aspects of metabolic regulation, micro-circulation, the electrical and mechanical activity of the heart, and their interactions. Modelling and optimisation of drugs delivery in clinical diseases will be addressed as well. This requires the understanding of transport, diffusion and reaction processes within the blood and organs of the body.  The emphasis of this project will be put on mathematical modelling, numerical analysis, algorithm implementation, computational efficiency, validation and verification. Our purpose is to set up a mathematical simulation platform eventually leading to the improvement of vascular diseases diagnosis, setting up of surgical planning, and cure of inflammatory processes in the circulatory system.   This platform might also help physicians to construct and evaluate combined anatomic/physiological models to predict the outcome of alternative treatment plans for individual patients.","1810992","2009-01-01","2014-06-30"
"MATHEF","Mathematical Thermodynamics of Fluids","Eduard Feireisl","MATEMATICKY USTAV AV CR V.V.I.","""The main goal of the present research proposal is to build up a general mathematical theory describing the motion of a compressible, viscous, and heat conductive fluid. Our approach is based on the concept of generalized (weak) solutions satisfying the basic physical principles of balance of mass, momentum, and energy. The energy balance is expressed in terms of a variant of entropy inequality supplemented with an integral identity for the total energy balance.

We propose to identify a class of suitable weak solutions, where admissibility is based on a direct application of the principle of maximal entropy production compatible with Second law of thermodynamics. Stability of the solution family will be investigated by the method of relative entropies constructed on the basis of certain thermodynamics potentials as ballistic free energy.

The new solution framework will be applied to multiscale problems, where several characteristic scales become small or extremely large. We focus on mutual interaction of scales during this process and identify the asymptotic behavior of the quantities that are filtered out in the singular limits. We also propose to study the influence of the geometry of the underlying physical space that may change in the course of the limit process. In particular, problems arising in homogenization and optimal shape design in combination with various singular limits are taken into account.

The abstract approximate scheme used in the existence theory will be adapted in order to develop adequate numerical methods. We study stability and convergence of these methods using the tools developed in the abstract part, in particular, the relative entropies.""","726320","2013-05-01","2018-04-30"
"MATHFOR","Formalization of Constructive Mathematics","Thierry Coquand","GOETEBORGS UNIVERSITET","The general theme is to explore the connections between reasoning and computations in mathematics. There are two main research directions. The first research direction is a refomulation of Hilbert&apos;s program, using ideas from formal, or pointfree topology. We have shown, with multiple examples, that this allows a partial realization of this program in commutative algebra, and a new way to formulate constructive mathematics. The second research direction explores the computational content using type theory and the Curry-Howard correspondence between proofs and programs. Type theory allows us to represent constructive mathematics in a formal way, and provides key insight for the design of proof systems helping in the analysis of the logical structure of mathematical proofs. The interest of this program is well illustrated by the recent work of G. Gonthier on the formalization of the 4 color theorem.","1912288","2010-04-01","2015-03-31"
"MATRICS","Modern Approaches to Temperature Reconstructions in polar Ice Cores","Hubertus Fischer","UNIVERSITAET BERN","The recent anthropogenic global warming makes a detailed knowledge of variations in the Earth climate system and of the coupling processes between climate and biogeochemical cycles of pressing importance. Studies of climate changes in the past represent a vital part of climate change research which is essential to assess the current warming against the background of natural climate variability. Due to strong limitations in direct observations, climate reconstructions for the past can only be achieved using natural climate archives. The paleoclimatic archive in ice cores provides not only information on climate variability over many thousands of years in high resolution but also on greenhouse gases, aerosol concentrations and more. Crucial questions on climate variability on interannual to orbital time scales and on the coupling processes and teleconnections in the climate system remain still open. To answer these questions novel climate parameters on polar ice cores are needed that go beyond previous studies in terms of temporal resolution, spatial coverage as well as quantitative representativeness. This proposal intends to develop such methods based on latest advances in analytical techniques and to apply them to polar ice cores. The common theme of the new approaches within MATRICs is the reconstruction of new, quantitative temperature information from different regions of the Earth all on the same core avoiding crucial crossdating issues. This comprises (i) continuous quantitative reconstructions of local temperature changes on polar ice sheets in seasonal resolution using new approaches, (ii) estimates of climate changes in continental, not permanently ice covered regions based on concurrent changes in the methane cycle and (iii) a new physical ice core gas thermometer for mean global ocean temperature. Successful implementation of the studies in MATRICs will make a significant contribution to maintain the world leading position of European ice core science.","2100000","2009-01-01","2014-12-31"
"MATRIX","MAchine for Time Reversal and Immersive wave eXperimentation","Johan Olof Anders ROBERTSSON","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","There is a need for a radically new laboratory experimental approach for studying the interaction of seismic waves with the complex media of the Earth’s subsurface. We present here a fundamental new approach to seismic wave experimentation that involves fully immersing a physical seismic experiment within a virtual numerical environment. This enormously challenging endeavour, which is relevant to many outstanding issues in seismology, has not been previously attempted. By continuously varying the output of numerous transponders closely spaced around the physical domain using a control algorithm that takes advantage of measurements made by a scanning Laser-Doppler Vibrometer and a novel theory of exact boundary conditions, waves travelling between the physical and numerical domains will seamlessly propagate back and forth between the two domains without being affected by reflections at the boundaries between the two domains. This will allow us to investigate diverse types of Earth materials using frequencies that are much closer to those of seismic waves propagating through the Earth than previously possible.

The novel laboratory enables experimentation under highly controlled conditions. A broad range of long-standing problems in wave propagation and imaging that have eluded Earth scientists and physicists for decades will be addressed. Fine scale heterogeneity, porosity and fluid saturation in real Earth media result in complex frequency-dependent amplitude and phase responses that we can characterize in the laboratory. Synthetically produced complex models can be used in wavefield-focussing experiments and to achieve complete elastic time-reversal for the first time ever. We will study coda waves that can be indicative of slight changes in stress fields before catastrophic fracturing and that might provide pre-cursory signs of earthquakes. Finally, the laboratory is highly relevant to applications such as non-destructive testing, medical imaging and lithotripsy.","3498330","2016-12-01","2021-11-30"
"MCATNNLO","High Precision Simulation of particle collisions at the LHC","Edward William Nigel Glover","UNIVERSITY OF DURHAM","The recent discovery of a Higgs-boson like resonance at the Large Hadron Collider (LHC) at CERN is a major landmark in the quest to understand the fundamental nature of the Universe. Precise measurements of the properties of the new boson are now mandatory and must be reflected in a similar quest for higher precision from the theory side. We aim to meet this challenge by developing a theoretical framework together with suitable high-precision tools that will guarantee the continued success of the LHC programme.

The aim of this proposal is therefore to develop and establish a new standard of theoretical precision in the description of physical observables at the LHC and other particle collider experiments, thereby leading to a more precise extraction of fundamental physics parameters, such as the couplings of the Higgs boson to other fundamental particles.   The necessary theoretical precision will be achieved by systematically including the next-to-next-to leading order (NNLO) corrections in the perturbative expansion in the relevant simulation tools, focusing on crucial
experimental benchmark processes.

The techniques and frameworks we will develop will be applicable to other processes and in particular, will be very relevant to searches for physics beyond the Standard Model, and in the further interpretation of any signals that would indicate such a discovery.

To achieve this ambitious goal, the PI will work very closely with a team of carefully chosen scientists with relevant overlapping and complementary expertise in precision calculations and event simulation: Professor Dr Thomas Gehrmann (University of Zürich), Professor Dr Aude Gehrmann-De Ridder (ETH Zürich) and Dr Frank Krauss (Durham University).","1941144","2014-03-01","2019-02-28"
"MCC","Mapping the Complexity of Counting","Leslie Ann Goldberg","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","""Just as advances in engineering rely on a deep foundational knowledge of physics, so too advances in algorithms and programming rely on a deep knowledge of the intrinsic nature of computation. Significant progress has been made, particularly in the field of computational complexity, which aims to discover which computational problems are feasible, which are inherently infeasible, and why. However, huge theoretical challenges remain: many problem classes are poorly understood, including those containing problems arising in practical applications. The MCC project will enable significant computational advances in a host of application areas through the development of a comprehensive theory of counting problems. These problems, which involve the computation of weighted sums, are common and important, arising in practical applications from diverse fields including statistics, statistical physics, information theory, coding, and machine learning. Thus, it is of fundamental importance to understand their complexity. We propose a coherent and systematic study of the complexity of counting problems. A sequence of exciting recent developments, pioneered by the PI and others, makes it plausible that we now have the tools needed to make substantial progress. The overall objectives of MCC are (1) Map out the landscape of computational counting problems (exact and approximate), determining which problems are tractable, and which are intractable (quantifying the extent of tractability), and (2) Develop complexity characterisations elucidating the features that make counting problems tractable or intractable, not only discovering which problems are tractable, but also discovering a characterisation telling us why. The project encompasses a large range of wide-open problems. However, the strength of the PI, the scale of the project, the timeliness of the project (given recent results), and the novel methods proposed make it certain that the research will produce breakthroughs.""","2499093","2014-03-01","2019-02-28"
"MCSK","""Moduli of curves, sheaves, and K3 surfaces""","Rahul Vijay Pandharipande","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","""Algebraic geometry is the study of varieties -- the zero sets of polynomial equations in several variables. The subject has a central role in mathematics with connections to number theory, representation theory, and topology. Moduli questions in algebraic geometry concern the behavior of varieties as the coefficients of the defining polynomials vary. At the end of the 20th century several fundamental links between the algebraic geometry of moduli spaces and path integrals in quantum field theory were made. I propose to study the moduli spaces of curves, sheaves, and K3 surfaces. While these moduli problems have independent roots, striking new relationships between them have been found in the past decade. I will exploit the new perspectives to attack central questions concerning the algebra of tautological classes on the moduli spaces of curves, the structure of Gromov-Witten and Donaldson-Thomas invariants of 3-folds including correspondences and Virasoro constraints, the modular properties of the invariants of K3 surfaces, and the Noether-Lefschetz loci of the moduli of K3 surfaces.  The proposed approach to these questions uses a mix of new geometries and new techniques. The new geometries include the moduli spaces of stable quotients and stable pairs  introduced in the past few years. The new techniques involve a combination of virtual localization, degeneration, and descendent  methods together with new ideas from log geometry. The directions discussed here  are fundamental to the understanding of moduli spaces in mathematics and their interplay with topology,
string theory, and classical algebraic geometry.""","2167997","2013-05-01","2018-04-30"
"MDDS","Mechanism Design for Data Science","Moshe TENNENHOLTZ","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","The way data science algorithms and techniques, central to the Internet and on-line media, are designed need to be revolutionized. Current designs ignore participants' strategic incentives. Our vision is the establishment of an entirely new repertoire  of incentive-compatible data science algorithms and techniques, obtained through pioneering the application of game-theoretic mechanism design for data science.  

Game theory is the branch of mathematics dealing with the modeling and analysis of  multi-agent interactions.  
 Mechanism design is the part of game theory that deals with the design of protocols/algorithms for environments consisting of self-motivated participants. Mechanism design has been central to bridging computer science and  game theory. It has been widely applied to electronic commerce, advertising and routing networks, and led to significant contributions. 
On the other hand, data science is flowering, with major applications in search and information retrieval, on-line recommendation systems, clustering and segmentation, and social networks analysis. 
Quite surprisingly, although the incentives of publishers/firms/customers in such data science contexts are of great importance, mechanism design in the related settings has been almost completely neglected.  
The proposal aims at building theoretical foundations, providing algorithms, as well as validating through experiments, a fundamental bridge between mechanism design and data science. The ultimate success of this research would be the replacement of classical relevance ranking, segmentation, on-line explore \& exploit, and influencers' detection  algorithms by incentive-compatible ones, creating the next generation of data science algorithms..","2493705","2017-07-01","2022-06-30"
"MEC","Macroscopic Entanglement in Crystals","Nicolas Robert Gisin","UNIVERSITE DE GENEVE","""Quantum theory is often presented as the theory of the microscopic world. However, over the last decade, things have changed dramatically. Today one can envision manipulating large quantum systems, while mastering individual degrees of freedom. It is thus timely to ask entirely new questions on the quantum/classical transition and to support these by experimental investigations of large entanglement. The vision of this project is to explore conceptually, experimentally and technologically the limits of large entanglement of macroscopically distinguishable quantum states. We propose to demonstrate entanglement between two or more macroscopic crystals with hundreds of entanglement bits (e-bits), hundred of thousands of excitations and billions of ions. For this purpose, we’ll use Neodymium doped YSO crystals and our AFC (Atomic Frequency Comb) protocol for multimode quantum memories.

Since entanglement is the signature of quantumness, this project will demonstrate “macroscopic” entanglement and help sharpening the meaning of “large” and “macroscopic”. By combining theory and experiments, questions like “What is large entanglement?” and “What deserves to be called entanglement between macroscopic systems?” will receive new insightful answers. This will deepen our understanding of Nature and in particular of the intricate meaning of meso- and macroscopic.

Two further goals of this project are to demonstrate large entanglement between two crystals separated by tens of kilometres, and to teleport many qubits stored in a crystal to a distant one.

Although this project is on fundamental questions, it will also contribute to improving the quantum memories required for continental scale ultra-secure quantum communications. We expect other surprising applications, in particular the high sensitivity of large entanglement to various decoherence mechanisms can be turned positively into quantum sensors.""","1693500","2014-02-01","2019-01-31"
"MECCA","Meeting Challenges in Computer Architecture","Per Orvar Stenström","CHALMERS TEKNISKA HOEGSKOLA AB","""Computer technology has doubled computational performance every 24 months, over the past several decades. This performance growth rate has been an enabler for the dramatic innovation in information technology that now embraces our society. Before 2004, application developers could exploit this performance growth rate with no effort.  However, since 2004 power consumption of computer chips exceeded the allowable limits and from that point and onwards, parallel computer architectures became the norm. Currently, parallelism is completely exposed to application developers and managing it is difficult and time-consuming. This has a serious impact on software productivity that may stall progress in information technology.

Technology forecasts predict that by 2020 there will be hundreds of processors on a computer chip. Apart from managing parallelism, keeping power consumption within allowable limits will remain a key roadblock for maintaining historical performance growth rates. Power efficiency must increase by an order of magnitude in the next ten years to not limit the growth rate. Finally, computer chips are also key components in embedded controllers, where stringent timing responses are mandatory. Delivering predictable and tight response times using parallel architectures is a challenging and unsolved problem.

MECCA takes a novel, interdisciplinary and unconventional approach to address three important challenges facing computer architecture – the three Ps: Parallelism, Power, and Predictability in a unified framework. Unlike earlier, predominantly disciplinary approaches, MECCA bridges layers in computing systems from the programming language/model, to the compiler, to the run-time/OS, down to the architecture layer. This opens up for exchanging information across layers to manage parallelism and architectural resources in a
transparent way to application developers to meet challenging performance, power, and predictability requirements for future computers.""","2379822","2014-02-01","2019-01-31"
"MechAGE","In Vivo Single-Cell Mechanomics of Bone Adaptation and Regeneration in the Aging Mouse","Ralph Müller","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Osteoporosis, one of the most prevalent degenerative diseases, is characterized by a reduction in bone mass and increased fracture risk and has been partly attributed to the decrease in mechanical usage of the skeleton. A detailed understanding of the molecular mechanisms governing load-regulated bone remodeling could therefore lead to the identification of molecular targets for the development of novel therapies. Bone remodeling is a multiscale process mediated through complex interactions between multiple cell types and their local 3D environments. However, the underlying mechanisms of how cells respond to mechanical signals are still unclear. By combining single-cell “omics” technologies with well-established tissue-scale models of bone mechanobiology, MechAGE proposes to develop the technology required to allow spatially resolved in vivo single-cell mechanomics of bone adaptation and regeneration. CRISPR/Cas technology will be exploited to generate fluorescent reporter mice to identify the different cell types involved in the bone remodeling process. By combining RNA-sequencing of single cells isolated by laser-capture microdissection with micro-finite element analysis and time-lapsed in vivo micro-CT, MechAGE will link the transcriptome of hundreds of single cells to their local mechanical in vivo environment (LivE). This will allow investigation of molecular responses of the cells to LivE changes with aging in established mouse models of bone adaptation and regeneration. In addition to in vivo mechanomics, MechAGE proposes to use cellular and multiscale computational modeling to run in silico simulations of real-world events for better understanding of diseases of aging in mice and to maximize the use of the high quality in vivo mechanomic data. Findings from MechAGE will lead to a systems level understanding of the spatio-temporal regulation of gene expression during the process of load-induced bone adaptation and regeneration in the aging mouse.","2500000","2017-10-01","2022-09-30"
"MECHMAM","Multiscale Extended Computational Homogenization for the 
Mechanical design of Advanced Materials","Marc Georges Denis Geers","TECHNISCHE UNIVERSITEIT EINDHOVEN","""The bottom-up design of advanced materials with unprecedented mechanical properties is a grand challenge, requiring reliable multiscale methods. This proposal targets a novel extended multiscale computational homogenization framework, in order to make a breakthrough in lifting scale separation limits restricting existing scale bridging methods. This method enables designs using groundbreaking concepts, as used in advanced mechanical metamaterials, offering superb properties in e.g. energy absorption or harvesting, dynamic and multi-functional properties. This novel route relies on a multiscale design that exploits, rather than avoids, the complex interactions between the scales involved.

The proposed methodology is fundamentally new with a potentially large impact on many multiscale methods. The computational homogenization method will be taken as the starting point, since it is one of the most powerful multiscale methods available. To enable the anticipated breakthrough, the coarse scale description will be enriched by key characteristics of the fine scale fluctuation fields that are responsible for the breakdown of scale separation. A generalized micromorphic continuum thus emerges at the coarse scale.

The analysis of the fine scale fluctuation fields will be established in a strongly coupled numerical-experimental approach, making use of integrated image and field correlation methods. Full kinematical fields at different scales and different stages of deformation will be used. Particular attention is given to computational efficiency, by a newly developed dedicated reduced order model for the extended multiscale scheme. The added value of the novel multiscale method and its practical applicability will be demonstrated by analysing the damage-to-fracture transition in a multi-phase steel. A full proof of principle is given on the design, processing and testing of a novel nonlinear micromorphic acoustic metamaterial, taking optimally benefit of scale interactions.""","2489197","2014-03-01","2019-02-28"
"MEDIGRA","Mechanics of Energy Dissipation in Dense Granular Materials","Ioannis Vardoulakis","NATIONAL TECHNICAL UNIVERSITY OF ATHENS - NTUA","Granular materials are of interest to different fields of the physical sciences and engineering. To model their behaviour, either a solid- or fluid mechanics approach is used. Rather than deforming uniformly, granular fluids develop thin shear-bands, which mark areas of flow, material failure and energy dissipation. The MEDIGRA project proposes a thorough experimental, theoretical and numerical study of the Mechanics of Energy DIssipation in dense GRAnular materials. The fundamental challenge faced by the project is to quantify the various energy dissipation mechanisms in dense granular materials using innovative thermo-poromechanical experiments. The measured characteristics are expected to lead to the formulation of appropriate analytical and numerical tools aimed to describe the mechanical behaviour of granular materials from the rigorous angle of energetics. In particular, the project proposes to: 1) Design, develop, install and exploit a novel Thermographic High Speed Cylinder Shear Apparatus (THSCSA) to study the properties of the mechanical and thermal boundary layer that is forming at the inner rotating-drum material interface, as well as determining the required thermographic properties of granular materials. 2) Convincingly quantify the way the total energy dissipation is split into heat production, grain breakage and other mechanisms, using the project-developed THSCSA apparatus and other advanced experimental apparatuses. 3) Develop physical models and robust numerical tools capable of incorporating the experimentally obtained dissipation characteristics. 4) Test the knowledge acquired within the project in two applications (shear segregation and landslide modelling). The project aims to advance our knowledge on the basic physics behind long-standing open problems such as the “heat-flow paradox” in earthquake mechanics, the lifetime prediction of imminent catastrophic landslides and the applicability of continuum approximations to segregation phenomena.","981600","2008-11-01","2011-10-31"
"MEDYMA","Biophysical Modeling and Analysis of Dynamic Medical Images","Nicholas Ayache","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","During the past decades, exceptional progress was made with  in vivo medical  imaging technologies  to capture the anatomical,  structural and physiological properties of tissues and organs in a patient,  with an ever increasing spatial and temporal resolution.

The physician is now faced with a formidable overflow of information, especially when a  time dimension is added to the already hard to integrate 3-D spatial, multimodal and multiscale dimensions of modern medical images. This increasingly hampers the early detection and understanding of subtle image modifications which can have a vital impact on the patient's health.

To change this situation, this proposal introduces a new generation of computational models for the  simulation and analysis of  dynamic medical images. Thanks to their generative nature, they will allow the construction of databases of  synthetic, realistic medical image sequences simulating various evolving diseases, producing an invaluable new resource for training and benchmarking. Leveraging on their principled  biophysical and statistical foundations, these new models will bring a remarkable  added clinical value after they are personalized with innovative methods to fit the medical images of any  specific patient.

By explicitly revealing the underlying evolving biophysical processes observable in the images, this approach will yield  new groundbreaking image processing tools to correctly interpret the patient's condition (computer aided  diagnosis), to accurately predict the future evolution (computer aided prognosis), and to precisely simulate and monitor an optimal and personalized therapeutic strategy (computer aided therapy).  First applications will concern high impact diseases including brain tumors, Alzheimer's disease, heart failure and cardiac arrhythmia and will open new horizons in computational medical imaging.","2498327","2012-04-01","2017-03-31"
"MeerTRAP","Discovering Fast Transients and Pulsars with MeerKAT for Cosmology and to Test the Laws of Gravity","Benjamin STAPPERS","THE UNIVERSITY OF MANCHESTER","""Short duration bursts of cosmic radio emission provide us with a potent tool for studying the extremes of physics. In the form of periodic pulses from pulsars they can be used as precision instruments to test theories of gravity, understand the emission processes and the equation of state of ultra-dense matter. In the form of transient bursts we can use them to study the physics governing explosive events such as gamma-ray bursts, merging neutron stars, annihilating black holes or hitherto unknown phenomena. Observing how these bursts are affected by their passage through the distant Universe allows us to probe its physical state. To harness the combined power of periodic and bursting radio sources to explore physics beyond that possible in a terrestrial laboratory I propose MeerTRAP: Meer (more) TRAnsients and Pulsars, a project to continuously use the MeerKAT telescope to search the radio sky for pulsars and fast transients and to rapidly and accurately locate them. Utilising the excellent sensitivity and sky coverage of MeerTRAP my team will discover many rare and scientifically important pulsar types: relativistic binaries, intermittent emitters, transitioning systems. Current radio telescopes have only explored the tip of the transients """"iceberg"""" and MeerTRAP will transform our knowledge of these manifestations of extreme physics. It will detect hundreds of new bursts, which will all be well localised, allowing us to identify hosts and distances, greatly enhancing their use as cosmological probes. Localisation also enables measurement of their true fluxes, polarisation and spectral indices which are all crucial to identify their origin. To achieve this we will design, implement and exploit state-of-the-art hardware and software. We will also use the MeerLICHT optical telescope, which will track MeerKAT, to give us a crucial glimpse of the optical sky immediately before and after any radio transient to further constrain their origin and the associated physics.""","3488956","2016-10-01","2021-09-30"
"MEFUCO","Precision Measurements of Fundamental Constants","Klaus Blaum","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","The fine structure constant α, the masses and magnetic moments of elementary particles like the electron or proton, and others are fundamental quantities which determine the basic structure of the universe. They can not be predicted by theory, but their precise determination is required to enable the comparison of theoretical models with experimental observations at the highest possible level. The improvement of these quantities beyond the present level of accuracy represents a significant challenge for modern metrology. We propose ambitious experiments and measurements to substantially improve the precision of a number of fundamental constants, namely the electron and proton mass, the fine structure constant α, the magnetic moment of the proton and of the bound electron, and the Q-value of the 3H-3He decay for an improved sensitivity limit in the determination of the electron-antineutrino rest mass. Using single-ion storage in a well-defined small volume with perfectly controlled electromagnetic fields for nearly unlimited periods of time, we will measure the eigenfrequencies of the particles with unprecedented precision in dedicated Penning traps. From those values, fundamental properties such as the magnetic moment, atomic and nuclear mass or, equivalent, binding energy can be extracted. This will reveal the strength of all interactions present in the quantum mechanical system. Our new results combined with established theories will yield values for fundamental constants like the electron mass and the fine structure constant α.  In order to meet these challenges novel trap geometries, ultra-sensitive and low-noise single-ion detection techniques will be developed and combined with other technological advances in order to enable us to reach up to one order of magnitude improved values for the quantities mentioned above.","2158800","2011-12-01","2016-11-30"
"MEMBRANE","MEMS made Electron Emission Membranes","Hendrik Van Der Graaf","STICHTING NEDERLANDSE WETENSCHAPPELIJK ONDERZOEK INSTITUTEN","We propose a radically new and generic type of detector for photons, electrons and energetic charged particles: a stacked set of curved miniature dynodes in vacuum, created through MicroMechanical Electronic Systems (MEMS) fabrication techniques on top of a state-of-the-art CMOS pixel chip. This combination in itself is an extremely efficient electron detector. By capping the system with a traditional photocathode, a highly sensitive timed photon counter can be realized, outperforming all existing photon detectors. By capping it with an Electron Emission Membrane, a timed particle tracking detector is realized with a time resolution far superior to current particle counters.

The core innovation, i.e., the stacked curved dynodes on top of a pixel chip, will revolutionize electron detection in solid-state, atomic and molecular physics experiments. As a photon detector, it will have pico-second time resolution, much better than classical photomultipliers, at low noise. This will have impact on the field of medical imaging, optical communication, night-vision equipment and even 3D image recording by measuring the time-of flight of photons from a flashlight. As a particle detector, it will allow faster and higher-resolution measurements of the trajectories of fast charged particles, essential in modern particle physics experiments. Its time resolution is three orders of magnitude better than state-of-the-art Si planar detectors, opening new horizons for (vertex) tracking, time-of-flight spectrometers, track pattern recognition and trigger detectors.

The realization of this detector concept requires high-risk/high-impact developments in the area of (1) fundamental understanding of electron emission, (2) the MEMS-based fabrication of novel curved transmission dynodes and (3) high-efficiency Electron Emission Membranes. To achieve these objectives, the PI will lead a concerted effort of technical physicists and theoreticians.","2396000","2013-02-01","2018-01-31"
"MEMS 4.0","Additive Micro-Manufacturing for Plastic Micro-flectro-Mechanical-Systems","Jürgen Peter BRUGGER","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The manufacturing of silicon-based MEMS today is well advanced because the micro-electro-mechanical devices for automotive, domestic, health-care and consumer electronics can be fabricated with methods from IC industry. Polymer-based MEMS have a great potential for flexible electronics and biomedical applications, but to date, the techniques to engineer functional polymers into 3D microsystems, are still at their beginning because a coherent fabrication platform with the right tools and processes does not yet exist. The field could tremendously benefit from a coordinated effort in materials and manufacturing, in particular with a focus on biocompatible plastic materials for biomedical applications. Additive manufacturing such as 3D printing and associated processing such as sintering has already started to transform traditional industry, but is not scalable much below a micrometer because the thermal processing is done in bulk or by lasers on surfaces. MEMS 4.0, in analogy with the industry 4.0 concept, aims to perform concerted research in additive manufacturing at the micro/nanoscale and associated key techniques. Using my expertise in MEMS and Nanotechnology, MEMS 4.0 will push the frontiers in new materials and new processing for MEMS by setting a focus on stencilling, printing, self-assembly and local thermal processing. This coherent processing framework will permit the use of delicate, soft, polymer materials to engineer the next generations of plastic MEMS. We are primarily targeting biodegradable implantable MEMS and permanently implantable glassy carbon MEMS. They are the most challenging to fabricate, but if successful, they also have an enormous impact for future wearables and implantables.","2500000","2017-10-01","2022-09-30"
"MEMSforLife","Microfluidic systems for the study of living roundworms (Caenorhabditis elegans) and tissues","Martinus Adela Maria Gijs","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","This proposal situated at the interfaces of the microengineering, biological and medical fields aims to develop microfluidic chips for studying living roundworms (Caenorhabditis elegans), living cultured liver tissue slices obtained from mice, and formaldehyde/paraffin-fixed human breast cancer tissue slices and tumors. Each type of microfluidic chip will be the central component of a computer-controlled platform having syringe pumps for accurate dosing of reagents and allowing microscopic observation or other types of detection. From an application point-of-view the work is focused on five objectives: (i) Development of high-throughput worm chips. Our goal is to build worm tools that enable high-throughput lifespan and behavioral measurements at single-animal resolution with statistical relevance. (ii) Linking on-chip microparticles (beads) to the C. elegans cuticle. We will use beads with electrostatic surface charges and beads that have a magnetic core for quantification of locomotion and forces developed by the worms. Moreover high-refractive index microspheres will be used as in situ microlenses for optical nanoscopic worm imaging. (iii) Realization of a nanocalorimetric chip-based setup to determine the minute amount of heat produced by worms and comparison of the metabolic activity of wild-type worms and mutants. (iv) Study of precision-cut ex vivo liver tissue slices from mice, in particular to evaluate glucose synthesis. The slices will be perifused with nutrients and oxygen in a continuous way and glucose detection will be based on the electrochemical principle using microfabricated electrodes. (v) On-chip immunohistochemical processing and fluorescent imaging of fixed clinical tissue slices and tumorectomy samples. These systems aim the multiplexed detection of biomarkers on cancerous tissues for fast and accurate clinical diagnosis.","2492400","2013-05-01","2018-04-30"
"MEQUANO","Mesoscopic Quantum Noise: from few electron statistics to shot noise based photon detection","D. Christian Glattli","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","We propose innovative approaches to electronic quantum noise going from very fundamental topics addressing the quantum statistics of few electrons transferred through conductors to direct applications with the realization of new types of versatile broadband photon detectors based on photon-assisted shot noise. We will develop electron counting tools which will not only allow to full characterization of electron statistics but also open the way to new quantum interference experiments involving few electrons or fractional charge carriers and will question our understanding of quantum statistics. Generation of few electron bunches will be obtained by the yet never done technique of short voltage pulses whose duration is limited to few action quanta, one quantum for one electron. Detection of electron bunches will be done by an unprecedented technique of cut and probe where carriers are suddenly isolated in the circuit for further sensitive charge detection. Using highly ballistic electron nanostructures such as Graphene, III-V semiconductors with light carriers, Carbone Nanotubes or simply tunnel barriers, we will bring mesoscopic quantum noise effects to higher temperature, energy and frequency range, and thus closer to applications. Inspired by late R. Landauer s saying: the noise IS the signal we will develop totally new detectors based on the universal effect of photon-assisted electron shot noise. These versatile broadband detectors will be used either for on-chip noise detection or for photon radiation detection, possibly including imaging. They will operate above liquid Helium temperature and at THz frequencies although projected operation includes room temperature and far-infrared range as no fundamental limitation is expected. The complete program, balanced between very fundamental quantum issues and applications of quantum effects, will open routes for new quantum investigations and offer to a broad community new applications of mesoscopic effects.","1999843","2009-02-01","2015-01-31"
"MERESPO","Mechanically Responsive Polymers","Christoph Weder","UNIVERSITE DE FRIBOURG","""“Intelligent” polymers which change their properties “on command”, that is upon exposure to a pre-defined stimulus in a highly selective and reversible manner, are of considerable academic interest and attractive for countless technologically relevant applications. Many examples of chemically, thermally, electrically, optically, or electrically responsive materials are known, but only few polymers have been studied, which respond in a useful and predictable manner to the exposure of mechanical stress.
The here-proposed program targets the design, synthesis, processing, exploration and exploitation of a radically new family of bio-inspired, mechanically responsive polymers in which mechanical stress provides the activation energy to trigger specific pre-programmed chemical reactions. These reactions, in turn, will be used to bestow polymers with unusual and previously unavailable functionalities, such as mechanical morphing, mechanically induced generation of light, mechanically controlled cell growth, auto-lubricating behavior, and the ability to release small molecules such as drugs, fragrances and antiseptics.
A three-pronged research approach is proposed. Thrust 1 will investigate carefully selected model systems with the aim to advance the predictive understanding for the relationships between the chemical structure of the mechanically responsive motifs or “mechanophores”, their connectivity with a matrix polymer, the morphology and mechanical properties of the system, and the mechanoresponse. Thrust 2 focuses on the exploration of new mechanophores and mechanochemical transduction schemes. Thrust 3 will apply the knowledge generated for the creation of novel materials that offer a wide variety of new and interesting functionalities.
The knowledge generated through these efforts will provide a broad intellectual basis for the future design, of advanced functional materials based on mechanochemical transduction schemes.""","1992493","2012-05-01","2017-04-30"
"MESOLIGHT","Mesoscopic Junctions for Light Energy Harvesting and Conversion","Michael Grätzel","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Research will focus on the generation of electric power by mesoscopic solar cells, a domain where the PI has an outstanding track record and leadership on the global scale. The target is to increase the photovoltaic conversion efficiency from currently 11 to over 15 percent rendering these new solar cells very attractive for applications in large areas of photovoltaic electricity production. The approach to reach this challenging target is highly creative and has a strongly interdisciplinary character. Successful implementation of the project goals is assured by the vast experience and know how of the PI and his team in the key areas of the project. The project is divided in four work packages. The first three introduce creative new concepts to enhance substantially the performance of single-junction dye sensitized nanocrystalline devices, while the fourth addresses multi-junction cells and photon up-conversion systems. The tasks to be accomplished comprise 1) The theoretically assisted conception and synthesis of new molecular sensitizers to extend the spectral response of dye sensitized photovoltaic cells into the near IR up to 900 nm, increasing substantially the short circuit photocurrent of the solar cell. 2) The implementation of highly innovative mesoscopic oxides structures to support the molecular dye or quantum dot and collect the photo-generated charge carriers. 3) The introduction of smart amphiphilic molecular insulators and ultra-thin ceramic barriers at the mesoscopic junction in order to retard the interfacial electron-hole recombination and 4) The exploration of radically new cell embodiments based on multi-junction tandem cells and photon up-conversion schemes, whose solar to electric power conversion efficiency can be raised beyond the Shockley-Queiser limit of 32 percent.","2046000","2010-03-01","2015-02-28"
"METALLICHYDROGEN","Exploring conductive and metallic hydrogen","Mikhail Eremets","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Hydrogen under ambient pressure and low temperature forms a  molecular crystal which  under high pressure of ~400 GPa is predicted turn into metal and at further compression up to ~500 GPa hydrogen molecules dissociate and are transformed to a monoatomic crystal. This simplest metal is predicted to be a superconductor with a very high critical temperature Tc ~200 K. Moreover, this superconductor might be recovered to ambient pressure. Metallic hydrogen might acquire a new quantum state, namely the metallic superfluid and the superconducting superfluid. Because the zero-point motions of the hydrogen nuclei (protons) are significant, they might stabilize metallic hydrogen in a zero-temperature liquid ground state similar to liquid helium. For this state, superconductivity for electrons and protons (Fermi-liquids) is expected in hydrogen, and superconductivity for electrons and superfluidity for deutrons in deuterium (an isotope of hydrogen). For astrophysics the study of metallic hydrogen is important because it might be a main constituent in giant planets and stars.
We plan to explore three directions to achieve and study metallic hydrogen: (a) Compression of pure hydrogen at room and lower temperatures to record pressures of 440 GPa which we currently achieve (b) Exploration of the higher temperature domain P> 150 GPa, T<1000 K; (c) Study of hydrogen dominant materials at low pressures P>50 GPa and low temperatures. We will give first preference to compression pure hydrogen to metallic state at low temperatures   to verify the theoretical prediction in the region of ~ 400 GPa. In case this pressure would not be sufficient our study will be focused on parallel tasks hydrogen dominant materials, and fluid hydrogen.","1901600","2011-03-01","2017-02-28"
"METBIOCAT","Metal catalysis in biological habitats: New strategies for optical bio-sensing and targeted therapy","Jose Luis Mascareñas Cid","UNIVERSIDAD DE SANTIAGO DE COMPOSTELA","This proposal aims at the discovery of robust transition-metal catalyzed transformations that can take place in aqueous media and cellular lysates, and are susceptible of being exported to living cells. Specifically, we will exploit the special coordination and activation ability of different metal complexes towards pi-systems to induce chemo-selective reactions of designed, abiotic, unsaturated substrates. Moreover, and importantly, the metal catalysts will be conjugated to designed ligands or biopolymers so that the catalytic power of the metal complex can be transferred to specific “in vivo” locations. Initial designs in this latter “high risk” endeavor will be guided by the current knowledge on metal-catalyzed bio-orthogonal chemistry as well as by some precedents on catalysis-based metal-sensing tactics.
Ultimately, we want to install catalytic power in specific cellular sites and/or endow catalytic properties to any selected bio-molecular target. The catalytic activity could then be used to trigger the amplified generation of fluorescent signals or boost the production of bioactive drugs from inert, non-toxic precursors. This will set the basis for the development of efficient bio-sensing and imaging tools, and “in cellulo” diagnosis tactics, and of novel target-directed therapeutic strategies. With the crescent identification of disease-related biomarkers, the development of biomarker-associated diagnosis and therapy protocols is becoming one of the more urgent challenges in modern life sciences. Advances in early diagnosis can have a profound impact in public health, and boost new technology developments.
The transversal expertise of my group in synthesis, metal catalysis, molecular recognition and chemical biology (see PI profile) places us in a rather unique position to tackle this type of interdisciplinary project.","2356276","2014-02-01","2020-01-31"
"METIQUM","Mesoscopic THz impedance microscopy for quantum materials","Teunis Martien Klapwijk","TECHNISCHE UNIVERSITEIT DELFT","An important frontier in condensed matter physics is the understanding of quantum materials in which different ground states compete, leading to electronic inhomogeneity and the concept of ‘quantum electronic liquid crystals’. The challenge for experiments is to measure the local electrodynamic properties in materials, which are electronically inhomogeneous, but atomically homogeneous.
I propose a new technique to determine these local variations of the electronic properties. The central objective is to measure with nanometer-scale spatial resolution the frequency-dependent electrodynamic properties, such as complex dielectric constant and complex conductivity of quantum materials at frequencies in the several hundreds of GHz range. The method is derived from the recent progress in astronomical instruments for the submillimeter (hundreds of GHz to THz) frequency band. This progress, to which I contributed extensively, is driven by the desire to study the universe. Now, with this technology and expertise in hand, the disciplinary boundaries can be crossed once more and directed to the other challenging frontier of quantum materials. With this instrument it will become possible to determine the local (and possibly frequency-dependent) electromagnetic properties, such as the dielectric constant and conductivity, for a range of materials.
Through this technique, I will make it possible to study the local properties of new materials and even to get access to the local energy-scales of their excitations. It is clear that the program is ambitious and risky, but if successful it provides a major step forward in experiments to reveal the various electronic states of quantum materials and a new scanning-probe technique operating in a new frequency range.","2451266","2014-05-01","2019-04-30"
"MFECE","Magnetostrophic Flow in Experiments and the Core of the Earth","Andrew Jackson","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","We describe here an innovative strategy for understanding the so-called magnetostrophic regime of fluid flow in the Earth s core, and thus the mechanisms by which the Earth s magnetic field is sustained over time. The magnetostrophic regime is the state in which Lorentz (magnetic) forces are balanced by Coriolis (rotational) forces and pressure gradients and is thought to be the zeroth order force balance in the core. We propose a series of ground-breaking experiments using liquid sodium contained in a rapidly rotating sphere containing a differentially rotating solid inner sphere. For the first time electric current is injected into the fluid in different configurations in order that the Lorentz force is everywhere significant. Various other magnetic fields can be applied from the exterior and the interior. The influence of turbulence, viscous and magnetic boundary layers will be examined. The presence of instabilities and wave motion will be studied, and the existence of steady solutions will be naturally determined. Diagnostic measurements of magnetic fields and electrical potentials, and Doppler velocimetry will characterise the experiment. These unique experiments are backed by numerical calculations. Complementary studies will analyse the observed magnetic field over the last 400 years in the same magnetostrophic framework. An inverse method will be developed to find the initial state of the field that evolves in a manner compatible with observations. This will elucidate the interior structure of the magnetic field for the first time, determining the amplitude and morphology of the field. The importance of magnetic diffusion (Joule heating) will arise naturally, and fluid motion in the entire core will be found, allowing comparison with geodetic observations.","3116900","2010-05-01","2016-04-30"
"MiCE","Microflow in Complex Environments","Julia Mary Yeomans","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","We will study the way in which the hydrodynamics of simple, complex and active fluids is affected by their environment, in particular by patterned surfaces and by confinement. We shall concentrate on micron and nanometric length scales where surfaces are often key in controlling fluid behaviour. The work is driven by current rapid and exciting advances in fabricating micropatterned substrates and by new experimental techniques probing the flow properties of fluids at these scales. Our work will be primarily computational and theoretical, but with an experimental component within Oxford, and with close experimental links to several groups internationally.

The systems we will concentrate on are:

1. simple fluids at micropatterned substrates: We aim to understand interface pinning, particularly on anisotropic surfaces, and superhydrophobic hydrodynamics. The knowledge will be used to help design devices, such as displays and condensers that exploit fluid-surface interactions at the mesoscale.

2. complex fluids in confinement and at patterned substrates: We shall concentrate on the f-d virus as a highly monodisperse system of colloidal rods which shows lyotropic liquid crystalline ordering. A close collaboration between experiment and simulation will investigate the interplay between elasticity, surface anchoring, flow, topological defects and interface instabilities.

3. active fluids at surfaces: Our aim is to understand low Reynolds number swimming in the vicinity of rough surfaces and in confined systems such as microchannels and fluid drops. Microswimmers provide an experimentally and theoretically accessible example of non-equilibrium statistical physics and have a range of striking behaviours, including clustering, low Reynolds number turbulence and anomalous flow field statistics, that remain exciting challenges.","1583887","2012-04-01","2017-03-31"
"MICROCHEMICALIMAGING","Enhancing microfabricated devices with chemical imaging for novel chemical technology","Sergei Kazarian","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","The development of microchemical systems is one of the most exciting recently developed research topics with numerous potential industrial applications. One of the greatest challenges to encourage these systems to be adopted by industry is successful high level integration with sensors for understanding, optimisation and control of microsystems for various processes. The proposed research will develop such systems and their integration via linking them with chemical imaging. The benefits of chemical engineering at smaller lengthscales are manifold; the design of microchemical processes is important where, by nature, it is essential to have microdevices, e.g. in cell biology manipulation and transformations. Other processes can be designed macroscopically, but a move to microprocesses gives process advantages, such as enhanced heat and mass transfer, novel flow regimes, bringing material and process time and lengthscales into the same region to allow material property and process interactions, which would be impossible in macro-reactors. In order to achieve this, it is essential to have the capability of rapid 3D chemical imaging on a nano/microscale, as only by devising these new techniques to image microchemical systems, it will be possible to optimise them for novel engineering. The proposal is aimed at providing chemical imaging capability to miniaturised devices for the engineering of new materials and processes. It is proposed (i) to use chemical imaging and micro-deposition methods for the generation of materials with responsive gradient structures; (ii) to engineer nanostructured materials aided by high-resolution chemical imaging; (iii) to combine microfluidics with chemical imaging as a prototype of miniaturised chemical factories. The overall aim is to utilise the advantages of spectroscopic chemical imaging to develop novel miniaturised devices and materials that will serve as suitable platforms for future industrial users with wide applicability.","1430607","2009-01-01","2014-10-31"
"MICROLIPIDS","Microbial lipids: The three domain ‘lipid divide’ revisited","Jacobus Smede SINNINGHE DAMSTE","STICHTING NEDERLANDSE WETENSCHAPPELIJK ONDERZOEK INSTITUTEN","Tremendous progress has been made in the last decade in the genetic characterization of microorganisms, both in culture and in the environment. However, our knowledge of microbial membrane lipids, essential building blocks of the cell, has only marginally improved. This is remarkable since there exists a dichotomy in the distribution of lipids between the three Domains of Life. Diacyl glycerols based on straight-chain fatty acids are produced by bacteria and eukaryotes, whereas archaea synthesize isoprenoidal glycerol ether lipids. From a microbial evolutionary perspectives, this ‘lipid divide’ is enigmatic since it has recently become clear that eukaryotes evolved from the archaea. Preliminary results of my research group show that when novel analytical methodology is used, there is a large hidden diversity in microbial lipid composition that may resolve this fundamental question. Here I propose to systematically characterize prokaryotic intact polar lipids (IPLs) with state-of-the-art analytical techniques based on liquid chromatography and high-resolution mass spectrometry to bring our knowledge of microbial lipids to the next level. To this end, we will characterize (i) 250+ bacterial and archaeal cultures and (ii) 200+ environmental samples for IPLs by HPLC-MS, complemented by full identification of fatty acids and other lipids released after acid hydrolysis of total cells. This approach will be complemented by the characterisation of functional genes for lipid biosynthesis. This will involve both mapping of known genes, based on the analysis of published whole (meta)genome data, as well as the identification of as yet unknown genes in selected groups of prokaryotes. The results are expected to make a fundamental contribution to (i) our understanding of the evolution of biosynthesis of membrane lipids, (ii) their application as microbial markers in the environment, and (iii) in the development and application of organic proxies in earth sciences.","2499426","2016-10-01","2021-09-30"
"MICROMEGAS","Nanofluidics inside a single carbon nanotube","Lydéric Bocquet","UNIVERSITE LYON 1 CLAUDE BERNARD","Nanofluidics is an emerging field aiming at the exploration of fluid transport at the smallest scales. Taking benefit of the specific properties of fluids in nanoconfinement should allow to challenge the limits of macroscopic continuum frameworks, with the ultimate aim of reaching the efficiency of biological fluidic systems, such as aquaporins. Carbon nanotubes have a decisive role to play in this quest, as suggested by the anomalously large permeabilities of macroscopic carbon nanotube membranes recently measured. This behavior is still not understood, but may be the signature of a ‘superlubricating’ behavior of water in these nanostructures, associated with a vanishing friction below a critical diameter, a result put forward by our preliminary theoretical results.
To hallmark this grounbreaking behavior, it is crucial to go one step beyond and investigate experimentally the fluidic properties inside a single carbon nanotube: this is the aim of this proposal. To this end, the project will tackle two experimental challenges: the integration of a single nanotube in a larger nanofluidic plateform; and the characterization of its fluidic properties. To achieve these tasks, we propose a fully original route to integrate the nanotube in a hierarchical nano to macro fluidic device, as well as state-of-the-art methods to characterize fluid transport at the ‘zepto-litter’ scale, based on single molecule fluorescence techniques and ‘patch-clamp’ characterization. In parallel, experimental results will be rationalized using modelization and molecular dynamics. This project will not only provide a thorough fundamental understanding of the properties of carbon nanotubes as fluidic transporter, but also provide an exceptional nanofluidic plateform, allowing to explore the limits of classical (continuum) frameworks. It will also allow to envisage future potential applications, eg for desalination, separation, energy converter, jet printing, ...","2418000","2011-01-01","2016-12-31"
"MICROMOTILITY","Multiscale modeling and simulation of biological and artificial
locomotion at the micron scale: from metastatic tumor cells and unicellular swimmers to bioinspired microrobots","Antonio De Simone","SCUOLA INTERNAZIONALE SUPERIORE DI STUDI AVANZATI DI TRIESTE","The project addresses the mechanical bases of cell motility by swimming and crawling, and the possibility of replicating the principles behind them in artificial systems.

The goals are to elucidate some key mechanisms governing bio-locomotion. In particular, actin- based motility of crawling cells and motility by swimming of unicellular organisms will be studied both in general and with reference to concrete model systems.

The study of biological examples of swimming and crawling motility will be used to produce a prototype of a micron-scale bio-inspired motile micro-robot exploiting the miniaturization that becomes possible from the extensive use of active materials.

This is a multi-disciplinary research project. The themes arise from the Mechanics of Soft and Bio- logical Matter. The methods are those of Computational Engineering, and take advantage of innovative techniques from Applied Mathematics. The planned research activities rest on the development of new tools and methods in mathematical modeling, numerical simulation, data acquisition on biological systems, and on the construction of prototype devices.

***","1302270","2014-04-01","2019-03-31"
"MICRONANO","Modeling Brain Circuitry using Scales Ranging from Micrometer to Nanometer","Pascal Fua","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","If we are ever to unravel the mysteries of brain function at its most fundamental level, we will need a precise understanding of how its component neurons connect to each other. Furthermore, given the many recent advances in genetic engineering, viral targeting, and immunohistochemical labeling of specific cellular structures, there is a growing need for automated quantitative assessment of neuron morphology and connectivity. Electron microscopes can now provide the nanometer resolution that is needed to image synapses, and therefore connections, while Light Microscopes see at the micrometer resolution required to model the 3D structure of the dendritic network. Since both the arborescence and the connections are integral parts of the brain&apos;s wiring diagram, combining these two modalities is critically important. In fact, these microscopes now routinely produce high-resolution imagery in such large quantities that the bottleneck becomes automated processing and interpretation, which is needed for such data to be exploited to its full potential. We will therefore use our Computer Vision expertise to provide not only the necessary tools to process images acquired using a specific modality but also those required to create an integrated representation using all available modalities. This is a radical departure from earlier approaches to applying Computer Vision techniques in this field, which have tended to focus on narrow problems. State-of-the-art methods have not reached the level of reliability and integration that would allow automated processing and interpretation of the massive amounts of data that are required for a true leap of our understanding of how the brain works. In other words, we cannot yet exploit the full potential of our imaging technology and that is what we intend to change.","2495982","2010-04-01","2016-03-31"
"MICRONANOTELEHAPTICS","Micro/Nano Exploration, Manipulation and Assembly: Telehaptics and Virtual Reality System Development and Investigation of Biomechanics and Neuroscience of Touch","Mandayam Anandanpillai Srinivasan","UNIVERSITY COLLEGE LONDON","The primary objective of the proposed project is to develop robot mediated human interface technologies to manually explore, manipulate and assemble progressively smaller objects ranging from micro- to nano-meter scales and a secondary objective is to demonstrate the power of the interface system in the investigation of the fundamental mechanics and neural mechanisms of touch. The proposed system will consist of a master-slave robotic teleoperation (TO) subsystem and a virtual reality (VR) subsystem. The master robot will enable the user to touch, feel and manipulate (1) real micro/nano structures through the slave robot or (2) computer models of micro/nano structures in the virtual reality environment. Specific aims of this effort are as follows: (1) design and develop a custom master system to enable the user to have real-time visual, auditory, and bimanual haptic interactions; (2) design and develop a slave system consisting of microscopes and manipulators progressively augmented to enable micro to nano-precision movements and forces; (3) develop modular software architecture with device abstraction to support multiple master and slave devices; (4) integrate virtual reality software to enable the user to have real-time visual, auditory, and bimanual interactions with virtual models at micro- to nano-meter scales based on empirical data or to test hypotheses; (5) use the system to perform biomechanics and neurophysiology experiments at progressively micro- to nano-precision movements and forces; (6) develop mathematical models of mechanotransduction for quantitative understanding of touch mechanisms at multiple scales.","3264188","2010-12-01","2016-11-30"
"MIL","Mixing Ionic Liquids","Thomas Welton","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","This proposal is will open a new field of study: Using ionic liquid mixtures to provide optimal solvent systems for chemical synthesis. It is challenging and has input from a wide range of techniques, from theory, physical property measurement to measuring effects on the rates of reactions. It will generate both fundamental understanding of the behaviours of ions in mixtures and the ability to manipulate these behaviours for process development and scale-up. This will enable these exciting new systems to be applied on the industrial scale. The potential impact of the capability building represented by this programme spreads across the chemicals industry and on to society in general through more efficient, greener chemicals processing and the consequent reduction of negative environmental impacts. During the last decade ionic liquids have transformed from interesting, but poorly known materials to the subjects of huge research activity for both academic and industrial chemists. European academics and companies have been in the forefront of the application of ionic liquids. This proposal will be an important contribution to maintaining that lead.  Ionic liquids are  designer solvents  that can be fine-tuned to be the optimum solvent for applications This project will provide a step-change in the synthetic flexibility available for ionic liquid properties  design by making mixtures of ionic liquids.  This is an entirely new approach and there is little information that can be used to make predictions. However, we know from our knowledge of molecular mixtures, molecular/ionic mixtures and mixtures of high melting salts, that the behaviours to be seen are likely to be complex and non-linear. This will provide opportunities to tune different properties independently to provide the optimum liquids for a wide range of applications.","1494157","2009-04-01","2015-03-31"
"MILEPOST","Microscale Processes Governing Global Sustainability","Maria Mercedes MAROTO-VALER","HERIOT-WATT UNIVERSITY","Reactive transport modelling is a key tool in understanding the extremely complex interplay of flow, transport and reactions occurring over various temporal and spatial scales in the subsurface. The most difficult challenge in reactive transport is the capture of scale dependence, and upscaling reactive transport will ultimately only be successful if there is a detailed understanding of fundamental mechanisms at the pore level and the supporting data are available. State-of-the-art tools (e.g. X-ray microtomography and on-chip porous media) are not sufficient to understand reactive flow, as they do not provide real-time mapping of propagation of fronts (e.g. temperature, pressure, concentration) that are critical to refine and validate simulations.

The ambition is to progress beyond the state of the art via additive manufacturing tools to print 3D replicas of porous cores that enable monitoring the properties within the pores. Our unique approach is to develop for the first time three-dimensional instrumented replicas of porous structures, so we can gain much needed dynamic data at the pore scale that can be incorporated into validated simulations coupling flow and reactive transport processes.

We combine expertise and integrating ground-breaking work in: (i) additive manufacturing to produce three dimensional replicas of porous structures; (ii) tools to embed sensors to determine in-vivo propagation of fronts (pressure, temperature, pH) within complex structures; and (iii) novel high-fidelity in-silico pore models coupling relative permeability functions and critical saturations with compositional changes and validated using virtual reality tools. The ERC MILEPOST project will transform our ability to analyse and predict the behaviour of a wide range of pore-scale processes governing the macroscopic behaviour of complex subsurface systems and open up new horizons for science in other areas, e.g porosity controlled in polymers and bioprinting.","2810198","2016-09-01","2021-08-31"
"MILESTONE","Multi-Scale Description of Non-Universal Behavior in Turbulent Combustion","Heinz Günter PITSCH","RHEINISCH-WESTFAELISCHE TECHNISCHE HOCHSCHULE AACHEN","Combustion is an extremely important field for our society. The development of new, step-change technologies is essential and greatly benefits from computational design. However, turbulent combustion physics are complex, highly non-linear, of multi-scale and multi-physics nature, and involve interactions at many time-scales. This makes modeling quite challenging such that accurate predictive models, especially for the formation of pollutants, are not available. Today, the two major challenges for developing predictive simulations of turbulent combustion are first to account for its multi-scale nature by considering the non-universal behavior of small-scale turbulence, which is known to be critically important for turbulence-chemistry interactions, and second, to provide data in sufficient detail for rigorous analysis of model deficiencies and unambiguous model development. These two issues are addressed in the proposed work. The main overall objectives are: 1) Establish a new multi-scale framework to analyze and model turbulent combustion phenomena based on a new way to describe turbulence using so-called dissipation elements, which are space-filling regions in a scalar field allowing to capture its small-scale morphology and non-universality. 2) Create new unprecedented datasets using direct numerical simulations (DNS) and provide new analysis methods to develop and validate combustion models; this will include automatically reducing and optimizing chemical kinetic mechanisms for use in DNS and developing an on-the-fly chemistry reduction technique. 3) Apply new modeling approaches to complex and highly non-linear modeling questions, such as pollutant formation in turbulent spray combustion. The successful outcome of the project will provide new and unprecedented datasets, a quantitative description of the impact of non-universality in small-scale turbulence on different aspects of turbulent combustion, and the basis for an entirely new multi-scale closure.","2499884","2016-06-01","2021-05-31"
"Milli-Tech","Milli-Volt Switch Technologies for Energy Efficient Computation and Sensing","Adrian IONESCU","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The Milli-Tech proposal aims at a novel technology platform serving both computation and sensing: electronic switch architectures, called steep slope switches, exploiting new device physics and concepts in emerging 2D materials to achieve operation at voltages below 100 millivolts. Such switches will have a subthreshold slope below 10mV/decade, significantly more abrupt than MOSFET thermal limit of 60mV/decade at room temperature and in great advance to any beyond CMOS switches. Such characteristics will dramatically improve both the energy efficiency of logic circuits and the transduction sensitivity for many classes of sensors.
The project will develop a technological platform called ‘millivolt technology’ focusing on low power digital and sensing/analog electronic functions exploiting steep slopes, with the goal of lowering the energy per useful function (computed and sensed bit of information) by a factor of 100x. 
Such ultra-low operation voltage will contribute to solving major challenges of nanoelectronics such as power issues and it will enable energy efficient super-sensitive sensors for Internet-of-Everything (IoE).
Milli-Tech includes fundamental research on new solid-state steep slope device concepts: heterostructure tunnel FETs in 2D Transition-Metal-Dichalcogenides (TMD), 2D Van der Waals super-lattice energy filter switch and hybrid architectures combining two switching principles: band-to-band-tunneling and metal-insulator-transition or negative capacitance in VO2, used as additive technology boosters.
Milli-Tech plans breakthroughs by precise demonstrators: (i) energy efficient computation blocks for Von-Neumann ICs at sub-100mV, (ii) Active Pixel Sensors based on 2D TMD/GeSn tunnel FETs for IR imagers, (iii) Terahertz detectors based on hybrid 2D VO2/TMD switches (iii) ultra-sensitive 2D steep slope charge detectors for biosensing. The high-risk ‘millivolt technology’ will be highly rewarding by enabling the energy efficient revolution for IoE.","2929875","2016-06-01","2021-05-31"
"MIMAS","Multi-dimensional interferometric amplification of ultrashort laser pulses","Andreas Tünnermann","FRIEDRICH-SCHILLER-UNIVERSITAT JENA","Ultrafast lasers, which allow the concentration of light in space and time, have been instrumental in revolutionizing industrial production technologies, medical applications and cutting-edge fundamental research. A common demand for many applications is the combination of maximum pulse peak powers with maximum average powers, in extreme cases involving petawatt (PW) peak powers and megawatt (MW) average powers. Additionally, these parameters must be achieved together with an optimum beam quality and high efficiency. The MIMAS project aims to address these demands and enable new realms of performance for ultrafast lasers. 
The basic idea is spatially and temporally separated amplification of ultrashort laser pulses followed by coherent combination. This overcomes all the scaling limitations known in single-emitter systems. Moreover, the spatially separated amplification will be developed to an integrated and highly compact configuration: an ytterbium-doped multicore fiber. In addition, it is proposed that a sequence of pulses be amplified with an encoded phase pattern, causing a coherent pulse stacking at the system output. 
The targeted laser pulse parameters are completely beyond the scope of current laser technology and therefore able to revolutionize many applications. The target is to generate a pulse energy of >1J at 10kHz repetition rate, i.e. an average power of >10 kW, with a wall-plug efficiency of >10%. Together with a pulse duration of <200fs, such performance results in a pulse peak power of >5 TW in a scalable architecture. This outstanding performance, which is three orders of magnitude above the capabilities of today’s laser systems, is emitted from only two fibers and features excellent beam quality.
I am deeply convinced that such an ultrafast laser source will be the key element in a number of experiments in modern sciences; not only in fundamental physics but also in biology and medicine, it will stimulate seminal discoveries and breakthroughs.","2373750","2015-09-01","2020-08-31"
"MIMEFUN","Biomimetics for Functions and Responses","Olli Tapio Ikkala","AALTO KORKEAKOULUSAATIO SR","Energy efficiency and sustainability encourage to develop lightweight materials with excellent mechanical properties, combining also additional functionalities and responses. Therein nature allows inspiration, as e.g. pearl of nacre and silk show extraordinary mechanical properties due to their aligned self-assemblies. However, biological complexity poses great challenges and in biomimetics selected features are mimicked using simpler concepts. Previously artificial nacre has been mimicked by multilayer and sequential techniques and ice-templating. However, concepts for aligned spontaneous self-assemblies are called for scalability. We will develop toughened nacre-inspired materials by templating functionalized polymers on colloidal sheets in suspension, followed by self-assembly by solvent removal. Similarly, we will develop silk-mimetic materials using aligned organic fibrous reinforcements in soft dissipative matrix. Nanofibrillated cellulose will be wet-spun using extrusion into coagulant bath, followed by post drawing, drying and functionalization to allow silk-like fibers with high mechanical properties. In another route, cellulose rod-like whiskers will be decorated with soft functional polymers allowing energy dissipation, followed by alignment and interlinking to mimick silk-assemblies. The colloidal routes allow also new functionalities by using functional polymers, e.g. electroactive and conjugated polymers and nanoparticles. Importantly, redox-active polymers are bound on the colloidal sheets. Incorporating in a planar electrochemical cell with flexible electrodes, electrochemical switching of stiffness is obtained using a small voltage, as the intercolloidal interaction is controlled by the charge state of the redox-active layers. This would allow a new class of material, eg. to interface users and devices. In summary, we present a colloidal self-assembly platform for biomimetic materials with exciting mechanical, functional, and switching properties.","2296320","2012-04-01","2017-03-31"
"MIST","Molecules, magnetic fields and Intermittency in coSmic Turbulence – Following the energy trail.","Edith, Gabrielle FALGARONE","ECOLE NORMALE SUPERIEURE","The discovery of molecules in the early universe is a challenging providence. Molecules unveil the truly cold universe in which stars form and their rich versatility provides unique diagnostics to unravel the ”relative importance of purely gravitational effects and gas dynamical effects involving dissipation and radiative cooling”, recognized 40 years ago by White and Rees to be a central issue in theories of galaxy formation. Molecules also reveal that cosmic turbulence is far less dissipative than predicted by cosmological simulations, with a broad equipartition in a vast variety of media between the thermal energy of the hottest phases and the turbulent energy of the coldest. Our project focuses on the physics of turbulent dissipation, and its link to the emergence of molecules, in the magnetized compressible medium where gravitational instability develops to form stars and seed galaxies in the early universe. It builds on a fundamental property of turbulence, its space-time intermittency: dissipation occurs in bursts. Our team will foster strong interactions between three main research axes: (1) observations of the chemical and thermal markers of turbulent dissipation in the high-redshift and local universe, (2) statistical analyses of the magnetic and velocity fields in samples of unprecedented size and sensitivity to study the non-Gaussian signatures of turbulent dissipation, and (3) numerical experiments dedicated to (a) the space-time structures of turbulent dissipation and the formation of molecules in their wake, and (b) the split of the energy trails between hot/thermal and cold/turbulent phases. This project will benefit from the prodigious capabilities of the ALMA and NOEMA interferometers, the launch of the JWST in 2018, and the Planck satellite data on polarized Galactic foregrounds. The ENS Physics Department, with its strong theoretical and experimental expertise on turbulence, is an ideal place to house such a project.","2500000","2017-10-01","2022-09-30"
"MISTIC","Mastering the dusty and magnetized Interstellar Screen to Test Inflation Cosmology","François Boulanger","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","How did the Universe begin? The prevailing idea is that the Bang of the Big Bang was an early burst of exponential expansion, dubbed inflation. A key prediction of inflation is that it generated gravitational waves. The corresponding ripples in the space-time geometry left an imprint in the polarisation of the Cosmic Microwave Background (CMB). A new generation of experiments are making a major step towards this signal by mapping the microwave sky polarisation with an unprecedented sensitivity and combination of sky coverage, angular resolution and wavelengths. These experiments may show whether the energy scale of inflation predicted by the simplest models is correct. However, unlike for temperature anisotropies, Galactic foregrounds have larger amplitude than any putative primordial B-mode signal. The signature of cosmic inflation will not be detected, unless contamination associated with the dusty and magnetized interstellar medium in the Galaxy is removed with the required accuracy and confidence. Within the competitive field of CMB studies, the MISTIC project proposes a unique contribution to the search for its B-mode polarisation. The MISTIC team has the unmatched ambition and capability to tie the analysis of polarisation data and component separation to state-of-the-art understanding and modelling of the dusty magnetized interstellar medium. Our project goal is to achieve the breakthroughs in the fields of Galactic astrophysics, microwave sky modelling, and component separation, required to achieve the best sensitivity on the B-mode CMB polarisation. Our work plan bridges data analysis, physical modelling, sky modelling and component separation into an iterative process that will take full advantage of the uniqueness and complementarities of expertise and data gathered in the MISTIC project.","1755000","2011-06-01","2016-05-31"
"MM-PGT","Modern Methods for Perturbative Gauge Theories","David A. Kosower","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","Gauge theories are the basis of modern theories of high-energy physics. Perturbative calculations are crucial to developing our quantitative understanding of these theories, as well as seeking new and deeper structures in these theories. Precision higher-order calculations in the SU(3) component of the Standard Model, perturbative Quantum Chromodynamics (QCD), will be crucial to understanding data at the CERN-based Large Hadron Collider (LHC) and finding and measuring physics beyond the standard model. Precision calculations in the electroweak theory will also play a role in confronting later precision data with theoretical models. The related maximally (N=4) supersymmetric gauge theory has served both as an important theoretical laboratory for developing new calculational techniques, as well as a link to string theory via the AdS/CFT duality. It is also emerging as a fruitful meeting point for ideas and methods from three distinct areas of theoretical physics: perturbative gauge theories, integrable systems, and string theory. The Project covers three related areas of perturbative gauge theories: computation of one- and two-loop amplitudes in perturbative quantum chromodynamics; incorporation of these amplitudes and development of a fully-matched parton-shower formalism and numerical code; and higher-loop computations in the N=4 supersymmetric theory. It aims to develop a general-purpose numerical-analytic hybrid program for computing phenomenologically-relevant one- and two-loop amplitudes in perturbative QCD. It also aims to develop a new parton shower allowing complete matching to leading and next-to-leading order computations. It seeks to further develop on-shell computational methods, and apply them to the N=4 supersymmetric gauge theory, with the goal of connecting perturbative quantities to their strong-coupling counterparts computed using the dual string theory.","961080","2009-01-01","2014-12-31"
"MMDYNASYS","Molecular Motors, powering dynamic functional molecular systems","Benard Lucas FERINGA","RIJKSUNIVERSITEIT GRONINGEN","In this proposal the unique properties of unidirectional light driven molecular rotary motors will be built upon to achieve dynamic control of function and develop responsive systems with a particular focus on systems in water. Light-driven molecular rotary motors are distinct from the majority of molecular switches, as they allow sequential access to multiple functional states in a responsive system through non-invasive stimulation. Importantly, continuous irradiation induces continuous rotary motion which provides a unique opportunity to design dynamic systems and responsive materials that can be driven out-of-equilibrium. The research program is divided in four work-packages: a) chemical and redox driven unidirectional motors; here we will develop processive unidirectional motors that can use (electro)chemical energy in a continuous manner, b) amplification of motion; here rotary motors operate in assemblies to amplify mechanical function over a wide range of length scales. Specifically we will use liquid crystal-water interfaces as a unique platform to control motion and organization. c) dissipative self-assembly: molecular motors offer fantastic opportunities to control self-assembly and drive such systems out-of-equilibrium. We aim at metastable aggregate formation (hydrogels) and the design of amphiphilic motors for responsive self-assembled nanostructures; d)triggering biomolecular function; the goal is to use rotary motors to regulate DNA transcription and ultimately as genuine powering device to control cardiac cell function. In the emerging field of photopharmacology, we take advantage of non-invasive high spatio-temporal control that switching with light provides.  The proposed research program is highly challenging but provides the comprehensive effort required to achieve control of complex nanomechanical systems and will opening a bright future for applications ranging from stimuli responsive materials to spatio-temporal control of biomolecular systems","2499524","2016-09-01","2021-08-31"
"MMFCS","Multiscale Models for Catalytic-Reaction-Coupled Transport Phenomena in Fuel Cells","Bengt Sundén","LUNDS UNIVERSITET","In proton exchange membrane fuel cells (PEMFCs) and solid oxide fuel cells (SOFCs) there are various transport processes strongly affected by catalytic chemical/electrochemical reactions in nano- or/and micro-structured and multi-functional porous electrodes. Due to the complexity of fuel cells, fundamental understanding of physical phenomena continues to be required for the coupled chemical and transport processes with two-phase flow/water management in PEMFCs, and internal reforming reactions/thermal management in SOFCs. The project deals with the coupling of micro scale reactions (such as the electrochemical reactions and catalytic reactions) with various transport phenomena to provide a comprehensive understanding of fuel cell dynamics. The methodology for the project is a combination of model development and integration, simulation/analysis and validation. For microscopically complex porous layers and active sites, submodels will be developed by considering the detailed elementary kinetic rates based on the intermediate chemical species and their reactions occurring on the surface of the involved materials.  As the inputs, the obtained data from the microscopic submodels will be implemented by the macroscopic CFD codes, previously developed for various applications, to examine local parameters in the porous electrodes and components. Both macro- and microscopic models will be validated by the experimental and/or literature data during the course of the project. The project will make progress beyond the state-of-the-art in modelling and analysis of advanced fuel cells, such as ultra low Pt loading (<0.1mgPt/cm2) and high temperature (120-200oC) PEMFCs, and intermediate temperature (600-800oC) planar SOFCs.","1320000","2009-06-01","2014-05-31"
"MOBILE","Modelling, Optimization and Control of Biomedical Systems","Efstratios Pistikopoulos","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","The main aim of the proposed project is to develop models and model based control and optimization methods and tools  for drug delivery systems, which would ensure: (i) Reliable and fast calculation of the optimal drug dosage without the need for an on-line computer while taking into account the specifics and constraints of the patient model (personalised health care) (ii) Flexibility to adapt to changing patient characteristics, (iii) Incorporation of the physician s performance criteria (iv) Safety of the patients (v) Reduced size effects by optimising the drug infusion rates The overall control and optimisation approach will rely on the novel multi-parametric model-based control technology developed by the PI over the past 15 years, which will be further extended and implemented in the context of the following biomedical systems (a) insulin delivery (b) control of anaesthesia and hemodynamic variables, (c) optimal chemotherapy design for anti-cancer (d) optimal control of the chemotherapy for HIV.","1782925","2009-01-01","2013-12-31"
"MOBILE-W","Exploring Mobile Interfaces:  Domain Walls as Functional Elements","Nava Setter","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Very recent findings by our group and others reveal that ferroelectric interfaces can show strongly enhanced properties and remarkable new effects with potential for exploitation in devices. In particular, new phases at broadened domain walls [DWs], conductivity and possibility for ferromagnetism in ferroelectric DWs and giant response in materials possessing high density ferroelectric DWs were shown
DWs form spontaneously in ferroelectrics. Their functionality, widely recognized is neither quantified nor controlled. Distinctly from other interfaces, ferroelectric DWs are mobile, can be modified dynamically by external forces (electrical, stress, temperature variation) and can, moreover, be annihilated and recreated
What are the mechanisms to functionalize DWs? How to gain control over the structure and dynamics of these DWs, and what are the potential breakthroughs that such control may lead to? What additional properties of DWs await discovery? We will address these questions through several interrelated objectives designed to cover both fundamental aspects, as well as limits of applicability
Considering a single DW as a device that can be created, displaced and eliminated reversibly in-situ is unique to this project. Working with ordered arrays of DWs is another central theme. To create controlled patterns of DWs we will use top-down and bottom-up approaches. Characterization of DWs will range from investigation of the internal structure by Spherical Aberration Corrected HRTEM through cryogenic Piezo Force Microscopy study of DW phase-transition, to macroscopic characterization over a broad frequency and driving stimulus range. Theory will guide the investigation. Device concepts will be demonstrated, such as DW-enhanced ultrasonic transducer, DW transistor and ferroelectric string memory
We believe that attainment of these objectives should lead to conceptual breakthroughs both in our understanding of ferroelectric interfaces and in their applications.","2475600","2011-06-01","2016-08-31"
"MOBOCON","Model-based optimizing control - from a vision to industrial reality","Johannes Karl Sebastian Engell","TECHNISCHE UNIVERSITAT DORTMUND","This proposal focuses on the operation of processing units in chemical and biochemical production plants. A new approach, demonstrated recently in the lab and pilot scales as being feasible, is to perform a dynamic online optimization of the plant operating parameters in order to achieve an economically optimal operation while meeting constraints on emissions, product quality and operating limits of the equipment. Traditionally, meeting the specifications on product purity and quality, yield etc. is achieved by feedback control of the processing conditions to set-points determined by experienced operators or by an infrequent stationary optimization. In the optimizing control approach, the main degrees of freedom of the plant are not used to regulate certain variables to set-points but adapted dynamically to achieve an optimal performance  In a joint effort by two internationally leading research groups in process operations and in numerical methods for dynamic optimization, we will tackle the main obstacles to the widespread industrial application of this extremely promising approach and will realize:
•	Increased reliability and robustness of the algorithms and of the overall control scheme
•	Reduction of the modelling effort and increase of the model accuracy by combining rigorous models with online adaptation
•	New concepts for human-controller interaction to increase acceptance and performance.
Moreover, we will extend the scope of the application of optimizing control to large transitions where discrete control variables are manipulated, e.g. during start-up and shut-down.
The robustness and the power of optimizing control and the new concepts for the interaction with the operators will be validated and demonstrated by an application to a very challenging and innovative process in the pilot scale. The approach and the methods are transferable to the broad domain of processes where materials are transformed (e.g. iron and steel, glass, food and beverages).","3491632","2012-06-01","2017-11-30"
"MOCOPOLY","Multi-scale, Multi-physics MOdelling and COmputation of magneto-sensitive POLYmeric materials","Paul Alfred Steinmann","FRIEDRICH-ALEXANDER-UNIVERSITAET ERLANGEN NUERNBERG","MOCOPOLY is a careful revision of an AdG2010-proposal that was evaluated above the quality threshold in steps1&2. In the meantime the applicant has made further considerable progress related to the topics of MOCOPOLY. Magneto-sensitive polymers (elastomers) are novel smart materials composed of a rubber-like matrix filled with magneto-active particles. The non-linear elastic characteristics of the matrix combined with the magnetic properties of the particles allow these compounds to deform dramatically in response to relatively low external magnetic fields. The rapid response, the high level of deformations achievable, and the possibility to control these deformations by adjusting the external magnetic field, make these materials of special interest for the novel design of actuators for a fascinating variety of technological applications. It is the overall objective of this proposal to uncover the process-microstructure-properties relations of the emerging novel multi-scale, multi-physics material class of magneto-sensitive polymers with the aim to better exploit its promising potential for future, currently unimagined technological applications. This objective will only be achieved by performing integrated multi-disciplinary research in fabrication, characterisation, modelling, simulation, testing and parameter identification. This proposal therefore sets up a work programme consisting of nine strongly interconnected work packages that are devoted to 1) fabrication of magneto-sensitive polymers, 2) microstructure characterisation by modelling and simulation, 3) microstructure characterisation by CT-scanning, 4) continuum physics modelling at the micro-scale, 5) computational multi-physics homogenisation, 6) continuum physics modelling at the macro-scale, 7) testing at the macro-scale, 8) multi-scale parameter identification and 9) macro-scale parameter identification. The work programme is therefore characterised by various feedback loops between the work packages.","2498000","2012-04-01","2017-03-31"
"MODAG","Model Theory and asymptotic geometry","Ehud Hrushovski","THE HEBREW UNIVERSITY OF JERUSALEM","The principal methods of model theory, in its connections to algebraic geometry, have been quantifier elimination (e.g. Tarski's theorem)
and structural stability (used here in a wide sense, including simplicity, NIP and structural o-minimality.)
We propose to move beyond current limitations on both fronts, by means of three interrelated projects.   (1)  A study of definable sets in global fields.  A successful quantifier elimination result in this setting would extend the reach of model theory to wide areas of number theory and geometry that have not been accessible before, including points of small height in number theory, and the Gromov-Witten invariants of a variety in geometry.    (2) A study of limits of o-minimal metric structures as quotients of non-archimedean structures,    extending similar measure and group-theoretic  work  that has led to a resolution of Pillay's conjectures in the o-minimal setting, and leading towards a model theory of Calabi-Yau degenerations.    (3)   Model theoretic asymptotic limits lead to  measure and dimension theories, with associated dependence theories,   that resemble known structures from stability theory but do not lie within the stable realm or its current extensions.    Preliminary stability-theoretic considerations have already led to significant applications in combinatorics.    We propose creating a structural stability theory based on pseudo-finite dimension, expected to create a long-term bridge between model theory and
additive combinatorics.","1393500","2012-01-01","2017-09-30"
"MoDaS","Mob Data Sourcing","Tova Milo","TEL AVIV UNIVERSITY","Crowd-based data sourcing is a new and powerful data procurement paradigm that engages Web users to collectively contribute data, analyze information and share opinions. This brings to light, out of the huge, inconsistent Web ocean, an important body of knowledge that would otherwise not be attainable. Crowd-based data sourcing democratizes data-collection, cutting companies and researchers reliance on stagnant, overused datasets, and has great potential for revolutionizing our information world. Yet, triumph has so far been limited to only a handful of successful projects such as Wikipedia or IMDb. This comes notably from the difficulty of managing huge volumes of data and users of questionable quality and reliability. Every single initiative had to battle, almost from scratch, the same non-trivial challenges. The ad hoc solutions, even when successful, are application specific and rarely sharable.

In this project we propose to develop solid scientific foundations for Web-scale data sourcing. We believe that such a principled approach is essential to obtain knowledge of superior quality, to realize the task more effectively and automatically, be able to reuse solutions, and thereby to accelerate the pace of the absorption of this new technology that is revolutionizing our life. Our goal is very ambitious. We will develop the logical, algorithmic, and methodological foundations for the management of large scale crowd-sourced data and for the development of applications over such information. This encompasses a formal model capturing all the diverse facets of crowd-sourced data. This also means developing the necessary reasoning capabilities for managing data sourcing, cleaning, verification, integration, sharing, querying and updating, in a dynamic Web environment. This technological breakthrough will open the way for developing new and otherwise unattainable universe of knowledge in a wide range of applications, from scientific to social and economical ones.","1706400","2012-04-01","2018-03-31"
"MODFLAT","""Moduli of flat connections, planar networks and associators""","Anton Alekseev","UNIVERSITE DE GENEVE","""The project lies at the crossroads between three different topics in Mathematics: moduli spaces of flat connections on surfaces in Differential Geometry and Topology, the Kashiwara-Vergne problem and Drinfeld associators in Lie theory, and combinatorics of planar networks in the theory of Total Positivity.

The time is ripe to establish deep connections between these three theories. The main factors are the recent progress in the Kashiwara-Vergne theory (including the proof of the Kashiwara-Vergne conjecture by Alekseev-Meinrenken), the discovery of a link between the Horn problem on eigenvalues of sums of Hermitian matrices and planar network combinatorics, and  intimate links with the Topological Quantum Field Theory shared by the three topics.

The scientific objectives of the project include answering the following questions:
1) To find a universal non-commutative volume formula for moduli of flat connections which would contain the Witten’s volume formula, the Verlinde formula, and the Moore-Nekrasov-Shatashvili formula as particular cases.
2) To show that all solutions of the Kashiwara-Vergne problem come from Drinfeld associators. If the answer is indeed positive, it will have applications to the study of the Gothendieck-Teichmüller Lie algebra grt.
3) To find a Gelfand-Zeiltin type integrable system for the symplectic group Sp(2n). This question is open since 1983.

To achieve these goals, one needs to use a multitude of techniques. Here we single out the ones developed by the author:
-	Quasi-symplectic and quasi-Poisson Geometry and the theory of group valued moment maps.
-	The linearization method for Poisson-Lie groups relating the additive problem z=x+y and the multiplicative problem Z=XY.
-	Free Lie algebra approach to the Kashiwara-Vergne theory, including the non-commutative divergence and Jacobian cocylces.
-	Non-abelian topical calculus which establishes a link between the multiplicative problem and combinatorics of planar networks.""","2148211","2014-02-01","2019-01-31"
"MODPHYSFRICT","Modeling the Physics of Nano-Friction","Erio Tosatti","SCUOLA INTERNAZIONALE SUPERIORE DI STUDI AVANZATI DI TRIESTE","In the next five years, novel nanoscale friction experiments will provide decisive data on the physics of a century-old, central, yet incompletely understood area of physics. Nano and mesoscopic phenomena without historical precedent in classic friction are emerging in tip based and other sliding and dissipation measurements. Opportunities for the control of friction, based on harnessing and modifying collective phenomena in the substrate are in the works; and fresh spectroscopic insights into the quantum phenomena in solids and surfaces are currently obtained by nanofrictional means. Finally, exciting laser based sliding nanosystems just appeared. All this experimental opulence demands a strong matching theoretical effort:  and just that is the scope of this project. I and my group will model, calculate and simulate the frictional anomalies accompanying structural and ferroelectric phase transitions as an opportunity for friction control; and will pursue the noncontact dissipation anomalies as a local spectroscopic tool of electronic, magnetic and quantum transitions. A strong priority will be the theory and simulation of sliding in trapped colloids and cold ions on laser generated periodic potentials, where nanoscale and mesoscale sliding phenomena promise to be uniquely accessible. The range of approaches will cover phenomenology, model building and testing, molecular dynamics atomistic simulations of sliding, both empirical and ab initio, electronic and magnetic dissipation modeling; and ordinary condensed matter theory, along with non-equilibrium statistical mechanics. To achieve that scope, I will put together students, postdocs, and leading condensed matter theorists operating mostly in SISSA Trieste, but also in ICTP Trieste and elsewhere, assembling a strong team in good contact with European experimental groups. Their complementarity will allow this unified project to attain results much beyond what the team members could individually achieve.","1550000","2013-05-01","2019-04-30"
"MODSIMCONMP","Modeling, Simulation and Control of Multi-Physics Systems","Volker Mehrmann","TECHNISCHE UNIVERSITAT BERLIN","This proposal is aimed at developing and analyzing a fundamentally new interdisciplinary approach for the modeling, simulation, control and optimization of multi-physics, multi-scale dynamical systems. The new innovative feature is to generate models via a network of modularized uni-physics components, where each component incorporates a mathematical model for the dynamical behavior as well as a model for the uncertainties, e.g. due to modeling, discretization or finite precision computation errors. Based on this new modeling concept also new numerical simulation, control, and optimization techniques will be developed and incorporated, that allow a systematic adaptive error control (including the appropriate treatment of different scales,and the uncertainties) for the components as well as for the whole multi-physics model. In order to cope with the  differential-algebraic and multi-scale character of the systems we will develop and analyze remodeling techniques for the components as well as for the whole network including the uncertainties and special structures. The new remodeled systems will be designed such that they allow an efficient and accurate  dynamical simulation with high order numerical integration techniques as well as efficient methods for model reduction and open and closed loop control.
In an interdisciplinary corporation with colleagues from computer science and engineering we will extend the modeling language MODELICA to be able to incorporate the new features (in particular the uncertainties and modeling errors) and we also plan to implement the complete approach as a new software platform.","1899924","2011-04-01","2016-03-31"
"MODULI","Geometry of moduli spaces and mapping class groups","Ursula Annemarie Hamenstädt","RHEINISCHE FRIEDRICH-WILHELMS-UNIVERSITAT BONN","The primary goal of the project is to obtain an understanding of geometric and dynamical properties of moduli spaces and mapping class groups. For a mapping class group of a surface of finite type, we are interested in subgroups, in particular in the trace fields of Veech groups beyond the case of genus 2. Convex cocompact surface subgroups are word hyperbolic surface-by-surface groups, and we aim at clarifying whether or not such groups exist.

Fine asymptotics of the distribution of periodic orbits for the Teichmüller flow on strata of quadratic or abelian differentials can be related to dynamical zeta functions. A Borel conjugacy of the Teichmüller flow on the moduli space of quadratic differentials into the Weil-Petersson flow will be used to analyze dynamical properties of the Weil-Petersson flow.

The handlebody is a finitely presented subgroup of the mapping class group which however is not quasi-isometrically embedded. A new geometric model for the group will be used towards obtaining a comprehensive understanding of the geometry of this group, in particular with respect to calculating the Dehn function and quasi-isometric rigidigy.

A similar geometric model for the outer automorphism group of the free group may yield hyperbolicity of the electrified sphere graph on which this group acts by simplicial automorphisms..","1536600","2012-04-01","2018-03-31"
"Mol-2D","Molecule-induced control over 2D Materials","Eugenio CORONADO MIRALLES","UNIVERSITAT DE VALENCIA","We propose to create heterostructures based on functional molecules and 2D materials. As molecular systems we focus on bistable magnetic molecules able to switch between two spin states upon the application of an external stimulus (temperature, light, pressure, electric field etc.). As 2D materials we concentrate on those exhibiting in particular superconductivity or magnetism. The driving idea is to tune/improve the properties of the “all surface” 2D material via an active control of the hybrid interface. This concept, which goes much beyond the conventional chemical functionalization of a 2D material, will provide an entire new class of smart molecular/2D heterostructures, which may be at the origin of a novel generation of hybrid materials and devices of direct application in highly topical fields like electronics, spintronics, molecular sensing and energy storage. Through this molecular approach, we will address major challenges in different areas of the 2D research: i) in 2D physics, we investigate the new properties that should appear in heterostructures involving 2D superconductors and 2D magnets or magnetic molecules; ii) in 2D electronics, we explore the possibility of tuning the superconducting/magnetic properties of a 2D material by applying an external stimulus (light for example), or to design smart electronic/spintronic devices able to respond to physical (light, magnetic field, etc.) or chemical stimuli (trapping of molecules); iii) in 2D composite materials, a general goal is to design hybrid molecular/2D materials with improved properties with respect to the pure 2D material to be used in the fabrication of energy storage devices. To reach these challenging goals an integrative and multidisciplinary approach is proposed in which various facets of chemistry – coordination, solid-state and supramolecular chemistry – are coupled with physics, materials science and nanotechnology.","2499950","2018-10-01","2023-09-30"
"MOLART","Surface-Confined Metallosupramolecular Architecture: Towards a Novel Coordination Chemistry for the Design of Functional Nanosystems","Johannes Barth","TECHNISCHE UNIVERSITAET MUENCHEN","The fascinating properties of transition metal complexes intrigued generations of scientists and spurred major technological developments. They are decisive for life processes and catalysis. More recently the pertaining coordination interactions were used to assemble discrete nanostructures and supramolecular networks. Here we aim at a rationale for the design of metallosupramolecular architectures in intimate contact with solid supports. We study and control individual functional molecules and their metal-directed assembly at well-defined surfaces in exquisite detail by molecular-level scanning tunneling microscopy and spectroscopy. The atomistic insight gained into the underlying mechanisms and interactions is used to steer the formation of nano-architectures, whose physicochemical properties are characterized by local and space-averaging techniques. We rationalize the full involvement of the surface atomic lattice in the metal-ligand interactions and coordination spheres using advanced spectroscopic techniques and complementary ab initio theoretical calculations. We engineer nanoporous coordination networks with tailored cavities for patterning purposes, confinement and host-guest systems. We develop new concepts for controled molecular motion in nanoscale coordination environments. We explore the redox chemistry and catalytic activity of the presented coordinatively unsaturated sites to develop novel single-site heterogenous catalysts and potentially biomimetic systems. It is suggested that with the described research a novel heading in coordination chemistry can be explored. The properties of metal centers in unique coordination environments challenge our current understanding, whereas their nanoscale control bears promise for distinct and tunable functionalities.","2568000","2010-04-01","2016-03-31"
"MOLCHIP","A molecular laboratory on a chip","Gerardus Johannes Maria Meijer","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","The manipulation of atoms above a chip using magnetic fields produced by current carrying wires is a mature field of research. This field was inspired by the notion that miniaturization of magnetic field structures enables the creation of large field gradients, i.e., large forces and tight potentials for atoms. It also has been crucial that present-day microelectronics technology makes it possible to integrate multiple tools and devices onto a compact surface area. Such atom chips have been used to demonstrate rapid Bose-Einstein condensation and have found applications in matter-wave interferometry and in inertial and gravitational field sensing. Likewise, the engineering of miniaturized electric field structures holds great promise for the manipulation of polar molecules above a chip. The ability of a molecule to rotate and to vibrate allows for the coupling to photons over a wide range of frequencies. This might enable, for instance, to implement proposed schemes of quantum computation that use polar molecules as qubits. The use of miniaturized traps also brings quantum-degeneracy for samples of polar molecules closer. However, experimentally proven concepts to load and detect molecules on a chip are still in their infancy. In this project, we will develop and exploit experimental methods to load and detect polar molecules on a chip. Molecules are directly loaded from a supersonic beam into miniaturized electric field traps above the chip. For this, these traps originally move along with the molecular beam at a velocity of several hundred meters per second and are then brought to a complete standstill over a distance of only a few centimeters. After a certain holding time, e.g., after the experiments on the chip are over, the molecules are accelerated off the chip again for detection. This methodology is applicable to a wide variety of polar molecules, enabling the realization of a molecular laboratory on a chip.","2171760","2010-02-01","2015-01-31"
"MOLECULAR MOTORS","Molecular Motors - Controlling movement at the nanoscale","Bernard Feringa","RIJKSUNIVERSITEIT GRONINGEN","The design of artificial molecular motors and machines is one of the major challenges in contemporary molecular sciences and bottom-up molecular nanotechnology. Whereas the protein-based molecular motors found in the living cell are amongst the most fascinating and complex structures found in nature and crucial to nearly every key biological process, the field of synthetic linear and rotary motors is still in its infancy. In a broader context moving molecular sciences from the current situation with a focus on static structures and operation under thermodynamic control to dynamic chemistries with systems under kinetic control will represent a major step beyond current frontiers of chemical sciences. Furthermore, a shift from control of structure to dynamic control of function and from molecules to molecular systems, where several components act in concert often at different hierarchical levels, makes it possible for fascinating and unique properties to be discovered. In this program the goal is to significantly push ahead the frontiers of the field of molecular motors and machines both with respect to control of translational and rotary motion, as well as the exploration of dynamic functions of molecular systems governed by molecular motors. A further extremely challenging goal is to explore synthetic systems that can undergo autonomous motion. This program builds on our recent discoveries of the first unidirectional light-driven rotary molecular motor, the chemical driven rotary motor that can complete a full rotary cycle in a repetitive manner and the first molecular defined autonomous translational motor powered by a chemical fuel.  As the basic principles, rules and parameters that govern molecular motion at the nanoscale are, largely, not yet understood, the focus of this proposal is on a multidisciplinary program addressing some of the most challenging fundamental issues in this uncharted territory.","2175970","2009-01-01","2013-12-31"
"MOLFACTORY","Machinery for Molecular Factories","David Leigh","THE UNIVERSITY OF MANCHESTER","""The widespread use of molecular-level motion in key natural processes suggests that great rewards could come from bridging the gap between the present generation of synthetic molecular systems—which by and large rely upon electronic and chemical effects to carry out their functions—and the machines of the macroscopic world, which utilize the synchronized movements of smaller parts to perform particular tasks. The aim of this project is to design, construct and investigate the operation of synthetic molecular machines capable of performing sophisticated tasks in chemical synthesis. Its successful demonstration would give mankind the beginnings of a potentially game-changing new approach for the synthesis of organic molecules.""","2142557","2014-02-01","2019-01-31"
"MolMacIP","Molecular Machines with Integrated Parts","David A LEIGH","THE UNIVERSITY OF MANCHESTER","The widespread use of molecular-level motion in key natural processes suggests that great rewards could come from bridging the gap between the present generation of synthetic molecular machines—which by and large rely upon switching of individual components to carry out their functions—and the machines of the macroscopic world, which utilize the synchronized behaviour of integrated components to perform tasks more complex than the sum of the components. The aim of this project is to learn how to integrate the movements and chemistries of different molecular machine components.

The proposed program is divided into three themes:

Theme 1: Transport and synthesis by nanomechanical substrate positioning. We will integrate switching mechanisms with components that can attach and release a substrate, in order to develop methodology for the nanomechanical manipulation of substrates. We see this theme as reminiscent of the way that substrates are passed between sites in various enzyme complexes in biology.

Theme 2: Sequence-selective synthesis using molecular assembly lines. We will integrate reactive molecular appendages with rotaxanes in which the macrocycle moves along a track through brownian motion, in order to develop molecular assembly lines. This theme is reminiscent of molecular-machine-mediated sequence specific polymerisation in biology, such as protein synthesis by the ribosome and DNA synthesis by some DNA polymerases.

Theme 3: Transport with chemically fuelled molecular linear motors. We shall integrate ratchet mechanisms with chemically fuelled molecular pumps and walkers and use them to transport cargo between phases and to carry out synthesis via energy ratchet mechanisms induced by chemical fuels. This theme is reminiscent of how biological molecular motors drive chemical systems away from equilibrium, thereby enabling tasks to be performed, cargo to be transported directionally and work to be done.","2471095","2019-02-01","2024-01-31"
"MOLNANOMAS","Molecular Nanomagnets at Surfaces: Novel Phenomena for Spin-based Technologies","Roberta Sessoli","UNIVERSITA DEGLI STUDI DI FIRENZE","Molecular nanomagnets, also known as Single Molecule Magnets (SMMs), are a class of molecules that at low temperature exhibit magnetic hysteresis of pure molecular origin and not related to a cooperative effect. In the past fifteen years they have attracted  great interest for their potentiality to act as magnetic memory units and for the many quantum effects in the dynamics of their magnetization. Recently our observation that the magnetic bistability is retained when a tetranuclear iron(III) molecular cluster is grafted to a metallic surface has renewed the interest in these materials, which appear the ideal candidates for fundamental investigations on the interplay between conducting electrons and magnetic degrees of freedom in the emerging field of molecular spintronics. In this project we plan to benefit from our leading position in the research on SMMs to explore novel phenomena originated by the combination of SMMs with conducting and magnetic substrates in hybrid structures. Our interdisciplinary approach starts  from the design and synthesis of SMMs and includes their assembling on surface from solution or in high vacuum as well as the tuning of the interaction with conducting, and magnetic substrates through chemical tuning of SMMs. We will focus on the quantum dynamics of the magnetization that seems particularly affected by the interaction with a magnetic substrate opening perspectives for novel spintronic devices and for an unexplored strategy to increase the blocking temperature of SMMs. Additional aspects will be investigated, in particular the use of switchable magnetic molecules as well as the possibility to modify the interface with yet unexplored approaches, for instance exploiting the magnetic torque on molecules with large magnetic anisotropy.","2269200","2011-01-01","2015-12-31"
"MOLNANOSPIN","Molecular spintronics using single-molecule magnets","Wolfgang Wernsdorfer","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","A revolution in electronics is in view, with the contemporary evolution of two novel disciplines, spintronics and molecular electronics. A fundamental link between these two fields can be established using molecular magnetic materials and, in particular, single-molecule magnets, which combine the classic macroscale properties of a magnet with the quantum properties of a nanoscale entity. The resulting field, molecular spintronics aims at manipulating spins and charges in electronic devices containing one or more molecules. The main advantage is that the weak spin-orbit and hyperfine interactions in organic molecules suggest that spin-coherence may be preserved over time and distance much longer than in conventional metals or semiconductors. In addition, specific functions (e.g. switchability with light, electric field etc.) could be directly integrated into the molecule.  In this context, the project proposes to fabricate, characterize and study molecular devices (molecular spin-transistor, molecular spin-valve and spin filter, molecular double-dot devices, carbon nanotube nano-SQUIDs, etc.) in order to read and manipulate the spin states of the molecule and to perform basic quantum operations. MolNanoSpin is designed to play a role of pathfinder in this  still largely unexplored - field. The main target for the coming 5 years concerns fundamental science, but applications in quantum electronics are expected in the long run.  The visionary concept of MolNanoSpin is underpinned by worldwide research on molecular magnetism and supramolecular chemistry, the 10-year long experience in molecular magnetism of the PI, his membership in FP6  MAGMANet  NoE, and collaboration with outstanding scientists in the close environment of the team. During the last year, the recently founded team of the PI has already demonstrated the first important results in this new research area.","2096703","2008-11-01","2013-10-31"
"MOLPROCOMP","From Structure Property to Structure Process Property Relations in Soft Matter – a Computational Physics Approach","Kurt Kremer","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","""From cell biology to polymer photovoltaics, (self-)assembly processes that give rise to morphology and functionality result from non-equilibrium processes, which are driven by both, external forces, such as flow due to pressure gradients, inserting energy, or manipulation on a local molecular level, or internal forces, such as relaxation into a state of lower free energy. The resulting material is arrested in a metastable state. Most previous work has focused on the relationship between structure and properties, while insight into the guiding principles governing the formation of a (new) material, has been lacking. However, a comprehensive molecular level understanding of non-equilibrium assembly would allow for control and manipulation of material processes and their resulting properties. This lag of knowledge can be traced to the formidable challenge in obtaining a molecular picture of non-equilibrium assembly. Non-equilibrium processes have been studied extensively on a macroscopic level by non-equilibrium thermodynamics. We take a novel route approaching the challenge from a molecular point of view. Recent advances in experimental, but especially computational modeling, now allow to follow (supra-) molecular structural evolution across the range of length and time scales necessary to comprehend, and ultimately control and manipulate macroscopic functional properties of soft matter at the molecular level. Soft matter is particularly suited for that approach, as it is “slow” and easy to manipulate. We take the computational physics route, based on simulations on different levels of resolution (all atom, coarse grained, continuum) in combination with recent multiscale and adaptive resolution techniques. This work will initiate the way towards a paradigm change from conventional Structure Property Relations (SPR) to molecularly based Structure Process Property Relations (SPPR).""","2025000","2014-02-01","2019-01-31"
"MolS@MolS","Controlling Molecular Spin at the Molecular Scale","Herre Van Der Zant","TECHNISCHE UNIVERSITEIT DELFT","Because of their internal structure, molecules provide novel functionality not realizable in conventional semiconductor-based electronics. One exciting new possibility is that of spintronics: electronic devices using the electron spin to carry and manipulate information. So far, spintronics has been explored in metals and semiconductors. Magnetic molecules in principle enable radically new approaches in using the spin degree of freedom, but their incorporation in solid-state devices is a daunting task. In particular, the main challenge is to control their spin for storing and reading information. We propose to use electric fields and light for this purpose.

Based on our recent breakthroughs in making nanoscale junctions of noble metals and graphene, we will fabricate and study planar spin transistors built up from individual magnetic molecules or nanoparticles. A key device feature is that electrodes are separated by a distance on the scale of the molecular object itself. This geometry allows for in-situ application of strong local electric fields as well as optical fields to modify magnetic states and hence influence the conductance.

The objective of this proposal is to study how the electric conductance through single molecules and nanoparticles can be used to probe their magnetic properties and how external stimuli can control them. We will perform proof-of-principle experiments divided into four challenging tasks: 1) Study of quantum aspects of transport through single magnetic molecules and nanoparticles; 2) Room-temperature studies of molecular magnetism on the molecular scale; 3) Measurement of spin-polarized currents through molecular-scale magnetic junctions; and 4) Control of molecular magnetism by local electric and optical fields.

By obtaining a detailed understanding of the interplay between molecular magnetism and transport we strive to establish new strategies towards in-situ spin-state control and the development of novel spintronic nanodevices.","1998747","2013-04-01","2018-03-31"
"MOLTENEARTH","Fluid Silicates at Extreme Conditions and the Magma Ocean","Lars Peter Stixrude","UNIVERSITY COLLEGE LONDON","Partial melting of silicates dominates the chemical evolution of Earth today and was even more important in Earth’s earlier history.  The Earth may have begun in a completely molten state, a global magma ocean, with silicate liquid extending from a dense silicate atmosphere to the boundary with the iron-rich core at a pressure of 140 GPa.  Deep melt may exist in the Earth today, and the magma ocean may have left signatures of its presence.  However, these signals are still uninterpretable because of a lack of basic knowledge of the behavior of fluid silicates at extreme conditions: very little is known of the physics and chemistry of fluid silicates beyond the conditions of ongoing shallow magma genesis (<3 GPa).  We propose to solve this problem by constructing a comprehensive thermodynamic model (HeFESTo) of multi-component silicate melting, vaporization, and reaction with iron, and the physical properties of liquid and vapor phases over the entire pressure-temperature range relevant to Earth, including impacts and early Earth processes.  To help constrain the thermodynamic model, we will perform new first principles quantum mechanical simulations in the range of pressure, temperature, composition relevant to the early Earth that have not yet been explored by experiment or theory.  Simulations will include key homogeneous and heterogeneous systems of fluid silicates in liquid, vapor, supercritical, and solid forms, including simulations of pure phases, and phase coexistence.  We expect HeFESTo to change our views of magma ocean evolution and lead to new scenarios of Earth’s earliest evolution.  What these scenarios might be is impossible to predict as they will be shaped by still unknown aspects of the physics and chemistry of silicate liquids at extreme conditions, which the MoltenEarth project aims to discover.","2498891","2012-04-01","2018-01-31"
"MOLUSC","Molecules under Light-Matter Strong Coupling","Thomas EBBESEN","CENTRE INTERNATIONAL DE RECHERCHE AUX FRONTIERES DE LA CHIMIE FONDATION","When molecules or molecular materials are placed in the confined field of an optical mode which is resonant with a molecular transition, new hybrid light-matter states can be formed through strong coupling. This can occur even in the dark due to strong coupling with the vacuum electromagnetic field.  The hybrid light-matter states are collective states involving a large number of molecules and they strongly modify the energy levels of the system. While light-matter strong coupling has been extensively studied in optics and quantum physics, the consequences for chemistry and molecular material properties are just beginning to be investigated. The overall aim of this proposal is understand in greater detail the fundamental properties of the hybrid light-matter states and to investigate the implications for the properties of molecules and materials. More specific objectives are: 
1) Deepen our understanding of the hybrid light-matter states from a physical chemistry perspective, including the dynamics and the thermodynamics. This is absolutely essential to develop this subject into a useful tool for chemists and materials scientists. 
2) Demonstrate that the chemical reactions, including enzymatic ones, in the ground state can be modified by selectively coupling individual vibrational modes involved in the chemistry. This could have consequences for site selective chemistry, homogeneous and heterogeneous catalysis among others. 
3) To further enhance molecular material properties, in particular functional solid state materials such as for organic electronics and photovoltaics. Here the key property is the extended nature of the hybrid light-matter state and the associated change in energy levels which modifies the absorption spectrum. 
4) Explore the possibilities of modifying phase transitions of materials under strong coupling and of playing with the quantum features of the hybrid states such as their entanglement to study molecular processes with entangled molecules","2468750","2018-10-01","2023-09-30"
"MOMB","Magneto-optics of layered materials: exploring many-body physics in electronic systems with unconventional bands","Marek Potemski","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""The project will explore many-body physics in emergent quantum Hall effect systems (graphitic layers and surface states of topological insulators) and in layered metals of transition metal dichalcogenides using magneto-optical spectroscopy - unconventional for this purpose, but uniquely applicable to these unconventional systems. Studying the inter Landau level excitations (with Raman scattering techniques) in graphene and its bilayer we will test the basic principles of the role of electron-electron interactions in the regime of the quantum Hall effect. Employing high sensitivity microwave absorption methods, we will attempt to solve one of the most controversial issues in the physics of graphene: the nature of the low temperature ground state of the graphene bilayer. The magneto-optical response (in the far-infrared range) of three dimensional topological insulators will be investigated with the aim of demonstrating a new (half odd-integer) quantum Hall effect of their surface states and possible new exotic ground states of single-cone Dirac fermions. Finally, with a fresh experimental approach (cyclotron resonance absorption on NbSe2 and TaS2 and their thin layers) we will shed new light on one of the most intriguing phenomena in strongly correlated systems: competition between an insulating behaviour (charge density wave state in our case) and the ideal-conductor, superconductivity phase.""","1934041","2013-03-01","2018-02-28"
"MOMENTUM","Angular momentum transfer in galaxy formation and evolution","Françoise Combes","OBSERVATOIRE DE PARIS","The project is dedicated to follow angular momentum within structures over a wide range of scales, to trace galaxy formation and the history of mass assembly. Angular momentum is a key parameter to determine galaxy morphology and kinematics. After primordial spin-up by tidal torques, the subsequent evolution may help to understand galaxy formation, although numerical models fail to reproduce large disk galaxies today, by lack of angular momentum. The project will focus on three areas: -- (i) detailed angular momentum transfer due to non-axisymmetric features, such as bars and spirals, in early galaxies, submitted to internal dynamical processes, but also external matter accretion --- (ii) census of angular momentum exchanges during galaxy interactions, transformation from orbital to internal spin, and exchange between the various components, role of the environment on the density of angular momentum, through cosmic filaments, and formation of large-scale structures  --  (iii) efficient fueling of super-massive black holes (SMBH) in the early universe, through angular momentum transfer, and study of complex feedback processes, in particular around bright cluster galaxies, at the centre of cooling flows. To address these three issues, we will carry on complementary simulations, with the best state-of-the art codes, tree-SPH, multi-phase including sticky particles, or Eulerian AMR code. The complex baryonic physics will be modelled, and comparisons made while varying methods and physical parameters. The highest resolution will be used to trace angular momentum transfer and resonances in idealised galaxies, but boundary conditions will be obtained from cosmological large-scale simulations.
At every step, the simulations will be confronted to observations.","2316000","2011-01-01","2015-12-31"
"MONACAT","Magnetism and Optics for Nanoparticle Catalysis","Bruno CHAUDRET","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","MONACAT proposes a novel approach to address the challenge of intermittent energy storage. Specifically, the purpose is to conceive and synthesize novel complex nano-objects displaying both physical and chemical properties that enable catalytic transformations with a fast and optimum energy conversion. It follows over 20 years of research on “organometallic nanoparticles”, an approach of nanoparticles (NPs) synthesis where the first goal is to control the surface of the particles as in molecular organometallic species. Two families of NPs will be studied: 1) magnetic NPs that can be heated by excitation with an alternating magnetic field and 2) plasmonic NPs that absorb visible light and transform it into heat. In all cases, deposition of additional materials as islands or thin layers will improve the NPs catalytic activity. Iron carbides NPs have recently been shown to heat efficiently upon magnetic excitation and to catalyse CO hydrogenation into hydrocarbons. In order to transform this observation into a viable process, MONACAT will address the following challenges: determination and control of surface temperature using fluorophores or quantum dots, optimization of heating capacity (size, anisotropy of the material, crystallinity, phases: FeCo, FeNi, chemical order), optimization of catalytic properties (islands vs core-shell structures; Ru, Ni for methane, Cu/Zn for methanol), stability and optimization of energy efficiency. A similar approach will be used for direct light conversion using as first proofs of concept Au or Ag NPs coated with Ru. Catalytic tests will be performed on two heterogeneous reactions after deposition of the NPs onto a support: CO2 hydrogenation into methane and methanol synthesis. In addition, the potential of catalysis making use of self-heated and magnetically recoverable NPs will be studied in solution (reduction of arenes or oxygenated functions, hydrogenation and hydrogenolysis of biomass platform molecules, Fischer-Tropsch).","2472223","2016-06-01","2021-05-31"
"MOQUACINO","Memory-enabled Optical Quantum Communications and Information Networks","Ian Walmsley","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","""The primary objective of this proposal is to build a large-scale photonic quantum network in order to enter a new and different regime of macroscopic quantum behaviour for light. We intend to implement a network of sufficient scale that the behaviour of the system is uncomputable, and that new physics will emerge from the study of its dynamics that will inform research in a number of fields dealing with complex systems, from fundamental physics to biology. This network will also enable new modes of quantum communications, sensing, simulation and computation that go well beyond anything that is possible using classical physics. Specifically we shall develop a truly scalable approach to building both linear and nonlinear photonic networks, and construct a 20-node, 20-qubit, 20-particle loss-tolerant photonic network that operates palpably beyond classical boundaries. It has not proven possible to break through into a new regime of complexity to date because of the intrinsically probabilistic character of the feasible network operations. In this proposal, we overcome this bottleneck. Our approach will integrate robust, simple, broadband photonic quantum memories together with novel pure-state light sources fabricated in precise 3-D photonic structures, coupled to integrated, highly-efficient photon-number-resolving detectors. Deterministic photon-photon interactions will be engendered either by measurement and storage or by nonlinear interactions in the memory made possible by coherent quantum feedback control. This will deliver a multi-node array of synchronized conditional quantum operations, novel quantum light-matter interactions and distillation of high-quality entangled states. The outcome of this project will be a viable means to explore new regimes of many-body quantum physics, with impact on information processing, including multi-particle quantum simulation, multiparty quantum repeaters, multimode quantum sensors and elementary quantum computing.""","1738404","2014-04-01","2019-03-31"
"MoQuOS","Molecular Quantum Opto-Spintronics","Wolfgang Wernsdorfer","KARLSRUHER INSTITUT FUER TECHNOLOGIE","The field of quantum materials has developed into an active playground for testing novel ideas and protocols for future devices governed by the principles of quantum mechanics. Quantum technologies might result in revolutionary improvements in terms of capacity, sensitivity, and speed, and will be the decisive factor for success in many industries and markets (http://qurope.eu/manifesto). 
Molecular magnets, realized by tailored molecules hosting magnetic ions, offer the possibility to obtain a few-spin object, necessary in order to perform basic quantum operations. The first molecular devices have enabled the read-out and manipulation of the spin states. Single-shot read-out times of one second have been achieved for a nuclear spin, which is, however, too slow for applications. Here, we will remove this bottleneck by developing reliable, fast, and scalable optical methods for the read-out of both electron and nuclear spin states allowing us to perform basic quantum-information processing protocols. 
The scientific approach is to use high-quality quantum emitters (NV-centers in diamond, ligand quantum emitters, 2D materials, or other optically active ions) to optically read-out efficiently the spin states of the magnetic molecules. Special care will be taken to minimize back action from the read-out emitter on the spin system and thereby preserving the quantum states. Various optical techniques (surface enhanced fluorescence, surface enhanced Raman scattering, and optical fiber scanning cavities) will be used to enhance the light-matter interaction to obtain a reliable and fast optical read-out. Due to of the possibilities of scanning the probing laser and using different fluorescence energies, the optical read-out is scalable to larger systems and 2D networks of molecules.
The project deals with the fundamental science of optical manipulation and characterization of molecular qubits and will advance the field of quantum optics and quantum electronics of single-spin systems.","2335416","2017-07-01","2022-06-30"
"MorePheno","Collider Phenomenology and Event Generators","Håkan Torbjörn Sjöstrand","LUNDS UNIVERSITET","Collider physics is about exploring the smallest constituents of matter, and unravelling the basic laws of the Universe. Unfortunately there can be a huge gap between a one-line formula of a fundamental theory and the experimental reality it implies. Phenomenology is intended to fill that gap, e.g. to explore the consequences of a theory such that it can be directly compared with data. 
Nowhere is the gap more striking than for QCD, the theory of strong interactions, which dominates in most high-energy collisions, like at the LHC (Large Hadron Collider) at CERN. And yet, when such collisions produce hundreds of outgoing particles, calculational complexity is insurmountable. Instead ingenious but approximate QCD-inspired models have to be invented.
Such models are especially powerful if they can be cast in the form of computer code, and combined to provide a complete description of the collision process. An event generator is such a code, where random numbers are used to emulate the quantum mechanical uncertainty that leads to no two collision events being quite identical.
The Principal Investigator is the main author of PYTHIA, the most widely used event generator of the last 30 years and vital for physics studies at the LHC. It is in a state of continuous extension: new concepts are invented, new models developed, new code written, to provide an increasingly accurate understanding of collider physics. But precise LHC data has put a demand on far more precise descriptions, and have also shown that some models need to be rethought from the ground up. 
This project, at its core, is about conducting more frontline research with direct implications for event generators, embedded in a broader phenomenology context. In addition to the PI, the members of the theoretical high energy physics group in Lund and of the PYTHIA collaboration will participate in this project, as well as graduate students and postdocs.","1990895","2015-11-01","2020-10-31"
"MOSILSPIN","Modeling Silicon Spintronics","Siegfried Selberherr","TECHNISCHE UNIVERSITAET WIEN","The breath taking increase in performance of integrated circuits became possible by continuous miniaturization of CMOS devices. On this exciting path many tough problems were resolved; however, growing technological challenges and soaring costs will gradually bring scaling to an end. This puts foreseeable limitations to the future performance increase, and research on alternative technologies and computational principles becomes important. Spin attracts attention as alternative to the charge degree of freedom for computations and non-volatile memory applications. Silicon as main material of microelectronics is characterized by negligible spin-orbit interaction and zero-spin nuclei and should display long spin coherence times. Combined with the potentially easy integration with CMOS, long spin coherence makes silicon perfectly suited for spin-driven applications, as confirmed by recent impressive demonstrations of spin injection, coherent propagation, and detection. The success of microelectronics technology has been well assisted by smart Technology Computer-Aided Design tools; however, support for spin applications is entirely absent. The objective here is to create, test, and apply a simulation environment for spin-based devices in silicon. Microscopic models describing the physical properties relevant to the spin degree of freedom are developed. Special attention will be paid to investigate, how to increase the spin coherence time. One option is based on completely removing the valley degeneracy in the conduction band by [110] uniaxial stress. Understanding spin-polarized transport in silicon and in compatible hysteretic materials allows using the spin-torque effect to invent, model, and optimize prototypes of switches and memory cells for the 21st century.","1678500","2010-03-01","2016-02-29"
"MoTIVE","Moments in Time in Immersive Virtual Environments","Mel SLATER","UNIVERSITAT DE BARCELONA","This project investigates how virtual reality (VR) can be used to live through an historical event so that participants perceive themselves to be there (Place Illusion) and take the events that are happening as real (Plausibility). To provide an application focus the research will be constructed around recreating a famous rock concert from the 1980s. The specific elements of the research involve an Agent Based Model (ABM) that populates the environment with thousands of virtual characters with their behaviour driven by the music. This ABM will run in VR embedding participants as a type of agent. Agents will have personality and emotional state that can influence one another, and the actions and state of participants will also influence the unfolding of the model. Based on the predictive coding model of brain functioning a theory of Place Illusion will be developed that results in a universal measurement. Similarly, the Plausibility Illusion will be modelled and corresponding universal measure derived. Participants in VR will be embodied, so that they will have a first person perspective life-sized virtual body that moves as they do. We will exploit the concept of body ownership and its consequences for attitudinal, behavioural, cognitive and agency changes to give people unique experiences of the virtual events, and carry out a series of experiments to assess the influence of being transported back in time in a younger body has on ageing. Our recent discovery that illusory agency can be realised through virtual embodiment will be used for research on improved motor learning. To allow people to move through the environment we will investigate paradigms for virtual walking, and in particular whether the multisensory principles involved in body ownership illusions can be used to lessen simulator sickness. The long term goal of the project is to understand how to capture treasured past moments lost in time, through their reproduction in ABM inspired virtual reality.","2199318","2018-01-01","2022-12-31"
"MPCPRO","Better MPC Protocols in Theory and in Practice","Ivan Bjerre Damgård","AARHUS UNIVERSITET","Multiparty computation (MPC) is a cryptographic technique allowing us to build distributed computer systems for handling confidential data. We can control exactly what information is released from the system, and privacy of the input data is maintained, even if an adversary breaks into several of the machines in the system. The efficiency of MPC protocols has been significantly improved in recent years. There are countless applications and the techniques are just now entering the commercial domain. However, the theory of the area has in several respects failed to keep up with this development, and we are still very far from being able to apply MPC to large-scale applications. In this project, we propose that state of the art for MPC protocols can be dramatically advanced by 

1) Developing a completely new theory for the performance of MPC protocols based on a more detailed model that better reflects what happens when protocols are executed on real platforms.
2) Use the new theory to guide development and implementation of new MPC protocols that will perform much better in practice. 
3) Explore the limits of what we can achieve by showing new lower bounds for MPC protocols, attacking a number of long-standing open problems. This will enable us to focus our attention to where improvements are possible.","2421995","2015-10-01","2020-09-30"
"MPOES","Mathematical Physics of Out-of-Equilibrium Systems","Antti Jukka Kupiainen","HELSINGIN YLIOPISTO","The purpose of the project is to develop  new tools for a mathematical analysis of out of equilibrium systems. My main goal is a rigorous proof of Fourier's law for a Hamiltonian  dynamical system. In addition I plan to study various fundamental problems related to transport in such systems.  I will consider extended dynamical systems consisting of a large number (possibly infinite) of subsystems that are coupled to each other. This set includes discrete and continuous  wave equations, non-linear Schrödinger equation and coupled chaotic systems.   I believe mathematical progress can be made in two cases: weakly nonlinear systems and strongly chaotic ones.  In the former class I propose to study the kinetic limit and corrections to it, anomalous conductivity in low dimensional systems, interplay of disorder and nonlinearity and weak turbulence.  In the latter class my goal is to prove Fourier's law. The methods will involve a map of the Hamiltonian problem to a probabilistic one dealing with random walk in a random environment and an application of rigorous renormalization group to study the latter.   I believe the time is ripe for a breakthrough in a rigorous analysis of transport in systems with conservation laws. A proof of Fourier's law would  be a major development in mathematical physics and would remove blocks from progress in other fundamental issues of non equilibrium dynamics.  I have previously solved hard problems using the methods proposed in this proposal and feel myself to be in a good position to carry out its goals.","1293687","2009-03-01","2014-02-28"
"MQC","Methods for Quantum Computing","Andris Ambainis","LATVIJAS UNIVERSITATE","""Quantum information science (QIS) is a young research area at the frontier of both computer science and physics. It studies what happens when we apply the principles of quantum  mechanics to problems in computer science and information processing. This has resulted in many unexpected discoveries and opened up new frontiers.

Quantum algorithms (such as Shor’s factoring algorithm) can solve computational problems that are intractable for conventional computers.  Quantum mechanics also enables quantum cryptography which provides an ultimate degree of security that cannot be achieved by conventional methods. These developments have generated an enormous interest both in building a quantum computer and exploring the mathematical foundations of quantum information.

We will study computer science aspects of QIS. Our first goal is to develop new quantum algorithms and, more generally, new algorithmic techniques for developing quantum algorithms. We will explore a variety of new ideas: quantum walks, span programs, learning graphs, linear equation solving, computing by transforming quantum states.

Secondly, we will study the limits of quantum computing. We will look at various classes of computational problems and analyze what are the biggest speedups that quantum algorithms can achieve. We will also work on identifying  computational problems which are hard even for a quantum computer. Such problems can serve as a basis for cryptography that would be secure against quantum computers.

Thirdly, the ideas from quantum information can lead to very surprising connections between different fields. The mathematical methods from quantum information can be applied to solve purely classical (non-quantum) problems in computer science. The ideas from computer science can be used to study the complexity of physical systems in quantum mechanics. We think that both of those directions have the potential for unexpected breakthroughs and we will pursue both of them.""","1360980","2013-05-01","2018-04-30"
"MSTAR","Massive Star Formation through the Universe","Jonathan TAN","CHALMERS TEKNISKA HOEGSKOLA AB","Massive stars are important throughout astrophysics, yet there remain many open questions about how they form. These include: What is the accretion mechanism of massive star formation? What sets the initial mass function of stars, especially at the highest masses? What is the relation of massive star formation to star cluster formation? How do massive star and star cluster formation vary with galactic environment? What was the nature of the first stars to form in the universe and could these have been the seeds for supermassive black holes? With recent advances in both theoretical/computational techniques and observational facilities, the time is now ripe for progress on answering these questions. 

Here we propose an ambitious research program that combines latest theoretical studies of massive star and star cluster formation, including analytic, semi-analytic and full numerical simulations, with state-of-the-art observational programs, including several large surveys. We will: 1) Develop new theoretical models for how individual massive stars form from gas cores, focusing on diagnostics and including study of how the process depends on galactic environment; 2) Test these protostar models against observations, especially with ALMA, SOFIA, JVLA, HST and in the near future with JWST and eventually TMT & E-ELT; 3) Develop theoretical models for star cluster formation, including both magneto-hydrodynamics of the gas and N-body modeling of the young stellar population, with the focus on how massive stars form and evolve in these systems; 4) Test these protocluster models against observational data of young and still-forming star clusters, especially with ALMA, HST, Chandra, JWST and ground-based near-IR facilities; 5) Explore new theoretical models of how the first stars formed, with potential implications for the origins of supermassive black holes - one of the key unsolved problems in astrophysics.","2500000","2018-09-01","2023-08-31"
"MULTI-SCALE FLOWS","Multi-scale modeling of mass and heat transfer in dense gas-solid flows","Johannes Alfonsius Maria Kuipers","TECHNISCHE UNIVERSITEIT EINDHOVEN","Dense gas-solid flows have been the subject of intense research over the past decades, owing to its wealth of scientifically interesting phenomena, as well as to its direct relevance for innumerable industrial applications. Dense gas solid flows are notoriously complex and its phenomena difficult to predict. This finds its origin in the large separation of relevant scales: particle-particle and particle-gas interactions at the microscale (&amp;lt; 1 mm) dictate the phenomena that occur at the macroscale (&amp;gt; 1 meter), the fundamental understanding of which poses a huge challenge for both the scientific and technological community. This proposal is aimed at providing a comprehensive understanding of large-scale dense gas-solid flow based on first principles, that is, based on the exchange of mass, momentum and heat at the surface of the individual solid particles, below the millimeter scale. To this end, we employ a multi-scale approach, where the gas-solid flow is described by three different models. Such an approach is by now widely recognized as the most rigorous and viable pathway to obtain a full understanding of dense-gas solid flow, and has become very topical in chemical engineering science. The unique aspect of this proposal is the scale and the comprehensiveness of the research: we want to consider, for the first time, the exchange of heat, momentum and energy, and the effects of polydispersity, heterogeneity, and domain geometries, at all three levels of modeling, and validated by one-to-one experiments. These generated insight and models will be extremely relevant for the design and scale-up of industrial equipment involving dispersed particulate flow, which is currently a fully empirical process, involving expensive and time-consuming experimentation.","2500000","2010-03-01","2015-02-28"
"MULTIAX","MULTIAX: Multiaxial and Multiscale Plasticity in Metals","Helena Van Swygenhoven","PAUL SCHERRER INSTITUT","""Our ambition is to probe the influence of non-proportional multiaxial straining on the multiscale aspects of metal plasticity with focus on three deformation mechanisms: dislocation plasticity in bcc metals, mechanical twinning in fcc metals and the martensitic phase transformation. These mechanisms play a key role in modern TWIP and TRIP steels, yet about their response to multiaxial loading not much is known.
The underlying hypothesis of this research project is that by performing biaxial deformation tests at the micro-, meso- and macro-scale meanwhile following the microstructure insitu, ground-breaking insight can be obtained on how a second strain path, a change in strain path with or without prior unloading affects the operation of the deformation mechanism, the defect accumulation and as a consequence, the evolving microstructure.
The expected outcome of the research will help the formulation of criteria to be implemented in micromechanical models, for which constitutive equations are now relying solely on a knowledgebase derived from uniaxial testing.
Operationally, the project contains a development phase and a research phase. First a micro- and meso-scale biaxial test rig will be developed, allowing deforming small samples in two orthogonal directions independently, compatible to be installed at various Xray beamlines of synchrotron facilities in Europe and in SEMs. The research phase will be multiscale: the response of each deformation mechanism will be investigated at the level of the mechanism itself, at the level of an oligocrystal focusing on transmission of strain across grain boundaries and at the macrosopic level focussing on the evolution of the microstructure. Experimental research will be accompanied by synergetic computational simulations.""","2499900","2014-03-01","2019-02-28"
"multiBB","Boron-boron multiple bonding","Holger Braunschweig","JULIUS-MAXIMILIANS UNIVERSITAET WUERZBURG","Multiple bonding between atoms is immensely important to chemistry, biology, physics and their associated industries; multiple bonds are both ubiquitous in everyday products and extremely useful functionalities for effecting chemical transformations. While very common with the elements carbon, nitrogen and oxygen, multiple bonding is in comparison extremely rare with other elements. Multiple bonding between heavier elements of the main group of the periodic table becomes less favourable the heavier the element becomes. However, this does not explain the relative paucity of multiple bonding with boron, which is immediately to carbon's left on the periodic table. In particular, isolable, stable compounds containing multiple bonds between two boron atoms are extremely rare, and until 2007 only a handful of charged examples existed.

A revolution in this field has recently been witnessed with the syntheses of the first neutral compounds with boron-boron double bonds, diborenes, and the first compounds with boron-boron triple bonds, diborynes. The first neutral diborenes were prepared in 2007, however, we have recently developed a number of rational, selective and more general routes to these compounds. The first diboryne compounds were prepared by our group in 2012. The significance of these two families of molecules is not only their unusual multiple bonding but also the extremely high electron density on the boron atoms, an unusual situation for an element that is known for its electron-poor character. This high electron density leads to strong boron-based nucleophilicity and extremely high reduction potentials – both highly novel phenomena.

This proposal aims to: (A) comprehensively explore the syntheses of these unique compounds and the limits thereof, and to (B) exploit the unusual reactivity of these electron-rich boron molecules in synthesis, small-molecule activation and materials science.","2500000","2016-05-01","2021-04-30"
"MULTICOMB","Multidimensional laser frequency comb spectroscopy of molecules","Theodor Hänsch","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","The advent of laser frequency combs a decade ago has revolutionized optical frequency metrology. Such combs have become enabling tools for a growing tree of applications, from optical atomic clocks to attosecond science.
Recently, the millions of precisely controlled laser comb lines produced with a train of ultrashort laser pulses have been harnessed for highly multiplexed molecular spectroscopy.
Fourier multi-heterodyne spectroscopy with frequency combs is emerging as a powerful new spectroscopic tool. Cavity-enhanced absorption spectroscopy with two frequency
combs just demonstrated a dramatically improved sensitivity, compared to conventional Fourier spectroscopy, with recording times shortened from seconds to microseconds.
Such capabilities open exciting opportunities for instantaneous trace gas analysis, time-resolved spectroscopy of short-lived molecular species, precision spectroscopy and
hyperspectral imaging. Moreover, since frequency combs involve intense ultrashort laser pulses, nonlinear interactions can be harnessed, such as saturation or coherent transient
phenomena including photon echoes, in analogy to multi-dimensional NMR spectroscopy. Envisioned applications range from optical labeling for the simplification and
disentanglement of complex spectra to coherent control for the selective microscopic imaging of unlabeled biomolecules.
Such new spectroscopic methods will be initially explored with state-of-the-art frequency comb sources, based on femtosecond fiber lasers and nonlinear conversion. Novel compact and reliable spectroscopic instruments with unprecedented capabilities will become possible with frequency comb generators based on cascaded four wave mixing in toroidal micro-resonators.","2389400","2011-09-01","2016-08-31"
"MULTIFLOW","Multiscale dynamics of turbulent flows","Javier Jiménez Sendin","UNIVERSIDAD POLITECNICA DE MADRID","Turbulence is a fundamental unsolved problem, at whose core are the multiscale processes that transfer, for example, energy across the inertial range of scales, or momentum across wall-bounded shear flows.  Turbulence is also key to applications, from industrial design and energy generation to climate dynamics, where the worst uncertainties are often due to its modelling.  Its practical computation and control have been hindered by empirical models and boundary conditions, in large part because of insufficient understanding of the multiscale transfer just mentioned. Direct simulations, without approximation, are expensive, but the past few years have seen the appearance of larger computers and reasonably-priced disks that allow, for the first time, the compilation of time-resolved  data sets of canonical turbulent flows with high enough Reynolds numbers to be truly multiscale, as well as the possibility of performing conceptual experiments on them.  The premise of this proposal is that  those new capabilities should allow us to elucidate, once and for all, the physics underlying the multiscale transfer processes in turbulence in the next five years, especially in shear flows near walls.  That will allow the formulation of more realistic engineering models, but the immediate goal of the proposal is to answer the fundamental questions that have resisted two centuries of attack by physicists and engineers. An important part of the work will involve adapting simulation codes to the new computer architectures expected in the next few years. Neither large-scale computing nor data mining are trivial activities, but our group has specialised in both during the past 20 years, particularly for the study of turbulence.","2200000","2011-02-01","2016-01-31"
"MULTILAT","Multi-phase Lattice Materials","Norman Fleck","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The integration of materials and architectural features at multiple scales into structural mechanics changed the way buildings were designed and gave us the Eiffel Tower, for example.  This approach led to the development of computational design approaches used in modern day construction and it is believed that similar principles can be applied to the design and manufacture of new lattice-based microstructures.  This vision, of fundamentally changing how materials are developed, is the inspiration behind this programme.  A systematic procedure for generating multi-phase lattice materials - MULTILAT - will be developed by micro-architectural design, in order to fill gaps in material property space.  New engineering devices and products frequently require materials with extreme properties, such as high strength and toughness at low density, and a systematic means of material invention is needed.  This proposal breaks much ground in developing new fundamental concepts, ranging from micro-architectured surface coatings to inter-penetrating bulk lattices of dissimilar materials.  Based on Fleck’s earlier work on the mechanics of foams and lattice materials, the unique and novel aspects of the proposed research are to design multi-phase lattice materials made from a wide range of materials, topologies and length scales.  The focus will be on 2 inter-penetrating lattices, but the topology of each can range from 1D fibres, through 2D meshes to 3D lattices and foams.  A focus will be lightweight strong and tough lattices, and surface lattices (as coatings).  Examples from Nature will be used to develop fundamental concepts, ranging from the high toughness and ductility of the root of a tree branch, to the high toughness of human skin and underlying fat. The successful project will lay scientific foundations for new engineering devices and solutions that will improve our competitiveness and quality of life.","2499870","2016-01-01","2020-12-31"
"MULTIMOD","Multi-Mathematics for Imaging and Optimal Design Under Uncertainty","Habib Ammari","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The aim of this interdisciplinary project is to develop new mathematical and statistical tools, probabilistic approaches, and inversion and optimal design methods to address emerging modalities in medical imaging, nondestructive testing, and environmental inverse problems. It merges the complementary expertise of the investigators in order to make a breakthrough  in the field of
mathematical imaging and optimal design by solving the most challenging problems posed by new imaging modalities. The PI and Co-PI are leading experts in their respective fields (applied
analysis and probability) and their researches have very strong interdisciplinary nature.

The goal of this project is to synergize asymptotic imaging, stochastic modelling, and analysis of both deterministic and stochastic wave propagation phenomena. We want to throw a bridge across the deterministic and stochastic aspects and tools of mathematical imaging. This requires a deep understanding of the different scales in the physical problem, an accurate modelling of the noise sources, and fine mathematical analysis of complex phenomena. The emphasis of this project will be put on deriving for each of the challenging imaging problems that we will consider, the best possible imaging functionals in the sense of stability and resolution. For optimal design problems, we
will evaluate the effect of uncertainties on the geometrical or physical parameters and design accurate optimal design methodologies.

In this project, we will build an exceptional interdisciplinary research and an innovative approach to training in applied mathematics.  We will train a new generation of applied mathematicians who will master both the probabilistic and analytical tools to best meet the challenges of emerging technologies.","1920000","2011-04-01","2016-03-31"
"MULTITHERMAN","Multiscale Thermal Management of Computing Systems","Luca Benini","ALMA MATER STUDIORUM - UNIVERSITA DI BOLOGNA","As computing platforms evolve into heterogeneous, 3D-integrated many-cores, increasing performance per unit area (or volume) comes unavoidably with growing power density, which becomes heat, leading to degradation, acceleration of chip aging and increase in cooling costs. Thermal dissipation is difficult to control at the micro-scale, where typically spatial and temporal power gradients are orders-of-magnitude higher than at the macro-scale.
MultiThermMan will move beyond the unsustainable worst-case design practices adopted in traditional thermal planning and reactive thermal management. We propose to integrate thermal-aware platform design, thermal control with workload management and shaping in a distributed, multi-scale strategy. This would enable to dynamically adjust the operating mode and active cooling control of each component in complex computing platforms to achieve the highest performance compatible with temperature constraints. The development of a synergistic performance, power and thermal management strategy requires major breakthroughs in several areas, namely architectures, run-time systems, resource management middleware, code optimization tools and programming models. To meet this challenge, MultiThermMan will bring together concepts and techniques from several disciplines: computer architecture and circuits, control theory, combinatorial and continuous optimization. statistical model-building and artificial intelligence.  Results will be demonstrated on physical and virtual prototypes, proving practical applicability and relevance for industrial applications.","2483397","2012-04-01","2018-03-31"
"Multiturbulence","Fractal-generated fluid flows: new flow concepts, technological innovation and fundamentals","John Christos Vassilicos","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","The unprecedented requirements set by the
dramatically evolving energy, environmental and climatic constraints
mean that industry needs new turbulent and vortical flow concepts for
new flow-technology solutions. The full potential and economic impact
of radically new industrial flow concepts can only be realised with a
step change (i) in the ways we generate and condition turbulent and
vortical flows and (ii) in our understanding of turbulent flow
dynamics and the consequent new predictive approaches.
Fractal/multiscale-generated turbulent and vortical flows are a new
family of flow concepts which I have recently pioneered and which
hold the following double promise:

(1) as the basis for a raft of conceptually new technological flow
solutions which can widely set entirely new industrial standards: this
proposal focuses on energy-efficient yet effective mixing devices;
low-power highly-enhanced heat exchangers; high-performance wings for
UAVs, cars, wind turbines; and realistic wind-field design
technologies for wind tunnel tests of tall structures such as
supertall skyscrapers and wind turbines.

(2) As the key new family of turbulent flows which will allow
hitherto impossible breakthroughs in our theory and modelling of fluid
turbulence.

I propose to realise this double promise by a combined
experimental-computational approach which will use cutting edge High
Performance Computing (HPC) and high-fidelity simulations based on a
new code which combines academic accuracy with industrial versatility
and which is specifically designed to perform very efficient massively
parallel computations on HPC systems. I will run these simulations in
tandem with a complementary wide range of wind tunnel, water channel
and other laboratory measurements in a two-way interaction between
laboratory and computer experiments which will ensure validations and
breadth of results.","2317265","2013-05-01","2018-04-30"
"MULTIWAVE","Multidisciplinary Studies of Extreme and Rogue Wave Phenomena","Frederic Dias","UNIVERSITY COLLEGE DUBLIN, NATIONAL UNIVERSITY OF IRELAND, DUBLIN","MULTIWAVE is an interdisciplinary project at the frontiers of mathematics, physics and engineering which will explore important open questions in nonlinear wave propagation and the emergence of extreme events. The work necessitates a Co-Investigator approach in order to carry out coordinated analytical, numerical and experimental studies of the nonlinear effects that form the subject of the proposal.  The project builds on recent international developments in the field of nonlinear waves led by the co-investigators that have shown how analogies between optical systems and the deep ocean provide new insights into the generation of the infamous hydrodynamic rogue waves on the ocean.  These results, which have led to the first experimental confirmation in 2010 of analytic predictions of hydrodynamics that have remained untested for 25 years, have now opened up the possibility for an optical system to directly study the dynamics and statistics of extreme nonlinear wave shaping.  This is a tremendous advance comparable to the introduction of optical systems to study chaos in the 1970s, and the co-investigators aim to be at the forefront of this research effort.  Core theoretical elements in the project will uncover the fundamental mechanisms underlying the emergence of large scale coherent structures from a turbulent environment, and resolve basic questions of energy transport in the presence of nonlinearity. These analytical studies will be complemented by numerical simulations and laboratory experiments in optical systems.  Specifically, recent advances in optical technology will enable the benchtop development of an “optical wave tank” that will accurately simulate multiple propagation scenarios in hydrodynamics and ocean systems.  Emphasis will be placed on extreme rogue wave events which are difficult or even impossible to study quantitatively in their natural oceanic environment.","1831800","2012-04-01","2016-09-30"
"MUNATOP","Multi-Dimensional Study of non Abelian Topological States of Matter","Stern","WEIZMANN INSTITUTE OF SCIENCE LTD","Non-abelian topological states of matter are of great interest in condensed matter physics,
both due to their extraordinary fundamental properties and to their possible use for quantum
computation. The insensitivity of their topological characteristics to disorder, noise,
and interaction with the environment may lead to realization of quantum computers with
very long coherence times. The realization of a quantum computer ranks among the foremost
outstanding problems in physics, particularly in light of the revolutionary rewards
the achievement of this goal promises.
The proposed theoretical study is multi-dimensional. On the methodological side the
multi-dimensionality is in the breadth of the studies we discuss, ranging all the way from
phenomenology to mathematical physics. We will aim at detailed understanding of present
and future experimental results. We will analyze experimental setups designed to identify,
characterize and manipulate non-abelian states. And we will propose and classify novel
non-abelian states. On the concrete side, the multi-dimensionality is literal. The systems
we consider include quantum dots, one dimensional quantum wires, two dimensional planar
systems, and surfaces of three dimensional systems.
Our proposal starts with Majorana fermions in systems where spin-orbit coupling, Zeeman
fields and proximity coupling to superconductivity are at play. It continues with “edge
anyons”, non-abelian quasiparticles residing on edges of abelian Quantum Hall states. It
ends with open issues in the physics of the Quantum Hall Effect.
We expect that this study will result in clear schemes for unquestionable experimental
identification of Majorana fermions, new predictions for more of their measurable consequences,
understanding of the feasibility of fractionalized phases in quantum wires, feasible
experimental schemes for realizing and observing edge anyons, steps towards their classification,
and better understanding of quantum Hall interferometry.","1529107","2013-10-01","2018-09-30"
"MUSiC","Quantum Metamaterials in the Ultra Strong Coupling regime","Jérôme Jean-Constant Faist","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Due to their mixed photon-electronic excitation character, cavity polaritons have highly interesting properties. Especially interesting is the so-called ultrastrong coupling regime, reached when the strength of the photon-two-level system coupling is larger than the energy of the resonant state. We have recently demonstrated that terahertz metamaterials coupled to high-mobility two-dimensional electron gases is an almost ideal, field-tunable system that enables the exploration of this ultra-strong coupling regime.
In this project, we want to explore four key physical questions opened by this new approach. First we plan to explore the limit of ultra-strong coupling in our systems, including the emission of Casimir-like squeezed vacuum photons upon non-adiabatic change in the coupling energy and parametric generation of light. Secondly we would like to test a theoretical prediction anticipating, in the ultra-strong coupling regime, a quantum phase transition to a Dicke superradiant state upon substitution of the GaAs/AlGaAs two-dimensional electron gas by a graphene layer or multilayers. Thirdly, we claim that our metamaterial-based system also enables the study of coupled polaritons by either direct meta-atom electromagnetic coupling or using a waveguide bus and superconducting circuits. Finally, we want to explore polaritonic emitters and non-linear elements.","2496560","2014-04-01","2019-03-31"
"MUSICOS","MUSE Imaging of the Cosmic Web – Ultra-Deep Observations of  Intergalactic and Circumgalactic Gas","Roland, Marie Bacon","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""This project aims at making major advances in our understanding of galaxy formation through the direct detection and spectroscopic mapping of gas outside of galaxies at high redshifts. This experiment will employ the 2nd generation ESO-VLT instrument MUSE, currently being finalised under my leadership. MUSE is a highly innovative integral field spectrograph featuring an unparalleled combination of large field of view, excellent angular resolution, and sensitivity to faint signals. The instrument will arrive at the observatory in mid 2013 and soon after begin with the buildup of a revolutionary set of “spectroscopic deep fields”, which will constitute a significant fraction of the 255 nights of observing time granted to the MUSE consortium in return for their investment.
The focus of this ERC proposal is the study of the gaseous environments of galaxies, and their links to the general inter¬galactic medium. Observationally it is extremely difficult to trace the inferred gas flows, because of their low predicted surface brightnesses and filamentary geometry. Only MUSE will have the sensitivity to detect and map them directly, through the Lyα emission line of recombining hydrogen. MUSE will also provide spectroscopic (kinematic) constraints on these gas flows, which will be essential for quantitative comparisons with numerical simulations.
A dedicated team will address the many challenges of this scientific endeavor in a comprehensive way. It will involve the characterization of the instrument and its systematics; the development of advanced data reduction and data analysis tools in a coordinated multidisciplinary approach, involving astronomers as well as experts in image and signal processing; and the interpretation of the results in conjunction with the latest high-resolution cosmological simulations. Upon completion, MUSICOS will deliver an entirely new view at the early stages of galaxy formation and the growth of baryonic mass in the young universe.""","2498400","2014-03-01","2019-02-28"
"mVITO","MILLIKELVIN VISUALISATION OF TOPOLOGICAL ORDER","James Charles DAVIS","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The urgent imperative to discover and understand topological quantum matter (TQM) is based both on the fundamental significance of its rich new physics and on the potential for unprecedented applications e.g. topological quantum computing. Specific topological ordered states, such as topological superconductors, ferromagnetic topological insulators, and topological Kondo insulators, are now the focus of physics research. Their microscopic quantum states have proven very difficult to address experimentally, one reason being the lack of instrumentation designed to deal with their novel measurement challenges, especially in high-resolution visualization of TQM.  However, new opportunities for this field, based on discovery of viable new materials and on the development of new visualization techniques, have emerged very recently. Therefore, we propose to develop and utilize a suite of new spectroscopic imaging scanning tunnelling microscope instruments capable of visualizing topological quantum matter at millikelvin temperatures. The objective is to achieve the extremely high energy-resolution required to access the energy gaps and exotic electronic states of TQM. Using these unique instruments, we propose a specific sequence of key experiments on direct visualization and quantitative understanding of topological quantum matter. These include direct measurement of the momentum-space structure of the energy gaps of topological superconductors, ferromagnetic topological insulators, and topological Kondo insulators. We plan atomic scale visualization of the Cooper-pair condensate, the ferromagnetic state, and the heavy-fermion hybridization to explore how the real-space structure of these ordered states influences their topological characteristics. Finally, we propose to visualize scattering interference of topological quasiparticles to detect the spectra of extremely exotic Majorana/Jackiw-Rebbi/Heavy-Dirac states that are predicted for these three topological orders.","3499176","2019-01-01","2023-12-31"
"MW-DISK","How the Milky Way Built Its Disk","Hans-Walter Rix","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Our Milky Way is a very typical galaxy, yet our position within it makes it a unique object of study: star-by-star we can obtain 3D positions, 3D velocities and chemical abundances. This wealth of information about our Galaxy’s stellar body holds the key to understanding how disk galaxies form and how dark matter acts on the scales of galaxies. Ongoing surveys have recently hundred-folded the number of stars with good distance estimates, radial and transverse velocities and abundance estimates; and this only forebodes the data wealth expected from ESA’s Gaia mission. Yet, practical approaches to extract the enormous astrophysical information content of these data have been sorely underdeveloped. Just within the last year, the PI and his collaborators have pioneered how to construct rigorously both mass density and kinematic maps of the Milky Way's stellar disk from existing spectroscopic surveys. It is proposed here to unleash the full potential of this approach.

This proposal builds on the PI's unique track record encompassing both extensive survey data analysis and detailed dynamical modeling, combined with his role in proprietary key data sets for this project. Focusing a group of experienced post-docs and PhD students for five years on these challenges will bring the critical mass in one single place to implementing such a comprehensive data/modeling machinery. The first years of the project will focus on the technique development and applications to ground-based data. The results will tell us how the Galaxy's disk was built and shaped, and will map dark matter in the inner parts of the MW. Such an analysis machinery is also indispensable for capitalizing  (astrophysically) on the catalogs that the Gaia mission will provide.","2421960","2013-05-01","2018-04-30"
"NANOANTENNAS","Nano-Optical Antennas for Tuneable Single Photon Super-Emitters","Niko Frans Van Hulst","FUNDACIO INSTITUT DE CIENCIES FOTONIQUES","Nano-optical antennas allow to confine light on a truly nanometer scale. Indeed, my group recently demonstrated efficient funneling of incident far field to antenna hotspots, i.e. nano-focusing down to 25 nm, and achieved for the first time steering of the angular photon emission of a single molecule. These pioneering results on close encounters between nano-antennas and photon emitters pave the way to a regime of new physical phenomena: super-emission, gradient effects, breakdown of the dipole approximation, near-field spectra, single photon beaming, quantized plasmons and potentially strong coupling. These are exactly the novel effects I plan to explore. Specific objectives are: - Nano-optical control: positioning of single photon emitters at antenna hotspots with &lt; 10 nm accuracy by top-down fabrication, optical forces and chemical recognition. - Super-emission-focusing: boosting of emission to ps Rabi periods and unity quantum efficiency by resonant coupling to the nano-antenna. Photons will be beamed in an antenna dominated angular cone, which in reciprocity acts as the acceptance cone for super-focusing. - Coherent antenna control: by shaping the phase content of broad band fs pulses and tuning the antenna load by optically active materials, I will control nanoscale fields, both in the temporal and spatial domain. - Quantized plasmons: by coupling single photon emitters across a nano-antenna I will explore strong coupling and uncover the quantum nature of plasmons. This research aims for a profound understanding of the fundamental limits of optical control at the nanoscale. The new tuneable photon super-emitters and nano-hot-spots open several new horizons: controlled single photon sources for quantum-information; light harvesting; energy conversion; efficient bio-sensors; optical imaging with 10 nm resolution.","2499600","2010-03-01","2015-08-31"
"NanoBioNext","Nanoscale Biomeasurements of Nerve Cells and Vesicles: Molecular Substructure and the Nature of Exocytosis","Andrew EWING","GOETEBORGS UNIVERSITET","I propose to develop and apply state of the art analytical methods to investigate cell membrane and vesicle substructure to elucidate the chemistry of the closing regulatory phase of individual exocytosis events. The general goal of this proposal is to develop a new brand of analytical nanoelectrochemistry (nanogap and nanopore electrochemical cytometry), combined with chemical nanoscopy imaging methods with STED and nanoscale mass spectrometry imaging.  I propose to apply this to the questions of the nature of exocytosis and the chemistry that initiates the process of a short-term memory. We have recently discovered that most neurotransmitter release is partial via an open and closed vesicle release process and this allows new mechanisms of plasticity and synaptic strength to be hypothesized. I propose to (i) test if partial release is ubiquitous phenomenon, (ii) develop new nanoscale analytical methods to measure exocytotic release from pancreatic beta cells and a neuron in Drosophila, and to elucidate the substructure of nanometer vesicles, (iii) use these analytical methods in model cells and neurons to test the hypothesis that lipid membrane changes are involved in the initiation of the chemical events leading to short-term memory, and (iv) test the effects of drugs and zinc on plasticity of vesicles and exocytosis. This work combines new method development with a revolutionary application of chemical analysis to test the hypothesis that lipids play a previously unanticipated role in synaptic plasticity and the chemical structures involved in the initiation of short-term memory. As long-term impact, this will provide sensitive analytical tools to understand how changes in these chemical species might be affected in relation to diseases involving short-term memory loss.","2500000","2018-08-01","2023-07-31"
"NANOCELLIMAGE","Ultrasmall Chemical Imaging of Cells and Vesicular Release","Andrew Ewing","CHALMERS TEKNISKA HOEGSKOLA AB","The long-term goal of this research is to establish the chain of molecular events associated with (1) neurotransmitter release at the single cell and subcellular level and (2) with cell differentiation and reprogramming. These are incredibly important goals for which there are few analytical chemistry methods that are available and useful. The immediate goal therefore includes development of three chemical methodologies at the cutting edge of analytical chemistry: 1) the development of arrays of nanometer electrodes that can be used to spatially measure the release of easily oxidized substances across the cell surface; 2) to improve the combination of MALDI and cluster SIMS ion sources on an orthogonal QStar instrument to enable protein and glycoprotein analysis at the single whole cell level, lipid domain analysis at the subcellular level, and importantly, depth profiling; and 3) the application of information discovered at single cells and of the methods developed in goals 1 and 2 to an in vitro model of cell-to-cell communication and regeneration. I intend to build on my expertise in both electrochemistry and SIMS imaging to develop these approaches. The work described here constitutes two new directions of research in my group as well as new analytical chemistry, and, if successful, will lead to researchers being able to gather incredibly important new data about cell-to-cell communication and cell differentiation and reprogramming as well as to a better understanding the role of lipids in exocytosis and endocytosis.","2491881","2011-01-01","2015-12-31"
"NANOCHEM","Nanopores for New Molecular Nitrogen Chemistry","Martin SCHRODER","THE UNIVERSITY OF MANCHESTER","We will study the adsorption, binding, transport and reactivity of N-containing substrates within the nanopores of functionalized and doped metal-organic framework (MOF) materials. A range of important target molecules (e.g., NH3, N2H4, N2O, NO, NO2 and N2O4) will be studied in this project, which aims to re-define the molecular chemistry for these energetic N-compounds in confined space, and to develop selective catalytic reduction (SCR) of NOx in the presence of NH3, urea and hydrocarbons. The PI has extensive experience in the field of coordination chemistry and hybrid materials, and seeks to change research direction to develop new gas phase catalysts and to gain fundamental understanding of molecular interactions, properties and function of specific energy, environmentally-related substrates within MOFs. Research objectives include the:
• Design and synthesis of new porous MOFs with emphasis on the decoration of their pore environment and improvement of their structural stability and function; 
• Characterisation of host and substrate-loaded materials by state-of-the-art in situ structural, dynamic and spectroscopic methods for the construction of structure-function relationships, supported by computational analysis and modelling;
• Adsorption, binding, release and separation of NH3, N2H4, N2O, NO, NO2 and N2O4 via both static and dynamic experiments;
• Tests of degradation and selective catalytic reduction (SCR) of captured NOx molecules with NH3, urea and hydrocarbons under mild conditions using nanoporous MOFs as host catalysts; 
• Assembly of a MOF-based (i) catalytic deNOx reactor and (ii) NH3 storage system for potential portable applications. 
This project will deliver new functional materials as high capacity portable NH3 stores, efficient capture medium for NOx, and new catalysts for reduction and mitigation of NOx to deliver significant impacts to academia, industry and society of direct relevance to clean energy and sustainable environment.","2498645","2017-06-01","2022-05-31"
"NANOFACTORY","Building tomorrow’s nanofactory","Olivier MARTIN","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The aim of this project is to translate the concept of production line to the nanoworld to develop what could become tomorrow’s nanofactory. So far, nanostructures are either chemically synthesized or produced using top-down approaches such as nanolithography, but no processes exist to take a few nanostructures and perform the basic operations required to assemble them into a more complex system. This proposal aims at addressing this need by realizing at the nanoscale the different functions that are required for a production line: receiving and moving raw nanomaterial in position, where it can be immobilized and worked on or transformed; combining different elements into more complex systems that support new functionalities. The project uses optical forces generated by plasmonic traps as enabling mechanism to act on raw material and the entire production line will be integrated into microfluidics, which will perform as an advanced conveyor belt. Local electrophoresis and photo-curable polymerization are used to locally modify and assemble raw nanoparticles. In addition to implementing challenging nanotechnologies, such as nanoscale electric contacts and perforated membranes, this project will also explore a fair amount of completely new physics, including the van der Waals interaction – which will be studied numerically and experimentally – the competition between optical and chemical forces or electrostatic attraction, and the detailed determination of the trapping potential produced by plasmonic nanostructures. The foreseen research is very comprehensive, including modelling, nanofabrication and explorations at the nanoscale. This ground-braking proposal will demonstrate how additive manufacturing can be implemented at the nanoscale.","2488190","2016-09-01","2021-08-31"
"NANOFORBIO","Nanostructures for biology","Cornelis Dekker","TECHNISCHE UNIVERSITEIT DELFT","I propose to employ our advanced capabilities for nanofabrication to explore new biology at the single-molecule and single-cell level. I choose to specifically address two directions of intense scientific interest: (i) With my team I will develop and exploit solid-state nanopores for the study of real-time translocation of individual biomolecules. In the past few years, my group has attained a leading position in this field and we want to apply our advanced knowledge to push the technology and use it to resolve some pressing questions in cell biology and biotechnology. Specifically, we will explore screening of DNA-protein complexes at the single-molecule level, and we will build biomimetic nanopores to address the physical mechanism of selection and controlled molecular transport of the nuclear pore complex. (ii) We will use nanofabrication to create well-defined landscapes for bacteria. This will allow biophysical studies of the interaction between bacteria and their habitat with an unprecedented control of the spatial structure and habitat parameters. I strongly believe that this approach constitutes a major new tool to experimentally address a number of fundamental issues in the ecology and evolution of bacteria for the first time in a controlled environment. Additionally, it opens up a way to explore the biophysics of bacteria in confined space, where we will study a new bacterial phenotype in nanofabricated slits which we recently discovered. While this research is primarily driven by the quest for understanding physical mechanisms in biology, it can also be expected to have profound impact on applications in antibiotics, gene therapy, and DNA sequencing.","2499091","2010-03-01","2015-02-28"
"NANOGRAPH","The Chemists Way of Making and Utilizing Perfect Graphenes","Klaus Müllen","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Graphenes, single sheets of graphite, hold enormous promise as a material of the future, since their unique electronic properties might allow us to combine advantages of silicon and plastics. We propose a concept for the synthesis and processing of mono- and multilayered graphenes and of graphene nanoribbons (GNRs), which are strips of graphene exhibiting a high aspect ratio. The key idea is the dehydrogenation (and planarization) of precursor molecules made from twisted benzene rings. Size and shape of the final graphenes will be chemically determined by the precursors themselves, which can be synthesized with great perfection. This elegant level of structural control of graphenes and GNRs discriminates our approach against existing literature efforts. Defined edges of GNRs are essential for creating finite electronic band gaps, since pristine graphene is a semimetal and thus not suitable for most electronic and optoelectronic applications. Graphenes at a size of several hundred nm will be targeted in solution, but mainly after deposition and transformation of the precursor molecules on substrate surfaces. The consequence is that we will apply and combine organic polymer synthesis and processing with methods of surface physics to create a new materials science of graphenes. Further characteristics of the work will include in-situ monitoring of chemical processes by scanning probe methods and interfacing of as-formed graphenes for in-situ measurements of charge carrier mobility and spin transport. Applications will be demonstrated for the construction of batteries, fuel cells, field effect transistors, and sensors. What we expect as key achievements will be the delineation of reliable structure-property relationships and improved device performance of graphene materials.","2500000","2011-02-01","2016-01-31"
"NANOGRAPH@LSI","Nanostructuring graphene and graphitic substrates for controlled and reproducible functionalization","Steven De Feyter","KATHOLIEKE UNIVERSITEIT LEUVEN","""Graphene is a new class of promising material with exceptional properties and thus warrants a plethora of potential applications in various domains of science and technology. However, due to intrinsic zero bandgap and inherently low solubility, a prerequisite for the use of graphene in several applications is its controlled and reproducible functionalization in a nanostructured fashion. Being a ‘surface-only’ nanomaterial, its properties are extremely sensitive not only to chemical modification but also to noncovalent interactions with simple organic molecules. A systematic knowledge base for targeted functionalization of graphene still eludes the scientific community. The present experimental protocols suffer from important shortcomings. Firstly, graphene functionalization occurs randomly in solution based methods and there is scarcity of methods that can exert precise control over how and where the reactions/interactions occur. Secondly, due to random functionalization, producing reproducible samples of structurally uniform graphene and graphitic materials remains a major challenge. Lastly, a molecular level understanding of the functionalization process is still lacking which precludes systematic strategies for manipulation of graphene and graphitic materials.

NANOGRAPH@LSI aims to develop systematic experimental protocols for controlled and reproducible (covalent, non-covalent as well as the combination of both) functionalization of graphene and graphitic materials in a nanostructured fashion at the liquid-solid interface (LSI), along with the implementation of new nanoscale characterisation tools, targeting a broad range of applications in the fields of electronics, i.e. graphene bandgap engineering, sensing, and separation. Supramolecular self-assembly of organic building blocks at the liquid-solid interface will be employed as a basic strategy. In view of the above mentioned applications, also upscaling protocols will be developed and implemented.""","2495740","2013-11-01","2018-10-31"
"NANOIMPACTS","Nano-Impacts: the chemistry of single nanoparticles","Richard Guy Compton","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Many fundamental issues at the cutting edge of nanoscience will be understood and exploited through the study of single nanoparticles (NPs). The phenomenon of particle-electrode impacts (PEI), due to Brownian collisions of NPs with an electrode held at a suitable potential, enables NPs to be individually addressed, chemically manipulated and interrogated via electrical contact during collisions.
We shall address experimental and theoretical aspects of PEI embracing the redox chemistry of metal, non-metal and organic nanoparticles; the use of tagged nanoparticles with tags varying from proteins/DNA (sensing applications) to organic moieties (synthesis and nanoarchitectures); the insertion chemistry of H, Li etc into metal and metal oxide NPs (with application to new battery materials); photoelectrochemistry of semiconducting and sensitised NPs; the aggregation of NPs, single molecule detection via electrochemistry, and controlling the impact environment via optimisation of the impact parameters for particular applications. Theoretical models will be developed to describe and predict the stochastic PEI phenomenon, including the testing of existing theories of electron transfer and transport to and from nanoscale electrodes (Frumkin and Levich exclusion effects).
We have pioneered early aspects of this fledgling field and are ideally placed to realise the full potential of PEI studies to a wide range of nanoelectrochemical, analytical, synthetic and sensing applications. We therefore request support for a comprehensive programme of work to expand and fully exploit the field, using the PEI phenomenon to advance the interfaces of electrochemistry with analytical chemistry, biochemistry, materials science and physics (offering myriad applications in synthesis, sensing, nanotechnology, batteries and solar cells) leading to a level of expertise and fundamental understanding prior to ambitious, world-leading experiments in nanoelectrochemistry and in analytical science.","2478320","2013-04-01","2018-09-30"
"NanoInspection","Near-Field Spectroscopic Imaging of the Assembly and Working of Nanosheets of Catalytic Porous Materials","Bert Marc Weckhuysen","UNIVERSITEIT UTRECHT","The pressing need for a more sustainable society has sparked intensive research efforts in search for novel materials with controlled structure, porosity and functionalities. Such porous materials may combine high catalytic activity and selectivity with a long-term stability in the conversion of renewable (e.g. biomass) and non-renewable feedstock when producing future transportation fuels and chemicals. Rational design and optimization of the catalytic properties of these materials is one of the keys for the transition from a fossil fuels based society to a sustainable society.
Useful porous catalytic solids are still largely discovered through a combination of trial-and-error, serendipity and high-throughput testing, mainly because not much is known about the molecular details of their formation and working. Such knowledge is needed to tailor these porous solids towards optimal functioning.
My goal is to obtain fundamental insights in the formation and catalytic functioning of crystalline porous materials. Nano-sized sheets of porous materials will be constructed as model systems amenable to nano-spectroscopic research. We will explore Tip Enhanced Raman Spectroscopy and Scanning Near-Field X-ray Microscopy as novel analytical tools in combination with a specially designed high-pressure/high-temperature in-situ Atomic Force Microscopy cell. In this way, Raman and X-ray spectra can be obtained at the nanoscale of e.g. a growing metal organic framework nanosheet. The novel tools and models will be used to study the chemistry of synthesis, self-assembly and catalysis. We will address catalyst stability by investigating catalyst corrosion in the presence of various polar molecules (water, alcohols, organic acids). It is expected that the new insights will make it possible to determine proper synthesis and reaction conditions leading to the assembly of improved catalytic porous materials, optimized for selective conversion of carbohydrates and lignin-related compounds.","2500000","2013-04-01","2018-03-31"
"NANOOXIDES","Nanosized porous molecular metal oxides with functionalizable cavities and soft matter behaviour allow studies of new phenomena","Achim Müller","UNIVERSITAET BIELEFELD","It is generally accepted that nano materials “will revolutionize our industries and our lives” (Nobel laureate R. Smalley). We wish to extend our work on unique metal-oxide nano materials, which offer signposting routes due to their structures/forms – like our worldwide highlighted and used spherical capsules with 20 gated pores – to study novel phenomena with impact for basic research as well as applications. These nano materials should now be adjusted regarding size, chemical composition, their linking to extended structures, and a variety of new tailored internal functionalities in order to get appropriate properties of interest for different purposes of nano science and technology.  Among the examples are: the protection/stabilization of intermediates and transport of the encapsulated materials after pore closing, systematic studies regarding the generation of hydrophobic cavities with and without water (of importance for protein research and even related to drug design), the removal of a variety of hydrophobic (potentially toxic) compounds from water based on molecular recognition (a paradigmatic shift) and finally, stepwise capsule closing and opening related to allosteric effects. The intended work on the capsules should open doors for the understanding of  phenomena in small spaces. Also materials based on the related wheel-shaped metal-oxides with unprecedented properties (i.e. soft matter behaviour) will be investigated.","1128400","2013-02-01","2016-01-31"
"NanoPaleoMag","Nanopaleomagnetism: a multiscale approach to paleomagnetic analysis of geological materials","Richard John Harrison","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Paleomagnetism has played a pivotal role in developing our modern understanding of the Earth, and remains one of the primary tools used to study the structure and dynamics of the Earth and other planets. However, some of the most interesting and controversial periods of Earth’s history occur far beyond the current limits of our confidence in the paleomagnetic signals used to study them. NanoPaleoMag will solve this problem by dramatically increasing the range of materials that are suitable for paleomagnetic study, thereby opening up periods of Earth history that have hitherto defied conventional paleomagnetic analysis.

Rocks are chemically, mineralogically, texturally and magnetically heterogeneous materials, with heterogeneity occuring at all length scales – from metres to nanometres. There is a pressing need to push the spatial resolution of paleomagnetic studies beyond their current limits and to extend the analysis into 3D. Adopting cutting-edge techniques from physics and materials science, NanoPaleoMag will perform paleomagnetic measurements at submicron length scales. 3D measurements of the volume, shape and spacing of all magnetic particles within a microscale region of interest will be made using a focused ion beam workstation. Combined with high-resolution paleomagnetic measurements and nanometre/nanosecond electron/X-ray magnetic imaging, NanoPaleoMag will characterise the magnetic properties of geological materials at fundamental length scales and time scales. Sample-return missions to asteroids, comets, moons and planets will soon provide unprecedented opportunities for extraterrestrial paleomagnetism. NanoPaleoMag will provide the methodology and instrumentation needed to analyse these precious materials.","2384543","2013-05-01","2018-04-30"
"NANOPARTCAT","Supported Nanoparticles for Catalysis: Genesis and Dynamics in the Liquid Phase","Krijn Pieter De Jong","UNIVERSITEIT UTRECHT","Supported metal nanoparticles are used as catalysts to accelerate and steer chemical conversions to produce, e.g., transportation fuels, chemicals and medicines. Albeit of eminent importance, supported metal catalysts are almost exclusively synthesized in liquid-phase processes that are often considered ‘an art rather than a science’. Although recent results from our laboratory and others on the fundamentals of catalysts synthesis have led to many new insights, the lack of methodology to investigate directly the formation of supported nanoparticles in the liquid phase hampers progress.
The key objective of this proposal is to image and thereby obtain a detailed understanding of both the genesis (synthesis) and the dynamics (catalysis) of supported metal nanoparticles in the liquid phase with nanometer resolution and in real time. To this end we will combine two recent developments: (1) a liquid-phase in situ cell for use in a transmission electron microscope (TEM) with (2) the element specificity of a Chemi-STEM that provides element specific images with nanometer resolution.. In this way we will image in the liquid phase the nucleation and growth of nanoparticles on a support. As support we plan to use materials with ordered porosity that allow imaging of genesis of nanoparticles in liquid confined in nanopores. The key objective of this proposal will be addressed in four projects (1) acquisition and implementation of a liquid-phase cell within a Chemi-STEM which is then used to study (2) ion adsorption of noble metal complexes onto silica and zeolites followed by liquid-phase reduction to form metallic nanoparticles, (3) crystallization of metal nitrates in nanopores of silica and carbon, (4) dynamics of palladium nanoparticles in liquid-phase catalysis.
The new insights will move catalysts synthesis ‘from art to science’ and provide control over the properties of supported nanoparticles to arrive at novel catalysts for sustainable processes.","2500000","2014-04-01","2019-12-31"
"NANOSCULPTURE","Exploration of strains in synthetic nanocrystals","Ian Robinson","UNIVERSITY COLLEGE LONDON","I plan to grow nanometre-sized crystals in confined geometries to examine the strain distributions that result.  The crystal growth will employ lithographic processing techniques, made possible by the local expertise in the central clean room facilities of the London Centre for Nanotechnology.  My group is world-leading in developing a method called Coherent X-ray Diffraction (CXD).  Our CXD strain images of a Pb nanocrystal were published in Nature in 2006.  CXD is sensitive to strain because the X-ray diffraction pattern surrounding a Bragg peak can be decomposed into symmetric and antisymmetric parts.  To a good approximation, the symmetric part can be considered to come from the real part of the electron density, while the antisymmetric part is a projection of the strain field.  The phasing of the data is a critical step that uses a computer algorithm, developed by us, which acts like the lens of a 3D X-ray microscope.  CXD works best for nanocrystal sizes between 40nm and 5µm, for crystals strongly attached to substrates and for isolated, fiducialised arrays of crystals that can be cross-referenced with other techniques.  To create nanocrystals in this size range, we will use both a bottom-up self-assembly of materials deposited onto  templated substrates, designed to introduce strain, and a top-down  nanosculpture  approach will use lithography techniques to create strain patterns in crystalline materials associated with shapes that are carved into them.   The interpretation of the images is the main intellectual output of the project.  This will be compared with finite element analysis, and the deviations interpreted as unique properties attributable to the nanoscale.  All project participants will work in a design, creation, analysis, interpretation, update cycle that will reveal the new basic principles of nanocrystal structure.  In the long run we will transfer CXD technology to Europe: beamline I-13 at Diamond will be ready for CXD in 2011.","2500000","2009-01-01","2013-12-31"
"NANOSENSOMACH","Nanoengineered Nanoparticles and Quantum Dots for Sensor and Machinery Applications","Itamar Willner","THE HEBREW UNIVERSITY OF JERUSALEM","""Chemically modified metallic nanoparticles (NPs) or semiconductor quantum dots (QDs) are central components for the future development of nanotechnology and nanobiotechnology. This program aims to introduce new dimensions into the field of nanotechnology and nanobiotechnology by synthesizing, characterizing and assembling molecule- or biomolecule-modified nanoparticles (NPs)/Quantum dots (QDs) hybrid nanostructures that perform tailored and programmable functionalities.  The project will include two complementary research activities.  One direction will include the generation of electropolymerized ligand-functionalized Au NPs matrices on electrode surfaces.  By tethering of appropriate ligands to the NPs, imprinted matrices for selective sensing, and signal-triggered NPs """"sponges"""" for the selective uptake and release of substrates will be designed.  Also, electrochemically induced pH changes by the NPs matrices will be used to control chemical reactivity (e.g., sol-gel transitions, activation of the ATP synthase machinery).  The second research direction will implement ligand-modified QDs for the sensing of ions or molecular substrates.  Similarly, nucleic acid-functionalized QDs will be used to develop new versatile sensing platforms exhibiting multiplexed analysis capabilities.  One platform will include the quenching of the QDs by G-quadruplexes, whereas the second platform will use biochemiluminescence resonance energy transfer (BRET) as readout signal.  Also, QDs-modified supramolecular DNA nanostructures will be designed to perform programmed machinery functions such as """"bi-pedal walker"""", """"seesaw"""", """"gear"""" or """"tweezers"""", and the machinery functions will be transduced by the optical properties of the QDs.  Finally, DNA-machines that trigger the isothermal amplified replication of the analyzed nucleic acid will be designed, and QDs tethered to the machine will optically transduce the replication process at real-time.""","2167400","2011-01-01","2016-12-31"
"NANOSHOCK","Manufacturing Shock Interactions for Innovative Nanoscale Processes","Nikolaus Adams","TECHNISCHE UNIVERSITAET MUENCHEN","Fluid dynamics are fundamental to a wide spectrum of natural phenomena and technological applications. Among the most intriguing fluid dynamics events are shockwaves, discontinuities in the macroscopic fluid state that can lead to extreme temperatures, pressures and concentrations of energy.The violence and yet the spatial localization of shockwaves presents us with a unique potential for in situ control of fluid processes with surgical precision. Applications range from kidney-stone lithotripsy and drug delivery to advanced aircraft design. How can this potential be leveraged/harnessed? What mechanisms and inherent properties allow for formation and control of shocks in complex environments such as living organisms? How can shocks be generated in situ and targeted for drug delivery with high precision while minimizing side effects? What is the potential of reactive/fluidic-process steering by shock-interaction manufacturing?
Our objective is to answer these questions by state of the art computational methods, supported by benchmark quality experiments. Computations will be based on advanced multi-resolution methods for multi-physics problems with physically consistent treatment of sub-resolution scales. Uncertainty quantification will be employed for deriving robust flow and shock-dynamic field designs. Paradigms and efficient computational tools will be delivered to the scientific and engineering community. Our group has strong foundations in complex-fluid physics and computational methods and a strong record of successfully integrating research and technical applications. Our goal is to provide un-precedented insight into shock generation and dynamics in complex environments and to unravel the path to technical solutions. Leveraging the enormous potential of manufactured shocks in situ gives access to breakthrough innovations and high-impact technologies, ranging from shock-driven nanoparticle reactors to non-invasive shock-mediated low-impact cancer therapies.","2353438","2015-12-01","2020-11-30"
"NANOSONWINGS","A new vision on nanocatalysts","Petrus Wilhelmus Nicolaas Maria Van Leeuwen","FUNDACIO PRIVADA INSTITUT CATALA D'INVESTIGACIO QUIMICA","In recent years it has been recognized that small metal nanoparticles hold the promise that their catalytic properties may be completely different from those of their bulk analogs or their monometallic complexes. Entirely new chemical conversions may be expected because of their shape and thermodynamic properties. So far, this promise has not led to important breakthroughs, as most findings can be categorized, mostly, as typical of homogeneous catalysis, or mimics of heterogeneous catalysts, especially hydrogenation. Nanoparticles need stabilizating reagents; polymers, dendrimers, ionic liquids, detergents, solid surfaces, and small ligands, have been discovered and used by trial and error. In this project we propose the use of concave, large organic molecules that will be developed and used to stabilize small nanoparticles by covering part of the vertices and apices, thereby controlling the size and the shape, leaving edges next to the molecular wings and uncovered surface available for interactions leading to catalysis. The controlling wings contain two or three strongly binding phosphines to prevent dissociation of the controlling agent and to modify, simultaneously the electronic properties of part of the metal atoms. The organic platforms have the advantage that additional groups can be connected to them which can serve as chiral modifiers, as recognition sites for larger molecules, as additional organic catalysts, or as ligands to hold a homogeneous co-catalyst. Three high-risk reactions will be investigated, (enantio)selective hydrogenation of aromatics, conversion of glycerol to high added value products and the selective conversion of syn gas by using devices derived from homogeneous and supramolecular catalysis.","3495000","2010-04-01","2015-03-31"
"NANOSQUID","Scanning Nano-SQUID on a Tip","Eli Zeldov","WEIZMANN INSTITUTE OF SCIENCE LTD","At the boundaries of physics research it is constantly necessary to introduce new tools and methods to expand the horizons and address fundamental issues. In this proposal, we will develop and then apply radically new tools that will enable groundbreaking progress in the field of vortex matter in superconductors and will be of great importance to condensed matter physics and nanoscience. We propose a new scanning magnetic imaging method based on self-aligned fabrication of Josephson junctions with characteristic sizes of 10 nm and superconducting quantum interference devices (SQUID) with typical diameter of 100 nm on the end of a pulled quartz tip. Such  nano-SQUID on a tip  will provide high-sensitivity high-bandwidth mapping of static and dynamic magnetic fields on nanometer scale that is significantly beyond the state of the art. We will develop a new  washboard frequency dynamic microscopy  for imaging of site-dependent vortex velocities over a remarkable range of over six orders of magnitude in velocity that is expected to reveal the most interesting dynamic phenomena in vortex mater that could not be investigated so far. Our study will provide a novel bottom-up comprehension of microscopic vortex dynamics from single vortex up to numerous predicted dynamic phase transitions, including disorder-dependent depinning processes, plastic deformations, channel flow, metastabilities and memory effects, moving smectic, moving Bragg glass, and dynamic melting. We will also develop a hybrid technology that combines a single electron transistor with nano-SQUID which will provide an unprecedented simultaneous nanoscale imaging of magnetic and electric fields. Using these tools we will carry out innovative studies of additional nano-systems and exciting quantum phenomena, including quantum tunneling in molecular magnets, spin injection and magnetic domain wall dynamics, vortex charge, unconventional superconductivity, and coexistence of superconductivity and ferromagnetism.","2000000","2008-12-01","2013-11-30"
"NanoStaph","Force nanoscopy of staphylococcal biofilms","Yves Dufrene","UNIVERSITE CATHOLIQUE DE LOUVAIN","Staphylococcus aureus is a leading cause of hospital-acquired infections, which are often complicated by the ability of this pathogen to grow as biofilms on indwelling medical devices. Because biofilms protect the bacteria from host defenses and are resistant to many antibiotics, biofilm-related infections are difficult to fight and represent a tremendous burden on our healthcare system. Today, a true molecular understanding of the fundamental interactions driving staphylococcal adhesion and biofilm formation is lacking owing to the lack of high-resolution probing techniques. This knowledge would greatly contribute to the development of novel anti-adhesion therapies for combating biofilm infections.

We recently established advanced atomic force microscopy (AFM) techniques for analyzing the nanoscale surface architecture and interactions of microbial cells, allowing us to elucidate key cellular functions. This multidisciplinary project aims at developing an innovative AFM-based force nanoscopy platform in biofilm research, enabling us to understand the molecular mechanisms of S. aureus adhesion in a way that was not possible before, and to optimize the use of anti-adhesion compounds capable to inhibit biofilm formation by this pathogen.

NanoStaph will have strong scientific, societal and economical impacts. From the technical perspective, force nanoscopy will represent an unconventional methodology for the high throughput and high resolution characterization of adhesion forces in living cells, especially in bacterial pathogens. In microbiology, the results will radically transform our perception of the molecular bases of biofilm formation by S. aureus. In medicine, the project will provide a new screening method for the fast, label-free analysis of anti-adhesion compounds targeting S. aureus strains, including antibiotic-resistant clinical isolates that are notoriously difficult to treat, thus paving the way to the development of anti-adhesion therapies.","2481438","2016-10-01","2021-09-30"
"NANOSYS","Nanosystems: Architectures, Design and Applications","Giovanni De Micheli","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Nanosystems are integrated systems exploiting nanoelectronic devices. In particular, this proposal considers silicon nanowire and carbon nanotube technologies as replacement/enhancement of current silicon technologies. This proposal addresses high-risk, high-reward research, unique in its kind. The broad objective of this proposal is to study system organization, architectures and design tools which, based on a deep understanding and abstraction of the manufacturing technologies, allow us to realize nanosystems that outperform current integrated systems in terms of capabilities and performance. Thus this proposal will address modelling of technological aspects, synthesis and optimization of information processing functions from high-level specifications into the nanofabric, and new design technologies for specific aspects of nanosystems including, but not limited to, sensing and interfacing with the environment. This proposal will address also cross-cutting design goals such as ultra-low power and high-dependability design, with the overall objective of realizing nanosystems that are autonomous (w.r. to energy consumption) and autonomic (i.e., self healing). The scientific novelty of this proposal stems from the use of a nanofabric, where computation, sensing and communication are supported by a homogeneous means as well as from the study of algorithmic tools for mapping high-level functions onto the nanofabric. The intrinsic benefit of this research is to provide a design flow that extends both the technological basis and the capabilities of integrated systems, thus strengthening the industrial European position in a key sector where disruptive innovation is key for survival. The extrinsic benefit of this research is to broaden the use of nanosystems to new domains, including mobile/distributed embedded systems, health/environment management, and other areas that are critical to our lives.","2499594","2010-04-01","2015-12-31"
"NAT_CAT","Nature Inspired Transition Metal Catalysis","Joost Nicolaas Hendrik Reek","UNIVERSITEIT VAN AMSTERDAM","""The development of new approaches in transition metal catalysis is of utmost importance since it provides the future tools required to arrive at a sustainable society.  Interestingly, the field of transition metal catalysis has been dominated by the relatively simple dogma that the activity and the selectivity of the catalyst is determined by the interplay between the metal and the ligands that are coordinated to the metal. By developing new ligands, new catalyst can be uncovered that display specific reactivity and selectivity. Nature on the other hand, uses a much larger tool-box to arrive at catalytic systems that are generally far more active and selective than the man-made catalysts.  Enzymes often use multimetallic sites, or multi functional groups that work in concert. Importantly, Enzymes are much larger than synthetic catalysts, and take advantage of the second sphere around an active site by 1) creating a sterically constrained cavity around it leading to entatic states, i.e. deformed intermediate states that lead to lower energy barriers to the product 2) positioning functional groups within the cavity to properly orient and activate the substrate, by lower the transition state via secondary interactions.
In the current proposal we control catalyst properties by encapsulation. Will will use isolated natural active sites (and models theirof) and install these in well-defined cavities and study their properties. Can we create a second coordination sphere such that we can get activities and selectivies similar to that of the original enzyme? For example, we aim for nitrogenase activity by putting isolated active sites in synthetic cages.""","2500000","2013-11-01","2018-10-31"
"NaTuRe","Nanotube Mechanical Resonator, Spin, and Superfluidity","Adrian Bachtold","FUNDACIO INSTITUT DE CIENCIES FOTONIQUES","Mechanical resonators based on carbon nanotubes are truly exceptional sensors of mass and force. In the last years, my group revealed these outstanding figures of merit of nanotube resonators. Here, the project NaTuRe will take advantage of these sensing capabilities to study physical phenomena in fascinating regimes that have not been explored thus far. Specifically, I will address three directions with major scientific interests:
1- I propose to perform electron spin resonance (ESR) measurements on single molecules using nanotube resonators. The goal is to see whether nature can provide molecular electronic spins endowed with long dephasing time. For this, we will measure molecular spins in a regime where the magnetic noise of the environment is reduced to an unprecedented level. In case of success, this work could open avenues in quantum science by allowing experiments not possible with the electronic spins of nitrogen-vacancy centres in diamond.
2- My team will carry out nuclear magnetic resonance (NMR) measurements on single nuclear spins. We will also perform magnetic-resonance force microscopy in order to image these individual nuclear spins. Achieving the objectives proposed here will be an unprecedented success in magnetic resonance imaging (MRI).
3- NaTuRe proposes a completely new experimental approach to investigate superfluidity. We will use a nanotube mechanical resonator to probe the superfluidity properties of helium-4 layers adsorbed onto the suspended nanotube. Our experimental approach will allow us to study various quantum phenomena in superfluidity of considerable interest and from a radically new perspective.
NaTuRe is a highly-interdisciplinary project with possible implications in quantum science, opto-mechanics, nano-science, structural biology, and low-temperature physics.","2503459","2017-01-01","2021-12-31"
"NCDFP","Non-Commutative Distributions in Free Probability","Roland Speicher","UNIVERSITAT DES SAARLANDES","""We intend to study new directions in free probability theory with high potential to lead to breakthroughs in our understanding of random matrix models and operator algebras. We will drive forward the study of """"free analysis"""" which is intended to provide a whole new mathematical theory for variables with the highest degree of non-commutativity and which lies at the crossroad of many exciting mathematical subjects.

More specifically, the objective of the proposal is to extend our armory for dealing with non-commutative distributions and to attack some of the fundamental problems which are related to such distributions, like: the existence and properties of the limit of multi-matrix models; the isomorphism problem for free group factors, and more generally, properties of free entropy and free entropy dimension as invariants for von Neumann algebras.

The main directions are:
(i) classifying non-commutative symmetries and describing the effect of invariance under such quantum symmetries for non-commutative distributions; this will rely on our recent theory of easy quantum groups
(ii) proving regularity properties for non-commutative distributions; for this we will develop the theory of free Malliavin calculus
(iii) providing algorithms for calculating non-commutative distributions; this will rely on advances of the analytic theory of operator valued free convolutions and will in particular lead to a master algorithm for the computation of asymptotic eigenvalue distributions for general random matrix problems""","2197000","2014-02-01","2019-01-31"
"NEDAG","New Directions in Derived Algebraic Geometry","Bertrand TOEN","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","New Directions in Derived Algebraic Geometry: In this proposal we propose to give a new impulsion on derived algebraic geometry by exploring new domains of applicability as well as developing new ideas and fundamental results. For this, we propose to focus on the, still very much unexplored, interactions of derived algebraic geometry with an extremely rich domain: singularity theory (to be understood in a broad sense, possibly in positive and mixed characteristics, but also singularities of meromorphic flat connections and of constructible sheaves). We plan to use the fruitful interactions between these two subjects in a two-fold manner: on the one hand derived techniques will be used in order to prove long standing open problems, and on the other hand we propose new developments in derived algebraic itself and thus open new research directions.The proposal has three major parts, interacting with each other in a coherent manner. In a first part we explore some direct applications of derived techniques to the study of singularities of degenerating families of proper schemes with the objective to prove a long standing major conjecture in the subject: the Bloch’s conductor formula. This is achieved by the introduction of a new trend of ideas in non-commutative geometry and more precisely by the introduction of a trace formula in the non-commutative setting. The second part is devoted to the exploration of trace and index formula for sheaves in two different, but very similar, setting: l-adic constructible sheaves and quasi-coherent sheaves with flat connections along a given algebraic foliations. In a third part we propose to make progress towards an unexplored domain: moduli spaces of flat, possibly irregular, connections on higher dimensional varieties and their relations with Poisson and symplectic geometry. The objective here is a far reaching generalization of  fundamental results on moduli spaces of flat connections on open curves and their symplectic aspects.","1255698","2017-09-01","2022-08-31"
"NEMESIS","Novel Energy Materials: Engineering Science and Integrated Systems (NEMESIS)","Christopher Rhys Bowen","UNIVERSITY OF BATH","The aim of NEMESIS is to establish a world leading research center in ferroelectric and piezoelectric materials for energy harvesting and energy generation. I will deliver cutting edge multi-disciplinary research encompassing materials, physics, chemistry and electrical engineering and develop ground breaking materials and structures for energy creation. The internationally leading research center will be dedicated to developing new and innovative solutions to generating and harvesting energy using novel materials at the macro- to nano-scale.

Key challenges and novel technical approaches are:

1. To create energy harvesting nano-generators to convert vibrations into electrical energy in hostile environments (e.g. wireless sensors in near engine applications).

2. To enable broadband energy harvesting to generate electrical energy from ambient vibrations which generally exhibit multiple time-dependent frequencies.

3. To produce Curie-temperature tuned nano-structured pyroelectrics to optimise the electrical energy scavenged from temperature fluctuations. To further enhance the energy generation I aim to couple thermal expansion and pyroelectric effects to produce a new class of thermal energy harvesting materials and systems.

4. To create nano-structured ferroelectric and piezoelectric materials for novel water-splitting applications. Two approaches will be considered, the use of the internal electrical fields present in ferroelectrics to prevent recombination of photo-excited electron-hole pairs and the electric charge generated on mechanically stressed piezoelectric nano-rods which convert water to hydrogen and oxygen.","2266020","2013-02-01","2018-12-31"
"NEMINTEM","In-situ NanoElectrical Measurements in a Transmission Electron Microscope","Hendrik Willem Zandbergen","TECHNISCHE UNIVERSITEIT DELFT","Nanocharacterization techniques are becoming increasingly important. They help us to determine local atomic arrangements, element compositions as well as electronic structures. High-Resolution Transmission Electron Microscopy (HRTEM) is the most powerful and widely accepted technique. However, until recently in-situ HRTEM did not show sufficient resolution to image changes on the atomic scale. In the last five years, my group has pioneered advanced specimen holders towards in-situ HRTEM. We have leading expertise in obtaining the high resolution in a range of controllable environments: temperatures, pressures, and liquids. In addition, combinations with other types of parallel measurements were pioneered, such as in-situ low-noise electrical characterization. Clearly it is indeed possible to operate the HRTEM as a nanolaboratory. It allows to really see what one is measuring. With this proposal I want to realize the equipment and methodology to perform nano-electrical measurements of nanostructures in-situ in a HRTEM. The NanoElectrical Measurements in a Transmission Electron Microscope (NEMinTEM) will be applied to nanostructures of a range of materials.  Furthermore the electron beam will be used to make well-controlled modifications of the nanostructure. The effects of these modifications on the electrical properties will be measured simultaneously. Semiconductor nanowires, graphene, metallic bridges and nanoelectrodes, and oxide multilayers will be studied, providing challenging examples with possible high-impact results It is to be expected that once NEMinTEM is mature, it will be applied to many more materials.","2500000","2011-04-01","2017-03-31"
"NEMO","Nearshore Monitoring and Modelling:
Inter-scale Coastal Behaviour","Marcellinus Stive","TECHNISCHE UNIVERSITEIT DELFT","""The unprecedented growth of coastal communities has led to € billions worth of developments and infrastructure within the coastal zone. Therefore, future coastal hazards, which are likely to be exacerbated by climate change, will result in massive socio-economic and environmental impacts. To develop informed coastal management strategies to mitigate such adverse impacts, robust large scale, long term forecasts of coastal change are urgently required. However, none of the currently adopted approaches for simulating large scale, long-term (LSLT) coastal change appear to be capable of producing robust forecasts.
The consideration of processes governing LSLT coastal morphodynamics at their operational scales presents similar challenges to those encountered in climate forecasting. The recognition of inter-scale process relationships and the scale aggregation challenge is conceptualised in a scale cascade concept.
State-of-the-art and new innovative field monitoring methods will be implemented over a 5 year period along a 20km stretch of the Holland coast to acquire unprecedented process information at different temporal and spatial scales. These data will then be used in conjunction with existing macro-scale field data and strategic process based numerical modelling to support the development of an original, generic, physics based scale-aggregated numerical model of LSLT coastal change.
Such an innovative physics based scale-aggregated approach to forecast LSLT coastal change has never been attempted, primarily due to lack of a clear vision and the non-availability of multi-scale field data. The development of the capability to robustly forecast LSLT coastal change will represent a ‘world first’ achievement which will significantly advance the state-of-the-art of coastal engineering/science. I firmly believe that this will place the young and exciting field of coastal engineering/science on par with the more developed disciplines such as meteorology and hydrology.""","2917144","2012-01-01","2016-12-31"
"NEMO","Network Motion","FRANCOIS BACCELLI","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","NEMO, NEtwork MOtion, is an inter-disciplinary proposal centered on network dynamics. The inter-disciplinarity spans from communication engineering to mathematics, with an innovative interplay between the two.
NEMO’s focus is on stochastic geometry. This emerges as one of the most important new conceptual  and operational tools of the last 10 years in wireless networking, with a major academic and industrial impact on architecture, protocol design, planning and economic analysis.
Nevertheless, the state of the art is unable to cope with the dynamics introduced in recent and future network functionalities. NEMO’s aim is to introduce dynamics in wireless stochastic geometry. The dynamic versions of stochastic geometry to be developed will capture these new functionalities and specifically tackle two core promises and challenges of the future of wireless networking: that of ultra-low latency networking, required for enabling the unfolding of future real time interactions, and that of draining to the Internet the unprecedented amount and structure of data stemming from
the Internet of Things.
Several fundamental types of random network dynamics underpinning these functionalities are identified. General mathematical tools combining stochastic geometry, random graph theory, and the theory of dynamical systems will be developed to analyze them. This will provide parametric models mastering the complexity of such networks, which will be instrumental in addressing the above challenges. The aim is to have, through these dynamical versions, the same academic and industrial impact on wireless networks as static stochastic geometry has today.
NEMO will leverage structural interactions of INRIA with Ecole Normale Supérieure on the mathematical side, and with Nokia Bell Labs and Orange on the engineering side. This will create in Europe a group focused on this mathematics-communication engineering interface, and to become the top innovation group of the field worldwide.","2498529","2019-01-01","2023-12-31"
"NEO-NAT","Understanding the mass scales in nature","Alessandro STRUMIA","EUROPEAN ORGANIZATION FOR NUCLEAR RESEARCH","The experimental results of the first run of the Large Hadron Collider lead to the discovery of the Higgs boson but have not confirmed the dominant theoretical paradigm about the naturalness of the electro-weak scale, according to which the Higgs boson should have been accompanied by supersymmetric particles or by some other new physics able of protecting the Higgs boson mass from quadratically divergent quantum corrections.

While the second LHC run is going to explore physics at higher energies in the next years, it is now the right moment to explore and develop new non conventional ideas about the origin of  mass scales in nature and in particular of the electro-weak scale.  Indeed, new theoretical ideas prompted by the fact that the standard paradigm is challenged by experiments, 
have been emerging in the past 1-2 years and are acquiring interest.  Furthermore, in view of the large backgrounds unavoidably present at the Large Hadron Collider,  unexpected discoveries could be delayed or even missed if 
experimentalists are not searching in the right direction.
The experimental signatures of the new non-conventional models need to be identified now.


Research performed by the PI and by the senior team members shows that concrete progress  can be achieved in developing new non-conventional ideas about how the electroweak scale and the gravitational Planck scale can be dynamically generated with a vastly different ratio, as observed in nature. However, dedicated funding is needed for younger researches that want to explore such directions outside the mainstream.  The main goal of this project is developing such new ideas and identifying their experimental signals.","1876215","2015-12-01","2020-11-30"
"NEOGAL","New frOntiers in Galaxy spectrAl modeLing","Stephane Charlot","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""NEOGAL is an intensive, coordinated effort to explore uncharted territory in the early chemical evolution of galaxies through the development of an innovative set of spectral analysis tools. This will be achieved through the accomplishment of four interdisciplinary projects using state-of-the-art techniques:

1. The development of a new approach overcoming the limitations of existing methods, to measure the chemical composition of ionized gas with unknown heavy element abundance ratios in high-redshift galaxies;

2. The combination of this approach with sophisticated models of the ultraviolet emission from young stellar populations, to allow differentiated constraints on the chemical composition of stars and gas;

3. The combination of these models with cosmological simulations of galaxy formation, to identify the spectral signatures of competing scenarios of early chemical evolution in observations of statistical samples of high-redshift galaxies;

4. The development of an integrated web service to allow the astronomical community at large to exploit these models to interpret observations of distant galaxies.

The spectral analysis tools developed in the framework of this project will open powerful new opportunities to probe the early star formation and chemical enrichment histories of galaxies. Moreover, by allowing the consistent interpretation of stellar and nebular emission, these tools will provide valuable constraints on parameters such as the escape fraction of ionizing photons and hence the contribution by galaxies to the ionizing background. These models therefore should have a profound impact on the optimal design of future surveys of primeval galaxies that will be carried out with the next generation of large facilities, such as the James Webb Space Telescope and the European Extremely Large Telescope.""","2400556","2013-05-01","2018-04-30"
"NERVI","From single neurons to visual perception","Olivier Dominique Faugeras","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","We propose to develop a formal model of information representation and processing in the part of the neocortex that is mostly concerned with visual information. This model will open new horizons in a well-principled way in the fields of artificial and biological vision as well as in computational neuroscience. Specifically the goal is to develop a universally accepted formal framework for describing complex, distributed and hierarchical processes capable of processing seamlessly a continuous flow of images. This framework features notably computational units operating at several spatiotemporal scales on stochastic data arising from natural images. Mean-field theory and stochastic calculus are used to harness the fundamental stochastic nature of the data, functional analysis and bifurcation theory to map the complexity of the behaviours of these assemblies of units. In the absence of such foundations the development of an understanding of visual information processing in man and machines could be greatly hindered. Although the proposal addresses fundamental problems its goal is to serve as the basis for ground-breaking future computational development for managing visual data and as a theoretical framework for a scientific understanding of biological vision.","1706839","2009-01-01","2013-12-31"
"NetSat","Networked Pico-Satellite Distributed System Control","Klaus Schilling","Zentrum fuer Telematik e.V.","A paradigm shift is emerging in spacecraft engineering from single, large, and multifunctional satellites towards cooperating groups of small satellites. This will enable innovative applications in areas like Earth observation or telecommunication. Related interdisciplinary research in the field of formation control and networked satellites are key challenges of this proposal.

Modern miniaturization techniques allow realization of satellites of continuously smaller masses, thus enabling cost-efficient implementation of distributed multi-satellite systems. In preparation my team has already realized two satellites at only 1 kg mass in the University Würzburg’s Ex¬perimental satellite (UWE) program, emphasizing crucial components for formation flying, like communication (UWE-1, launched 2005), attitude determination (UWE-2, launched 2009), and attitude control (UWE-3, launched 2013).

My vision for the proposed project is to demonstrate formation control of four pico-satellites in-orbit for the first time worldwide. To realize this objective, innovative multi-satellite networked orbit control based on relative position and attitude of each satellite is to be implemented in order to enable Earth observations based on multipoint measurements. Related sensor systems used in my laboratory in research for advanced characterization of teams of mobile robots will be transferred to the space environment. Breakthroughs are expected by combining optimal control strategies for coordination of relative motion with a robust flow of information in the network of satellites and ground stations, implemented via innovative use of ad-hoc networks in space. Based on my team’s expertise in implementing very small satellites, first time a system composed of four satellites will be launched to demonstrate autonomous distributed formation control in orbit. This research evaluation in space is expected to open up significant application potential for future distributed satellite system services in Earth observation.","2500000","2014-08-01","2019-07-31"
"NEUCOD","Neural coding, specification, design and test of message passing neural machines","Claude Yves Marie Berrou","INSTITUT MINES-TELECOM","The way information is represented, stored, recalled and processed in the neocortex is assuredly one of the most puzzling enigmas that science will have to solve during this century. Setting the basic principles of the Mental Information Theory is a high challenge potentially opening huge fields of research and progress in various domains.

This research project is at the crossroads of neuroscience, computational intelligence and information theory, with a particular emphasis on the properties of distributed information processing architectures. Precisely, this project aims to identify clearly, deepen and exploit the strong analogies (distributed structure and message passing, storage capacity, discrimination ability, resilience, importance of cycles and correlation, etc.) that can be found between the structures and properties of the cerebral cortex and those of modern error correcting decoders studied in the communications science area.

It is possible today to deduct from all the observations and discoveries made recently about cerebral biology a minimum material that can help information theory (communication, coding, graphs, etc.) contribute to the understanding and imitation of the neocortex functioning. In particular, the recently introduced biological concepts of neural clusters, neural cliques and sparse coding are exploited in order to devise original and efficient brain-inspired networks. We have already demonstrated that combining these concepts in a judicious approach opens the way to store and retrieve a number of messages proportional to the square of the number of neurons (to be compared for instance with the well-known sub-linear law of Hopfield networks).

The objective of this project is twofold: 1) implementing electronic machines having the ability to learn a lot of information and to produce new one by association, fusion or crossbreeding within a 5-year period, 2) contributing to the understanding of the biological long and short term memories","1880063","2012-02-01","2017-01-31"
"NEUROCMOS","Seamless Integration of Neurons with CMOS Microelectronics","Andreas Reinhold Hierlemann","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","We propose to seamlessly integrate advanced microelectronics and living neuronal cells in a comprehensive and interdisciplinary approach to significantly advance the understanding of neuronal behaviour. The project includes (a) the development of a novel multifunctional microelectronics chip platform in complementary metal oxide semiconductor (CMOS) technology, which serves to enable (b) key neurobiological and neuromedical research on network dynamics and plasticity of rodent neuronal networks and visual encoding in retinae, and (c) the necessary concurrent development of algorithms and models to efficiently process and maximally harness the unprecedented quality of the obtained data.
Neuronal or retinal preparations, such as acute and organotypic brain slices (retinae) or primary cultured, dissociated cells, will be directly placed or grown atop dedicated CMOS microelectronics chips. The chips will feature multiple functions, since neurons carry and pass signals to each other using electro-chemical mechanisms: electrophysiological recording & stimulation, in closed loop & real time, as well as highly spatially resolved impedance measurements and detection of neuroactive chemical compounds. The chips will be capable of delivering any of these functions to arbitrarily selectable individual cells or even subcellular units, and, at the same time, of interacting with a multitude of cells or complete neuronal networks. Along with imaging (light, fluorescence), pharmacological, and/or genetic methods, the developed chip platform will be used to study neuronal network dynamics, synaptic and axonal plasticity, relevant for many brain diseases, as well as visual encoding in the retina. Efficient data handling and spike sorting algorithms will be developed to facilitate these investigations. The multidimensional data will then be used to establish detailed models of neurons and neuronal networks.","2498000","2011-06-01","2016-05-31"
"neuroXscales","Microtechnology and integrated microsystems to investigate neuronal networks across scales","Andreas Hierlemann","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","To advance knowledge in electrophysiology and information processing of neuronal networks, we propose employing microtechnology and microelectronics to rigorously study neural networks in vitro across scales. Across scales pertains to the spatial domain - from details of subcellular components through single neurons to entire networks - and the temporal domain - from single action potentials to long-term developmental processes. Besides our CMOS-microelectronics-based high-density microelectrode arrays for recording and stimulation, the methodology will encompass patch-clamping directly on the microelectrode chips, high-resolution microscopy, genetic methods, large-scale data handling strategies, and dedicated data analysis and modeling algorithms. We will use mammalian cortical neuron cultures and brain slices.
We will potentially have access to every neuron and every action potential. We aim at studying - at the same time in the same preparation - details of specific neurons and subcellular components (somas, axons, synapses, dendrites) in their functional context and the characteristics of the corresponding networks (functional connectivity, emergent properties, plasticity). We will study alterations of components and networks over time and upon defined perturbations and mutual interdependence of network and component characteristics.
The high-spatio-temporal-resolution methodology will enable new fundamental neuroscientific insights through, e.g., facilitating investigation of axonal and axonal initial segment signaling characteristics, with the “axonal” side of neuronal activity being largely inaccessible to established methods. It will also enable the mapping of the overall synaptic input to a specific neuron, or the high-throughput monitoring of all action potentials in a network over extended time to see developmental effects or effects of disturbances. Potential applications include research in neural diseases and pharmacology.","3495000","2016-10-01","2021-09-30"
"NEUTRAL","Neutral Quasi-Particles in Mesoscopic Physics","Heiblum","WEIZMANN INSTITUTE OF SCIENCE LTD","I propose to study ‘neutral excitations’ in 2d and 1d electronic systems.  Such excitations, rarely studied, are unique since they are chargeless but may carry energy.  Being byproducts of electron interaction, they come in a few flavors:  (i) Downstream modes in composite edge channels of the integer quantum Hall effect (IQHE) regime; (ii) Upstream modes in the fractional quantum Hall effect (FQHE) regime; and (iii) Zero energy Majorana states (localized or propagating quasi-particles), in non-abelian FQHE states and in 1d topological P-wave superconductors.  My main interests in neutral modes in the QHE regime are: (a) Their direct association with the nature of the wavefunction of the quantum state; (b) Being excited when a charge mode is being partitioned (say, by a quantum point contact), they may play a prime role in dephasing interference of quasi-particles due to the energy they rob (in the partitioning process).  As for detecting Majorana quasi-particles, and aside from the exciting physics, their non-abelian nature makes them attractive as building blocks in ‘decoherence resistant’ systems.  Based on our acquired  abilities, such as material growth, processing techniques, and sensitive measurement techniques, I plan to perform experiments, which include: thorough studies of downstream and upstream neutral modes via shot noise and thermoelectric current measurements; proving (or disproving) their involvement in dephasing fractionally charged quasi-particles; growing and processing structures that harbor Majorana states (in 1d nano-wires and in 2d FQHE regime; and, possibly, eventually, manipulate Majorana states (by coupling and braiding).  Experiments will employ, e.g., ultra-low temperatures, sensitive shot noise measurements, cross-correlation of current fluctuations, and interference of quasi-particles (charge and neutral) in novel interferometers.","2428042","2014-03-01","2019-02-28"
"NEW ORGANOMETALLICS","Preparation of polyfunctional organometallics: new key intermediates for synthetic organic chemistry","Paul Knochel","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","The use of organometallic reagents in the last decades completely changed synthetic organic chemistry. These reactive intermediates made now available new materials with outstanding physical properties, novel drugs with excellent activity spectra, modern highly efficient and ecologically benign agrochemicals. The goal of this work is the development of new synthetic routes and a practical access to polyfunctional organometallic reagents using affordable and non-toxic metals such as Mg, Al, Ti, Ca, Mn, and Fe. These new types of organometallics will find an extremely broad range of applications in contemporary organic synthesis. New advanced approaches to these reagents will be based on the recently discovered beneficial influence of environmentally friendly salts such as LiCl on the preparation of these reactive intermediates. Our experience will allow us to design organometallic reagents bearing practically all important functional groups used in organic synthesis. This work will lead to ground-breaking advances in main-group synthetic chemistry and make a broad range of completely novel organometallic compounds available for applications in industry. The availability of such intermediates will revolutionize from the point of view of economy and environmental impact the syntheses of important drugs, modern agrochemicals, as well as perspective compounds with new groundbreaking physical properties for sophisticated electronics and adjacent applications. We plan to study the applications of these new reagents in the field of pharmacology, natural product synthesis, and life and material sciences. The new approach will concentrate on ecologically friendly organometallics of low toxicity and finding the novel synthetic ways in order to minimize the environmental impact of the synthetic intermediates.","2061000","2008-12-01","2014-11-30"
"NEWCLUSTERS","A new window on the Universe: The formation and evolution of galaxy clusters and proto-clusters","Hubertus Jacobus Alfonsus Ro¨ttgering","UNIVERSITEIT LEIDEN","The formation and evolution of clusters and proto-clusters of galaxies will be studied using unique diagnostic tools provided by the new pan-European radio telescope LOFAR and the APERTIF phased arrays on WSRT radio telescope. Combined with new ultra low frequency antennas (an extension to LOFAR here proposed), these new facilities will for the first time enable sensitive observations from the lowest possible frequencies accessible from the ground (~15 MHz) up to 1400 MHz. The guaranteed time projects (PI HR) to carry out ultra deep pointed observations and to survey the entire northern sky will be unique in terms of angular resolution, depth, and extremely large frequency range. This enables a coherent study of clusters of galaxies over the entire history of the universe up to the formation of the first proto-clusters.

Studies of the associated shock waves produced by cluster mergers and the magnetic field properties of the cluster gas will constrain models of the formation of galaxy clusters.The large field of views of both LOFAR  will enable the detection of radio emission from millions of star-forming galaxies up to z=2-3, at the epoch at which the bulk of galaxy formation is believed to have occurred. In combination with infrared surveys, the first significant sample of proto-clusters of galaxies will be obtained. This will enable the first complete study of the overall properties of proto-clusters and their galaxy contents.With LOFAR’s ability to pinpoint radio sources with extremely steep radio spectra, we will detect radio galaxies at unprecedented distances. As our previous radio and optical investigations have established that distant radio galaxies are often located in proto-clusters, the most distant LOFAR radio galaxies would be excellent targets to locate and study the first proto-clusters close to or even at the epoch of reionisation.","2500000","2014-01-01","2019-12-31"
"NEWISOTOPEGEOSCIENCE","New isotope systems for the geosciences","Alexander Halliday","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Isotope geochemistry provides much of our understanding of the workings of the Earth s system from core to clouds. This proposal requests funds to develop and apply new MC-ICPMS methods to tackle geochemical issues that are poorly understood and for which new archives and techniques should lead to significant breakthroughs. We will focus on 3 areas. First, we will study vanadium (V), which is analytically challenging because of its extreme composition but which could be immensely powerful for tracing changes in oxidation. The V isotopic composition of magnetite will be investigated as a proxy for oxidation state and to provide new constraints on the cause of precipitation of V rich magnetite ores in the world s biggest deposits. We will also measure V isotopic compositions of hydrocarbons to address (a) the origin of vanadyl-porphyrins with studies of soils and their associated plant material, (b) subsurface geoporphyrin transformations and the fate of V locked within sedimentary organic matter as it becomes buried and (c) the degree to which degradation in crude oils changes isotopic composition. Second, we will develop isotope geochemistry of fluid immobile elements. Titanium will be used to elucidate: (a) the fate of the lithosphere in convecting mantle; (2) mobility of immobile elements in sedimentary to metamorphic systems and (3) the scavenging history of seawater recorded in marine Fe-Mn crusts. Zirconium will be used with Ti to evaluate changes in physical weathering over time. Lastly, we will develop new kinds of detectors based on photon burst spectroscopy and resonance ionisation spectroscopy, to detect rare isotopes in the Earth. This will be of relevance to the measurement of cosmogenic nuclides and the record of putative supernova events in our galaxy.","2347355","2010-03-01","2015-09-30"
"NEWLOG","New Directions Linking Ocean Geochemistry, Biomineralization and Palaeoclimate","Henry Elderfield","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","This proposal bridges three fields, geochemical oceanography, biomineralization and palaeoclimatology. The link that will be advanced is the oceanic carbon system now and in the past and its relationship to climate change. The major focus will go into marine calcification, the building block of the skeletons and shells for a large number of marine organisms. This is a key research area because: (i) storage of carbon in oceanic deposits of calcium carbonate plays an important but  poorly understood role in controlling atmospheric CO2; (ii) trace element and isotopic compositions of marine calcifying organisms have been used for reconstructing environmental parameters to understand past changes in climate; (iii) increasing ocean acidification will lead to reduced calcification of modern ecosystems as well as enhanced dissolution of carbonate sediments that will play an increasingly important role in the future chemistry of  the ocean and its ability to take up atmospheric CO2.

Work on biomineralization and biomineralogy of marine calcifiers is expanding rapidly but is almost completely divorced from work on their use in palaeoceanography. Bringing these two aspects together has enormous potential and is a key goal of this proposal.

There are a large number of opportunities given recent breakthroughs in understanding, within this proposal and in the future. The most important are to (i) understand incorporation of proxies into foraminifera; (ii) produce accurate estimates of pH and CO2 over earth history on tectonic, orbital and rapid time scales; (iii) explain how changes in deep-sea storage of calcium carbonate affects atmospheric CO2; and (iv) develop research on evolution of ocean chemistry concurrent with biotic innovations.","2581643","2011-03-01","2016-02-29"
"NEWMATS","New Directions in Hybrid Inorganic-Organic Framework Materials","Anthony Kevin Cheetham","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The proposed work is in the field of inorganic-organic hybrid materials, focusing mainly on dense rather than nanoporous materials. There are a huge number of opportunities in this area, especially now that is has become relatively straightforward to control the conditions under which dense, often anhydrous phases with excellent thermal stability can be synthesized. The latter have a wide range of interesting properties in terms of optics, magnetism, electronic conductivity, catalysis and so on. We shall work on such functional hybrid materials with potential applications in areas such as lighting and displays, photovoltaic cells, data storage, ferroelectrics, catalysis, and gas storage.  We shall also explore some of the fundamental questions concerning hybrid frameworks: What factors control their crystalline structures? Can we reliably simulate their structures and calculate their energies? What kinds of defects can be incorporated into hybrid frameworks? How do these affect their properties? What factors influence the mechanical properties of hybrid frameworks? Can we develop a general method for preparing nanoparticles of hybrids?  The work will involve a great deal of solution-based synthesis, chemical and thermal analysis, structure determination by single crystal X-ray methods, physical property measurements (mainly optical, magnetic, electronic, and mechanical), computer simulations, and some calorimetry. Most of the facilities needed for this work are available in Cambridge, though I shall collaborate with others where appropriate (e.g. computer simulation, calorimetry).","2018579","2008-11-01","2014-04-30"
"NEWPHYS-MOLECULES","Probing Physics beyond the Standard Model from Molecules","Wilhelmus Ubachs","STICHTING VU","The Standard Model of physics is incomplete. Gravity is not understood at the quantum level, dark matter and dark energy are not explained, and (string)-theories searching to cover these shortcomings are only consistent in higher-dimensional spaces, while only four of those dimensions are observed. The mystery of finely tuned strengths of the fundamental forces, providing us with a Universe of complexity, remains unexplained. This calls for new physics that can also be explored at the atomic scale in the low energy domain. That is the paradigm underlying the present proposal: Effects of new physics – either related to hitherto unknown particles or to symmetry-breaking phenomena – will manifest themselves as minute shifts in the quantum level structures of atoms and molecules, in minute drifts over time or dependencies on environmental conditions.

I propose to perform precision metrology measurements on the H2 molecule in a search for new physics. Deviations between experimental results and QED-theory will scan unexplored territory beyond the Standard Model. Molecular metrology results of the fundamental ground tone vibration in H2 will be confronted with QED-theory calculations to search for the existence of new forces at the Angström length scale. If extra dimensions beyond the known 3+1 would be compactified at the same length scale of 1 Å, this would lead to strongly enhanced gravitational effects, measurable in a molecule. Our current research on experimental probes for varying constants on a cosmological time scale, is redirected into the investigation of chameleon scenarios: by studying H2 molecules in white dwarf stars by uv-astronomy, and by studying methanol molecules in our own galaxy by radio astronomy, searching for a possible dependence of fundamental constants on strong gravity or on density. 

If any of these targeted phenomena could be uncovered, it would have great impact on science as a whole, and on our view on the Universe and its origin.","2500000","2015-09-01","2020-08-31"
"NewTURB","New eddy-simulation concepts and methodologies 
for frontier problems in Turbulence","Luca Biferale","UNIVERSITA DEGLI STUDI DI ROMA TOR VERGATA","Advances in transportation, energy harvesting, chemical processing, climatology, atmospheric and marine pollution are obstructed by the lack of understanding of turbulence. The turbulent energy transfer toward small-scales is characterized by highly non-Gaussian and out-of-equilibrium fluctuations that cannot be described by mean-field theories or traditional closure approximations. State-of-the-art computers and algorithms do not allow to perform brute-force direct numerical simulations of any realistic turbulent configuration:  modelling is mandatory. On the other hand,  turbulence models are often strongly limited by our lack of understanding of fundamental mechanisms. As a result, we have a deadlock:  turbulence is thought of as ‘unsolvable’ theoretically and computationally ‘intensive’. Indeed, progress by using conventional methods has been slow. Last year, however, something new happened. Two unconventional conceptual and numerical  methodologies to study Navier-Stokes equations appeared based on: (i) a surgery of nonlinear interactions with different Energy and Helicity contents, (ii) a fractal-Fourier decimation. These unexplored tools are potential breakthroughs to unravel the basic mechanisms governing the turbulent transfer in isotropic, anisotropic and bounded flows, e.g. the mechanism behind the growth of small-scales vorticity and formation/stability of coherent structures, a challenge that has defeated all numerical and theoretical attempts, up to now. The ultimate goal of NewTURB is to integrate the fresh knowledge achieved by using these novel  numerical instruments to push forward the frontiers of  turbulence modelling, exploiting the possibility to reduce the number-of-degrees-of-freedom in an innovative way to deliver alternative frontier ‘multiscale eddy-simulations’ methodologies for both unbounded and bounded flows with smooth walls or with heterogeneous landscapes, e.g. flows over a rough surface.","1986000","2014-03-01","2019-02-28"
"NewWorlds","Magnetic Fields and the Formation of New Worlds","Jean-Francois, Nicolas, Jacques DONATI","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Magnetic fields impact the formation of low-mass stars and their planets, and contribute to setting adequate conditions for life to appear. They control the amount of material and angular momentum from which stars and their planets form and mature, and can save newborn close-in planets from falling into their host stars. Magnetic fields can also affect planets by eroding their atmospheres and limiting their habitability, while hampering at the same time their detectability. Our understanding of these issues is however limited and critically needs observational guidance.

To achieve a breakthrough on these forefront topics, we will exploit SPIRou, a state-of-the-art near-infrared spectropolarimeter / velocimeter integrated in our group and borne to become in 2017 the best instrument worldwide for such science. By pushing SPIRou to its ultimate performance in terms of sensitivity and precision, we will unveil magnetic fields and planets of young stars and nearby M dwarfs using our expertise in tomographic imaging and activity filtering. NewWorlds will in particular:
- explore magnetic topologies of low-mass forming stars & accretion discs, study how they depend on stellar mass & age and how they control the angular momentum evolution of young stars;
- discover newborn hot Jupiters around young stars, investigate their impact on the formation and evolution of planetary systems, and assess the role of magnetic fields in their survival;
- detect and characterize planetary systems around nearby M dwarfs, focussing on terrestrial planets within the habitable zone to assess the impact of fields on their habitability.

With this ERC grant, I will form a team dedicated to addressing these challenging goals in a comprehensive way. Ambitious, feasible and timely, NewWorlds will allow our team to optimally exploit SPIRou to unlock our understanding of how new worlds form, and will clear the path for future follow-up programmes with major facilities like the JWST and the E-ELT.","3487580","2017-10-01","2022-09-30"
"NEXT","Towards the NEXT generation of bb0nu experimets","Juan José Gomez Cadenas","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","Neutrinoless double beta decay is a hypothetical, very slow radioactive process whose observation would establish unambiguously that massive neutrinos are Majorana particles --- that is to say, identical to their antiparticles ---, which implies that a new physics scale beyond the Standard Model must exist. Furthermore, it would prove that total lepton number is not a conserved quantity, suggesting that this new physics could also be the origin of the observed asymmetry between matter and antimatter in the Universe.

In recent years, many innovative ideas have been put forward to improve the sensitivity of \bbonu\ experiments. In general, these propositions have sought to increase the number of experimental signatures available to reject backgrounds while attempting to use isotopes and detector techniques which can be more easily scaled to large masses.

The objective of this project is to realize the NEXT experiment, an innovativedetector based on a high-pressure xenon gas (HPXe) TPC that will run at the Laboratorio Subterr\'aneo de Canfranc (LSC), in Spain.

Our primary goal is to complete the construction and commissioning of a 150 kg HPXe TPC (NEXT-100) by 2014, and start a physics run in 2015 that can improve the present bound set by the EXO experiment and perhaps discover the Majorana nature of neutrinos. In addition, we will carry out an R\&D program focused in demonstrating the scalability of the technology to the ton scale.","2791771","2014-02-01","2019-01-31"
"NLST","Nonlocality in space and time","Sandu Popescu","UNIVERSITY OF BRISTOL","Quantum mechanics is our most successful theory of nature. Yet more than eight decades after its inception there is a general agreement that a deep and intuitive understanding of it is still missing; we know how to compute quantum effects but we clearly do not have the full story. Even to this day, surprising and even paradoxical quantum effects continue to be frequently discovered. They are paradoxical only because our understanding of quantum behaviour is not yet good enough to have anticipated them.

However, for the first time there are glimmers of hope. It is the main thesis of this project that what makes quantum mechanics so counterintuitive is the fact that it is nonlocal.

One nonlocal phenomenon, namely Bell-type nonlocality, is at present investigated intensively. It is by now universally accepted that it holds at least part of the key to truly understanding quantum behaviour.

However, it is the main point of this project that there exist two other types of nonlocality, namely dynamic nonlocality and nonlocality in time. Dynamic nonlocality is the nonlocality of the quantum equations of motion, discovered in the context of the Aharanov-Bohm effect. Nonlocality in time is the ability to impose independent initial and final boundary conditions on the evolution of a quantum system. In contrast to Bell-type nonlocality, these two other types of nonlocality have received far less attention. Research into temporal-nonlocality (pre- and post-selection) has been evolving slowly over the last twenty years, whilst dynamical nonlocality has been virtually untouched. I believe that only by understanding all three types of nonlocality can the key to quantum behaviour be found.

This project will develop an intensive research program on dynamical and temporal nonlocality whilst pursuing a vigorous investigation into the many fundamental open questions related to Bell-type nonlocality.","1664126","2012-03-01","2018-02-28"
"NMCEL","A modular micro nuclear magnetic resonance in vivo platform for the nematode Caenorhabditis elegans","Jan Gerrit Korvink","KARLSRUHER INSTITUT FUER TECHNOLOGIE","""Using state-of-the-art microsystems simulation, design and micro-engineering, the NMCEL project will result in a highly integrated and modular and low cost platform that is a high throughput micro-fluidic lab-on-a-chip and a sophisticated nuclear magnetic resonance (NMR) detector in one package, suitable for use in a wide-bore commercial NMR magnet. This unique platform targets the controlled in vivo NMR spectroscopy and imaging of the model organism C. elegans, for this purpose the high throughput micro-fluidic lab-on-a-chip has the necessary infrastructure to feed, hold, move and immobilise a large population of C. elegans on demand and over its lifecycle and lifetime, with operations synchronised with the remaining functions of the platform. In turn, the co-integrated NMR detector is specially adapted to the range of shapes of the nematode, and its motility, and is optimised with regard to NMR signal-to-noise ratio and spectral resolution. In order to attain high resolution, the NMR system is optimised for strong magnetic fields, through the computed layout of devices, the adaptation of microstructurable materials, and the design of electronics and control circuits. The targeted user of the platform is a C. elegans micro-biologist with a requirement to detect or discover small molecule (< 1000 Da) metabolites in vivo. High throughput in vivo metabolomic mapping, with a platform that generates molecular data on an individual-by-individual basis for populations of thousands of individuals, has the potential to open up a completely new window of research in systems biology. The NMCEL project aims to address this important step ahead.""","3396960","2012-07-01","2017-06-30"
"NMNAG","New Methods in Non Archimedean Geometry","Francois Loeser","UNIVERSITE PIERRE ET MARIE CURIE - PARIS 6","During the last decade, spectacular achievments have been completed in Algebraic and Arithmetic Geometry and in Representation Theory by using powerful new tools provided by Motivic Integration and Berkovich spaces. We propose to develop a general framework for geometry over non archimedean valued field that will provide common foundations for Motivic Integration and Berkovich spaces. This will allow to broaden the range of potential applications. A main originality of our approach is the use of advanced tools from modern Model Theory, like definability and stable domination, together with methods from Algebraic Geometry. The relevance of Model Theory to non archimedean geometry may be illustrated as follows: geometry over valued fields ultimately combine geometry over the residue field and geometry over the value group. Model theorically these geometries correspond respectively to stable and o-minimal theories. These are of a very different nature and Model Theory provides unifying concepts allowing to treat them on equal footing. This approach will in particular allow us to solve several fundamental open questions on the tame nature of the topology of Berkovich spaces and should open new perspectives towards outstanding conjectures like the Monodromy conjecture. Our goal is also to use model theoretic tools in order to give new applications of Motivic Integration to Algebraic Geometry and Singularity Theory.","1541800","2010-03-01","2015-02-28"
"NMNP","Nonlinear Micro- and Nano-Photonics: nonlinear optics at the micrometer scale and below","Mordechai Segev","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","We will investigate, experimentally and theoretically, the dynamics of nonlinear optical waves at mesoscopic scales, ranging from several wavelengths (~10 microns) down to the sub-wavelength regime (~0.2 microns). Our studies will cover a variety of optical settings: from various kinds of periodic systems (photonic lattices) with and without disorder, to bulk materials and nano-suspensions. Under proper conditions, light propagating nonlinearly in these systems can display complex nonlinear dynamics, giving rise to a variety of fascinating phenomena.   Perhaps the most intriguing are associated with the suspensions containing dielectric nano-spheres, upon which light acts, by virtue of the gradient force, to modify the local density of spheres, thereby varying the effective refractive index. We will use light to alter the properties of the fluid (e.g., surface-tension, viscosity), which, in turn, will affect the pattern of optical wave in space and time. We will study nonlinear optics coupled directly to nonlinear fluid dynamics. Our preliminary results demonstrate optically-induced convection and optically-driven waves in the fluid. In the same system, we will explore sub-wavelength optical spatial solitons. Our preliminary experimental results clearly show very narrow solitons, narrower than imaging optics can resolve.   In another effort, we will explore arrays of sub-wavelength waveguides with a sharp index contrast, and will study a variety of nonlinear phenomena unique to such structures.   Other efforts include linear and nonlinear wave phenomena in photonic lattices, such as Anderson localization of lightt, the optical realization of the famous Hofstadter butterfly, waves in honeycomb lattices exhibiting unique features arising from symmetry (diabolic points, Berry phase effects, backscattering, etc.), Anderson localization in quasi-crystals and in honeycomb structures, transport of solitons in random potentials, and more.","2100000","2008-10-01","2014-09-30"
"NNNPDF","Proton strucure for discovery at the Large Hadron Collider","Stefano FORTE","UNIVERSITA DEGLI STUDI DI MILANO","""The objective of this project is to revolutionize the way the structure of the proton is accessed, determined, and used in the computation of physical processes at hadron colliders such as the Large Hadron Collider (LHC) of CERN.  At a hadron accelerator, predictions require a precise, detailed, and accurate description and understanding  of the structure of the colliding protons, as encoded in parton distributions (PDFs) - the distributions of quarks and gluons. At the LHC, PDFs are at present the major source of uncertainty, and in the near future they will be the main hurdle for discovery. The vision of this project is to remove this hurdle by attacking the problem using recent results from artificial intelligence (AI). I will lead a research team of two staff scientists, four postdocs and three PhD students, who will apply to PDF determination the recent methods of deep reinforcement learning and Q-learning, which will be coupled with deep residual networks to achieve a fully parameter- and bias-free understanding of proton structure. I will bring into high-energy physics a methodology so far used for object recognition in self-driving cars and automatic game playing, leading both to new physics, and new computational techniques. The application of these techniques to  PDFs will enable me to reach two secondary goals. The first is theoretical: the full use for PDF determination of recent high perturbative order (next-to-next-to leading order or NNLO) computations, which will be integrated by means of a new approximation method which relies on combining known exact results with all-order information in various kinematic limits to extend the scope of the former to a more detailed (""""more exclusive"""") description of the final state.The second is phenomenological: the integration in PDF determination of the Monte-Carlo event generators which are used to turn field theoretical prediction into a realistic description which may be directly compared to experimental data.
""","1602862","2017-10-01","2022-09-30"
"NOBLE","The Origin, Accretion and Differentiation of Extreme Volatiles in Terrestrial Planets","Christopher John Ballentine","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Identifying the processes by which the terrestrial planets acquired, retained and redistributed extreme volatile elements remains a fundamental science question. Although the halogens, Br, Cl and I, represent a powerful potential tracer, the concentrations of Br and I within most samples is too low for most laboratories - and the halogens have been ignored. We detail a Manchester innovation which enables halogens in these samples to be readily analysed.

We propose to: 1) Make the first direct experimental measurements of the partitioning behaviour of the halogens relevant to terrestrial mantle melting and planetesimal differentiation; 2) Establish the character of halogens available for accretion from chondrites; 3) Investigate the effect of planetesimal differentiation on the halogens from evolved meteorite glass/mineral systems; 4) Identify the role of massive impactors from lunar volcanic glasses; and 5) Identify the effect of planetary surface processes in the absence of life from alteration products in Martian meteorites.

On the Earth we will establish the first halogen mid ocean ridge and ocean island basalt halogen data base.  This will enable us to: 1) Characterise the respective mantle source halogen-noble gases; 2) Test models of volatile element recycling into the mantle by exploiting the high I/Cl of the ocean sediments caused by organic sequestration; 3) Identify whether the relationship between halogens and noble gases in subducted fluids (pilot data) is preserved in the mantle; 4) Use the mantle iodine concentration together with 129Xe as a tool to quantify the mass of dead iodine recycled into the respective reservoirs. 5) Identify or devise mantle convection and primordial reservoir source models compatible with the new data set.","2747271","2011-04-01","2017-03-31"
"NOCO2","Novel combustion principle with inherent capture of CO2 
using combined manganese oxides that release oxygen","Jan Anders Lyngfelt","CHALMERS TEKNISKA HOEGSKOLA AB","Conventional CO2 capture processes have significant cost and energy penalties associated with gas separation. Chemical-looping combustion (CLC), an entirely new combustion principle avoids this difficulty by inherent CO2 capture, using metal oxides for oxygen transfer from air to fuel. The process has been demonstrated in small scale with gaseous fuels. However, with solid fuels it would be difficult to reach high fuel conversion, with the oxygen-carrier materials used so far. But a new type of combined oxides based on manganese has the ability not only to react with gaseous fuel, but also to release gaseous oxygen, which would fundamentally change the concept.

The programme would provide 1) new oxygen-carrier materials with unique properties that would make this low-cost/high-efficiency option of CO2 capture possible, 2) cold-flow model investigation of suitable reactor system configurations and components, 3) a demonstration of this new combustion technology at the pilot plant level, 4) a model of the process comprising a full understanding, including kinetics, equilibria, hydrodynamics of fluidized reactors, mass and heat balances.

The basis of this programme is the discovery of a number of oxygen-releasing combined manganese oxides, having properties that can make a CLC with solid fuels a break-through process for CO2 capture. The purpose of the programme is to perform a comprehensive study of these materials, to demonstrate that they work in real systems, to achieve a full understanding of how they work in interaction with solid fuels in fluidized beds and to assess how this process would work in the full scale.

Climate negotiations and agreements could be significantly facilitated by this low cost option for CO2 capture which, in principle, should be applicable to 25% of the global CO2 emissions, i.e. coal fired power plants. It would also provide a future means of removing CO2 from the atmosphere at low cost by burning biofuel and capture CO2.
.","2500000","2012-03-01","2017-02-28"
"NOGAT","NOBLE GAS TRACING OF SOURCES AND SINKS OF VOLATILE ELEMENTS IN THE ATMOSPHERE","Bernard Marty","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","This proposal has the objective to greatly enhance our understanding of sources, sinks and processes fixing the composition of the atmosphere at different time periods of time, from 3.8 Gyr ago to Present. For achieving this goal, I shall develop the high precision analysis of noble gases, which are key tracers of atmospheric evolution.
The core of the proposal is : (i) the development of multi-collector mass spectrometry analysis of noble gas isotopes coupled with standard bracketing, aimed at reaching the per mil or better precision level, which will constitute a world premiere, (ii) the analysis of unique cometary samples, of ancient sediments already partly available at my laboratory, and of present-day air sampled at different geographical and altitudinal scales, (iii) the quantification of sources and sinks of atmospheric volatiles through the study of the fluxes of noble gas isotopes.
With this proposal, I develop a new and extremely competitive area of geochemistry, aimed at better understanding the early evolution of our planet habitability, as well as at improving our knowledge of fluxes of volatile elements triggering anthropogenic climate change. This proposal will establish the leadership of Europe in high precision geochemistry of exceptional tracers, the noble gases.","2281806","2011-01-01","2016-12-31"
"NOLIMITS","NOvel Large-scale InP-Membrane based Integration Technology & Science","Meint Koert Smit","TECHNISCHE UNIVERSITEIT EINDHOVEN","It is the long‐term objective of NOLIMITS to develop a unique technology, that will provide combined electronic and photonic functionality in a single chip and that will overcome the scale and speed limits in today’s photonic integration technology. Our dream is a technology that will be in every laptop and that will contain photonic interconnects and small but ultrafast and ultra low power photonic processors on top of an electronic IC, that contains the large scale but lower speed memory and control functions. While this is a very ambitious and long‐term objective, it is the aim of NOLIMITS to demonstrate the potential of InP Membranes on Silicon (IMOS) for realising this goal up to a level that large companies can be interested in the large‐scale development of the technology. We believe that this can be done by 5 PhD students and postdocs that will be funded from this project, in close cooperation with three PhD students that are presently working on IMOS‐related technology. NOLIMITS will address the following challenges in two subprojects:
• Providing proof of concept for integration of a full set of basic photonic components in IMOS technology on top of a CMOS chip which contains advanced electronic functionality.
• Developing and demonstrating the technology for an application where the combination of photonics and electronics brings additional functionality.
• Integration of plasmonic lasers in IMOS technology and demonstrating THz switching speeds with very low switching powers, that allow for application in ultrafast digital photonic signal processors with up to 1 million lasers.
If successful, NOLIMITS will provide a powerful technology for integrating photonics and electronics at a complexity level that exceeds today’s photonic technology by more than three orders. Such a technology will create a whole field of new application and will become a game‐changer in the world of photonic integrated circuits.","2419002","2012-05-01","2018-01-31"
"NOMBQUANT","Novel phases in quantum gases: from few-body to many-body physics","Georgy Shlyapnikov","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The project is aimed at developing new methods to create ultracold gases with unexplored many-body properties, and we construct the theory to realize the proposed opportunities. We intend to develop new ideas to induce resonant, long-range, and many-body interaction between particles. This includes novel near-zero-field Feshbach resonances in gases tightly confined to 1 or 2 dimensions that will enable the exploration of the physics of spinor gases in ultralow magnetic fields (<1mG). The idea is that the resonating state of the closed channel is a field-tunable confinement-induced weakly bound state. By the low field one avoids field-induced accumulation of particles in a given Zeeman state and encounters a variety of novel many-body states with interaction-broken spin rotation symmetry. The new resonances for polar molecules in 2 dimensions (layers) are provided by their coupling to the interlayer 2-molecule bound state. This allows one to reduce the short-range 2-body interaction making a 3-body repulsion important for bosons, so that the resulting many-body states can be various supersolids. It is further proposed to work on intriguing open problems. The creation of an itinerant ferromagnet of 2-component fermions is blocked in 3D by the formation of weakly bound dimers at the strong intercomponent repulsion required by the Stoner mechanism. In 1D this state is impossible for contact interactions. Our idea is to include an antisymmetric interaction (p-wave in 3D), which can practically make the ground state ferromagnetic. We then focus on non-conventional transport of rotational excitations of polar molecules randomly distributed in a deep optical lattice. The amplitude of hopping of an excitation from an excited to a ground state molecule decays as a cubic power of the distance between them. This is a long-range behavior which may lead to Levy flights, antilocalization, algebraic localization of the excitations, and we develop a theory of all these regimes.","1584207","2014-02-01","2019-01-31"
"NoNaCat","Development of Molecular-defined Non-noble Metal Complexes and Nano-structured Materials for Sustainable Redox ReactionsDevelopment of Molecular-defined Non-noble Metal Complexes and Nano-structured","Matthias Heinrich Beller","LEIBNIZ - INSTITUT FUR KATALYSE EV AN DER UNIVERSITAT ROSTOCK","The major objective of this proposal is the development of new active and selective catalysts based on earth abundant metals (e.g. Fe, Mn, Co, Cu). These catalysts will be used for improved synthetic transformations which are of interest for organic chemistry in general and which are also of significant practical value for the chemical and life science industries. Traditional catalysts based on non-noble metals are not efficient for hydrogenation and dehydrogenation processes under mild conditions. However, by creating a suitable microenvironment with M-N interactions they are becoming active and selective. According to our concept the suitable surrounding will be created either by using nitrogen-containing pincer ligands or nitrogen-doped graphenes. Consequently, a variety of both molecular-defined homogeneous catalysts as well as nano-structured heterogeneous materials will be prepared, characterized and tested in various catalytic applications. More specifically, the following redox transformations will be investigated: Hydrogenation and transfer hydrogenation of carboxylic acids, esters, and nitriles; hydrogenation of amides and peptides; hydrogenation of carbon dioxide and selective oxidative coupling of alcohols to esters, amides, and nitriles. Furthermore, “waste-free” carbon-carbon bond forming reactions such as alkylations with alcohols and domino-synthesis of heterocycles from alcohols will be exploited. Finally, homogeneous and heterogeneous catalysts from earth abundant metals will be used in industrially relevant oxidative carbonylation reactions. With respect to methodology this proposal combines homogeneous with heterogeneous catalysis, which will result in new ideas for both fields.","2499375","2015-12-01","2020-11-30"
"NONFLU","Non-local dynamics in incompressible fluids","Diego CORDOBA GAZOLAZ","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","The goal of this project is to pursue new methods in the mathematical analysis of non-local and non-linear
partial differential equations. For this purpose we present several physical scenarios of interest in the context
of incompressible fluids, from a mathematical point of view as well as for its applications: both from the
standpoint of global well-posedness, existence and uniqueness of weak solutions and as candidates for blowup.
The equations we consider are the incompressible Euler equations, incompressible porous media equation
and the generalized Quasi-geostrophic equation. This research will lead to a deeper understanding of the nature
of the set of initial data that develops finite time singularities as well as those solutions that exist for all time for incompressible flows.","1779369","2018-09-01","2023-08-31"
"NonSequeToR","Non-sequence models for tokenization replacement","Hinrich SCHUETZE","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","Natural language processing (NLP) is concerned with
computer-based processing of natural language, with
applications such as human-machine interfaces and
information access.  The capabilities of NLP are currently
severely limited compared to humans. NLP has high error
rates for languages that differ from English (e.g.,
languages with higher morphological complexity like Czech)
and for text genres that are not well edited (or noisy) and
that are of high economic importance, e.g., social media
text.

NLP is based on machine learning, which requires as basis a
representation that reflects the underlying structure of the
domain, in this case the structure of language.  But
representations currently used are symbol-based: text is
broken into surface forms by sequence models that implement
tokenization heuristics and treat each surface form as a
symbol or represent it as an embedding (a vector
representation) of that symbol.  These heuristics are
arbitrary and error-prone, especially for non-English and
noisy text, resulting in poor performance.


Advances in deep learning now make it possible to take the
embedding idea and liberate it from the limitations of
symbolic tokenization.  I have the interdisciplinary
expertise in computational linguistics, computer science and
deep learning required for this project and am thus in the
unique position to design a radically new robust and
powerful non-symbolic text representation that captures all
aspects of form and meaning that NLP needs for successful
processing.

By creating a text representation for NLP that is not
impeded by the limitations of symbol-based tokenization, the
foundations are laid to take NLP applications like
human-machine interaction, human-human communication
supported by machine translation and information access to
the next level.","2500000","2017-10-01","2022-09-30"
"NORDIA","Non-Rigid Shape Reconstruction and Deformation Analysis","Ron Kimmel","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","Deformable and non-rigid objects, both natural and artificial, surround us at all scales from nano to macro, and play an important role in many applications ranging from medical image analysis to robotics and gaming. Such applications require the ability to acquire, reconstruct, analyze, and synthesize non-rigid three-dimensional shapes. These procedures pose challenging problems both theoretically and practically due to the vast number of degrees of freedom involved in non-rigid deformations. While modelling and analysis of non-rigid shapes has greatly advanced in the past decade, existing solutions are largely based on parametric models restricting the objects of interest to a narrow class of similar shapes. Broadly speaking, reconstruction, analysis, and synthesis of arbitrary deformable shapes remain unsolved problems, a practical solution of which would be a major milestone in computer vision and related fields. This proposal aims at answering these fundamental questions by adopting tools from modern metric geometry, a field of theoretical mathematics which in the past few decades has undergone a series of revolutions that remained largely unnoticed and unused in applied sciences. We believe that metric geometry tools could systematically answer these questions, and, coupled with modern numerical optimization techniques and novel hardware architectures, pave the computational way to the next generation in deformable shape analysis. We plan to develop such numerical tools while demonstrating their efficiency on several challenging real-life applications such as surgery prediction and planning, biometry, and computer-aided diagnosis.","2121295","2011-01-01","2016-12-31"
"NOTES","New Opportunities in Terahertz Engineering and Science","Alexander Giles Davies","UNIVERSITY OF LEEDS","Over the last 20 years, the study of mesoscopic quantum-confined electronic systems has revealed a wealth of exciting physics. The characteristic energy scale in many important mesoscopic devices such as two-dimensional electron systems, layered semiconductor structures, semiconductor quantum dots, and laterally-confined wires, dots, and other geometries, corresponds to the terahertz (THz) frequency range, which until recently has been difficult to access. Furthermore, invaluable information on the states and dynamics of carriers in condensed matter systems, not obtainable by other techniques, can potentially be accessed though the dynamic (high frequency) electronic response. My vision is to create a step-change in the study of mesoscopic electronic systems by developing and exploiting THz frequency technology to probe the THz frequency/picosecond response of quantum-confined electronic systems. I will develop quasi-optical guided-wave techniques to generate (and detect) single-cycle THz/picosecond electronic pulses adjacent to the mesoscopic system in a cryostat, avoiding the RC bandwidth-limiting problems inherent in previous high frequency (up to the GHz range) electrical measurements. In parallel, I will develop a series of original imaging and spectroscopy technologies based on the THz quantum cascade laser, including continuous wave coherent detection. These will be exploited in a range of proving projects, including phase-stepping interferometry, coherence-gated imaging and, ultimately, depth-resolved THz holography. This programme, comprising the symbiotic development of THz engineering and science, will be unique internationally and will open new opportunities and directions in the study and exploitation of THz frequency electronics and photonics.","1542600","2008-11-01","2013-10-31"
"NOVAMOX","Novel niches for anaerobic methane oxidation and their biogeochemical sigificance","Bo THAMDRUP","SYDDANSK UNIVERSITET","Motivated by a series of recent discoveries, NOVAMOX provides the first comprehensive biogeochemical and microbial ecological analysis of methane consumption in anoxic freshwater systems and oceanic oxygen minimum zones, environments where such processes to date were largely ignored. I propose that anaerobic microbial methane oxidation pathways are important sinks in for methane in these environments, thereby affecting methane emissions and the cycling of nitrogen, iron, and sulfur, as the cycling of these elements is coupled either directly or indirectly to methane oxidation. With the development of new incubation and sensing techniques necessary to detect the processes in their environment, we will identify and quantify active pathways of anaerobic methane oxidation, identify the organisms that catalyse these transformations, analyse their environmental distribution, characterize kinetic controls of their growth and metabolic activity, and analyse the isotopic signatures they may leave behind. The project will generate robust estimates of the biogeochemical significance of anaerobic methane oxidation in these overlooked niches, and provide a quantitative mechanistic framework for analysis of the role of these processes in Earth’s biogeochemical evolution as well as for their implementation in forecasts of global change. The project will also provide fundamental new insights to the ecology of the highly specialized microorganisms involved in methane oxidation, for use in potential biotechnological applications.","2462500","2016-10-01","2021-09-30"
"NOVCAT","Design of Novel Catalysis by Metal Complexes","David Milstein","WEIZMANN INSTITUTE OF SCIENCE LTD","Global concerns regarding the economy, environment and sustainable energy resources dictate an urgent need for the design of novel catalytic reactions. We have recently discovered novel, environmentally benign reactions catalyzed by pincer complexes, including an entirely new reaction, namely the direct coupling of alcohols with amines to produce amides and H2 (Science, 2007, 317, 790). We believe that the mechanisms of these reactions involve a new concept in catalysis: metal-ligand cooperation by aromatization-dearomatization of the ligand. Such cooperation can play key roles also in the activation of H2, C-H, and other bonds. Remarkably, we have very recently discovered a new strategy towards light-induced water splitting into H2 and O2, also based on metal-ligand cooperation in a pincer system, and have observed an unprecedented O-O bond formation process (Science, in press). The design of efficient catalytic systems for splitting water into hydrogen and oxygen, driven by sunlight, and without use of sacrificial reagents, is among the most important challenges facing science today, underpinning the potential of hydrogen as a clean, sustainable fuel. In this context, it is essential to enhance our understanding of the fundamental chemical steps involved in such processes. We plan to (a) explore the scope of bond activation and catalysis based on the new concept of metal ligand cooperation by aromatization-dearomatization (b) study the mechanism and scope of the newly discovered novel approach towards water splitting by light (c) develop novel environmentally benign catalytic reactions involving O-H, C-H and other bonds, such as anti-Markovnikov hydration of alkenes (d) develop unprecedented asymmetric catalysis using chiral cooperating ligands (e) develop new CO2 chemistry, including its hydrogenation to methanol and photolytic splitting to CO and O2. The research is expected to lead to novel catalysis, of importance to environment and sustainable energy.","1912018","2010-04-01","2015-03-31"
"Novel Calorimetry","Exploring the Terascale at LHC with Novel Highly Granular Calorimeters","Tejinder Virdee","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","The recent discovery of a Higgs boson by the ATLAS and CMS Collaborations at the Large Hadron Collider at CERN, Geneva, has undoubtedly opened a portal to widely expected new physics, anticipated to manifest itself in the Tera-electron-Volt range: the Terascale. New physics is needed to understand some of the deepest mysteries of our universe, that include its composition, where dark matter and dark energy seem to comprise 95% of its energy/matter density, and its evolution from the Big Bang to today, where we see much structure and where matter dominates over antimatter, and whether we live in more dimensions than the familiar four. Substantial improvements to the current experiments at the LHC are planned, and new experiments are being proposed or discussed at future new energy frontier accelerators to tackle these scientific issues. A key element of the improved and newer generation detectors is the use of very high performance calorimeters for the measurement of the energies of particles produced in the high-energy collisions at colliders. At the upgraded LHC these must operate in an unprecedentedly challenging experimental environment. This proposal deals with a novel, yet untested, high-risk approach to calorimetry that combines state of the art techniques so far only used independently either in charged particle tracking or conventional calorimeters. New technologies will have to be developed for such a ground-breaking calorimeter. These include very fine feature size, powerful, radiation hard electronics in emerging technologies using feature sizes of 130 nm or 65 nm; low-cost silicon sensors in the emerging technology using 8” silicon wafers, also in Europe; environmentally-friendly cooling technologies using liquid carbondioxide; high performance and fast decision making logic using new more powerful FPGA, all to be produced at an industrial scale.","3034107","2015-10-01","2020-09-30"
"NOVGRAPHENE","Novel uses for graphene","Francisco Guinea Lopez","FUNDACION IMDEA NANOCIENCIA","""Models for novel uses of graphene, not feasible in other materials, will be developed. Emphasis will be made on properties unique to graphene, like its extremely high stiffness, flexibility, tunable metallic features, and very low mass density. Novel applications will be studied in the areas of i) structural deformations and modulation of electronic properties, ii) spin manipulation, and iii) optoelectronics and plasmonics.""","991691","2012-05-01","2017-04-30"
"NOVOX","Perfectly interfaced nanomaterials for next generation oxide electronics","Judith Louise Driscoll","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Oxide thin film heterostructures hold the key to a wide range of novel, and energy efficient devices of many different sorts. In the last few years, there has been a plethora of very exciting reports in the top journals on the basic science of single layer oxide films or heterostructure devices. However, the holy grail of applications is still just an event on the horizon. An innovative and emerging materials science led approach is now required to understand the factors at play limiting these highly promising materials, thus opening the door to realising their functional potential. As time progresses the interfaces are playing an ever stronger role in the functionality and multifunctionality. New kinds of interfaces, new ways to control them, and state-of-the art probing of them are all needed to understand how to control and tune them. This proposal strikes at the heart of all these issues and aims to realise the true power of oxide electronics.","1565873","2010-05-01","2016-04-30"
"NPW","Novel Process Windows - Boosted Micro Process Technology","Volker Hessel","TECHNISCHE UNIVERSITEIT EINDHOVEN","Novel Process Windows (NPW) is an entirely new way of process design to boost micro process technology for the production of high-added value fine chemicals. Such process intensification demands for microstructured reactors with their excellent capabilities on mass and heat transfer and short residence times with prime constructional and functional features. This proposal is truly comprehensive and holistic as it includes four projects with different NPW facets; starting from a molecular-mechanistic- (New Chemical Trans¬formations) and kinetic-scale (High-Temperature / Pressure Processing) via the scale of reaction environment (Solvent-free Operation and Tuneable / Reactive Solvents) up to a process scale (Process Simplification and Integration). These four individual measures are bundled and directed by a generic project for cross-cutting insight, evaluation through cost and life-cycle analysis, and transfer to a large number of reactions.
High-p,T processing will enable for the Claisen rearrangement to shrink reaction times by orders of magnitude and to increase space-time yields consequently. Substantial selectivity increases are targeted for this reaction and the hydroformylation. The latter reaction will make use of tuneable solvents and near-critical water processing. As new chemical transformations with  process simplification and integration, the direct oxidation of cyclohexene to adipic acid as one-step synthesis and the copper-catalysed triazole Click Chemistry as one-flow multi-step synthesis will be tested.
These new and challenging processing technologies provide highly promising perspectives for future ‘green’ chemical factories to boost sustainability, covering the whole manufacturing chain in one system and providing a multi-purpose infrastructure.","2496100","2011-04-01","2016-03-31"
"NSYS","Nonlinear System Identification and Analysis in the Time, Frequency, and Spatio-Temporal Domains","Stephen Alec Billings","THE UNIVERSITY OF SHEFFIELD","Recent advances in biology, neuro-imaging, the observation of space by satellites, and many other disciplines has lead to an explosion of data and it is now absolutely imperative that a theoretical framework is developed that can be used to analyse this data to identify system dynamic behaviours in a transparent manner to reveal core dynamic behaviours and features. Complex nonlinear behaviours are ubiquitous in the life sciences, neuro-imaging and many other domains but the problems that these challenges raise are also fundamental in many other disciplines and problem domains. The study of complex systems that evolve as a function of time has received enormous attention over the last century and many important results have been established. While there is still much work to do to fully understand this class of systems recent results demonstrate that there is now a unique opportunity to significantly enhance and extend this purely temporal focus both to include nonlinear frequency domain analysis, and to derive results for the important class of spatio-temporal complex systems. The main aim of this proposal is to develop methods for the identification and analysis of the class of severely nonlinear systems, to develop complimentary nonlinear frequency domain identification and analysis methods, and to study the large class of systems that are defined by both spatial and temporal dynamics. In each case the aim is to develop core generic systems approaches that allow the construction of transparent models from recorded data sets that can be related back and analysed in terms of the components of the underlying system. Exemplars will be used throughout as case studies; these will include modelling the magnetosphere, stem cell dynamics, understanding the visual processing in drosophila or fruit fly brain, modelling the link between the recorded fmri signals and neural activity in brain, and the modelling of chemical systems and crystal growth far from equilibrium.","1947104","2009-04-01","2014-03-31"
"NTSC","New Techniques for Secure Computation","Yuval Mordechai ISHAI","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","Secure computation is a general-purpose tool for processing sensitive data without compromising its
confidentiality or integrity. In recent years, significant research efforts have been invested in optimizing
and implementing secure computation protocols and related cryptographic primitives such as fully
homomorphic encryption and program obfuscation.
The proposed project will explore new techniques for secure computation and related primitives.
The new techniques are expected to enhance the asymptotic and concrete efficiency of current solutions
and extend the known relations between secure computation and other problems in cryptography and
computational complexity theory. The research will span the following directions:
• Succinct secure computation via homomorphic secret sharing, exploring new research directions
that were opened by recent works of the PI;
• Protecting protocols against malicious parties, with relevance to efficient general-purpose program
obfuscation;
• Protecting protocols against general information leakage","1903833","2017-10-01","2022-09-30"
"NuBSM","From Fermi to Planck : a bottom up approach","Mikhail SHAPOSHNIKOV","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The Standard Model of particle physics is a hugely successful theory that has been tested in experiments at ever increasing energies, culminating in the recent discovery of the Higgs boson. Nevertheless, some major riddles cannot be addressed by the Standard Model, such as neutrino oscillations, the existence of Dark Matter, the absence of antimatter in the Universe. New fundamental principles, interactions and unknown yet particles are required to address these questions. Much of the research done during the last three decades on physics ‘beyond the Standard Model’ (BSM) has been driven by attempts to find a ‘natural’ solution of the hierarchy problem: why the Planck and the electroweak scales are so different. The most popular approaches to this problem predict new particles with the masses right above the electroweak scale.

This project explores an alternative idea that the absence of new particles with masses between the electroweak and Planck scales, supplemented by extra symmetries (such as scale invariance) may itself explain why the mass of the Higgs boson is much smaller than the Planck mass. This calls for a solution of the BSM problems by extremely feebly interacting particles with masses below the electroweak scale. Along the same lines we also explore the possibility that cosmological inflation does not require a new field, but is driven by the Higgs field of the Standard Model.
The proposed model offers solutions for BSM puzzles and is among a few ones that can be tested with existing experimental technologies and are valid even if no evidence for new physics is found at the LHC.

Constructing such a theory requires consolidated efforts in domains of high-energy theory, particle physics phenomenology, physics of the early Universe, cosmology and astrophysics as well as analyses of the available data from previous experiments and from cosmology. We will make predictions and establish the sensitivity goals for future high intensity experiments.","2371132","2016-10-01","2021-09-30"
"NUDEV","New Organic Semiconductor Device Concepts","Karl Leo","TECHNISCHE UNIVERSITAET DRESDEN","The goal of the present project is to investigate novel device concepts based on organic  semiconductor materials: the number of device principles is still rather small for organic devices and many concepts are not explored. The concept of controlled molecular doping successfully introduced to optoelectronic devices will be used here to achieve Fermi level control and  well-defined junctions. Preferentially, we plan to work in this project with oligomer substances which are deposited by vacuum technology. The main reason for this choice is that this approach allows depositing multilayer structures with comparatively high reproducibility. However, the principles developed here should also applicable with other low-cost deposition technologies such as printing. Based on previous experiments such as the realization of the first pn-homojunction, this work will concentrate on the following devices: First, we want to further develop the Zener diodes which have in first experiments shown that the basic effects exist in organic devices as well. The goal is to significantly reduce the forward offset voltage and the improve controllability of the reverse breakdown voltage. Second, we want to try to realize
efficient triodes allowing larger currents and higher switching speed as conventional approaches. Furthermore, we strive to realize an organic bipolar transistor, which is challenging in organic devices due the rather small diffusion lengths. Third, we will approach new devices with more complex layer structures, such as thyristors. While certain device types will be in the focus of the projects, we expect that the general understanding of organic semiconductors in switching device applications can be improved.","1953280","2011-04-01","2016-09-30"
"NUHGD","Non Uniform Hyperbolicity in Global Dynamics","Sylvain CROVISIER","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","An important part of differentiable dynamics has been developed from the uniformly hyperbolic systems. These systems have been introduced by Smale in the 60's in order to address chaotic behavior and are now deeply understood from the qualitative, symbolic and statistic viewpoints. They correspond to the structurally stable dynamics. It appeared that large classes of non-hyperbolic systems also exist. Since the 80's, different notions of relaxed hyperbolicity have been introduced: non-uniformly hyperbolic measures, partial hyperbolicity, ... They allowed to extend the previous approach to other families of systems and to handle new examples of dynamics: the fine description of the dynamics of Hénon maps for instance.
The development of local perturbative technics have brought a rebirth for the qualitative description of generic systems. It also opened the door to describe more globally the spaces of differentiable dynamics. For instance, it allowed recent progresses towards the Palis conjecture which characterizes the absence of uniform hyperbolicity by the homoclinic bifurcations — homoclinic tangencies or heterodimensional cycles. We propose in the present project to develop technics for realizing more global perturbations, yielding a breakthrough in the subject. This would settle this conjecture for C1 diffeomorphisms and imply other classification results.

These past years we have understood how qualitative dynamics of generic systems decompose into invariant pieces. We are now ready to describe more precisely the dynamics inside the pieces. We propose to combine these new geometrical ideas to the ergodic theory of non-uniformly hyperbolic systems. This will improve significantly our understanding of general smooth systems (for instance provide existence and finiteness of physical measures and measures of maximal entropy for new classes of systems beyond uniform hyperbolicity).","1229255","2016-09-01","2021-08-31"
"NUMERIWAVES","New analytical and numerical methods in wave propagation","Enrique Zuazua","BCAM - BASQUE CENTER FOR APPLIED MATHEMATICS","This project is aimed at performing a systematic analysis, providing a real breakthough, of the combined effect of wave propagation and numerical discretizations, in order to help in the development of efficient numerical methods mimicking the qualitative properties of continuous waves. This is an important issue for its many applications: irrigation channels, flexible multi-structures, aeronautic optimal design, acoustic noise reduction, electromagnetism, water waves, nonlinear optics, nanomechanics, etc. The superposition of the present state of the art in Partial Differential Equations (PDE) and Numerical Analysis is insufficient to understand the spurious high frequency numerical solutions that the interaction of wave propagation and numerical discretizations generates. There are some fundamental questions, as, for instance, dispersive properties, unique continuation, control and inverse problems, which are by now well understood in the context of PDE through the celebrated Strichartz and Carleman inequalities, but which are unsolved and badly understood for numerical approximation schemes. The aim of this project is to systematically address some of these issues, developing new analytical and numerical tools, which require new significant developments, much beyond the frontiers of classical numerical analysis, to incorporate ideas and tools from Microlocal and Harmonic Analysis. The research to be developed in this project will provide new analytical tools and numerical schemes. Simultaneously, it will contribute to significant progress in some applied fields in which the issues under consideration play a key role. In parallel with the analytical and numerical analysis of these problems, a mathematical simulation platform will be set to perform computer simulations and explore and visualize some of the most relevant and complex phenomena.","1663000","2010-02-01","2016-01-31"
"NuQFT","The Hall Plateau Transition and non-unitary Quantum Field Theory","Hubert Saleur","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","I propose to solve the Quantum Field Theory (QFT) describing the transition between plateaus of quantized Hall conductance in the Integer Quantum Hall Effect (IQHE). 
The existence of the plateaus and their topological origin are certainly well understood. In sharp contrast, the transition, which mixes the effects of disorder, magnetic field and possibly interactions, remains very mysterious. Numerical studies of lattice models are plagued by disorder. The QFT description involves physics at very strong coupling, and requires a non-perturbative solution before quantitative predictions can be made.
Finding such a solution is very difficult because the QFT for the plateau transition is ‘non-unitary’ -  it involves a non-Hermitian ‘Hamiltonian’. Non-unitary QFT is a challenging, almost unexplored topic, that must be first developed before the plateau transition can be addressed. 
I propose to carry out this task with a cross-disciplinary strategy that uses ideas and tools from conformal field theory, statistical mechanics, and mathematics. Key to this strategy is a new and powerful way of analyzing lattice regularizations of the QFTs by focussing on their algebraic properties directly on the lattice, with a  mix of advanced representation theory and numerical techniques.
The results - in particular, concerning  conformal invariance and renormalization group flows in the non-unitary case - will then be used to solve the QFT models for the plateau transition in the IQHE and in other universality classes of 2D Anderson insulators. This will be a landmark step in our understanding of the localization/delocalization transition in two dimensions, and allow a long delayed comparison of theory with experiment. The results will, more generally, impact many other areas of physics where non-unitary QFT plays a central role - from disordered systems of statistical mechanics to the string theory side of the AdS/CFT duality, to the effective description of open quantum systems.","2098158","2015-10-01","2020-09-30"
"NVS","Nano Voltage Sensors","Shimon Weiss","BAR ILAN UNIVERSITY","To understand how the brain works, tools need to be developed that will allow neuroscientists to investigate how interactions between individual neurons lead to emergent networks. Towards this goal, we will develop targetable voltage sensing nanorods that self-insert into the cell membrane and optically and non-invasively record action potentials at the single particle and nanoscale level, at multiple sites and across a large field-of-view. In semiconductors, absorption and emission band edges are modulated by an external electric field, even more so when optically excited electron-hole pairs are confined, giving rise to the quantum confined Stark effect. The physical origin of this effect is in the separation of photoexcited charges, creating a dipole that opposes the external field. The proposed sensors will optically record action potential with unique advantages not offered by other methods: much larger voltage sensitivity, high brightness, and hence single-particle voltage sensitivity, large spectral shift (affording noise-immune ratiometric measurements), fast temporal 
response, minimal photobleaching, large Stokes shifts, large two-photon excitation cross sections, excellent performance in the NIR, and compatibility with lifetime imaging. The proposed sensors could afford, for example, the recording of pre- and post-synaptic membrane potentials, sub-threshold events, ultrafast spiking, individual ion channel activity, or a release of ions from single Ca+2 stores. In addition, deep tissue imaging could be afforded by two photon microscopy and far-field non-linear temporal focusing combined with lifetime imaging. Here we seek to optimize all aspects of the sensors’ synthesis, functionalization, delivery, targeting and detection, in order to provide neuroscientists and physiologists a viable and user-friendly technology that will be generally useful for the study of action potential signals in the brain and in healthy 
or diseased heart and muscle tissues.","3497553","2016-01-01","2020-12-31"
"NWFV","Nonlinear studies of water flows with vorticity","Adrian Mircea Constantin","UNIVERSITAT WIEN","The aim of the project is to build and promote a team of excellence in the mathematical theory of water flows, with emphasis on nonlinear aspects. Our aim is to advance the state-of-the-art of water flows with vorticity. Flows within a fixed fluid domain as well as free surface flows will be considered and we strive to provide an accurate description of the entire flow; for example, the flow beneath a water wave and not just a description of the water wave profile. Problems of this type are currently of great interest, for example in the context of wave-current interactions and for a better understanding of tsunami waves. In addition to methods from the theory of partial differential equations to investigate the governing equations for water waves, the use of simplified models with a rich structure (e.g. integrable systems arising in the shallow water regime) will identify and highlight qualitative features. Numerical simulation in conjunction with experimental feedback and the gathering of field data will be of great support. Provision is made for consultation and collaboration with research groups in engineering and physics. Due to the interest of the general public in tsunamis, one of the objectives is to have a positive impact on the perception of science by society and on the raising of scientific interest of the younger generation through public lectures and contacts with high-schools.","1324797","2011-04-01","2016-03-31"
"OACFT","Operator Algebras and Conformal Field Theory","Roberto Longo","UNIVERSITA DEGLI STUDI DI ROMA TOR VERGATA","The project has two fundamental aims:    A) Open new research horizons exploiting the interplay between Operator Algebras and Conformal Quantum Field Theory.   B) Use Operator Algebraic methods for a deeper understanding of the internal structure of Conformal Quantum Field Theory, with a possible feedback for Operator Algebras.    A) concerns two points in particular:  - Find Noncommutative Geometrical structures associated with certain representations of Conformal Nets of von Neumann algebras and provide index theorems for quantum systems with infinitely many degrees of freedom in this framework. - Set up relations between Vertex Algebras and Local Conformal Nets and so provide new methods and results in each of these two subjects by importing and developing methods of the other subject.   B) concerns in particular the analysis of following points, some motivated by A): - Structure and classification of conformal nets of von Neumann algebras on the circle and on two-dimensional spacetimes, and of their representations; in particular conformal supersymmetric models.  - KMS and super-KMS functional structure in conformal models.  - Boundary Conformal Field Theory, in particular regarding new thermalization effects. - Conformal subnet structure, in particular restrictions on the possible index values for conformal subnets.  - Nuclearity and trace class properties for representations and modularity properties.","1044750","2008-12-01","2013-11-30"
"OAFPG","Operator Algebras, Free Probability, and Groups","Uffe Valentin Haagerup","KOBENHAVNS UNIVERSITET","Starting with Voiculescu&apos;s pioneering work on free probability from 1983, there has for more than 25 years been a strong development in mathematical research on the interrelations between operator algebra theory, probability theory and group theory. The PI has over the last 10 years made several important contributions to this development. The aim of the project is to target some of the main open problems in this area, such as the isomorphism problem for free group factors and Connes&apos; embedding problem for finite factors. The project will take place at University of Copenhagen in close contact with its well established group in operator algebras.","1612171","2010-01-01","2014-12-31"
"OCAL","Optimal Control at Large","Ioannis LYGEROS","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","""Despite wide ranging progress on both the theory and applications of optimal control for more than half a century, considerable challenges remain when it comes to applying the resulting methods to large scale systems. The difficulties become even greater when one moves outside the classical realm of model based optimal control to address problems where models are replaced by data, or macroscopic behaviours emerge out of microscopic interactions of large populations of agents. To address these challenges, we propose here to develop a framework for approximating optimal control problems using randomised optimisation. The starting point will be formulations of optimal control problems as infinite dimensional linear programs. Our recent work suggests that randomised methods on the one hand can serve as a basis for algorithms to approximate such infinite programs and on the other enjoy close connections to statistical learning theory, providing a direct link to data driven approaches. Turning these intuitions into an approximation framework for optimal control that rests on solid theoretical foundations and provides explicit accuracy guarantees will be the methodological contribution of the proposed research. The resulting methods can find a range of applications in engineering and beyond; here we will investigate two such applications. One is motivated by our work on energy management in buildings and districts. The challenge here is the dimensionality of the system, especially if one would like to include weather and other forecast information and the corresponding uncertainty. The second application will be to so-called population systems, that involve the interaction of many agents with local decision-making capabilities coupled through the use of common resources. Here the main challenges are defining suitable """"features"""" to abstract the individual states and the integration of uncertainty due to the presence of non-participating agents.
""","2497058","2018-11-01","2023-10-31"
"OCLOC","From Open to Closed Loop Optimal Control of PDEs","Karl Kunisch","UNIVERSITAET GRAZ","The proposal addresses some of the most pressing topics in optimal control of partial differential equations (PDEs): Non-smooth, non-convex optimal control and computational techniques for feedback control. These two topics will be applied to the large scale optimal control problems for the bidomain equations, which are the established model to describe the electrical activity of the heart. Due to their rich dynamical systems behavior these systems are particularly challenging.

The use of non-smooth functionals is of great practical relevance in many diverse situations. They promote sparsity, and provide a perfect formulation for switching and multi-bang controls, and for the optimal actuator location problem. For inverse problems the case $L^{p}$ with $p\in (0,1)$ is of special statistical importance, and $L^0$ can be the basis of a new formulation for topology optimization problems. But lack of Lipschitz continuity and of convexity are significant obstacles which can only be overcome by the development of new analytical and numerical concepts. The new algorithmic concepts will also be applicable to important non-smooth problems in continuum mechanics, as for instance the quasi-static evolution of fractures.

Closed loop control is of paramount importance due to its {\bf robustness} against system perturbations. Nevertheless, numerical realization of optimal feedback strategies for nonlinear PDEs has barely been touched since the curse of dimensionality makes direct numerical treatment of the Hamilton-Jacobi-Bellman equation unfeasible. We shall therefore develop and analyze suboptimal strategies based on model reduction and interpolation techniques, and on model-predictive control. The availability of boundary and near-to-the boundary measurements together with dynamic observer techniques will allow to test the proposed methods to obtain suboptimal feedback controls for the bidomain equations.","1678325","2016-01-01","2020-12-31"
"OCONTSOLAR","Optimal Control of Thermal Solar Energy Systems","Eduardo Francisco de Asis FERNANDEZ CAMACHO","UNIVERSIDAD DE SEVILLA","OCONTSOLAR aims to develop new control methods to use mobile sensors mounted on drones and unmanned ground vehicles (UGV) as an integral part of the control systems. Sensors mounted on vehicles have been used for surveillance and for gathering information, however these mobile sensors have not been used so far as an integral part of control systems. 
Solar power plants will be used as a case study, with the aim of optimizing their operation using spatial irradiance estimations and predictions. Many results will be applicable to other systems such as traffic control in highways and cities, energy management in buildings, micro-grids, agriculture (irrigation and plague control) and flood control. The main objectives and challenges are:
1. Methods to control mobile sensor fleets and integrate them as an essential part of the overall control systems. 
2. Spatially distributed solar irradiance estimation methods using a variable fleet of sensors mounted on drones and UGVs. 
3. New model predictive control (MPC) algorithms that use mobile solar sensor estimations and predictions to yield safer and more efficient operation of the plants allowing the effective integration of solar energy in systems delivering energy to grids or other systems while satisfying production commitments.
OCONTSOLAR includes proofs of concepts by implementation on the Solar Platform of Almeria and on a solar air conditioning plant installed at the host institution.","2500000","2018-09-01","2023-08-31"
"OFAV","Open intelligent systems for Future Autonomous Vehicles","Alberto Broggi","UNIVERSITA DEGLI STUDI DI PARMA","The objective of this proposal is the development of an open architecture for future autonomous vehicles to become a standard platform shared by car makers in the design of next generation intelligent vehicles. It is based on 360 degrees sensorial suite which includes perceptual and decision making modules, with the ultimate goal of providing the vehicle with autonomous driving capabilities and/or supervise the driver's behavior. The perception module also includes vehicle-to-vehicle and vehicle-to-infrastructure subsystems, to increment the vehicle s sensing capabilities.  The research is based on the extended know-how and experience of the Principal Investigator s group at the Univ of Parma, which already marked fundamental milestones worldwide in the field of vehicular robotics. Car manufacturers and automotive suppliers are extremely interested in this research stream, but at the same time are very cautious in investing in long term and risky research like this.  Besides providing clear advantages on safety for road users, the availability of an open architecture will encourage and make possible the sharing of knowledge between public and private research communities (academic and automotive industry) and thus speed up the design of a standard platform for future vehicles.  Further research steps will be eased -and therefore made more effective-thanks to the common and open architectural layer proposed by this project.","1751067","2008-12-01","2013-10-31"
"OGLEIV","Optical Gravitational Lensing Experiment: New Frontiers in Observational Astronomy","Andrzej Udalski","UNIWERSYTET WARSZAWSKI","We apply for financial support for the new, fourth phase of the Optical Gravitational Lensing Experiment (OGLE-IV) - one of the largest scale sky surveys worldwide, operating continuously since 1992. During its operation the OGLE project contributed significantly to many fields of modern astrophysics including gravitational microlensing, extrasolar planets searches, stellar astrophysics, Galactic structure and many others. The main scientific goal of the OGLE-IV phase will be the second generation planetary microlensing survey. It should result in top rank discoveries of the Earth mass planets and should provide the full census of planets down to Earth masses orbiting their hosts at 1-5 AU orbits. This parameter space is only accessible to the microlensing technique. Complementary census of planets orbiting at the distances smaller that 1 AU is to be made by space missions using transit technique.  OGLE-IV survey will also conduct research in many other top rank astrophysical topics like the search for Pluto size dwarf planets from the Kuiper Belt, search for free-floating black holes, microlensing in the Magellanic Clouds and Galactic disk. Hundreds of new discoveries in the variable star field are also guaranteed. Moreover, OGLE-IV will operate on-line services providing real time photometry of variable objects of many types. The OGLE-IV data will be placed in public domain and available to the astronomical community.","2498000","2010-01-01","2014-12-31"
"OMNES","Open Many-body Non-Equilibrium Systems","Tomaz PROSEN","UNIVERZA V LJUBLJANI","We shall study non-equilibrium many-body quantum systems, considering local interactions in one or two spatial dimensions in situations where the generator of time evolution in the bulk of the system is unitary whereas the incoherent processes are limited to the system's boundaries. We foresee a mathematical theory of dynamical quantum phases of matter with applications in the theory of quantum transport and nanoscale devices that manipulate heat, information, charge or magnetization. 

Our steady-state setup represents a fundamental paradigm of mathematical statistical physics which has been pioneered by the PI, who gave the first explicit solution for boundary driven/dissipative strongly interacting many-body problem (XXZ spin 1/2 chain) which answered a long debated question on strict positivity of the spin Drude weight at high temperature. 

The main focus of OMNES will be centered on exploring the following three interconnected pathways: Most importantly, we shall develop a general framework for exact solutions of non-equilibrium integrable quantum many-body models, in particular the steady states and relaxation modes, and develop quantum integrability methods for non-equilibrium many-body density operators. Fundamentally new concepts which are expected to emerge from these studies, relevant beyond the context of boundary-driven/dissipative systems, are novel quasilocal conservation laws of the bulk Hamiltonian dynamics. Second, we shall investigate relevance of exact solutions in physics of generic systems which are small perturbations of integrable models and explore the problem of stability of local and quasilocal conserved quantities under generic integrability-breaking perturbations. Third, we shall formulate and study the problem of quantum chaos in clean lattice systems, in particular to establish a link between random matrix theory of level statistics and kinematic and dynamical features of lattice models with sufficiently strong integrability breaking.","2041000","2016-10-01","2021-09-30"
"OMSAMA","Optimisation of Multiscale Structures with Applications to Morphing Aircraft","Michael  Ian Friswell","SWANSEA UNIVERSITY","The performance of engineering structures is continuously increasing, enabled by the accurate simulation and subsequent optimization of these systems. The ACARE Vision 2020 document set the ambitious goal of a 50% reduction in aircraft emissions that can only be achieved through a step change in aircraft technology. Adaptive structures and morphing aircraft are novel technologies that can provide this step change, and this proposal provides an efficient method to model, optimize and realize these structures. Morphing aircraft have the ability to alter the shape of their wings to improve fuel efficiency or to increase control effectiveness. The Wright brothers employed wing warping for roll control, but as aircraft speeds increased compliant structures were replaced with small, rigid control surfaces. Bird flight motivates the search for more efficient solutions, where a compliant structure is continuously optimized in flight using distributed sensors and actuators. From the structural perspective the objective is to produce fully integrated, hierarchical structures with compliance control. However the requirements are conflicting: the structure must be stiff to withstand the external loads, but must be flexible to enable shape changes. The solution to this conflict is to design the structure to decouple the two actions, through components with significant anisotropy and integrated actuation. The components may be modelled at the micro scale, but these models are too large for system optimization studies. This proposal provides a step change to existing methods by developing a framework where multi-scale and multi-physics modelling may be achieved efficiently, though significant improvements in the way in which the different models of varying fidelity communicate.","2481462","2010-05-01","2015-04-30"
"OMSQC","Orthogonalization Models in Semiempirical Quantum Chemistry","Walter Thiel","MAX PLANCK INSTITUT FUER KOHLENFORSCHUNG","""The proposal aims at the development of a generally applicable semiempirical approach that goes beyond the current standard model by including explicit orthogonalization and dispersion terms into the semiempirical Hamiltonian. We have recently shown in preliminary work on organic molecules that such orthogonalization models (OMx = OM1, OM2, OM3) are significantly more accurate than standard semiempirical methods (AM1, PM3, PM6) both for ground-state and excited-state properties, at comparable computational costs. We plan to improve the OMx models by incorporating dispersion corrections (OMx-D) and by extending the formalism from an sp to an spd basis (OMx-DE). The resulting approaches will be parameterized for all chemically important main-group elements and transition metals to generate the next generation of generally applicable semiempirical methods. These methods are designed to fill the currently existing gap between density functional theory (DFT) and classical force field approaches. Being about 1,000 times faster than DFT, and being capable of treating electronic events (unlike classical force fields), OMx-based methods are expected to enable realistic electronic structure calculations, with useful accuracy, on large complex systems in all branches of chemistry. Especially when applied in a multi-method strategy, with synergistic use of different computational tools, this will allow the modelling of many chemically relevant systems that are currently beyond reach for computational chemistry. Proof-of-concept applications will address the reaction mechanisms of enzymatic reactions (biocatalysis) and electronically excited states (organic solar cells, photoactive proteins, excited-state dynamics in complex systems). The successful development of generally applicable OMx-based methods will provide a breakthrough in computational chemistry by opening up new areas of application.""","1996000","2014-01-01","2018-12-31"
"ONE","Unified Principles of Interaction","Michel Beaudouin-Lafon","UNIVERSITE PARIS-SUD","Most of today’s computer interfaces are based on principles and conceptual models created in the late seventies. They are designed for a single user interacting with a closed application on a single device with a predefined set of tools to manipulate a single type of content. But one is not enough! We need flexible and extensible environments where multiple users can truly share content and manipulate it simultaneously, where applications can be distributed across multiple devices, where content and tools can migrate from one device to the next, and where users can freely choose, combine and even create tools to make their own digital workbench.

The goal of ONE is to fundamentally re-think the basic principles and conceptual model of interactive systems to empower users by letting them appropriate their digital environment. The project will address this challenge through three interleaved strands: empirical studies to better understand interaction in both the physical and digital worlds, theoretical work to create a conceptual model of interaction and interactive systems, and prototype development to test these principles and concepts in the lab and in the field. Drawing inspiration from physics, biology and psychology, the conceptual model will combine substrates to manage digital information at various levels of abstraction and representation, instruments to manipulate substrates, and environments to organize substrates and instruments into digital workspaces. 

By identifying first principles of interaction, ONE will unify a wide variety of interaction styles and create more open and flexible interactive environments.","2456028","2016-10-01","2021-09-30"
"ONE-MIX","Mid-infrared optical dual-comb generation and spectroscopy with one unstabilized semiconductor laser","Ursula Keller","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","ONE-MIX proposes to develop single-source dual-comb lasers for mid-infrared (2-5 μm wavelength)
spectroscopy, potentially enabling many new applications in science and industry, such as environment, safety,
pharma, and health. This proposal extends our recent demonstration of a dual-comb MIXSEL (Modelocked
Integrated eXternal-cavity Surface Emitting Lasers) in the near-infrared, validated by measuring weak water
absorption at 968 nm. Many more relevant gas absorption lines, however, are in the mid-infrared, where
sensitivities of parts-per-billion can be achieved.
Dual-comb mid-IR spectroscopy applications are currently limited by the cost, complexity, and size of
conventional optical comb systems, based on two modelocked lasers with four active stabilization loops. The
single-source dual-comb MIXSEL, however, substantially reduces the complexity of existing systems to a
single compact free-running laser. In comparison to other competing new approaches such as quantum cascade
lasers or micro resonator combs, the MIXSEL provides substantially more power per comb line with low
linewidth and noise, and is ideally suited for a 1 to 5 GHz comb spacing, which is optimal for many molecular
spectroscopy applications, allowing for fast, accurate, and sensitive absorption measurements.
This proposal leverages our know-how in MIXSEL design combined with III-V semiconductor epitaxy to
demonstrate this new class of lasers in the mid-IR, enabling simpler, lower-cost systems with sufficient speed
and sensitivity for many relevant and commercially interesting applications in the 2 to 5 μm spectral region,
such as CO2 , CH4, or NOx trace gas detection. We propose to extend the near-IR MIXSELs to the mid-IR by
combining two validated semiconductor approaches – using type-II gain quantum wells combined with a type-
I saturable absorber quantum wells, fabricated with existing GaSb (gallium-antimonide) molecular beam
epitaxy systems operated out of the FIRST lab at ETH Zurich.","3210000","2019-01-01","2023-12-31"
"OPTELOMAC","Opto-Electronic Organic Materials by New Acetylene Chemistry","François Nico Diederich","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","""Atom-economic, atom-economic (""""click""""-type)transformations of donor (N,N-dialkylaniline, TTF, ferrocene)-activated acetylenes with strong electron-accepting olefins (TCNE, TCNQ, tricyanovinyl derivatives) are applied to the construction of stable molecular and supramolecular chromophores with unusual electronic and optical properties. Their properties are characterized in interdisciplinary collaboration with the objectives to provide new classes of chromophores for opto-electronic device applications, to investigate pi-electron delocalization in acetylenic molecular architectures extending into one, two, and three dimensions, and to advance fundamental knowledge in an interplay between experiment and theory allowing prediction and tuning of opto-electronic properties. Specific aims are: 1. New &quot;super-electron acceptors&quot; and investigation of their intra- and intermolecular charge-transfer interactions. These non-planar, stable, and sublimable chromophores are expected to possess high third-order optical nonlinearities and are investigated for formation of amorphous, high-optical quality films and conductive or magnetic charge-transfer complexes and salts with various electron donors. 2. Optically pure alleno-acetylenic macrocycles and oligomers adopting helical conformations. The chiroptical properties of these chromophores are exceptional and will be further enhanced in supramolecular assemblies. 3. Covalently modified fullerenes with increased electron uptake capability for applications in photovoltaic devices. 4. Regular [AB]-type oligomers and polymers using the formation of charge-transfer chromophores from acetylenic precursors as the chain-propagation step. 5. Zwitterionic, redox-amphiphilic dendrimers for mono- and multi-layer formation in organic electronic devices.""","1690200","2010-03-01","2015-02-28"
"OPTINF","Optimization and inference algorithms from the theory of disordered systems: theoretical challenges and applications to large-scale inverse problems in systems  biology","Riccardo Zecchina","POLITECNICO DI TORINO","The project is  focused on two objectives:  the study of  optimization and inference algorithms based on advanced  statistical physics methods for disordered systems, and their  application to  large-scale inverse problems in computational  systems biology.
In last years,   fundamentally  new approaches to large-scale optimization and inference problems have emerged at the interface between  Statistical Mechanics  and Computer Science. Partly this was made possible by extending ideas from the statistical physics of disordered systems to applications in computer science. Indeed, the application of methods originally developed for the analysis of spin glasses  to  hard optimization problems led to the definition of message passing algorithms (MPAs),  a new class of algorithms that on many difficult problems showed  performance definitely superior to Monte Carlo schemes.   The field presents many conceptual open problems   and  applications of great potential impact.
MPAs are intrinsically parallel and can be used to tackle optimization problems over large networks of constraints.  Their  probabilistic foundations are still largely unexplored and thus their study can contribute greatly to computational  statistical physics.
At the same time,  these new techniques  are  becoming key  tools in fields such as   computational  systems biology, where  the  exponential increase of molecular data   is posing  new computational challenges  in the study of biological systems  composed by many  interacting molecular components. It is a fact that the advances in sequencing and other high throughput technologies deeply transformed the world of biological research over the last 10-15 years. This project aims  at bringing the  MPAs  techniques to the full benefit of biological research.","1260105","2011-07-01","2016-06-30"
"OPUS","Optical Ultra-Sensor","Markus Pollnau","KUNGLIGA TEKNISKA HOEGSKOLAN","This project aims at pushing the limits of optical sensing on a microchip by orders of magnitude, thereby allowing for ultra-high sensitivity in optical detection and enabling first-time-ever demonstrations of several optical sensing principles on a microchip. My idea is based upon our distributed-feedback lasers in rare-earth-ion-doped aluminum oxide waveguides on a silicon chip with ultra-narrow linewidths of 1 kHz, corresponding to Q-factors exceeding 10^11, intra-cavity laser intensities of several watts over a waveguide cross-section of 2 micrometer, and light interaction lengths reaching 20 km. Optical read-out of the laser frequency and linewidth is achieved by frequency down-conversion via detection of the GHz beat signal of two such lasers positioned in the same waveguide or in parallel waveguides on the same microchip.
The sensitivity of optical detection is related to the laser linewidth, interaction length, and transverse mode overlap with the measurand; its potential of optically exciting ions or molecules and its optical trapping force are related to the laser intensity. By applying novel concepts, we will decrease the laser linewidth to 1 Hz (Q-factor > 10^14), thereby also significantly increasing the intra-cavity intensity and light interaction length, simplify the read-out by reducing the line-width separation between two lasers to the MHz regime, and increase the mode interaction with the environment by either increasing its evanescent field or perpendicularly intersecting a nanofluidic channel with the optical waveguide, thereby allowing for unprecedented sensitivity of optical detection on a microchip. We will exploit this dual-wavelength distributed-feedback laser sensor for the first-ever demonstrations of intra-laser-cavity (ILC) optical trapping and detection of nano-sized biological objects in an optofluidic chip, ILC trace-gas detection on a microchip, ILC Raman spectrometry on a microchip, and ILC spectroscopy of single rare-earth ions.","2499958","2014-11-01","2019-10-31"
"ORBITMOL","'Orbital molecules' - self-organised states for orbitronics","John Paul Attfield","THE UNIVERSITY OF EDINBURGH","‘Orbital molecules’ are made up of coupled orbital states on several metal ions within an orbitally-ordered (and sometimes also charge-ordered) solid such as a transition metal oxide. Spin-singlet dimers (a weak metal-metal bond) are known in several materials, but recent discoveries of more exotic species such as 18-electron heptamers in AlV2O4 and 3-atom trimerons in magnetite (Fe3O4) have shown that a general new class of quantum electronic states that we call ‘orbital molecules’ awaits exploration.

The discovery of trimerons is particularly important as it provides the solution to the important and long-running problem of the low temperature Verwey phase of magnetite. This was discovered in 1939 but remained contentious as the complex superstructure was unknown. The applicant and co-workers recently used a synchrotron microcrystal technique to solve the structure. This showed that the Verwey transition is driven by Fe2+/3+ charge ordering in a first approximation, but with the formation of a self-organised network of trimeron orbital molecules that had not been predicted in over 70 years of previous study.

To expand the magnetite discovery into a general breakthrough in understanding quantum matter, this project will explore chemical tuning of orbital molecule self-organisation, discovery of novel orbital molecule orders in frustrated networks, and investigations of trimeron glass and liquid phases in magnetite. Evidence for liquid phases is key to possible applications. The project will develop high resolution diffraction and total scattering methods to determine long range and local orbital molecule orders, with further characterisation from magnetisation and conductivity measurements.  Samples will be synthesised at ambient and high pressures.

This study will pioneer a new area of research in the electronic properties of solids, and may help to underpin future post-silicon orbitronic technologies based on the creation and manipulation of orbital states.","2315142","2014-02-01","2019-01-31"
"ORISTARS","Toward a Complete View of Star Formation: The Origin of Molecular Clouds, Prestellar Cores, and Star Clusters","Philippe Jacques Antoine André","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","Understanding star formation from large to small scales is a major unsolved problem of modern astrophysics, fundamental in its own right and having a profound bearing on both galaxies and planet formation.To achieve a breakthrough in our observational and theoretical knowledge of star formation, we propose to confront numerical simulations with observations obtained with state-of-the-art instrumentation, including instruments developed by our group. We will investigate three key areas: a) Origin of filamentary molecular clouds; b) Core formation within filaments; c) Fragmentation of prestellar cores into binary stars and protoplanetary disks. Our approach is novel in that we combine observational, theoretical, and instrumental efforts within the same team, and we address star formation coherently from large to small scales, allowing us to establish links between the « macrophysics » (eg. global star formation in galaxies) and the « microphysics » of the problem (eg. formation of individual solar systems). This project will be based on extensive (sub-)millimeter continuum and spectral line studies with the Herschel Space Observatory, IRAM 30m telescope/interferometer, APEX 12m telescope, and ALMA interferometer. ORISTARS will benefit from our large « Gould Belt Survey » key project with Herschel providing the first complete census of prestellar cores and young protostars in nearby star-forming complexes. We will extensively use our lab’s magnetohydrodynamic code RAMSES. We will also take advantage of technological innovations made at CEA for the Herschel-PACS and ArTéMiS instruments to develop a large 1.2mm bolometer array/polarimeter (« Polar-Channel ») for the next-generation millimeter continuum camera at IRAM. Polar-Channel will be a new powerful tool to map polarized dust continuum emission and help clarify the role of magnetic fields in forming prestellar cores and generating the rich filamentary cloud structure revealed by our ongoing Herschel observations.","2267880","2012-04-01","2018-03-31"
"OSIRIS","Open silicon based research platform for emerging devices","Lars Mikael Östling","KUNGLIGA TEKNISKA HOEGSKOLAN","The OSIRIS proposal will address the crucial and ultimately strategic area for the future emerging nanoelectronics, i.e. how structures and devices actually will be fabricated as physical dimensions approaches a few nanometer minimum feature size. The project title is  Open silicon based research platform for emerging devices  and indicates that many of the future emerging devices will be based on a silicon fabrication base platform but may not be fully based on silicon as the active semiconductor material. Over the past 10 years this research team has established a versatile fabrication technology platform in excellent condition to open up a variety of new technologies to explore nanometer minimum feature size in realizable electrical repeatable devices structures.

The proposed project has five different focus areas outlined. It covers a broad range of critical research issues that can be foreseen as groundbreaking topics for the period beyond 2015. the different topics addressed are;

1) Three dimensional FET nanostructures based on SiNW and GeNW with advanced configuration.
2) New applications of SiNW with build-in strain for fast silicon-base optoelectronic devices.
3) Low frequency noise in advanced nanoelectronic structures
4) THz devices for IR-detection
5) Bio-sensor nanoelectronics for extreme bio-molecule sensitivity and real time detection of DNA.

These areas are carefully chosen to assemble the right mix with predictable research success and with a few areas that can be called high gain/high risk. In particular we want to mention that focus area 2 and 4 have a great potential impact when successful but also at a certain higher risk for a more difficult implementation in future devices. There is in no cases any risk that the research will not generate high quality scientific results.","1999500","2009-06-01","2014-05-31"
"OSYRIS","Open SYstems RevISited: From Brownian motion to quantum simulators","Maciej Lewenstein","FUNDACIO INSTITUT DE CIENCIES FOTONIQUES","""This proposal concerns open systems, i.e. systems interacting with the environment, and their fundamental role in natural sciences. The main objectives are: i) to develop theory of Brownian motion for molecules in biological environments; ii) to adapt classical many-body open systems such as kinetic or/and diffusion-aggregation models to the quantum domain; iii) to develop theory of open systems as quantum simulators; finally iv) to develop theory of quantum Brownian motion in inhomogeneous media. Although all these objectives may seem to be quite unrelated, our main goal will be to connect them in order to unambiguously asses the relevance of open systems in specific areas of physics, biology and beyond. Accordingly, objective i) will be explored in close collaboration with experimentalists in which the diffusion of biomolecules on cell membranes requires a description in terms of Brownian motion in correlated disordered potentials. In ii) we will search for many-body kinetic and growth models that provide the configurations that may serve as samples of random potentials desired in i). These models can be regarded as quantum models with non-Hermitian generators of evolution; in some situations they can be generalized to genuine quantum ones, described by a quantum master equation, linking ii) and iii). In iii) we will look for applications of quantum open systems as quantum simulators of condensed matter/high energy physics. We will also look at single particle interactions with quantum many body environment, linking the objectives iii) with iv) and i). Expected results are: a) understanding the relationship between biological function and the spatiotemporal dynamics of single molecules in living cells; b) understanding of the structure of classical many body stochastic models and their relation to quantum ones; c) concrete proposals for open systems quantum simulators;  and  d) development of tools to characterize and observe quantum Brownian motion.""","1787565","2014-01-01","2018-12-31"
"OUTEFLUCOP","Out of Equilibrium  Fluctuations in Confined  Phase Transitions","Sergio Ciliberto","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","This project aims at studying experimentally the out of equilibrium fluctuations
in strongly confined fluids. Three main problems will be analyzed :
a) The effects on the dynamics when the fluctuations are confined in a volume smaller than the spatial correlation length; b) The fluctuations of the injected and dissipated power in out of equilibrium in highly confined systems, where extreme events may produce
an instantaneous ''negative entropy production rate''. c) Are fluctuations a limiting factor for application ? Might they be useful ?
Our strategy is to enhance the role of fluctuations and correlations working close to the critical point of a second order phase transition. We will work  at the critical point
of mixing  of either  a binary mixture of fluids or of   polymer blends, whose microscopic time scales and correlation lengths are  much longer than those of binary mixtures of simple fluids. The local   measurements and the confinement will be realized using an original ultra low noise Atomic Force Microscopy (AFM)  developed  in our laboratory. This AFM will be used in association with a near field aperture   free light scattering technique, local and global dielectric   techniques and evanescent waves imaging. This experimental set up, measuring local and global variables, will give new insight to two other interesting phenomena that are present in the critical regions : the finite size effects (such as dimensional crossover and time dependent critical Casimir effect) and the relaxation towards equilibrium after a quench at the critical point. These two phenomena have been widely investigated both theoretically and numerically butonly a few experiments have tried to measure directly the local fluctuations of confined fluids. Due to the universal nature   of phase transitions the results can be applied to many other systems in   which measurements are more complicated.","2376117","2011-03-01","2016-12-31"
"OUTREACH","Overlooked Unresolved Toxic Organic Pollutants: Resolution, Identification, Measurement and Toxicity:OUTREACH","Steven John Rowland","UNIVERSITY OF PLYMOUTH","Organic pollution exceeds an area greater than that of France+Germany. Toxic, persistent, unresolved and unidentified, complex mixtures (UCMs) of organic chemicals are abundant all over the globe. These UCMs, which encompass a number of chemical classes, are outside present EU and world pollutant regulations. It is very important that these toxicants are identified, measured and their effects studied. Until recent work in my lab, few methods allowed such UCMs to be identified. Even now, we have only made studies of unresolved alkylated derivatives of benzene; there are numerous other unstudied classes of UCM chemicals widespread in the environment. These will be amenable to study by the new methods once optimised and new high throughput screening methods will then allow 10,000 chemicals per day to be tested for toxicity. In the present project, firstly, novel gas chromatography x gas chromatography-time of flight-mass spectrometry (GCxGC-ToF-MS) methods will be used to separate, identify and measure, novel UCM compounds isolated from water  soluble  fractions of a range of crude oils; secondly, the same methods will be used to identify previously UCM toxicants in polluted mussels collected from a range of global locations. Thirdly, representatives of each compound class will be synthesised and toxicities assayed using mussel feeding rates, amphipod reproduction and growth rates and fish hepatocyte assays. Finally, the UCMs, newly resolved and identified in polluted mussels will be isolated and fractionated by preparative high performance liquid chromatography and their toxicities assayed, in an effects-directed approach. This will establish definitively the range of compounds in the UCMs that are toxic, and will deliver new methods by which UCMs may be routinely assayed and no longer overlooked. The major outcome of this investigation will be much improved assessment of the effects of organic chemical pollutants on the health of the global coastal marine environment","2000000","2009-07-01","2014-12-31"
"OXIDESURFACES","Microscopic Processes and Phenomena at Oxide Surfaces and Interfaces","Ulrike Diebold","TECHNISCHE UNIVERSITAET WIEN","Metal oxide surfaces and interfaces play a key role in energy-related applications and in novel schemes for electronic devices that exploit the special physical and chemical properties of these promising materials.

For progress in both areas, a detailed, mechanistic understanding of the atomic and molecular processes that occur at oxide surfaces and interfaces is critical.  Experiments on well-characterized model systems in conjunction with computational modelling can provide such insights, but current investigations are limited in the range of materials and scope of phenomena that can be studied, and to experiments in a low-pressure environment.

Research conducted in this project will push these limits by:
•	Developing new methodologies for atomic-scale investigations of the subsurface region of oxides with mixed electronic and ionic conduction to measure mass and charge transport across oxide interfaces.
•	Combining cutting-edge molecular beam epitaxy techniques with atomically-resolved scanning tunneling microscopy to synthesize samples of multi-component metal oxide materials with tailored surface properties.
•	Establishing a new research thrust that will combine both ex-situ and in-situ electrochemical surface science techniques to study structurally well characterized metal oxide surfaces in an aqueous environment.","2496100","2012-02-01","2017-01-31"
"OXLEET","Oxidation via low-energy electron transfer. Development of green oxidation methodology via a biomimetic approach","Jan Erling Bäckvall","STOCKHOLMS UNIVERSITET","Oxidation reactions are of fundamental importance in Nature and are key transformation in organic synthesis. There is currently a need from society to replace waste-producing expensive oxidants by environmentally benign oxidants in industrial oxidation reactions. The aim with the proposed research is to develop novel green oxidation methodology that also involves hydrogen transfer reactions. In the oxidation reactions the goal is to use molecular oxygen (air) or hydrogen peroxide as the oxidants. In the present project new catalytic oxidations via low-energy electron transfer will be developed. The catalytic reactions obtained can be used for racemization of alcohols and amines and for oxygen- and hydrogen peroxide-driven oxidations of various substrates. Examples of some reactions that will be studied are oxidative palladium-catalyzed C-C bond formation and metal-catalyzed C-H oxidation including dehydrogenation reactions with iron and ruthenium. Coupled catalytic systems where electron transfer mediators (ETMs) facilitate electron transfer from the reduced catalyst to molecular oxygen (hydrogen peroxide) will be studied. Highly efficient reoxidation systems will be designed by covalently linking two electron transfer mediators (ETMs). The intramolecular electron transfer in these hybrid ETM catalysts will significantly increase the rate of oxidation reactions. The research will lead to development of more efficient reoxidation systems based on molecular oxygen and hydrogen peroxide, as well as more versatile racemization catalysts for alcohols and amines.","1722000","2010-01-01","2015-12-31"
"OXYGEN","How oxygen regulates the structure and function of microbial ecosystems","Donald Eugene Canfield","SYDDANSK UNIVERSITET","Our project is called OXYGEN.  Our aim is to develop and use cutting-edge high-sensitivity oxygen detection systems to explore how oxygen regulates the metabolism of aerobic and anaerobic organisms in experimental systems and in nature. We will explore and understand the complex structure of low oxygen ecosystems on the present Earth, and the evolution of  these ecosystems through time in the face of changing concentrations of atmospheric oxygen.","2568194","2011-04-01","2016-03-31"
"PACART","Free space photon atom coupling - the art of focusing","Gerhard Leuchs","FRIEDRICH-ALEXANDER-UNIVERSITAET ERLANGEN NUERNBERG","A conceptually simple but radically new approach will be explored and developed: the interaction of light with a single atom in free space. No experiment has yet come close to the highest possible coupling efficiency attainable in such a fundamental system. The usual way of enhancing light-matter coupling is to place an atom inside a cavity. Another approach involves setting the atom in the near field of a plasmonic antenna. The free space approach, however, is special: a light field matched to the atomic dipole provides many desired aspects of fully efficient coupling. The birth of this new research area was marked by the PI's pioneering publication in 2000 arguing that efficient coupling of an atom to a light field is possible in free space without modifying the density of modes of the light field such as in a cavity or having competing radiative or non-radiative decay channels such as in plasmonic enhancement. At the time of writing, the highest probability achieved for exciting a single atom with a single photon in free space is less than 1%. At the heart of the project proposed here is a deep diffraction-limited parabolic mirror, which can provide the required aberration-free focusing of a vectorial dipole wave over the full 4π solid angle – a true challenge to optics. Perfectly efficient free space coupling to a single quantum system will be a novel building block for numerous applications. In addition, the experimental set-up will allow for the studying of other open questions in the realm of classical and quantum optics related to full solid angle focusing.","1499704","2014-03-01","2019-02-28"
"PACE","Programming Abstractions for Applications in Cloud Environments","Ermira Mezini","TECHNISCHE UNIVERSITAT DARMSTADT","""Cloud computing is changing our perception of computing: The Internet is becoming the computer and the software: (a) vast data centers and computing power are available via the Internet (infrastructure as a service), (b) software is available via the Internet as a service (software as a service). Building on the promise of unlimited processing/storage power, applications today process big amounts of data scattered over the cloud and react to events happening across the cloud. Software services must be both standard components to pay off for their provider and highly configurable and customizable to serve competitive needs of multiple tenants.
Developing such applications is challenging, given the predominant programming technology, whose fundamental abstractions were conceived for the traditional computing model.
Existing abstractions are laid out to process individual data/events. Making the complexity of applications processing big data/events manageable requires abstractions to intentionally express high-level correlations between data/events, freeing the programmer from the job of tracking the data and keeping tabs on relevant events across a cloud. Existing abstractions also fail to reconcile software reuse and extensibility at the level of large-scale software services.

PACE will deliver first-class linguistic abstractions for expressing sophisticated correlations between data/events to be used as primitives to express high-level functionality. Armed with them, programmers will be relieved from micromanaging data/events and can turn their attention to what the cloud has to offer. Applications become easier to understand, maintain, evolve and more amenable to automated reasoning and sophisticated optimizations.  PACE will also deliver language concepts for large-scale modularity, extensibility, and adaptability for capturing highly polymorphic software services.""","2280998","2013-03-01","2018-02-28"
"PACEMAKER","Past Continental Climate Change: Temperatures from marine and lacustrine archives","Jaap Sinninghe Damste","STICHTING NIOZ, KONINKLIJK NEDERLANDS INSTITUUT VOOR ONDERZOEK DER ZEE","Global climate change is a topic of major interest as it has a large impact on human societies. Computer models used to predict directions of future climate change are validated by means of retrospective analysis of past climate changes. Detailed reconstruction of past climates, especially temperature, is, therefore, of considerable importance. Several tools (proxies) are available to reconstruct absolute sea surface temperatures. Continental temperature reconstructions, however, are hampered by a lack of quantitative temperature proxies and, consequently, are often qualitative rather than quantitative. Recently, my group discovered a new quantitative continental temperature proxy, the MBT index, which is based on the distribution of membrane lipids of soil bacteria. Their composition is a function of annual mean air temperature (MAT). These lipids are transported by rivers to the ocean and deposited in marine sediments. Determination of the MBT index in cores from river fans can, thus, potentially be used to reconstruct continental, river basin-integrated, temperatures from a marine record in front of large river outflows. We will study the mechanisms of transport of the soil bacterial membrane lipids to the ocean in many river systems and compare the down-core changes in their composition with conventional MAT proxies. We will also investigate the potential of lake sediments as archives of continental climate change using our new MBT palaeothermometer and apply this thermometer in the assessment of continental climate change during the transition from a hothouse to an icehouse Earth in the last 100 million years. This project that combines aspects of microbiology, molecular ecology, lipid biogeochemistry and paleoclimatology will bring this novel continental palaeothermometer to maturity.  If we can ground-truth the use of the MBT-proxy, it will open up new windows in palaeoclimatological research and thus contribute to improvement of current climate models.","2498040","2009-01-01","2014-12-31"
"PAHS","The Role of Large Polycyclic Aromatic Hydrocarbon molecules in the Universe","Alexander Godfried Gerardus Maria Tielens","UNIVERSITEIT LEIDEN","Strong emission features dominate the IR spectra of the interstellar medium of the Milky Way, galaxies in the local Universe and out to redshifts of ~3. These features are generally attributed to IR fluorescence of large Polycyclic Aromatic Hydrocarbon (PAH) molecules pumped by UV photons. These species must be abundant, ubiquitous, and an important component of the ISM. However, despite extensive experimental and theoretical efforts, no specific PAH or even classes of PAHs have been unambiguously identified. Hence, we do not really know the intrinsic physical and chemical properties of these species and therefore cannot quantify their role in the Universe. I propose a highly interdisciplinary program combining observational, theoretical, and experimental studies to determine the IR emission characteristics of large PAH molecules, their origin and evolution, and their influence on the Universe around us. The proposed program will analyze interstellar PAH spectral maps obtained with Spitzer and ISO and relate the spatial variations in the spectral diversity to variations in the characteristics of the radiation field and the physical conditions of these regions that will be obtained with Herschel. This observational program will be aided by an innovative laboratory program that will measure the IR characteristics of large, astrophysically relevant PAH molecules in the gas phase and by a modelling program of the emission characteristics of these PAHs in space. In addition, the photochemistry of gaseous PAHs will be studied in the laboratory using novel techniques, allowing us to model the chemical evolution of PAHs and their reaction products in the interstellar medium. In this way, key astronomical questions involving interstellar PAHs can be addressed.","2383980","2010-05-01","2015-04-30"
"PAIRPLASMA","Creating an electron-positron plasma in a laboratory magnetosphere","Thomas Sunn PEDERSEN","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","The visible Universe is predominantly in the plasma state. On Earth, plasmas are less common, but they find many applications in industry and are also studied with the goal of providing an abundant energy source for mankind through fusion energy. The behaviour of plasmas studied thus far, in particular those that are magnetized, is very complex. The complexity manifests itself first and foremost as a host of different wave types, many of which are generically unstable and evolve into turbulence or violent instabilities. This complexity and the instability of these waves stems to a large degree from effects that can be traced back to the difference in mass between the positive and negative species, the ions and the electrons.

In contrast to conventional ion-electron plasmas, electron-positron (pair) plasmas consist of equal-mass charged particles. This symmetry results in unique behaviour of the pair plasmas, a topic that has been intensively studied theoretically and numerically for decades but experimental studies are only just starting. These studies are not only driven by curiosity: Strongly magnetized electron-positron plasmas are believed to exist ubiquitously in pulsar magnetospheres and active galaxies in the Universe, and the entire Universe is believed to have been a matter-antimatter symmetric plasma in its earliest epochs after the Big Bang.

We propose here to create and study the first long-lived and confined pair plasmas on Earth. This is now possible by combining novel techniques in plasma and beam physics. We will develop a levitated dipole confinement device and will fill it with readily available electrons and low-energy positrons from the world-leading steady-state positron source.","2378958","2017-08-01","2022-07-31"
"PALAEO-RA","A Palaeoreanalysis To Understand Decadal Climate Variability","Stefan BRÖNNIMANN","UNIVERSITAET BERN","Climatic variations at decadal scales, such as phases of accelerated warming, weak monsoons, or widespread subtropical drought, have profound effects on society and the economy. Understanding such variations requires insights from the past. However, no data sets of past climate are available to study decadal variability of large-scale climate with state-of-the-art diagnostic methods. Currently available data sets are limited to statistical reconstructions of local or regional surface climate. The PALAEO-RA project will produce the first ever comprehensive, 3-dimensional, physically consistent reconstruction of the global climate system at a monthly scale for the past six centuries. This palaeoreanalysis is based on combining information from early instrumental measurements, historical documents (e.g., capitalizing on large amounts of newly available data from China), and proxies (e.g., tree rings) with a large ensemble of climate model simulations. To achieve this novel combination, a completely new data assimilation system for palaeoclimatological data will be developed. The unique data sets produced in this project will become reference data sets for studying past climatic variations (i) for diagnostic studies of interannual-to-decadal variability, (ii) as a benchmark for model simulations and (iii) for climate impact studies. Using the data produced, the project will analyse episodes of slowed or accelerated global warming, decadal subtropical drought periods, episodes of expanding or contracting tropics, slowed or strengthened monsoons, changes in storm tracks, blocking and associated weather extremes, and links between Arctic and midlatitude climate. The analyses will provide new insights into the processes governing decadal variability of weather and climate.","2499975","2018-10-01","2023-09-30"
"PALEONANOLIFE","Responses of precambrian life to environmental changes","François Michel Raoul Robert","MUSEUM NATIONAL D'HISTOIRE NATURELLE","This multidisciplinary proposal has the objective to enhance our knowledge on the early steps of the evolution of life on Earth by providing a foundation for better deciphering the molecular fossil record as well as the geochemical signals hidden in ancient rocks. Based on the multiscale and multitechnique study of morphologically preserved microorganisms fossilized within ancient siliceous nodules, I propose to chronologically reconcile the evolution of metabolisms of life forms during the Precambrian with the variation of (sea)water paleo-temperatures registered by the silica matrix in which the investigated organic microfossils are embedded.
Spatially-resolved information on fossil organic constituent speciation and their structural relationships with the silica matrix will be obtained at the nanometer scale using a unique combination of spectroscopy and microscopy techniques, notably including STXM and TEM. Crucial information on paleo-metabolisms will be obtained from NanoSIMS experiments by measuring the stable H-C-N-S isotope composition of the investigated fossilized objects at the scale of individual cells. In parallel, laboratory experiments will be conducted to better assess the potential isotopic and molecular evolution of organic molecules during the fossilization process. Estimations of water paleo-temperatures – likely corresponding to oceanic paleo-temperatures – will be achieved based on the distribution of the silicon and oxygen isotopic composition of silica closely associated to the fossil cells, measured at the very high spatial resolution of the NanoSIMS. Furthermore, the study of natural proxies will provide a more profound understanding of the significance of the temperature registered by the isotopic compositions of Precambrian cherts. In addition to radically change scientific ideas about Precambrian Paleontology, the technical and scientific developments resulting from this work will be broadly applicable and serve numerous communities.","1468852","2012-07-01","2017-12-31"
"PALGLAC","Palaeoglaciological advances to understand Earth’s ice sheets by landform analysis","Christopher David CLARK","THE UNIVERSITY OF SHEFFIELD","Ice sheets regulate Earth’s climate by reflecting sunlight away, enabling suitable temperatures for human habitation. Warming is reducing these ice masses and raising sea level.  Glaciologists predict ice loss using computational ice sheet models which interact with climate and oceans, but with caveats that highlight processes are inadequately encapsulated. Weather forecasting made a leap in skill by comparing modelled forecasts with actual outcomes to improve physical realism of their models.  This project sets out an ambitious programme to adopt this data-modelling approach in ice sheet modelling. Given their longer timescales (100-1000s years) we will use geological and geomorphological records of former ice sheets to provide the evidence; the rapidly growing field of palaeoglaciology.

Focussing on the most numerous and spatially-extensive records of palaeo ice sheet activity - glacial landforms - the project aims to revolutionise understanding of past, present and future ice sheets. Our mapping campaign (Work-Package 1), including by machine learning techniques (WP2), should vastly increase the evidence-base. Resolution of how subglacial landforms are generated and how hydrological networks develop (WP3) would be major breakthroughs leading to possible inversions to information on ice thickness or velocity, and with key implications for ice flow models and hydrological effects on ice dynamics. By pioneering techniques and coding for combining ice sheet models with landform data (WP4) we will improve knowledge of the role of palaeo-ice sheets in Earth system change. Trialling of numerical models in these data-rich environments will highlight deficiencies in process-formulations, leading to better models. Applying our coding to combine landforms and geochronology  to  optimise modelling (WP4) of the retreat of the Greenland and Antarctic ice sheets since the last glacial will provide ‘spin up’ glaciological conditions for models that forecast sea level rise.","2425299","2018-10-01","2023-09-30"
"PALP","Physics of Atoms with Attosecond Light Pulses","Anne L'huillier Wahlström","LUNDS UNIVERSITET","""The field of attosecond science is now entering the second decade of its existence, with good prospects for breakthroughs in a number of areas. We want to take the next step in this development: from mastering the generation and control of attosecond pulses to breaking new marks starting with the simplest systems, atoms. The aim of the present application is to advance the emerging new research field “Ultrafast Atomic Physics”, where one- or two-electron wave packets are created by absorption of attosecond pulse(s) and analyzed or controlled by another short pulse. Our project can be divided into three parts:

1. Interferometric measurements using tunable attosecond pulses
How long time does it take for an electron to escape its potential?
We will measure photoemission time delays for several atomic systems, using a tunable attosecond pulse source. This type of measurements will be extended to multiple ionization and excitation processes, using coincidence measurements to disentangle the different channels and infrared ionization for analysis.

2. XUV pump/XUV probe experiments using intense attosecond pulses
How long does it take for an atom to become an ion once a hole has been created?
Using intense attosecond pulses and the possibility to do XUV pump/ XUV probe experiments, we will study the transition between nonsequential double ionization, where the photons are absorbed simultaneously and all electrons emitted at the same time and sequential ionization where electrons are emitted one at a time.

3. """"Complete"""" attosecond experiments using high-repetition rate attosecond pulses
We foresee a paradigm shift in attosecond science with the new high repetition rate systems based on optical parametric chirped pulse amplification which are coming to age. We want to combine coincidence measurement with angular detection, allowing us to characterize (two-particle) electronic wave packets both in time and in momentum and to study their quantum-mechanical properties.""","2047000","2014-03-01","2019-02-28"
"PALs","The calm before the storm: Pre-stellar cores as Astrophysical Laboratories","Paola Caselli","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","""Stars like our Sun and planets like our Earth form in dense regions within interstellar molecular clouds, called pre-stellar cores (PSCs).  PSCs provide the initial conditions in the process of star and planet formation, but large uncertainties exist concerning basic astrophysical processes and parameters, such as surface chemistry, the cosmic-ray ionization rate, the H2 ortho-to-para ratio, the abundance of atomic Oxygen and """"metals"""". In current models, these parameters/processes are typically fixed to some """"canonical"""" values and variations across PSCs are neglected. With the new generation of telescopes and the advances in radiative transfer and dynamical/chemical modelling, the time has now come to develop theoretical models without highly uncertain parameters.

PCSs are dark, cold and quiescent. They are the simplest units in the process of star formation. Thus, they provide a unique opportunity for the study of fundamental astrophysical processes in a """"calm"""" environment, just before the battering of the protostellar """"storm"""".  For this reason, PSCs can be used as ideal laboratories to refine our understanding of how stars and planets form. With this advanced grant fellowship, I plan to connect state of the art dynamical and chemical models and test them against detailed observations of prototypical PCSs to first deliver parameters and processes that are needed to understand basic physical mechanisms.  I will then explore in detail the formation, evolution and physical/chemical structure of PSCs in different environments. Finally, with the help of ALMA data, I will focus on the central few thousands AU and study the first steps toward the formation and early evolution of proto-planetary disks (PPDs).

This is sorely needed to enable us to understand the initial conditions in the process of star and planet formation and to link PSCs with PPDs, currently studied by different communities, with the ultimate aim of understanding our chemical/physical heritage.""","2499291","2013-03-01","2018-02-28"
"PANDORA","Performance Active Nanoscale Devices Obtained by Rational Assembly","Mathias Brust","THE UNIVERSITY OF LIVERPOOL","""The key challenge of this proposal is to create artificial nanostructures that perform active tasks inspired by biology such as transport, chemical synthesis and molecular motion. Instead of bio-molecules, ligand-stabilized metal nanoparticles will be used as the main components that will operate embedded within a biological or non-biological matrix, i.e. a cell compartment, a membrane, an emulsion or a gel. Chemical multi-functionality of the nanoparticles will be achieved through a simple but conceptually new modular approach by which rationally selected different functional moieties are attached to the surface of the particles. While the global vision behind this proposal is to build the foundation for a technological implementation of active nanomaterials that operate far from chemical equilibrium, the research itself is purely fundamental. It seeks to demonstrate that the thermodynamic conditions essential to living organisms can also be applied to artificial nanoscale assemblies and enable these to imitate some functional aspects of life.""","2494730","2013-06-01","2018-05-31"
"PanScales","Spanning TeV to GeV scales for collider discoveries and measurements","Gavin SALAM","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Over the coming years, the Large Hadron Collider (LHC) will search for new physics at ever higher energies, firmly establish many facets of the Standard Model that relate to the Higgs boson, and carry out a broad range of precision measurements. A tool that is central to this endeavour is the parton shower. Parton showers are immensely flexible tools, which simulate the strong-interaction physics that occurs between the TeV energy scales that the LHC was designed to probe, and the GeV mass scale of the protons and other hadrons that the LHC collides and detects. They are used in almost every LHC experimental analysis. Of all the first-principles theoretical methods used at the LHC, the parton shower is the only one that has not seen substantial advances in its underlying precision in the past 20 years. As a result, parton showers are becoming the critical weak link in LHC physics. This project will radically transform the way in which parton showers are conceived, by introducing innovative methods that establish the relation with another field of research called resummation, to which the PI has made ground-breaking contributions.

The main outcome of the project will be a novel parton shower with accuracies up to an order of magnitude higher than in current approaches. This will be essential for reliably exploiting the information that is present across the full range of energy scales at high-energy colliders.","2339381","2018-10-01","2023-09-30"
"PantaRhei","Interdisciplinary Integrated Forecasting System for Fluid Flows","Piotr Krzysztof Smolarkiewicz","EUROPEAN CENTRE FOR MEDIUM-RANGE WEATHER FORECASTS","A high performance modelling system is proposed for simulating multi-scale flows with an unprecedented range of multidisciplinary physical applications. Computer simulations of global weather at horizontal resolutions in the order of a kilometre (i.e. nonhydrostatic) will become operational for numerical weather prediction (NWP) beyond 2020. Existing NWP models operate at hydrostatic scales and are not equipped to resolve convective motions where nonhydrostatic effects dominate, thus impairing the fidelity of forecasts. While NWP strives to extend the skill towards finer scales, nonhydrostatic research models endeavour to extend their realm towards the global domain. The two routes of development must cross, but the approach how to merge the diverse expertise is far from obvious. The proposed work will synthesise the complementary skills of two exceptionally successful modelling systems: ECMWF's Integrated Forecasting System (IFS) and the nonhydrostatic research model EULAG formulated by the principal investigator. The IFS is one of the most comprehensive Earth-system models available in the world, while EULAG offers unprecedented expertise in multidisciplinary computational fluid dynamics (CFD) ranging from simulations of laboratory flows to magneto-hydrodynamics of solar convection. The essence of the proposal is a pioneering numerical approach, where a nonhydrostatic global model is conditioned by global hydrostatic solutions within a single code framework. The key technology are EULAG's numerical procedures expressed in time-dependent generalized curvilinear coordinates, pairing the mathematical apparatus of general relativity with modern CFD. The new model will predict with greater fidelity extreme weather events that are critical to the protection of society while sustaining Europe’s role as the world leader in operational NWP. Moreover, this model will be one of the most advanced computing tools available to the European community for research and education.","999559","2013-03-01","2018-02-28"
"PARADIGM","New Paradigm in the Design of Degradable Polymeric Materials - Macroscopic Performance Translated to all Levels of Order","Ann-Christine Albertsson","KUNGLIGA TEKNISKA HOEGSKOLAN","A new generation of polymeric materials is needed promptly that does not behave like traditional commodity plastics in terms of environmental interaction, degradation pattern, fragmentation tendency, and biological persistency. I herein propose a new paradigm in the design of polymeric materials; the design of polymeric materials through a retro-structural approach where the macroscopic performance is translated to every scale level of structural order so that appropriate molecular recognitions are identified and subsequently synthetically generated in a bottom-up procedure. Inspiration on how to design such materials is best drawn from Nature which is unsurpassed in its ability to combine molecular building blocks into perfectly designed versatile super- and supramolecular structures with well-defined properties, disassembly patterns, and biological functions. A closer look into the structural build-up of biological materials gives important clues on how to design synthetic functional materials with desirable environmental interaction. In addition to advanced synthesis, surface modification and processing, the materials and their degradation behavior will be thoroughly characterized by using traditional characterization techniques in combination with latest spectroscopic and imaging techniques. I have chosen to focus on two areas that stand out as highly prioritized in maintaining or even raising our quality of life; sustainable materials for commodity applications and tissue engineering systems in biomaterials science. This is a bold high risk proposal which if successful will have a ground-breaking influence on how we design polymeric materials.","2500000","2010-03-01","2016-02-29"
"PARIS","PARticle accelerators with Intense lasers for Science (PARIS)","Victor Malka","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Particle and radiation beams are commonly used in our daily life. For example, accelerated electrons are deflected in the cathode tube of televisions or computer screens. X rays are routinely used for non destructive material or body inspection, for example to check human bodies (to visualize tumour cells, dental caries and osseous fractures) or to increase the safety of travellers by inspecting their luggage. Ionizing radiations are efficiently used in radiotherapy to cure cancer by damaging irreversibly the DNA of cells. From the fundamental point of view, the development of ultra short bunches of energetic particles and X ray photons is of crucial importance in biology, chemistry, and solid state physics, where these beams could be used to diagnose the electronic, atomic or molecular dynamics with unprecedented, simultaneous time and space resolution. The interaction of laser beams with matter in the relativistic regime has permitted to demonstrate new approaches for producing energetic particle beams, thanks to the tremendous electric fields that plasmas can support. The incredible progress of laser plasma accelerators has allowed physicists to produce high quality beams of energetic radiation and particles. These beams could lend themselves to applications in many fields, including medicine (radiotherapy, and imaging), radiation biology, chemistry (radiolysis), physics and material science (radiography, electron and photon diffraction), security (material inspection), and of course accelerator science. Stimulated by the advent of compact and powerful lasers, with moderate costs and high repetition rate, this research field has witnessed considerably growth in the past few years, and the promises of laser plasma accelerators are in tremendous progress. The PARIS ERC/AdG proposal aims at developing actively this new field of research which is of major interest for a broad scientific community and which has the potential to provide new societal applications.","2250000","2009-04-01","2014-03-31"
"PATCH","Computational Theory of Haptic Perception","Vincent Hayward","UNIVERSITE PIERRE ET MARIE CURIE - PARIS 6","During mechanical interaction with our environment, we derive a perceptual experience which may be compared to the experience that results from acoustic and optic stimulation. Progress has been made towards the discovery of mechanisms subserving the conscious experience of interacting with mechanical objects. This progress is due in part to the availability of new instruments that can tightly control mechanical stimulation of both the ascending, i.e. sensory, and descending, i.e. motor, pathways. The program describes the design of new mechanical stimulation delivery equipment capable of fine segregation of haptic cues at different length scales and different time scales so that controlled stimuli may be delivered with the ease and accuracy which is today possible when studying vision or audition. The purpose of this equipment is to disentangle and recombine the individual cues used by the brain to recover the attributes of an object, leading to the identification of the computations that must be performed to achieve a perceptual outcome. In vision and audition, much is known of the nature of the peripheral and central computations, but in touch, for lack of proper equipment, little is known. From this knowledge, I aim at developing a theory of haptic perception which rests on the observation that these computations are distributed in the physics of mechanical contact, in the biomechanics of the hand, including the skin, the musculoskeletal organization, innervation, and in central neural processes. This research program is rich in applications ranging from improved diagnosis of pathologies, to rehabilitation devices, to haptic interfaces now part of consumer products and virtual reality systems.","2302000","2010-08-01","2016-07-31"
"PATCHYCOLLOIDS","Patchy colloidal particles: a powerful arsenal for the fabrication of tomorrow new  super-molecules . A theoretical and numerical study of their assembly processes","Francesco Sciortino","UNIVERSITA DEGLI STUDI DI ROMA LA SAPIENZA","An unprecedented development in particle synthesis  is  providing methods to generate high yield  quantities of nano- and micro-particles of  different shapes, compositions, patterns and functionalities and  an unprecedented diverse spectrum of particle  patchiness, significantly extending the naturally available choices.  These methods draw from the diverse fields of chemistry, physics, biology, engineering and materials science, and, in combination, provide a powerful arsenal for the fabrication of new particulate building blocks, the molecules of tomorrow materials, self-assembling into molecular-mimetic and unique structures, fluids, and gels made possible solely by their design.
The new  particles offer the possibility to go beyond the spherical interaction case, to move from the colloidal atom to the colloidal molecule --- providing valence to colloids --- and to further  strength  the analogies between colloids and globular proteins.  The present theoretical and computational project aims at providing
new ideas for  developing   effective methodologies of  bottom-up manufacturing, at providing  the scientific community with the background  necessary to fully control the self-assembly of these new building blocks as well as solutions to relevant condensed-matter physics problems.  The project also aims at developing realistic models of DNA-functionalized nano and micro particles, presently the most promising and versatile building block of  bio-colloid materials.  Understanding the assembly of patchy particles will  offer fine control over the three-dimensional organization of materials, as well as the combination of different materials over several length scales, making it possible to design a spectrum of crystal polymorphs and self-assembled ordered and disordered structures unprecedented in colloid science.","1559160","2009-04-01","2014-03-31"
"PaVeS","Parametrized Verification and Synthesis","Javier ESPARZA","TECHNISCHE UNIVERSITAET MUENCHEN","Parameterized systems consist of an arbitrary number of replicated agents with limited computational power, interacting to achieve common goals. They pervade computer science. Classical examples include families of digital circuits, distributed algorithms for leader election or byzantine agreement, routing algorithms, and multithreaded programs. Modern examples exhibit stochastic interaction between mobile agents, and include robot swarms, molecular computers, and cooperating ant colonies.

A parameterized system is in fact an infinite collection of systems, one for each number of agents. Current verification technology of industrial strength can only check correctness of a few instances of this collection. For example, model checkers can automatically prove a distributed algorithm correct for a small number of processes, but not for any number. While substantial progress has been made on the theory and applications of parameterized verification, in order to achieve large impact the field has to face three ``grand challenges'':

- Develop novel algorithms and tools for p-verification of classical p-systems that bypass the high complexity of current techniques.

-Develop the first algorithms and tools for p-verification of modern stochastic p-systems.

-Develop the first algorithms and tools for synthesis of correct-by-construction p-systems.

Addressing these challenges requires fundamentally new lines of attack. The starting point of PaVeS are two recent breakthroughs in the theory of Petri nets and Vector Addition Systems, one of them achieved by the PI and his co-authors. PaVeS will develop these lines into theory, algorithms, and tools for p-verification and p-synthesis, leading to a new generation of verifiers and synthesizers.","2354000","2018-09-01","2023-08-31"
"PBL-PMES","Atmospheric planetary boundary layers: physics, modelling and role in Earth system","Sergej Zilitinkevich","ILMATIETEEN LAITOS","This project aims to systematically revise the planetary-boundary-layer (PBL) physics accounting for the non-local effects of coherent structures (long-lived large eddies especially pronounced in convective PBLs and internal waves in stable PBLs). It focuses on the key physical problems related to the role of PBLs in the Earth system as the atmosphere-land/ocean/biosphere coupling modules: the resistance and heat/mass transfer laws determining the near-surface turbulent fluxes, the entrainment laws determining the fluxes at the PBL outer boundary, the PBL depth equations, and turbulence closures. In this project the first round of revision will be completed, the advanced concepts/models will be empirically validated and employed to develop new PBL parameterization for use in meteorological modelling and analyses of the climate and Earth systems. The new parameterizations and closures will be implemented in state-of-the-art numerical weather prediction, climate, meso-scale and air-pollution models; evaluated through case studies and statistical analyses of the quality of forecasts/simulations; and applied to a range of environmental problems. By this means the project will contribute to better modelling of extreme weather events, heavy air pollution episodes, and fine features of climate change. The new physical concepts and models will be included in the university course and new textbook on PBL physics. This project summarises and further extends our last-decade works in the PBL physics: discovery and the theory of the new PBL types of essentially non-local nature:  long-lived stable  and  conventionally neutral ; quantification of the basic effects of coherent eddies in the shear-free convective PBLs including the non-local heat-transfer law; physical solution to the turbulence cut off problem in the closure models for stable stratification; and discovery of the stability dependences of the roughness length and displacement height.","2390000","2009-01-01","2013-12-31"
"PCELLS","Synthetic Cellularity via Protocell Design and Chemical Construction","Stephen Mann","UNIVERSITY OF BRISTOL","We propose to undertake an ambitious 5-year multidisciplinary programme that seeks to pioneer and establish a fundamentally new paradigm in protolife research that is based on novel conceptual and experimental advances in the design and construction of rudimentary forms of synthetic cell-like micro-ensembles (protocells). Our approach is positioned at the interface between materials chemistry, soft matter science and synthetic biology, and will address the following aspects of protocell design and construction: (i) functional complexity in protocell phenotypes, (ii) protocell self-structuring and metamorphosis, (iii) multi-compartmentalization and protocell endosymbiosis, and (iv) collective behavior in protocell communities. We will initiate unprecedented increases in the complexity of individual protocells by developing new types of structural architectures with advanced functions including photosynthetic protocells and motile proteinosomes, and develop innovative strategies for the chemical secretion of spatially extended extra-protocellular hydrogel matrices and induction of protocell metamorphosis. We will develop a modular micro-engineering approach to protocell multi-compartmentalization with the aim of generating coordinated enzyme- and gene-activated endosymbiotic interactions, and pioneer the experimental study of collective behaviour in communities of synthetic protocells. Our overall aim is to pioneer a modern approach to synthetic cellularity that advances the chemical and physical basis of protocell structure and function, and spearheads the development of future technologies based on autonomously functioning chemical micro-compartments with applications in bioinspired micro-storage and delivery, micro-reactor technologies, cytomimetic engineering, and the development of integrated constructs for diverse procedures in synthetic biology.","2499238","2017-07-01","2022-06-30"
"PDECP","Partial differential equations of Classical Physics","Demetrios Christodoulou","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","I shall pursue two projects both of which belong to the fields of partial differential equations, geometric analysis and mathematical physics. The first project, ``the shock development problem&quot;, belongs also to the field of fluid dynamics and aims at a full understanding of how, in the real world of 3 spatial dimensions, hydrodynamic shocks evolve, my previous work having analyzed in detail how they form. The second project, ``the formation of electromagnetic shocks in nonlinear media&quot; aims at establishing how electromagnetic shocks form by the focusing of incoming electromagnetic wave pulses in a nonlinear medium. The case of an isotropic nonlinear dielectric will be studied first, to be followed by the case of a general isotropic medium. The methods of geometric analysis introduced in my previous work shall be employed, in particular the ``short pulse method&quot; introduced in my work on the formation of black holes by the focusing of incoming gravitational waves in general relativity. The application of these methods to the problem for a general isotropic medium will require the development of new geometric structures. My three Ph. D. students shall purse the following three projects, belonging also to the fields of partial differential equations, geometric analysis and mathematical physics. The first project is in nonlinear elasticity. It is the study of the equilibrium configurations, in free space, of a crystalline solid in which a continuous distribution of dislocations is present, and aims at analyzing the relationship between the dislocation distribution and the resulting internal stress field. The second is in general relativity and aims at a theoretical understanding of the phenomena discovered by M. Choptuik in his numerical study of the gravitational collapse of a self-gravitating scalar field in spherical symmetry. The third is the study of hydrodynamic shock interactions and focusing in spherical symmetry.","1278000","2010-03-01","2015-02-28"
"PEPS","Exploring the physics of Proto-stars and Extra-solar PlanetS","Gilles Chabrier","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The PEPS project is dedicated to the understanding of low-mass star, brown dwarf and giant planet formation and evolution and to the characterization of their internal structure and observable properties. The aim is to develop a sound theoretical foundation and a new generation of modelling tools, in close interaction with observing and experimental programs. The ultimate goal of the project is to provide a consistent description of the different stages from the very formation process to the long term evolution for these objects, characterizing the initial conditions of star/planet formation and exploring their impact on the subsequent mechanical (mass, radius, internal structure and composition), thermal (surface temperature, luminosity) and spectral properties. A dedicated part of the project will focus on exo-Earth planets and on the identification of bio-signatures in their atmosphere, opening an avenue to exobiology.","2376000","2010-06-01","2016-05-31"
"PERCY","Personal Cryptography","Jan Leonhard Camenisch","IBM RESEARCH GMBH","""The amount of personal data stored in digital form has grown tremendously. All aspects of our lives are concerned. Our data include family pictures, insurance documents, bills and receipts, health records, cryptographic keys, electronic identities, certificates, and passwords. We store and process them on several personal devices as well as in the cloud via services such as Flickr or Facebook. Managing these data is challenging: they have to be updated, backed up, synchronised across devices, and shared. In case of emergency, health records must be accessible to doctors or designated family members. Many of these data are sensitive, but adequately protecting them is virtually impossible for private users with current tools.
Encrypting data makes managing them only harder. It destroys much of the functionality that users have come to expect such as synchronising and sharing; mismanagement of encryption keys might even render data illegible to the owner himself.

Our goal is to develop fundamentally new cryptographic primitives, protocols, and policy languages that let human users deal with cryptographic keys and encrypted personal data. We will invent mechanisms that 1) enable humans to securely store and retrieve cryptographic keys based on a single human-memorisable password, on biometrics, on hardware tokens; 2) enable end users to manage their various cryptographic keys and encrypted data via these keys; and 3) enable users and cloud hosts to perform useful operations on encrypted data without needing to decrypt. Our mechanisms will run on resource-constrained devices, i.e., they will be efficient and yet secure in the sense that they provide security guarantees, especially in the presence of untrusted cloud hosts.

Our basic cryptographic research aims at infusing growth of a research community around protection mechanisms for end-user keys and data and to initiate follow-up collaborative projects to deploy our theoretical results in the real world""","2467700","2013-03-01","2018-02-28"
"PERFUME","Smart Device Communication: A paradigm for  high PERformance FUture Mobile nEtworking","David GESBERT","EURECOM","Advances in theory, integration techniques and standardization have led to huge progress in wireless technologies. Despite successes with past and current (5G) research, new paradigms leading to greater spectral efficiencies and intelligent network organizations will be in great demand to absorb the continuous growth in mobile data. Our ability to respond suitably to this challenge in the next decade will ensure sustained competitiveness in the digital economy.

With few exceptions such as ad-hoc topologies, classical wireless design places the radio device under the tight control of the network. Promising technologies envisioned in 5G such as (i) Coordinated MultiPoint (CoMP) techniques, (ii) Massive MIMO, or (ii) Millimeter-wave (MMW) by-and-large abide by this model. Pure network-centric designs, such as optical cloud-supported ones raise cost and security concerns and do not fit all deployment scenarios. Also they make the network increasingly dependent on a large amount of signaling and device-created measurements. 
Our project envisions a radically new approach to designing the mobile internet, which taps into the device’s new capabilities. Our approach recasts devices as distributed computational nodes solving together multi-agent problems, allowing to maximize the network performance by exploiting local measurement and information exchange capabilities. The success of the project relies on the understanding of new information theory limits for systems with decentralized information, the development of novel device communication methods, and advanced team-based statistical signal processing algorithms.
 The potential gains associated with exploiting the devices’ collective, network friendly, intelligence are huge. The project will demonstrate long-term impact of the new paradigm, in pushing the frontiers of mobile internet performance, as well as short- to mid-term impact through its adaptation to currently known communications scenarios and techniques.","2358769","2015-10-01","2020-09-30"
"PertQCD","Automatization of perturbative QCD at very high orders.","Charalampos ANASTASIOU","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","In recent months, we broke new ground in perturbative Quantum Chromodynamics computing for the first time a physical cross-section of a hadron collider process - Higgs production - at the fourth order in the strong coupling constant expansion. This breakthrough  improved the perturbative precision of a fundamental cross-section by a factor of four,  paving the way for a very precise testing of the Standard Model theory against LHC data.  

The aim of our proposal is to fully automate all calculations which are needed for LHC and future collider physics at similarly high perturbative orders. Our work will improve the precision of theoretical predictions across the spectrum of LHC phenomenology, matching or superseding the accuracy of 
experimental measurements. In turn, we will be able to draw firm conclusions about the validity of theories which aspire to describe nature at TeV energies and search confidently for signals of new physics through precision measurements at the LHC.","2045095","2016-10-01","2021-09-30"
"PESM","Towards the Prototype Probabilistic Earth-System Model for Climate Prediction","Timothy Noel Palmer","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","A programme of research is described which will revolutionise the mathematical formulation of comprehensive Earth-System models, potentially leading to a step-change improvement in the reliability of our predictions of climate change, both globally and regionally. This programme of research is intended to make climate simulations more consistent both with the multi-scale nature of climate, and with related scaling symmetries of the partial differential equations which govern climate. This will be achieved by moving away from the traditional deterministic approach to the closure problem in computational fluid dynamics, and towards a more novel description of physical processes near and below the truncation scale of climate models, using contemporary nonlinear stochastic-dynamic mathematics. A detailed technical account of how this will be achieved in given in the full proposal. Leveraged on the proposer's many contacts in Europe and around the world, the aim of the proposed research is to produce the world's first Probabilistic Earth System Model. The consequences are enormous: a comprehensive climate model with reduced biases against observations, a model which will be capable of producing estimates of uncertainty in its own predictions, and a model which can make use of emerging energy-efficient probabilistic processor hardware. key to practical success as we approach the era of the exascale supercomputer. The development of the prototype Probabilistic Earth-System Model will open a new era of international scientific collaboration on climate model development, and has the potential to influence climate policy, on mitigation, adaptation and on geoengineering, a the highest governmental and intergovernmental levels.","2137014","2012-04-01","2017-07-31"
"PHAROS","Photocatalytic Generation of CarbAnions for Organic Synthesis","Burkhard KOENIG","UNIVERSITAET REGENSBURG","Light is a fascinating reagent for chemistry as it provides energy to drive reactions, but leaves no trace. In visible light photoredox catalysis the initial electron transfer from the excited dye to a substrate yields radical anion or radical intermediates, which dominate the subsequent chemistry. Carbanions, which are the most important nucleophiles in organic chemistry, are typically not available from photocatalysis. The project PHAROS aims to overcome the current limitation of visible light photocatalysis to radical chemistry and extend its use to carbon nucleophiles. To obtain carbanions for organic synthesis using visible light, we propose three specific project tasks:

1) We develop the next generation of visible light photocatalysts extending the current energetic limit of bond activation required for carbanion generation. This task is based on our recently developed consecutive photoinduced electron transfer (conPET) strategy accumulating the energy of more than one photon for synthesis. Now, the reduction power is further increased reaching potentials of alkali metals and allowing sequential two-electron transfers as needed for preparing carbanions.

2) This technology is then used to generate carbanions from neutral starting materials by visible light photoinduced one- or two-electron transfer. The concept allows a light-driven synthetic carbanion chemistry without the stoichiometric use of reducing reagents, such as magnesium, zinc or lithium.

3) Faster and cleaner reactions, longer catalyst lifetimes and selective photocatalytic sequences are achieved by sensitized photocatalysts and pulsed light excitation. This will enhance the overall energy efficiency of photoredox catalysis facilitating practical applications.

The energy of visible light provides the redox energy to generate carbanions for organic synthesis and thereby broadens the synthetic use of the most abundant and sustainable energy source on earth, visible light.","2458200","2017-09-01","2022-08-31"
"PhaseNanoCracker","The Metallurgical Nutcracker: Probing at the Nanoscale the Structure and Properties of Hard Second Phases in Alloys and Composites","Andreas Mortensen","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Our ambition is to probe the strength of “second” phases in multiphase metal alloys and composites, meaning of hard particles added to strengthen a metal, or alternatively of brittle inclusions that weaken it. Such phases are ubiquitous in structural metals; yet not much is known of the microstructural features that govern their strength.
The underlying hypothesis of this project is that defects that limit the strength of such hard second phases can be identified and then altered by processing. Motivations for this enquiry stem from our previous research on metal composites, coupled with the fact that modern methods of nanoscale mechanical characterization now make such a quest feasible.
Operationally, we plan to apply and extend nanomechanical testing to probe the strength of micrometric, irregularly shaped, hard particles currently used to strengthen metals. We aim to test such particles whole, and also for their local internal properties. Testing will rely on focused ion beam machining and adapted mechanical nanoprobing. These techniques will be combined to probe, using nanoindentation and original testing procedures, local and global strength values for hard second phase particles. Materials systems to be investigated are: (i) ceramic particles for the reinforcement of metal composites; (ii) silicon in aluminium, (iii) cementite and MC carbides in steel. Defects limiting the strength of these hard brittle phases suggested by nanoscopic mechanical testing will be identified using in-depth microstructural characterization, by electron microscopy notably, of both virgin and tested particles. The data will be supplemented by mechanical testing of macroscopic samples containing the hard particles in question. Processing routes will be explored, towards identification of strategies by which the strength of such second phases can be improved to improve, in turn, the performance of several important engineering materials.","2496000","2012-05-01","2017-04-30"
"PHOSFUN","Phosphorene functionalization: a new platform for advanced multifunctional materials","Maurizio Peruzzini","CONSIGLIO NAZIONALE DELLE RICERCHE","2D materials have attracted a great deal of interest due to their variety of applications. Since its discovery in 2004, graphene has monopolized the attention given the unparalleled combination of outperforming structural and functional properties which pave the way for a plethora of different applications. Though its applicability in micro- and nanoelectronic has been later demonstrated to be strongly limited due to its inherent lack of a band gap. This limitation could be overcome using phosphorene, a recently discovered 2D sheet formed by phosphorus atoms prepared by exfoliation of black phosphorus and endowed with a natural band gap. Up to now, only theoretical and basic research has been carried out without the demonstration of reliable and reproducible implementation into real electronic devices.
The PHOSFUN proposal focuses the unexplored chemical reactivity of phosphorene and gathers together chemists mastering the chemistry of phosphorus with physicists expert in advanced nanostructured systems. First, we aim to set-up a scalable and reproducible synthesis of mono and multilayer phosphorene. Then the functionalization of phosphorene with organic and inorganic moieties will be carried out. Doping the phosphorene will provide new functionalized heterostructured 2D sheets. The functionality of the innovative advanced 2D materials will be validated by implanting phosphorene derivatives into different device platforms addressed to applications in material science, catalysis, microelectronics and optoelectronic devices. The final aim of the project is to demonstrate the feasibility of a chain-of-value based on phosphorene platform from synthesis to device realization and implementation. Our results will give an overview on how the chemical and physical properties of phosphorene may be modulated. This will expand enormously the fundamental knowledge on phosphorus-based materials and will open the way to novel applications in different areas.","1995554","2015-07-01","2019-06-30"
"PHOSPhOR","Photonics of Spin–Orbit Optical Phenomena","Lorenzo MARRUCCI","UNIVERSITA DEGLI STUDI DI NAPOLI FEDERICO II","Spin-orbit optical phenomena can be broadly defined as those phenomena in which the polarization (“spin”) and the spatial structure (“orbit”) of an optical wave interact with each other and become spatially and/or temporally correlated, leading to novel effects or photonic applications.
The project vision is a full-fledged spin-orbit photonic science and technology, and its achievement will be pursued by moving in three main directions:
1) We will develop innovative systems based on spin-orbit optical media for generating light fields exhibiting a complex spatial vector structure, both in two dimensions (transverse plane and transverse fields) and in three (i.e. involving time- and space-dependent polarization fields and longitudinal field components). We will extend these ideas to other spectral domains (terahertz waves) and explore the possible applications of these fields in areas such as optical manipulation, plasmonics, space-division multiplexing in optical fibers, time-domain terahertz spectroscopy, ultrafast optics.
2) We will exploit spin-orbit quantum correlations generated within single photons and/or among few correlated photons to demonstrate novel quantum-information protocols using both the polarization and the transverse modes to encode and manipulate multiple qubits in each photon and for the implementation of quantum simulations of material systems based on photonic quantum walks in the Hilbert space of the light transverse modes.
3) We will investigate novel or unexplained physical processes occurring in structured optical media and light-sensitive material systems which respond both to the optical polarization and to its spatial inhomogeneity. Such materials will then be used to manipulate and characterize spin-orbit vector states of light.","1680833","2016-06-01","2021-05-31"
"PHOTBOTS","Nano Photonics-Based Micro Robotics","Diederik Sybolt Wiersma","LABORATORIO EUROPEO DI SPETTROSCOPIE NON LINEARI","The general goal is to bring together different fields of research in order to create a new research area of photonic micro robotics. That is to create, study, and implement truly microscopic structures with nano scale accuracy that can perform robotic tasks and that are entirely powered and controlled by light. This idea brings immense challenges both from the point of view of the physics involved as well as the chemistry needed to create the appropriate materials, but if successful can also have a huge impact.
To achieve this, we will combine our expertise on complex photonic materials and direct laser writing, to create micro structured patterns in liquid crystal elastomers, which are rubber-like polymers with liquid crystalline properties that can be triggered with light. In our view, this opens up a new strategy to create robots of various kinds, on a truly micrometer length scale. That is, micro robots that can swim, walk, or crawl, and when at destination perform specific tasks, controlled and driven by light.
This proposal, in the first instance, deals with fundamental, curiosity-driven research and wishes to address the wealth of physics and chemistry that arises when combining nano photonics with micro robotics. Having said that, the range of potential applications is very broad. Our photonic micro robots would be able to penetrate otherwise difficult to access environments and perform tasks such as sensing or sampling. They could be made in large quantities which means they could also be put into action collectively in swarms (using mechanical and/or optical interaction between the individual robots).
The project is truly interdisciplinary, which makes it very challenging but also exciting. The photonic micro robotic structures will be created by bringing together concepts from physics and chemistry, while the inspiration for designs comes partly from biology and potential applications can be foreseen in medicine.","2200000","2012-01-01","2017-12-31"
"PHOTMAT","Photonically fused molecular materials","William BARNES","THE UNIVERSITY OF EXETER","Molecular materials are ubiquitous, encompassing smart phone displays, plastic electronics and the molecular machinery of photosynthesis. Many of these remarkable uses depend on interactions between the molecules. Until now these interactions have been electric in character, and have been dictated by how electric charge is distributed over the molecules. PHOTMAT will transform the world of molecular materials by adding a new ingredient – photons. I will fuse photons and molecules together to create new hybrid states – part molecule and part photon – that are dramatically different from those of the constituent molecules and photons. The idea of coupling molecules with photons is a radical new approach with implications that reach across physics, quantum information, chemistry, materials science, nanotechnology and biology.

I propose a pioneering research programme that will catalyse the transition from embryonic early results to the creation of a new conceptual framework to unveil a new frontier in nanoscience and nanotechnology. We will perform new experiments that will provide clear proof-of-principle demonstrations of the incredible opportunities opened up by coupling molecules with photons. As examples, we will show how the range over which energy (excitons) can be transport may be extended by a factor of 1000, and we will show how the process of photosynthesis can be modified and controlled. This research has enormous potential, from transforming artificial photosynthesis for clean fuel production to inspiring a new generation of molecular metamaterials. 

My goal is to explore the rich array of possibilities that arise when photons are made an integral part of molecular materials. At present much of the underlying physics is unclear and controversial. I will resolve the important open questions and show how photonic coupling of molecules leads to new molecular materials, new ways to control chemical and biological processes, and a new type of nanophotonics.","2447699","2017-09-01","2022-08-31"
"PHOTOMETA","Photonic Metamaterials: From Basic Research to Applications","Costas Soukoulis","FOUNDATION FOR RESEARCH AND TECHNOLOGY HELLAS","Novel artificial materials (photonic crystals (PCs), negative index materials (NIMs), and plasmonics) enable the realization of innovative EM properties unattainable in naturally existing materials. These materials, called metamaterials (MMs), have been in the foreground of scientific interest in the last ten years. However, many serious obstacles must be overcome before the impressive possibilities of MMs, especially in the optical regime, become real applications.
The present project combines NIMs, PCs, and aspects of plasmonics in a unified way in order to promote the development of functional MMs, and mainly functional optical MMs (OMMs). It identifies the main obstacles, proposes specific approaches to deal with them, and intends to study unexplored capabilities of OMMs. The project objectives are: (a) Design and realization of 3d OMMs, and achieve new metasurface designs applying Babinet’s principle. (b) Understanding and reducing the losses in OMM by incorporating gain and EM induced transparency (EIT). (c) Achieving highly efficient PC nanolasers and surface plasmons (SPs) lasers. (d) Use chiral MMs and SPs to reduce and manipulate Casimir forces, and (e) Using MMs, combined with nonlinear materials, for THz generation, and tunable response.(f)Calculate electron- phonon scattering and edge collisions in graphene and in graphene-based molecules. The unifying link in all these objectives is the endowment of photons with novel properties through imaginative use of EM-field / artificial-matter interactions. Some of these objectives seem almost certainly realizable; others are more risky but with higher reward if accomplished; some are directed towards new specific applications, while others explore new physical reality.
The accomplishment of those objectives requires novel ideas, advanced computational techniques, nanofabrication approaches, and testing. The broad expertise of the PI and his team, and their pioneering contributions to NIMs, PCs, and plasmonics qualifies them for facing the challenges and ensuring the maximum possible success of the project.","2100000","2013-03-01","2019-02-28"
"Photonis","Isotope Fractionation of Light Elements Upon Ionization: Cosmochemical and Geochemical Implications","Bernard MARTY","UNIVERSITE DE LORRAINE","Light elements such as hydrogen and nitrogen present large isotope variations among solar system objects and reservoirs (including planetary atmospheres) that remain unexplained at present. Works based on theoretical approaches are model-dependent and do not reach a consensus. Laboratory experiments are required in order to develop the underlying physical mechanisms. The aim of the project is to investigate the origins of and processes responsible for isotope variations of the light elements and noble gases in the Solar System through an experimental approach involving ionization of gaseous species. We will also investigate mechanisms and processes of isotope fractionation of atmophile elements in planetary atmospheres that have been irradiated by solar UV photons, with particular reference to Mars and the early Earth. Three pathways will be considered: (i) plasma ionisation of gas mixtures (H2-CO-N2-noble gases) in a custom-built reactor; (ii) photo-ionisation and photo-dissociation of the relevant gas species and mixtures using synchrotron light; and (iii) UV irradiation of ices containing the species of interest. The results of this study will shed light on the early Solar System evolution and on processes of planetary formation.","2810229","2017-01-01","2021-12-31"
"PHOTPROT","The Dynamic Protein Matrix in Photosynthesis: From Disorder to Life","Rienk Van Grondelle","STICHTING VU","In photosynthesis solar light is harvested by an antenna, the energy is transferred to the photosynthetic reaction center where a charge separation occurs. These processes occur on an ultrafast timescale and result in a stable product. The photosynthetic apparatus consists of a complex set of pigment-proteins that perform these delicate processes with a quantum efficiency close to 1. Proteins are intrinsically disordered and display dynamics over a fast range oof times, from femtoseconds to seconds. In this proposal I wish to explore how this dynamic protein matrix facilitates or maybe even drives the primary events of photosynthesis. Together with my co-investigator Bruno Robert I plan to investigate four aspects of how the protein matrix may affect this important biological process. In the first project we will study if the charge separation in Photosystem 2 occurs along multiple pathways, depending on the realization of the disorder. Project 2 concerns the possible role of quantum coherence in charge separation in Photosystem 2. In project 3 we will investigate how the dynamic protein matrix maybe even actively stabilizes the early charge separation. Project 4 aims to find out how functional transitions in photosynthetic proteins are coupled to conformational changes. The latter relates to the idea that the peripheral light-harvesting complex of plants, LHCII, plays a role in photoprotection by switching between a light-harvesting state and a quenching state. The project combines a number of state-of-the-art biophysical approaches and furthermore aims to develop new techniques: 2-dimensional electronic spectroscopy and plasmon wave resonance spectroscopy. The final result will deliver a unique view on how the physics of protein matter manifests itself in biology.","2864400","2011-02-01","2017-01-31"
"PHYSAPS","The Physics of Active Particle Suspensions","Wilson Che Kei Poon","THE UNIVERSITY OF EDINBURGH","‘Active matter’ is matter that is intrinsically out of equilibrium. In particular, an ‘active suspension’ is made up of self-propelled particles or droplets dispersed in a liquid. Active matter is not in thermal equilibrium even in the absence of external driving, and display fascinating properties. Thus, e.g., a so- lution of the filament-forming protein actin and the ‘molecular motor’ protein myosin can ‘burn’ ATP as fuel to produce a gel that flows in the absence of any external pressure gradient; while a suspension of swimming bacteria can have a viscosity that is lower than that of the suspending liquid. There is yet no gener- ally accepted statistical mechanics of active matter, where the absence of detailed balance means that small differences in microscopic dynamics can in principle lead to very different macroscopic behaviour. Moreover, there is no a priori reason to believe that a reduced description in terms of just a few macroscopic parameters (such as effective temperature and density) is possible. I propose a systematic pro- gramme of experiments to discover when and how microscopic dynamics affect the macroscopic behaviour of active suspensions, whether any of their behaviour has analogues in suspensions of passive particles and droplets, and how activity can be described using coarse-grained variables. To ensure that the experiments can be tightly coupled to theory and simulations, I will use well-characterised, model systems of active particles. Developing model systems is therefore a subsidiary, but crucial, goal of my programme. Some of these systems will be designed to be as similar as possible in their passive properties, but quite distinct in terms of their microscopic dynamics – a ‘luxury’ that is typically only available to theo- rists and simulators. Experimenting with such model systems should reveal what phenomena are generic to activity, and what phenomena are specific to particular kinds of microscopic dynamics.","2491601","2014-02-01","2019-01-31"
"PHYSBOIL","Physics of liquid-vapor phase transition","Detlef Lohse","UNIVERSITEIT TWENTE","""Liquid-vapor phase transitions and boiling are omnipresent in science and technology, but, as far as basic understanding of the hydrodynamics, these phenomena remain """"terra incognita''. The objective of the proposed work is to achieve a fundamental understanding of the fluid dynamics and heat transfer of the liquid-vapor phase transition - in particular of boiling - both on a micro- and on a macro-scale, through experiments under well-defined and controlled conditions, accompanied by theoretical and numerical modeling. Up to now """"boiling'' has been nearly exclusively an engineering subject. We want to change this and make it a physics subject as we are convinced that boiling involves very interesting and practically relevant physics still in need of understanding.

On the micro-scale the planned experiments include nucleation studies of individual and interacting vapor bubbles on superheated, geometrically and chemically micro- and nano-structured surfaces. In the bulk of the flow, nucleation will be achieved through laser heating, through local pressure gradients, and through acoustically triggered vaporization of metastable perfluorcarbon nanodroplets in a superheated liquid. The vapor bubbles will be monitored with ultra-high-speed digital imaging, micro particle velocimetry, infrared thermography, and heat flux measurements. On the theoretical side we will use molecular dynamics simulations and the level-set method.

On the macro-scale the focus is on closed boiling turbulent flows, namely Rayleigh-Benard and Taylor-Couette flow. We will measure how the vapor bubble formation affects global quantities such as the heat flux and the angular momentum flux and thus the drag, and local flow properties such as the vapor bubble concentration. The numerical simulations, with one generic code for both geometries, will be based on discrete particle models.""","2108000","2011-03-01","2016-02-29"
"PhysSF","Physics of Star Formation and Its Regulation","Eva SCHINNERER","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","In the past decade we learned when and where stellar mass was built up in galaxies through cosmic time, now we must understand the physical causes in order to answer `How do galaxies form and evolve?’. This ERC project is designed to greatly advance our understanding of the physics of the star formation (SF) process and its regulation in typical star forming galaxies. The ERC project consists of 2 complementary parts: (A) an unparalleled characterization of the SF process in nearby galaxies through full exploitation of the revolutionary capabilities of the latest millimeter interferometers (ALMA) and optical integral field units (MUSE). This study will constrain the key physical parameters for the SF process on only 50pc scales - the scale of large HII regions and their predecessors, giant molecular clouds. At this crucial scale, the MUSE-ALMA-HST Survey will provide a characterization of the SF history, stellar/gaseous surface densities, metallicities of stars and gas, the stellar radiation field, extinction, and stellar/gas kinematics, and thus uncover the physical conditions that control and regulate the SF process. Part (B) will place the results of part (A) in a cosmological context, by characterizing key galaxy quantities (e.g., gas mass fraction, specific SF rates, gas depletion times) in fully representative galaxy samples after (z<3) and before (z>3) the peak epoch of cosmic star formation density. In addition to providing the critically needed constraints on the conditions that govern the SF process, this ERC project will provide the observational benchmark for state-of-the art galaxy simulations and models. The PI is internationally recognized as a leader in SF studies in nearby and distant galaxies, and has successfully led large international collaborations that strongly shaped our current understanding of the SF process. Through her track record and access to the required data, the PI is uniquely positioned to successful lead this ambitious program.","2495000","2016-10-01","2021-09-30"
"PICOPROP","Photo Induced Collective Properties of Hybrid Halide Perovskites","Laszlo Forro","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The recent discovery of the organo-inorganic perovskite CH3NH3PbI3 as very efficient material in photoelectric conversion is multifaceted: it turns out that this compound is promising not only in photovoltaics, but it is lasing, it gives bright light emitting diodes, promising in water splitting and we are persuaded that it can play an important role in basic sciences, as well.
We have recently realized that under white light illumination the photoelectrons, due to their very long recombination time, stay in the conduction band and the resistivity of a single crystal shows a metallic behavior. If the lifetime is sufficiently long and the density of these excited carrier is high enough they could condense into a Fermi sea. The project’s goal is to realize this highly unusual state and to document its properties by magneto-transport and spectroscopic techniques. We will check in our model compound the long-sought superconductivity of photo-excited carriers, extensively searched for in cuprates, if we could stabilize it by fine tuning the interactions by hydrostatic pressure under constant illumination. 
The availability of high quality samples is primordial for this program. It turns out that CH3NH3PbI3 is ideal compound, it seems to be almost free of charged defects (its room temperature resistance is 5 orders of magnitude higher than that of Phosphorus doped Silicon at 1013 cm-3 doping concentration) and we can grow excellent single crystals of it. Furthermore, it has a flexibility in material design: one can vary all the constituents, and even the dimensionality by making layered materials with the main chemical motifs. A special effort will be devoted to tune the spin-orbit coupling by different elements, since this could be at the origin of the long recombination time of the photo-electrons. 
We suspect that the highly tunable, clean and disorder-free doping obtained by shining light on these ionic crystals opens a new era in material discovery.","2495712","2015-09-01","2020-08-31"
"PISA","Polymerisation-induced self-assembly","Steven Peter Armes","THE UNIVERSITY OF SHEFFIELD","The efficient, reproducible synthesis of bespoke organic nanoparticles of controlled size, morphology and surface functionality in concentrated solution is widely regarded to be a formidable technical challenge. However, recent advances by the Principal Investigator (PI) suggest that this important problem can be addressed by polymerisation-induced self-assembly (PISA) directly in aqueous solution to form a range of diblock copolymer 'nano-objects'. The proposal combines three synergistic themes within the PI's group: (i) controlled-structure water-soluble polymers, (ii) living radical polymerisation and (iii) novel polymer colloids. More specifically, the PI will work closely with four post-doctoral scientists and a PhD student to design a series of diblock copolymer nanoparticles with either spherical, worm-like or vesicular morphologies under dispersion polymerisation conditions in either water, alcohol or n-alkanes. This exciting and timely fundamental research programme will produce world-leading scientific innovation. Moreover, the targeted nanoparticles will be evaluated for various potential applications, such as (i) intracellular delivery of various biomolecules (e.g. DNA, proteins, antibodies), (ii) readily sterilisable biocompatible hydrogels, (iii) bespoke Pickering emulsiifiers and foam stabilisers, (iv) tough nanocomposite monoliths, (v) new components for next-generation paints, (vi) novel boundary lubricants for high performance engine oils. Informal collaborations with four academic partners and four industrial companies will ensure that maximum scientific value and economic impact is extracted from this ambitious work programme. All research findings will be published in top-quality scientific journals and the PI will provide appropriate mentoring to inspire his research team to become the next generation of creative, productive scientists for the EC.","2480300","2013-02-01","2018-07-31"
"PLANETDIVE","Planetary diversity: the experimental terapascal perspective","Guillaume, Marie, Bernard Fiquet","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The discovery of extra-solar planets orbiting other stars has been one of the major breakthroughs in astronomy of the past decades. Exoplanets are common objects in the universe and planetary systems seem to be more diverse than originally predicted. The use of radius-mass relationships has been generalized as a means for understanding exoplanets compositions, in combination with equations of state of main planetary components extrapolated to TeraPascal (TPa) pressures. 

In the most current description, Earth-like planets are assumed to be fully differentiated and made of a metallic core surrounded by a silicate mantle, and possibly volatile elements at their surfaces in supercritical, liquid or gaseous states. This model is currently used to infer mass-radius relationship for planets up to 100 Earth masses but rests on poorly known equations of states for iron alloys and silicates, as well as even less known melting properties at TPa pressures.

This proposal thus aims at providing experimental references for equations of state and melting properties up to TPa pressure range, with the combined use of well-calibrated static experiments (laser-heated diamond-anvil cells) and laser-compression experiments capable of developing several Mbar pressures at high temperature, coupled with synchrotron or XFEL X-ray sources. I propose to establish benchmarking values for the equations of states, phase diagrams and melting curves relations at unprecedented P-T conditions. The proposed experiments will be focused on simple silicates, oxides and carbides (SiO2, MgSiO3, MgO, SiC), iron alloys (Fe-S, Fe-Si, Fe-O, Fe-C) and more complex metals (Fe,Si,O,S) and silicates (Mg,Fe)SiO3. In this proposal, I will address key questions concerning planets with 1-5 Earth masses as well as fundamental questions about the existence of heavy rocky cores in giant planets.","3498938","2016-01-01","2020-12-31"
"PLAQNAP","Plasmon-based Functional and Quantum Nanophotonics","Sergey Bozhevolnyi","SYDDANSK UNIVERSITET","""Plasmon-based nanophotnics, an explosively growing research field concerned with surface-plasmon waveguides and circuitry, is oriented towards exploiting unique perspectives opened for radiation guiding along metal surfaces: extreme mode confinement (i.e., far beyond the diffraction limit) and seamless interfacing of electronic and photonic circuits (that both utilize the same metal circuitry). At the same time, unavoidable radiation absorption by metals results in the fundamental trade-off between the mode confinement and propagation loss, so that the problem of making the most of the above unique features becomes of paramount importance. The proposal encompasses two ground-breaking research directions in plasmonics that explore and utilize extremely confined plasmon-waveguide modes for functional and quantum nanophotonics. These directions of in-depth investigations concentrate within two interrelated and largely unexplored research areas within plasmonics: development of ultra-compact plasmonic configurations exhibiting unique functionalities and realization of strong coupling between extremely confined plasmonic modes and individual quantum emitters. Fundamental studies of ultimate mode confinement and coupling to quantum emitters would evolve into investigations carried out within forefront topics including (i) dynamic control of plasmon-waveguide modes using the same metal circuitry for both radiation guiding and its control with electrical signals; (ii) moulding the radiation flow by gradually varying waveguide cross sections in order to realize efficient nanofocusing of radiation, miniature ultra-dispersive wavelength-selective components and table-top models of plasmonic black holes, and (iii) quantum plasmonics with individual quantum emitters being strongly coupled to deep subwavelength surface plasmon modes, targeting the realization of a saturable waveguide mirror, single-photon transistor and long-distance entanglement of two remote quantum emitters.""","2278636","2014-02-01","2019-01-31"
"PLASILOR","Plasmonic-Silicon-Organic Hybrid – a Universal Platform for THz Communications","Juerg Leuthold","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","The PLASILOR project aims at combining the best of three worlds by bringing silicon, organic and plasmonic technologies onto one common platform. Within PLASILOR, we will develop novel devices that outperform the current state-of-the-art in terms of functionality, speed and size thanks to unique characteristics only offered by organics and plasmonics. 
    The focus of the device activities will be on novel transmitters and receivers and their subcomponents. Key to the project will be high-speed plasmonic-organic hybrid modulators with 200 GHz bandwidth and novel plasmonic detectors with a similar bandwidth. The project will also pursue the development of novel waveguide and coupler concepts for optical, THz and plasmonic modes but also work actively towards the development of new organic sources and make them accessible to the communications community by benefiting from recent developments in the field of organic lighting.
    Finally, the potential of the new platform will be put at test by demonstrating a 240 GHz beam-steering link on a chip - an undertaking that would be difficult – if not impossible to realize by other means. This radio-over fiber link will be based on a cointegration of both high-speed photonics and RF elements such as antennas. The demonstrator will benefit from the unique large scale integration capabilities offered by silicon CMOS, the strong linear-electro-optic effect of tailored organic compounds and the ultra-fast and compact size offered by plasmonics. 
    The project is disruptive and challenging but it builds on the device and system expertise of the applicant. For instance, Juerg Leuthold and his group have only recently demonstrated the first ultra-compact high speed plasmonic modulator and they introduced the first wireless 100 Gbit/s link. 
    The new platform will not only be a solution for THz communications – but also for the wider field of THz applications.","2487475","2015-11-01","2020-10-31"
"PLASMAQUO","Development of plasmonic quorum sensors for understanding bacterial-eukaryotic cell relations","Luis Manuel Liz Marzán","ASOCIACION CENTRO DE INVESTIGACION COOPERATIVA EN BIOMATERIALES- CIC biomaGUNE","This proposal aims at the development of novel nanostructured materials based on crystalline assemblies of anisotropic plasmonic (gold/silver) nanoparticles, to be used for the surface enhanced Raman scattering (SERS) detection of quorum sensing (QS) signaling molecules, and to the demonstration of applications of such materials to monitor population kinetics in bacterial colonies and the determination of the interaction mechanisms between mixed colonies and their manipulation through external parameters. This will involve a first stage related to the careful design of the most appropriate nanoparticle morphology and composition, as well as an understanding of their specific assembly processes (both on substrates and in solution), so that the collective plasmonic response will be optimized towards the enhancement of the Raman signal of the probe molecular codes. Coating of the nanoparticle supercrystals with a mesoporous layer will be required to protect them against contact with bacteria and cells, while permitting contact with the QS signaling molecules. Ultimately, when the sensing system has been optimized and its performance demonstrated for monitoring of QS signals and colony growth, two final and important goals will be pursued. First, the interaction between mixed colonies (bacteria-bacteria and bacteria-eukaryotic cell) will be monitored in order to get information about synergic or antagonist (toxicity) QS mechanisms during the growth and proliferation of different bacteria and interspecies. This goal will permit the design of in vitro experiments where a bacterial strain may be manipulated by means of external introduction of the appropriate QS signaling molecules. Finally, the major challenge will be the practical demonstration of the ability of these new materials in this particular configuration for understanding and manipulating the growth and communication of different types of prokaryotic and peukaryotic cells.","2247630","2011-03-01","2017-02-28"
"PLASMETA","Plasmonic Metamaterials","Albert Polman","STICHTING NEDERLANDSE WETENSCHAPPELIJK ONDERZOEK INSTITUTEN","IIn this program I will demonstrate control of light at length scales well below the free-space wavelength, leading to entirely new fundamental phenomena and important applications. The research program is built on specially engineered metamaterials composed of metal nanostructures that support surface plasmons that are embedded in a dielectric. The program is composed of three strongly related topics:
1) I will experimentally demonstrate an entirely new class of optical metamaterials that posses a refractive index that can be tuned over a very large range: -10 < n < +10. Based on coupled plasmonic waveguides, these materials will, for the first time, show true left-handed behaviour of light (n < 0) in the UV/blue spectral range. I will demonstrate negative refraction of light and use these materials to demonstrate the “perfect lens” which enables sub-wavelength imaging of (biological) nanostructures.
2) I will use plasmonic metamaterials to engineer the flow of light in thin-film solar cells. By controlling the scattering and trapping of light using plasmonic nanostructures integrated with semiconductor waveguide slabs I will demonstrate ultra-thin solar cells with efficient collection and conversion of infrared light, aiming at beating the ergodic light trapping limit.
3) I will demonstrate strong coupling between light and mechanical motion in the smallest possible volume. Light trapped in plasmonic metamaterials exerts a force that can lead to a shift in the plasmonic resonance frequency which in turn provides feedback on the mechanical motion. We will use this nanoscale coupling mechanism to actively cool and heat mechanical motion in plasmonic nanostructures and use this phenomenon in a new type of plasmon-based quartz oscillator.","2286000","2011-07-01","2016-06-30"
"PLASMONANOQUANTA","""Frontiers in Plasmonics: Transformation Optics, Quantum and Non-linear phenomena""","Francisco José Garcia Vidal","UNIVERSIDAD AUTONOMA DE MADRID","""The overall objective of this proposal is to work in depth along three ground-breaking lines of research that are at the cutting edge of the current research in Plasmonics. These three subjects have strong overlap and are:

1) Non-linear phenomena and Plasmonic lasing: the introduction of optical-gain media into plasmonic waveguides has proven to be a feasible way to overcome the inherent losses within the metal. In order to reveal the physics behind this phenomenon, we intend to develop a new ab-initio theoretical framework that should combine the resolution of classical Maxwell’s equations with a quantum-mechanical treatment of the molecules forming the optical-gain medium. Within this formalism we also aim to analyze in depth very recent proposals of plasmon-based nano-lasers, the design of active devices based on surface plasmons and the use of optical-gain media in metallic metamaterials.

2) Transformation Optics for Plasmonics:  we plan to apply the idea of Transformation Optics in connection with the concept of Metamaterials to devise new strategies for molding the propagation of surface plasmons in nanostructured metal surfaces. Additionally, we will use the Transformation Optics formalism to treat quasi-analytically non-local effects in plasmonic structures.

3) Quantum Plasmonics: several aspects of this new line of research will be tackled. Among others, fundamental studies of the coherence of surface plasmons that propagate along different metal waveguides after being generated by quantum emitters. A very promising line of research to explore will be plasmon-mediated interaction between qubits, taking advantage of the quasi-one-dimensional character of plasmonic waveguides. Strong-coupling phenomena between molecules and surface plasmons and the design of practical scenarios in which entanglement of surface plasmons could take place will be also addressed. We also plan to study how to generate surface plasmons with orbital angular momentum.""","1347600","2012-04-01","2017-03-31"
"PLASMONICS","Frontiers in Surface Plasmon Photonics - Fundamentals and Applications","Thomas Ebbesen","CENTRE INTERNATIONAL DE RECHERCHE AUX FRONTIERES DE LA CHIMIE FONDATION","Surface plasmons have generated considerable renewed interest through a combination of scientific and technological advances. In particular with the progress nanofabrication techniques, the properties of surface plasmons (SP) can now be controlled by structuring metals at the nanometer scale. The overall objective of this proposal is to manipulate and control the properties of the SPs to analyze fundamental phenomena through which new capacities can emerge. The project is divided in four parts with strong overlap:   1) SP enhanced devices: We plan to use the benefits provided by SPs to enhance devices or create new device architectures. Textured metal surfaces, and the associated SP modes, can be used as antennas to extract, capture and control light in a variety of applications that include imaging and polarization sensing, nano-optical elements and detectors.   2) SP circuitry:  To achieve complete miniature SP photonic circuits, a number of components to launch SP, control their propagation and finally decouple SP back to light are necessary.  Much progress has been made in this direction but many challenges remain at the level of individual components and complete circuits that will be explored. 3) Molecule   SP interactions: Molecule - SP strongly coupled interactions are expected to modify extensively photophysical and photochemical processes that will be studied by time resolved techniques. This issue also has implications for generating all optical control needed in SP circuitry.  4) Casimir effect and SPs: The tailoring of  the Casimir force by enhancing the contribution of SP modes has been proposed by theoretical studies. Experiments will be undertaken to test the relationship between Casimir physics and plasmonics using nanostructured metal surfaces which could have significant consequences for nano-electro-mechanical systems.  For each of these subjects, the objectives are at the cutting edge of the surface plasmon science and technology.","2200000","2009-01-01","2014-12-31"
"PMELT","New Frontiers in Protein-based Nanomaterials","Stephen Mann","UNIVERSITY OF BRISTOL","We propose to undertake an ambitious 5 year interdisciplinary programme that introduces a fundamentally new paradigm in protein-based nanomaterials research. The new approach involves two main project themes based respectively on fundamental studies on the structure, function and properties of molten protein polymer surfactant nanoconstructs, and the development of these novel nanomaterials as smart fluids, biotechnological devices and health care products. This proposal represents a new and adventurous area of work for the PI, and will allow unprecedented access to a novel class of nanomaterials with controllable architectures, unique physical properties and inherent biological functionality. In so doing, the work will open up promising new avenues of bionanomaterials research and offer significant advantages over current methods for producing protein-based nanomaterials at extremely high concentration and dosage. In general we expect the research programme to pioneer new frontiers in fundamental research and generate significant economic and societal impact as nanomaterials become increasingly integrated into medical and technological products, and new commercial markets based on nanoscience are discovered.","2168862","2011-03-01","2016-02-29"
"POLARITRONICS","Manipulation of trapped quantum polariton fluids","Benoît Deveaud","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Despite their incredibly short lifetime, around a few picoseconds only, it has now been amply demonstrated that polaritons may undergo Bose Einstein condensation (BEC) and such quantum fluids have demonstrated very interesting properties such as superfluidity. This project aims at introducing a new paradigm in solid-state physics trough the manipulation of polaritons quantum fluids and condensates in properly designed traps : such fluids will bring a wide variety of novel properties. With my team, I have recently made major advances towards this by preparing high quality polariton traps and evidencing some of the aspects of the rich physics of polaritons fluids in traps.
The whole field of polariton fluids is still in its infancy and I am convinced that major discoveries will be made during the coming years both for propagating polariton fluids and for confined geometries. I intend to stay at the forefront the field of confined polaritons and to provide high quality structures to other labs. My studies will be oriented along three major lines, each requesting a significant effort.
- The study of polariton BECs and quantum fluids in planar microcavities, both in II-VIs and III-Vs,
- The manipulation of coherent polariton fluids in geometry controlled environments,
- The realization of BEC and quantum fluid based polaritronic devices.
Each of these three parts represents a major challenge with great potentialities. First, these topics are a really novel contribution in solid-state physics. Second, the possible manipulation of polariton condensates opens up a vast domain, which covers both fundamental and applied physics and which limits we are absolutely unable to assess yet. I feel that the transition from atom condensates to polariton condensates may bring similar improvements for possible devices than it has been the case for the transition between the electronic tube and the transistor. I aim to keep my research group at the head of these very promising changes.","2000000","2012-02-01","2017-01-31"
"POLMAG","Polarized Radiation Diagnostics for Exploring the Magnetism of the Outer Solar Atmosphere","Javier Trujillo Bueno","INSTITUTO DE ASTROFISICA DE CANARIAS","POLMAG aims at a true breakthrough in the development and application of polarized radiation diagnostic methods for exploring the magnetic fields of the chromosphere, transition region and corona of the Sun via the interpretation of the Stokes profiles produced by optically polarized atoms and the Hanle and Zeeman effects in ultraviolet (UV), visible and near-infrared spectral lines. To this end, POLMAG will combine and expand expertise on atomic physics, on the quantum theory of radiation, on high-precision spectropolarimetry, on advanced methods in numerical radiative transfer, and on the confrontation of spectropolarimetric observations with spectral synthesis in increasingly realistic three-dimensional (3D)  numerical models of the solar atmosphere.
POLMAG targets the following very challenging issues:
-  Which are the optimum spectral lines for probing the magnetism of the outer solar atmosphere?
-  How to compute efficiently the Stokes profiles taking into account partial frequency redistribution, J-state quantum interference and the Hanle and Zeeman effects?
-  How to determine the magnetic, thermal and dynamic structure of the outer solar atmosphere through confrontations with spectropolarimetric observations?

POLMAG will go well beyond the current state of the art as follows:
-   Applying and extending the quantum theory of light polarization
-   Developing and applying efficient radiative transfer codes
-   Modeling the Ly-alpha and Mg II h & k observations of our CLASP suborbital rocket experiments  
-   Developing novel coronal magnetometry methods by complementing for the first time the information provided by forbidden and permitted lines
-   Developing the plasma diagnostic techniques needed for the scientific exploitation of spectropolarimetric observations with the new generation of solar telescopes and putting them at the disposal of the astrophysical community
 
POLMAG will open up a new diagnostic window in astrophysics.","2478750","2018-01-01","2022-12-31"
"POLPBAR","Production of Polarized Antiprotons","Hans Ströher","FORSCHUNGSZENTRUM JULICH GMBH","Hadrons, the building blocks of all matter in Nature, are not fundamental but composed of quarks and gluons. Up to now we do not know HOW NATURE MAKES HADRONS one of the most important questions of contemporary structure-of-matter physics. Major breakthroughs are to be expected with new experimental facilities such as FAIR. Most studies in hadron physics at HESR/FAIR will employ beams of unpolarized antiprotons, but the most spectacular opportunities will arise for polarized antiprotons the physics case is exceptional. The flag-ship experiment, Drell-Yan production in double polarized proton-antiproton scattering, gives direct access to transversity , the terra incognita of nucleon spin structure. The provision of such beams presents enormous scientific / technological challenges and has never been achieved with intensities sufficient for the crucial experiments. State-of-the-art techniques are capable of producing intensities less than ~10^5 s-1, which cannot be efficiently accumulated. It is the aim of this project to develop an efficient method for POLARIZING ANTIPROTON BEAMS by in-situ build-up in a storage ring. The only viable method to do this effectively is through &quot;spin-filtering&quot; by the repeated interaction of an antiproton beam with a polarized hydrogen gas target in a cooler storage ring. This technique works with protons, but it is not clear how the polarization build-up happens in detail. Spin-filtering needs to be optimized and, in particular, it must be extended to antiprotons. Within the framework of this project, the aim is to provide polarized antiproton beams in a storage ring with at least WITH 10 ORDERS OF MAGNITUDE higher intensity than previously possible. A very experienced team of scientists and engineers is needed, and this is available within my group. We will also strongly benefit from our collaboration partners. Thus, it is a &quot;now or never&quot; opportunity. If successful, a new era will open with fascinating experiments.","2448376","2010-05-01","2016-04-30"
"POLTDES","Interacting polaritons in two-dimensional electron systems","Atac Imamoglu","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Reversible coupling of excitons and photons in a microcavity leads to the formation of mixed light-matter quasiparticles, called cavity-polaritons. Weakly interacting polaritons constitute a rich system for studying nonequilibrium condensation and superfluidity. While exciton-polaritons have been studied mostly in intrinsic semiconductors with no free electrons, two-dimensional modulation-doped semiconductors with strong interactions between electrons have played a central role in unravelling many-body physics using transport. In this project, we combine these two fields of research and explore the complex interplay between cavity-polaritons and strongly correlated states of two dimensional electrons embedded inside microcavities. Our principal objective is the realization of polariton mediated superconductivity of electrons in gallium arsenide. Besides demonstrating a new mechanism for Cooper-pair formation, such an observation could revolutionize the search for systems that exhibit topological order. In a reciprocal approach, we will exploit the many-body nature of optical excitations in a two-dimensional electron gas to enhance polariton-polariton interactions. This will allow us to reach the polariton blockade regime, paving the way for realization of nonequilibrium strongly interacting polaritons. In parallel, we will explore cavity-magneto-polariton excitations out of fractional quantum Hall ground states: the objective in this part is to use the strong filling factor dependence of polariton splitting to realize nonlinear optical devices which derive their photon-photon interaction from light-absorption induced transition between compressible and incompressible ground states. Concurrently, we will study charged-exciton-polaritons in monolayer transition metal dichalcogenides positioned inside a microcavity, where a large polariton Berry-curvature allows for the observation of valley Hall effect and could be used to realize topological polaritons.","2482250","2015-11-01","2020-10-31"
"POLYBIOLUB","Polymeric Analogs to Biolubrication Systems","Nicholas Spencer","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Lubrication in nature is based on water, but only functions due to the presence of a host of biomolecules. In articular (e.g. hip) joints, it appears that the lubrication system is even more complex than previously believed, involving multiple gradients in structures and properties. Gels and brushes appear to play an important role in biolubrication, while highly confined, highly hydrated charged polymers are key to the important mechanical properties of cartilage. Components such as hyaluronic acid, glycoproteins, and lipids all appear to act synergistically to yield the extraordinary tribological properties of the cartilage-synovial-fluid system. POLYBIOLUB seeks to mimic the mechanisms revealed by the latest studies of cartilage, by means of the synthesis of functionally similar polymeric structures. This is a completely novel approach to the problem, which has the promise not only of revealing structural dependencies of cartilage function, but also of yielding radically new, biomimetic, lubricious, low-wear materials that could find applications in either industrial or clinical environments. The principal synthetic tool will be controlled radical polymerization in a flow environment, involving in situ growth monitoring, followed by a series of postmodification and grafting steps, to yield structures that are lubricious, wear resistant, and tough. These efforts will be accompanied by extensive characterization of composition, structure and mechanical/tribological properties at each stage. 

Our group has extensive experience in controlled radical polymerization in a flow system, postmodification of polymers, brush-gel and layered polymer systems,. We also have over a decade of experience in water-based lubrication, natural lubrication, synovial-joint studies, and multidimensional gradient systems. The challenge now is to combine these skills to fabricate a completely new biomimetic material.","2456570","2015-08-01","2020-07-31"
"POLYCAT","Polymeric catalysts and supports: A new paradigm for biomass processing","Franz Ferdinand Schüth","MAX PLANCK INSTITUT FUER KOHLENFORSCHUNG","Transforming part of the feedstock base of the chemical industry from fossil feedstocks to biomass, as expected by many researchers and companies, requires fundamental changes in the technologies for processing. While fossil feedstocks are non-polar and processed at high temperatures in the gas phase, most biomass derived feedstocks are polar molecules, which are present in aqueous solution and are typically processed at relatively low temperatures in defunctionalization reactions. Yet, most approaches rely on the conventional wisdom of using catalysts developed for fossil feed processing where the conditions almost always exclude the use of polymeric catalysts. However, for the conditions of biomass processing, adapted polymeric catalysts could be ideally suited. This defines the goals of the proposed research program: known and newly designed polymeric catalysts will be explored with respect to their potential in biomass conversion reactions, which could lead to a new paradigm in catalytic biofeedstock processing. In order to achieve this objective, it will be attempted to produce four selected, exemplary classes of polymers in porous form with adjustable porosity. The porous polymer types will then be post-treated with different methods to introduce catalytic functionality (acid/base and redox functionality). The target processes in biomass conversion are prototypical examples which are representative for whole classes of reactions. These include the depolymerisation of the major components of lignocellulose (cellulose, hemicellulose and lignin) and the targeted synthesis of selected platform molecules starting from glucose as the depolymerisation product of cellulose. Successful completion would provide more efficient access to many novel value chains and establish a novel class of catalytic materials.","1764000","2010-03-01","2015-02-28"
"PoroFrac","A high-fidelity isogeometric simulation methodology for fracture in porous media","René De Borst","THE UNIVERSITY OF SHEFFIELD","Fracture in heterogeneous, (partially) fluid-saturated porous media is a multi-scale problem with moving internal boundaries, characterised by a high degree of complexity and uncertainty. Nevertheless, in spite of an abundance of research on fracture in solid materials, there is relatively little work on fluid-saturated porous materials. Herein, a robust, flexible simulation technology will be developed for existing faults and propagating fractures in such media. The project consists of three pillars, each of which will have a scientific impact in its own right, complemented by a horizontal, application-oriented theme, which links the pillars, creates synergy and added value, and applies and elaborates the technology for hydraulic fracturing and for fault dynamics during earthquakes. In pillar 1 a mesoscopic, multi-phase model will be developed for fluid transport in cracks which are embedded in a fluid-saturated porous medium. The development of an adaptive spline technology in pillar 2 will enable to capture crack propagation and branching in arbitrary directions on arbitrary discretisations. The reliability method of pillar 3 will make it possible to make a quantitative assessment of the probability that, in a layered, heterogeneous medium, a crack propagates in a certain direction. Its successful completion will pave the way for a wider acceptance and use of reliability methods in fracture analyses, well beyond the primary application area of porous media. The linking theme will showcase some direct applications, in hydraulic fracturing and in earthquake analysis, but has a much wider range of applicability, e.g. for the safety analysis of CO₂ or nuclear waste storage in sub-surface formations, or fracture in fluid-saturated human tissues. Thus, the project will result in a robust simulation tool for fracture propagation in fluid-saturated porous media with unprecedented predictive capabilities for societal issues in energy, health, environment, and safety.","2329520","2016-01-01","2020-12-31"
"PORTWINGS","Decoding the Nature of Flapping Flight by port-Hamiltonian System Theory","Stefano STRAMIGIOLI","UNIVERSITEIT TWENTE","Flapping flight is one of the wonders of nature and has been vastly studied by biologists and fluid dynamicists. Many artifacts that mimic biological systems have been built at different scales.
For example, we have managed to create a system that resembles the steady flapping behaviour of its biological counterpart and can fly untethered, stably up to 80 km/h in up to 5 Beaufort wind speed. This is the Robird developed at the University of Twente, which got the 2016 ERF Tech Transfer Award, and is commercially exploited by Clear Flight Solutions. Even if this technology and results are unique and recognised worldwide, we still do not fully understand flapping flight to the scientific depth needed to go even further. The Robird cannot take off on its own, cannot perch, uses symmetric flapping, steers using a number of manifolds placed on the tail and has a minimal autonomy and a restricted operation time due to power consumption. In this project I propose to gain a much deeper structured understanding of flapping flight and experimentally validate these understandings. This will be done using port-Hamiltonian (PH) system theory and its physically unifying character, which will couple fluid dynamics theory to dynamically changing surfaces and their actuation. Once models will be validated by wind tunnel tests with flow visualisation, numerical optimisation will be used to fine tune models and search for uncertain parameters. Based on these findings, artifacts will be built to validate the generated models with real systems. Based on the insight gained, a new robotic bird will be realised with unprecedented flight dexterity, able to flap asymmetrically, adapt to the flow and take off and land as birds do, in order to validate the scientific understandings.","2800000","2018-10-01","2023-09-30"
"POSTCELL","Post-Cellular Wireless Networks","Angel LOZANO SOLSONA","UNIVERSIDAD POMPEU FABRA","POSTCELL aims at laying the foundation for future generations of wireless networks as they move past the reigning cell-centric paradigm and into the post-cellular era. This entails the definition of a new architecture for such networks and the characterization of the ensuing performance. For the future of wireless communications, the implications would be far-reaching.

The growth of wireless traffic is relentless, and it is actually gaining new momentum on account of fresh mechanisms: smartphones, cloud computing, and machine-to-machine communication. As a result, the volume of wireless traffic is poised to increase to truly staggering levels and, to face this challenge, wireless networks need to enter a new stage.

There is a fledging awareness that this challenge can only be fended off by a process of network massification, with two views about it. In the first view, densification is the only strategy through which dramatic improvements can be attained hereafter; this leads to a vision where base stations become tiny and exceedingly abundant. The second view, in turn, is built on the idea of dramatically scaling the number of colocated antennas per base station from the current handful to possibly hundreds. One of the seeds of POSTCELL is that, since neither form of massification can by itself resolve the challenge facing wireless systems, the two forms will have to end up coexisting.

Reconciling these two forms of massification and enabling a truly phenomenal scaling calls for an entirely new architecture where cells and physical base stations become things of the past, replaced by dynamically defined virtual base stations, powerful caches, and the possibility of device clustering, among other leaps forward. The signal processing needs to shift away from base stations, which become deconstructed, so as to gather at new places. POSTCELL seeks to drive this transformation and to gauge the performance of post-cellular wireless networks.","1876846","2016-10-01","2021-09-30"
"POTENTIALTHEORY","Potential theoretic methods in approximation and orthogonal polynomials","Vilmos Totik","SZEGEDI TUDOMANYEGYETEM","The project is aimed at systematic applications of potential theoretical
methods in approximation theory and in the theory of orthogonal polynomials.
Various open problems are proposed in different fields which
can be attacked with tools that have been developed in the
near past or are to be developed within the project.
The main areas are asymptotic behavior of Christoffel functions on the
real line and on curves, the universality problem in random matrices,
orthogonal polynomials and their zeros, polynomial inequalities, approximation
by homogeneous polynomials and some
questions in numerical analysis. The research problems and areas
discussed in the proposal are intensively investigated in current research. As has been the
case in the past, PhD students will be actively involved in the project.","402000","2011-01-01","2016-12-31"
"POWVER","Power to the People. Verified.","Holger Hermanns","UNIVERSITAT DES SAARLANDES","Twenty years ago we were able to repair cars at home. Nowadays customer services repair coffee machines. By installing software updates. Soon you will no longer be able to repair your bike.

Embedded software innovations boost our society; they help us tremendously in our daily life. But we do not understand what the software does, regardless of how well educated or smart we are. Proprietary embedded software has become an opaque layer between functionality and user. That layer is thick enough to possibly induce malicious or unintended behaviour.  Proprietary embedded software locks us out of the products we own.

We need a turn to open and hence customisable embedded software. However, a minor customisation might well have strong unexpected impact, for instance on the longevity of an embedded battery, or the safety of the battery charging process. We thus need means to detect, quantify and prevent such implications.

The POWVER project lays the foundations. It provides quantitative verification technology for system-level correctness, safety, dependability, and performability. In this endeavour, POWVER takes up a hard scientific challenge, a challenge where discrete and continuous, real-time, stochastic as well as data- and user-dependent aspects are all deeply intertwined: embedded software for electric power management. Electric power is intricate to handle by software, is safety-critical, but vital for mobile devices and their longevity. Since ever more tools, gadgets, and vehicles run on batteries and use power harvesting, power management is a pivot of the future.

POWVER will demonstrate that quantitative verification of open embedded software is feasible, and can ensure safe and dependable operation of safety-critical devices. A proof of concept will target the field of electric mobility, set up as a blueprint for other battery-powered appliances. As such, POWVER is the nucleus for a radical change in the way embedded software quality is assured in general.","2425000","2016-09-01","2021-08-31"
"PREAS","Predicting the arsenic content in groundwater of the floodplains in SE Asia","Diederik Jan Postma","Geological Survey of Denmark and Greenland","More than 100 million people living on the floodplains of the Ganges-Brahmaputra-Meghna, Mekong and Red River, all draining the Himalayas, are consuming arsenic contaminated water. Providing safe drinking water for these people requires a quantitative understanding of the processes regulating the groundwater arsenic content and this knowledge is presently not available. In PREDIAS we propose a revolutionary new approach to study these arsenic contaminated aquifers where sediments and groundwaters are considered as one reacting unit that is changing over time. The key hypothesis is that it is the aquifer sediment burial age that is the overall controlling parameter for the arsenic content. This new approach is explored by studying the groundwater chemistry as a function of sediment burial age, which is equivalent to the geological evolution over time, in part of the Red River floodplain in Vietnam. The investigations comprise delineating the sedimentological development over the last 9000 yrs as well as reconstructing hydrogeological conditions over that period. Process studies will reveal the effect of burial age on the chemical properties of the sediments and the arsenic release mechanisms. They comprise the binding and release mechanisms of arsenic to the aquifer sediment, and the reactivity of sedimentary organic carbon and iron oxides which drive the redox reactions controlling the water chemistry and arsenic mobilization. Information on the sedimentological and hydrogeological development over time as well as a quantification of the geochemical processes will be incorporated in a  3-D reactive transport model which over the last 9000 years, in steps of about 1000 years, can predict the evolution of the arsenic content over space and time in the groundwater of the studied area. The model can be extended in a more conceptual form to larger parts of the Red River delta and Bangladesh using satellite imaging to reveal the geological development in those areas.","1619932","2014-01-01","2018-03-31"
"PreCoMet","Predictive Computational Metallurgy","William Arthur Curtin","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Why is there no “Moore’s Law” for the creation of stronger and more durable metals?  Because there is a unique complexity to mechanical properties of metals: the strength, hardening, embrittlement, fracture, and fatigue are controlled by multi-defect interactions (dislocations/solutes/precipitates/grain boundaries).  However, such multi-defect interactions are beyond the scope of analytical elasticity theory, and thus require a deeper inquiry at atomistic and quantum scales.  And observed macroscopic mechanical behaviour arises from the collective interactions among such defects over large length and time scales.  The PI will tackle the fundamental challenge of the multi-defect, multi-scale problem in metal alloys through a combined theory/simulation effort that will push forward the frontiers of computational metallurgy and yield new, quantitative, predictive models of the mechanical performance of metals alloys that will accelerate metal design.  Three specific thrusts are proposed to predict the role of solute chemistry on : (i) fundamental dislocation phenomena, and the resulting effects on plastic flow and ductility (Solute/Dislocation/Dislocation interactions); (ii) dislocation transmission/absorption and damage nucleation along boundaries (Solute/Grain-boundary/Dislocation interactions); and (iii) the propagation of cracks under monotonic and fatigue loading (Solute/Crack/Dislocation/Grain-boundary interactions).  Small-scale Quantum, Atomistic, and/or Dislocation-level simulations will be designed to probe mechanistic concepts and to validate new predictive theories and new material constitutive models.  This approach is now feasible due to new multiscale modeling techniques developed by the PI and his recent quantitative models that resolve long-standing problems in metallurgy.  The theories and models emerging from this research will allow for generalization of the mechanisms across metals, and will enable the enhancement and design of new metal alloys.","2347920","2014-02-01","2019-01-31"
"PRECRIME","Self-assessment Oracles for Anticipatory Testing","Paolo TONELLA","UNIVERSITA DELLA SVIZZERA ITALIANA","One of the long-lasting dreams in science fiction is the ability to arrest criminals before they even commit crimes. Software testing researchers have a similar dream: when the context for a bug manifestation occurs in the field, the goal is to discover and fix the bug before it causes any in-field failure. In fact, current practice of pre-release testing is severely limited when dealing with autonomous AI (Artificial Intelligence) systems (such as self-driving cars, robots, automated traders, virtual doctors and customer service chatbots), running in complex, rapidly changing environments, which cause their run-time adaptation, learning and knowledge acquisition, because pre-release testing cannot exhaustively explore all different contexts and states in which the software will be running.

The PRECRIME project introduces a new, disruptive view on testing, called anticipatory testing and aimed at fixing bugs before they even manifest themselves in the field. Anticipatory testing is activated at run-time by a new type of oracles, called self-assessment oracles, which observe and report unexpected execution contexts. A self-assessment oracle is an estimator of the system’s confidence in being able to handle a new execution context correctly. The goal of anticipatory testing is to anticipate any failure that might occur in the field due to unexpected execution contexts. Whenever an execution context monitored at runtime by self-assessment oracles is estimated as a low confidence context for the system, anticipatory testing exercises the software automatically and extensively in similar contexts. Timely activation of anticipatory testing by runtime observations results in early, anticipatory fault detection. Combined with automated patch synthesis, anticipatory testing leads to the release of a patch for the fault before any software failure occurs in the field.","2313250","2019-01-01","2023-12-31"
"PREMOL","At the crossroad of molecular physics, quantum optics and spectroscopy:ultra-high-precision molecular spectroscopy for fundamental physics","Stephan SCHILLER","HEINRICH-HEINE-UNIVERSITAET DUESSELDORF","Molecules have an enormous potential in the field of frequency metrology-based fundamental physics, yet have so far not played an important role, due to difficult experimental challenges. The goal of this project is to overcome these difficulties by developing new techniques, thus opening a new chapter in precision molecular spectroscopy. Furthermore, it will have far-reaching impact in fundamental physics: 

(1) A 500-fold improved limit to the existence of a “fifth force” with range on the 0.1 nm scale.
(2) An independently determined set of the fundamental constants me/mp, me/md, and the Rydberg constant R∞. Their uncertainties will be reduced compared to CODATA2014 by up to a factor of 23 for me/mp, 4 for me/md, and 2 for R∞; 
(3) A test of the current muonic atom discrepancies of proton and deuteron charge radii rp and rd at the 30% and 50% level, respectively;
(4) Achieve a precision 1×10-16 in molecular ion spectroscopy, thus exploring the feasibility of using, in the future, molecular ions for testing the time-dependence of me/mp and mp/md.

Molecular hydrogen ions (MHIs) are the systems in principle suitable for providing these results: indeed, the required ab initio theory has made outstanding advances, reaching the 8×10-12 inaccuracy level. To date, experimental results are orders of magnitude less precise. 
In order to achieve an accuracy surpassing the theoretical one, this project shall develop new quantum optical techniques, including:
- Doppler-free spectroscopy, rotational and ro-vibrational;
- preparation of a single molecular ion in a single internal quantum state;
- resolution and control of systematic shifts at levels from 1×10-12 to 1×10-16; 
- novel spectroscopy laser systems.
These techniques will be of general applicability in the field of molecular ion spectroscopy.

The proposed work is based on the wide experience of the PI in precision measurements and will make a strong and overdue contribution to spectroscopy and fundamental physics.","2500000","2018-09-01","2023-08-31"
"PREPROCESSING","RIGOROUS THEORY OF PREPROCESSING","Fedor Fomin","UNIVERSITETET I BERGEN","The main research goal of this project is the quest for  rigorous  mathematical theory explaining  the power and failure of  heuristics.  The incapability of current computational models to explain the success of  heuristic algorithms in practical computing is the subject of wide   discussion for more than four decades. Within this project we expect a significant breakthrough in the study of a large family of heuristics: Preprocessing (data reduction or kernelization). Preprocessing  is a reduction of  the problem to a simpler one and this is the type of algorithms  used in almost every application.
As key to novel and groundbreaking results,  the proposed project aims to develop  new theory of polynomial time compressibility.    Understanding the origin of compressibility   will serve to  build more powerful  heuristic algorithms, as well as to explain the behaviour of preprocessing.
The  ubiquity of preprocessing  makes the theory of compressibility  extremely important.
The new theory  will  be able to transfer the ideas of efficient computation beyond the established borders.","2227051","2011-04-01","2016-03-31"
"PRIMEGAPS","Gaps between primes and almost primes. Patterns in primes and almost primes. Approximations to the twin prime and Goldbach conjectures","Janos Pintz","MAGYAR TUDOMANYOS AKADEMIA RENYI ALFRED MATEMATIKAI KUTATOINTEZET","The twin prime conjecture, that n and n+2 are infinitely often primes simultaneously, is probably the oldest unsolved problem in mathematics. De Polignac (1849) conjectured that for every even value of h, n and n+h are infinitely often primes simultaneously. These are the most basic problems on gaps and patterns in primes. Another one is the  conjecture of Waring (1770), stating that there are arbitrarily long arithmetic progressions (AP) of primes. For the newest developments we cite Granville (Bull. AMS 43 (2006), p.93): ): Despite much research of excellent quality, there have been few breakthroughs on the most natural questions about the distribution of prime numbers in the last few decades. That situation has recently changed dramatically with two extraordinary breakthroughs, each on questions that the experts had held out little hope for in the foreseeable future. Green and Tao proved that there are infinitely many k-term arithmetic progressions of primes using methods that are mostly far removed from mainstream analytic number theory. Indeed, their work centers around a brilliant development of recent results in ergodic theory and harmonic analysis. Their proof is finished, in a natural way, by an adaptation of the proof of the other fantastic new result in this area, Goldston, Pintz and Yildirim s proof that there are  small  gaps between primes.  The proposal's aim is to study these types of patterns in primes with possible combination of the two theories. We quote 3 of the main problems, the first one being the most important. 1) Bounded Gap Conjecture. Are there infinitely many bounded gaps between primes?  2) Suppose that primes have a level of distribution larger than 1/2. Does a fixed h exists such that for every k there is a k-term AP of generalised twin prime pairs (p, p+h)? 3) Erdôs'  conjecture for k=3. Suppose A is a sequence of natural numbers, such that the sum of their reciprocals is unbounded. Does A contain infinitely many 3-term AP's?","1376400","2008-11-01","2013-10-31"
"PrintPack","Arranging the Particles: Step Changing Chemical Measurement Technology","Gert DESMET","VRIJE UNIVERSITEIT BRUSSEL","The progress in liquid chromatography (LC), basically following Moore’s law over the last decade, will soon come to a halt. LC is the current state-of-the-art chemical separation method to measure the composition of complex mixtures. Driven by the ever growing complexity of the samples in e.g., environmental and biomedical research, LC is constantly pushed to higher efficiencies. Using highly optimized and monodisperse spherical particles, randomly packed in high pressure columns, the progress in LC has up till now been realized by reducing the particle size and concomitantly increasing the pressure. With pressure already up at 1500 bar, groundbreaking progress is still badly needed, e.g., to fully unravel the complex reaction networks in human cells.
For this purpose, it is proposed to leave the randomly packed bed paradigm and move to structures wherein the 1 to 5 micrometer particles currently used in LC are arranged in perfectly ordered and open-structured geometries. This is now possible, as the latest advances in nano-manufacturing and positioning allow proposing and developing an inventive high-throughput particle assembly and deposition strategy. The PI's ability to develop new parts of chromatography will be used to rationally optimize the many possible geometries accessible through this disruptive new technology, and identify those structures coping best with any remaining degree of disorder. Using the PI's experimental know-how on microfluidic chromatography systems, these structures will be used to pursue the disruptive gain margin (order of factor 100 in separation speed) that is expected based on general chromatography theory. 
Testing this groundbreaking new generation of LC columns together with world-leading bio-analytical scientists will illustrate their potential in making new discoveries in biology and life sciences. The new nano-assembly strategies might also be pushed to other applications, such as photonic crystals.","2488813","2016-10-01","2021-09-30"
"Probiotiqus","Processing of biomolecular targets for interferometric quantum experiments","Markus Arndt","UNIVERSITAT WIEN","Recent studies in Vienna have shown that surprising quantum phenomena, such as matter-wave interferometry with molecules composed of hundreds of covalently bound atoms, are actually feasible.

PROBIOTIQUS will now be the first project world-wide to develop experimental tools for matter-wave physics with large biomolecules from amino acid clusters up to proteins and self-replicating molecules.

First, we shall exploit the full potential of coherent molecule metrology for biomolecules, and molecules in a biomimetic environment.  This research connects quantum physics with chemistry and biophysics, since already a restricted number of precisely determined geometrical, electrical, magnetic or optical properties may provide tell-tale analytical information. Embedding the biomolecules in a hydrate layer will allow us to study their properties in a context that approaches the ‘natural’ environment.

Second, we will develop molecular beam methods, optical manipulation tools and detection schemes to prepare proteins and other large biomolecules for advanced quantum experiments. This includes new laser-assisted acoustic and thermal volatilization methods, slowing and focusing in optical forces, diffraction at ionization and neutralization gratings as well as tagging of proteins with ionizable small biomolecules.

Third, we will prepare a cryogenic biomolecular sample in a buffer-gas loaded ion trap, where optical ionization and neutralization will be optimized in order to enable optical diffraction gratings. The target temperature of 10 K will be the starting point for interference experiments with proteins and self-replicating RNA, on the way towards full viruses.

Quantum interference with large biomolecules at the edge to life has remained an outstanding challenge throughout the last two decades. The ERC advanced grant will now focus on this goal with novel and interdisciplinary strategies, in world-wide unique experiments.","2266904","2013-04-01","2018-03-31"
"PROGEOCOM","Avenues in Probabilistic and Geometric Combinatorics","Gil Kalai","THE HEBREW UNIVERSITY OF JERUSALEM","We consider problems in geometric and probabilistic combinatorics and discuss some applications to and connections with other areas.One underlying theme of our proposal is  discrete isoperimetric relations.

On the probabilistic side we discuss applications of Fourier analysis of Boolean functions to the study of threshold behavior of random graphs and other stochastic models, and propose ten directions for this emerging theory. One crucial problem is the study of near equality cases of Harper's isoperimetric inequality.

On the geometric side we discuss the relation between the number of (k-1)-dimensional faces and the number of k-dimensional faces for complexes that can be embedded in 2k-dimensions. We also consider metrical and algorithmical problems on graphs of polytopes and Helly-type theorems.","1376504","2013-05-01","2018-04-30"
"ProgrES","Programmable Enzymatic Synthesis of Bioactive Compounds","Sabine FLITSCH","THE UNIVERSITY OF MANCHESTER","Enzymes are now established as highly selective biocatalysts in organic synthesis with the range of catalysts and reactions rapidly increasing through access to large protein databases and high-throughput molecular biology tools for biocatalyst engineering. The diversity of biocatalytic reactions is now at a stage where they can be linked in (chemo)-enzymatic reaction cascades, where two or more chemical and/or enzymatic reactions can be catalysed simultaneously generating de novo biosynthetic pathways for chemical synthesis not found in Nature. These reaction cascades have demonstrated important prior art, however they have been mostly limited to few steps and lack the complexity provided by the natural pharmacopeia. ProgrES aims to achieve a step-change by introducing unprecedented structural complexity into de novo pathways and by moving away from manual to automated, high-throughput cascade design and implementation. The proposal is to use a transdisciplinary approach that addresses three important bottlenecks: i. Identification of enzymatic reactions that allow early functionalisation and late stage diversification of the cascade toolkit to increase structural complexity, building on C-H activation mediated by monooxygenases, which are well established in our research group. ii. As diversity of targets increases, high resolution structural analysis of pathway intermediates and products becomes a bottleneck, which is addressed by high-throughput label free mass spectrometry based analytical tools that will match the demands on timescale and numbers. iii. In parallel, we will establish bioinformatics tools adapted from both chemical synthesis and biosynthesis, to allow programmable enzymatic synthesis for cascade design. As proof-of-concept and test bed for the ProgrES platform we aim to generate a library of diverse synthetic imino sugars. This proposal will lead to a major breakthrough in creating a diverse range of scaffolds of therapeutic interest.","2399830","2018-07-01","2023-06-30"
"PROMETHEUS","Pattern formation and mineral self-organization in highly alkaline natural environments","Juan Manuel Garcia Ruiz","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","The precipitation of alkaline-earth carbonates in silica-rich alkaline solutions yields nanocrystalline aggregates that develop non-crystallographic morphologies. These purely inorganic hierarchical materials, discovered by the IP of this project, form under geochemically plausible conditions and closely resemble typical biologically induced mineral textures and shapes, thus the name ‘biomorphs’. The existence of silica biomorphs has questioned the use morphology as an unambiguous criterion for detection of primitive life remnants. Beyond applications, the study of silica biomorphs has revealed a totally new morphogenetic mechanism capable of creating crystalline materials with positive or negative constant curvature and biomineral-like textures which lead to the design of new pathways towards concerted morphogenesis and bottom-up self-assembly created by a self-triggered chemical coupling mechanism. The potential interest of these fascinating structures in Earth Sciences has never been explored mostly because of their complexity and multidisciplinary nature. PROMETHEUS proposes an in depth investigation of the nature of mineral structures such as silica biomorphs and chemical gardens, and the role of mineral self-organization in extreme alkaline geological environments. The results will impact current understanding of the early geological and biological history of Earth by pushing forward the unexplored field of inorganic biomimetic pattern formation. PROMETHEUS will provide this discipline with much needed theoretical and experimental foundations for its quantitative application to Earth Sciences. The ambitious research program in PROMETHEUS will require the development of high-end methods and instruments for the non-intrusive in-situ characterization of geochemically important variables, including pH mapping with microscopic resolution, time resolved imaging of concentration gradients, microscopic fluid dynamics, and characterization of ultraslow growth rates.","2431771","2014-08-01","2019-07-31"
"ProMotion","Probing Majorana quasi-particles and ballistic spin-momentum locking in topolocical insulatornanostructures","Karl Dieter WEISS","UNIVERSITAET REGENSBURG","Three-dimensional topological insulators (3D-TI) feature an insulating bulk and conducting surface states. The energy spectrum of these surface states mimics the one of relativistic Dirac electrons, i.e., it is linear in wave-vector k, not spin-degenerate and the spin locks to the k-vector (spin-momentum locking (SML)). The lack of spin degeneracy makes TIs a promising system for detecting quasi-particles with properties of Majorana fermions (MF), being potentially useful for fault tolerant quantum computing. The other feature, SML, promises electrical manipulation of magnetization and is at the heart of spintronics. The focus of this project is on testing new concepts, set ups and experiments to probe MFs and SML in 3D-HgTe nano- and hybrid structures. Strained films of 3D-HgTe constitute a TI with unprecedented high charge carrier mobility enabling the observation of ballistic and quantum effects, thus being a very promising material system for these studies. For hunting MF we focus on clean 3D-HgTe nanowires (NW) with superconductor (SC) contacts inducing topological superconductivity in the TI. Here, we utilize the peculiar energy spectrum of TI-NWs, tunable from topologically trivial to topological by adding half a flux quantum through the wire’s cross section. The existence of MFs forming at the superconducting wire’s end shall be proven by (i) anomalous conductance quantization at point contacts, (ii) measuring the anomalous height of Shapiro steps in SC-NW-SC junctions and by (iii) probing quantized conductance in SC-NW-normal metal junctions. In a complementary new approach (iv) we measure the quantum capacitance in an array of magnetic vortices in a SC-TI heterojunction to probe the density of states in the proximity induced superconducting gap, where MFs are expected to form. For probing SML in ballistic ferromagnet-HgTe hybrid systems we resort to a novel geometry (v) which measures the asymmetry of current flow rather than local or non-local voltages.","2498887","2018-07-01","2023-06-30"
"PROOFCERT","ProofCert: Broad Spectrum Proof Certificates","Dale Allen Miller","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","There is little hope that the world will know secure software if we
cannot make greater strides in the practice of formal methods:
hardware and software devices with errors are routinely turned against
their users.  The ProofCert proposal aims at building a foundation
that will allow a broad spectrum of formal methods---ranging from
automatic model checkers to interactive theorem provers---to work
together to establish formal properties of computer systems.  This
project starts with a wonderful gift to us from decades of work by
logicians and proof theorist: their efforts on logic and proof has
given us a universally accepted means of communicating proofs between
people and computer systems.  Logic can be used to state desirable
security and correctness properties of software and hardware systems
and proofs are uncontroversial evidence that statements are, in fact,
true.  The current state-of-the-art of formal methods used in
academics and industry shows, however, that the notion of logic and
proof is severely fractured: there is little or no communication
between any two such systems.  Thus any efforts on computer system
correctness is needlessly repeated many time in the many different
systems: sometimes this work is even redone when a given prover is
upgraded.  In ProofCert, we will build on the bedrock of decades of
research into logic and proof theory the notion of proof certificates.
Such certificates will allow for a complete reshaping of the way that
formal methods are employed.  Given the infrastructure and tools
envisioned in this proposal, the world of formal methods will become
as dynamic and responsive as the world of computer viruses and hackers
has become.","2201589","2012-01-01","2016-12-31"
"PROSPERITY","Probing Stellar Physics and Testing Stellar Evolution through Asteroseismology","Conny Aerts","KATHOLIEKE UNIVERSITEIT LEUVEN","Our goal is to achieve a physical description of stellar interiors with an order of magnitude better precision in the physical quantities than we have now. We will concentrate on three outstanding critical issues in current stellar structure theory and solve them through a novel approach termed asteroseismology. 1. We will obtain a quantitative estimate of the amount of convective mixing and of the internal rotation profile for a broad range of stellar masses and evolutionary states, with specific emphasis on massive stars and on red giant stars. This will be done using new seismic data assembled by the space missions MOST, CoRoT and Kepler, which have a factor 1000 better precision than the ground-based data we had to rely on so far.  2. We will include, for the first time, the effect of a radiation-driven stellar wind on the theoretical description of stellar oscillations.  This opens a new avenu: the seismic calibration of  stellar evolution models of the most massive stars from the core-hydrogen burning up to the supernova stage. 3. We will build a new dedicated camera, MAIA, for the Mercator telescope at La Palma (Canary Islands), to investigate the badly understood common envelope phase of close binary stars. There are large unknowns in their evolution, mainly during the red giant phase when the two stellar components may share a common envelope. The recently discovered pulsating subdwarf O and B binaries must have lost their hydrogen envelope during a common envelope phase near the tip of the red giant branch. We will put tight seismic constraints on their outer hydrogen layer and mass and use these two diagnostics to perform a critical evaluation of close binary evolution theory along the giant branch. Our project encompasses engineering, observational astronomy, theoretical astrophysics, time series analysis and statistical clustering. It will revolutionise stellar evolution theory for a variety of stars and all topics in astrophysics that build on it.","2491200","2009-01-01","2013-12-31"
"ProtonPump","Structural mechanism coupling the reduction of oxygen to proton pumping in living cells","Richard Neutze","GOETEBORGS UNIVERSITET","Every breath you take delivers oxygen to mitochondria within the cells of your body. Mitochondria are energy transducing organelles that accept electrons liberated from the food that you eat in order to generate a transmembrane proton concentration gradient. Cytochrome c oxidase is an integral membrane protein complex in the mitochondria that accepts four electrons and reduces molecular oxygen to two water molecules while simultaneously pumping protons against a transmembrane potential. Cytochrome c oxidase homologues are found in almost all living organisms. Because oxygen is the final destination of the transferred electrons, this enzyme family is referred to as the terminal oxidases. Crystal structures of terminal oxidases have been known for more than two decades and these enzymes have been studied with virtually all biophysical and biochemical methods. Despite this scrutiny, it is unknown how redox reactions at the enzyme’s active site are coupled to proton pumping. Here I aim to create a three dimensional movie that reveals how proton exchange between key amino acid residues is controlled by the movements of electrons within the enzyme. This work will utilize state-of-the-art methods of time-resolved serial crystallography, time-resolved wide angle X-ray scattering and time-resolved X-ray emission spectroscopy at European X-ray free electron lasers (XFELs) and synchrotron radiation facilities to observe structural changes in terminal oxidases with time. I will develop new approaches for rapidly delivering oxygen or electrons into the protein’s active site in order to initiate the catalytic cycle in microcrystals and in solution. This project will yield completely new insight into one of the most important chemical reactions in biology while opening up the field of time-resolved structural studies of proteins beyond a handful of naturally occurring light-driven systems.","2500000","2019-01-01","2023-12-31"
"PROWAT","Proton conduction in structured water","Huib BAKKER","STICHTING NEDERLANDSE WETENSCHAPPELIJK ONDERZOEK INSTITUTEN","In recent years water near surfaces and solutes has been observed to be differently structured and to show slower reorientation and hydrogen-bond dynamics than in bulk. Aqueous proton transfer is a process that strongly relies on the structure and dynamics of the hydrogen-bond network of liquid water and that often occurs near surfaces. Examples are thylakoid and mitochondrial membranes and the nanochannels of transmembrane proteins and fuel cells. An important but experimentally largely unexplored area of research is how the rate and mechanism of aqueous proton transfer change due to the surface-induced structuring of the water medium. Theoretical work showed that the structuring and nano-confinement of water can have a strong effect on the proton mobility. Recently, experimental techniques have been developed that are capable of probing the structural dynamics of water molecules and proton-hydration structures near surfaces. These techniques include heterodyne detected sum-frequency generation (HD-SFG) and two-dimensional HD-SFG (2D-HD-VSFG).

I propose to use these and other advanced spectroscopic techniques to study the rate and molecular mechanisms of proton transfer through structured aqueous media. These systems include aqueous solutions of different solutes, water near extended surfaces like graphene and electrically switchable monolayers, and the aqueous nanochannels of metal-organic frameworks. These studies will provide a fundamental understanding of the molecular mechanisms of aqueous proton transfer in natural and man-made (bio)molecular systems, and can lead to the development of new proton-conducting membranes and nanochannels with applications in fuel cells. The obtained knowledge can also lead to new strategies to control proton mobility, e.g. by electrical switching of the properties of the water network at surfaces and in nanochannels, i.e. to field-effect proton transistors.","2495000","2016-10-01","2021-09-30"
"PSOPA","Phase-sensitive optical parametric amplifiers","Peter Avo Andrekson","CHALMERS TEKNISKA HOEGSKOLA AB","Optical amplifiers are essential in optical communication systems as they compensate loss induced by the transmission fiber ensuring signal integrity of the information being transmitted, as well as in other applications such as spectroscopy.

This research proposal deals with phase-sensitive optical parametric amplifiers (PSA) that have unique and superior properties compared with all other optical amplifiers, most notably the potential of noiseless amplification, very broad optical bandwidth, and being an enabler of a range of ultrafast all-optical functionalities. In communication, there is an urgent need to develop new technologies that can break the ‘nonlinear Shannon capacity limit’, which is considered a serious barrier for continued capacity increase needed to meet the exponentially growing demand for bandwidth. The use of PSAs is expected to be an essential part of this development.

The objective is to unleash the unexplored potential of PSAs by generating knowledge and implementing experimental demonstrations that go substantially beyond current state-of-the-art. This involves a mix of engineering and scientific challenges with telecom and non-telecom applications in mind. We will leverage advances in other areas e.g. low loss photonic crystal fibers and highly nonlinear materials to realize compact PSAs with unprecedented performance. Specifically, we will demonstrate:

• Significant merits (reach, spectral efficiency, capacity) of PSAs in optical transmission systems
• High coherence, low noise lasers by utilizing ultralow noise amplifier as gain element
• Very broad gain bandwidth, low noise PSAs using specially tailored nonlinear gain medium
• Compact (hybrid integration compatible) PSA using new nonlinear materials
• Novel ultrafast all-optical operations/signal processing using PSAs
• Capability of PSAs for detection of very weak optical signals for e.g. and quantum optics","2499264","2012-03-01","2017-02-28"
"PTRELSS","Phase transitions in random evolutions of large-scale structures","Fabio Martinelli","UNIVERSITA DEGLI STUDI ROMA TRE","Large scale structures typically consist of a large number of mutually interacting components and the collective behavior of this huge number of degrees of freedom leads to a host of intriguing phenomena: anomalous fluctuations, phase transitions, complex time evolutions and metastable behavior. By phase transition, a key concept for the proposal, we mean here an abrupt, qualitative change in the properties of the model when its parameters are changed. Mathematically, phase transitions are signaled by the appearance of different probability measures describing the possible states of the system but they are also signaled by a dramatic change in the convergence time of certain Markov chain Monte Carlo algorithms that are used to sample from the probability distribution of the sample. Systems of this type are well known and studied in statistical physics, but in the last decade their importance has been recognized in other frontier areas of scientific research, such as biology, genetics or theoretical computer science. At the same time it is widely recognized that a deep mathematical understanding of the behavior of these systems is essential. The latter requires the development and use of powerful probabilistic ideas and techniques. Probability is, in fact, the basis of a common language, that allows the sharing, adaptation and combination of key notions and techniques from the different areas involved. Around these topics and guided by the PI a small group of young and brilliant researchers formed in the last years and later developed into a somewhat larger team whose members are all very active and tightly collaborating, with already several important results which opened new directions and perspectives. I propose to build up on these advances by gathering around the host institution the team and guide it to further research on the interplay between equilibrium and dynamical aspects of phase transitions in large-scale structures.","1248067","2009-01-01","2012-12-31"
"PUSHBOUND","Pushing the Boundaries of Molecular Dynamics Simulations","Michele Parrinello","UNIVERSITA DELLA SVIZZERA ITALIANA","Atomistic computer simulation has established itself as one of the most important methodologies in modern science, its impact being felt in areas as diverse as physics, chemistry, geophysics, materials science and biophysics, to name but a few. Yet in spite of great progress in computer power and algorithms, we are still not able to simulate such important phenomena as nucleation, phase transitions or protein folding. The purpose of this proposal is to strongly push back the limits of length, time scale and accuracy of present-day methods, greatly enhancing the scope of atomistic simulations. We expect the impact of a successful outcome of this proposal to revolutionize the field. We shall make use of three technical innovations: an extension of Langevin-type equations to include correlated (colored) noise; the use of h-matrices to speed up electronic structure calculations; and an intelligent use of neural networks. Our strategy will be complex. We plan to speed up ab-initio molecular dynamics calculations considerably and also to generate new and highly accurate effective potentials based on electronic structure calculations. A large part of our effort will be devoted to the time scale problem. In this respect we shall improve metadynamics so that its implementation becomes as general and as automatic as possible, and we shall also introduce methods for reconstructing the real dynamics from metadynamics. Finally, highly innovative and powerful sampling methods based on specially designed colored noise Langevin equations will be developed.","2499600","2010-08-01","2015-07-31"
"QBH Structure","The Quantum Structure of Black Holes and the Recovery of Information","Nicholas WARNER","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","The detection of black-hole mergers in 2015 was a spectacular confirmation of General Relativity (GR). Yet, it is also in black holes that the fundamental conflict between GR and quantum mechanics (QM) is most acute.  Black holes are known to have a vast entropy.  Consistency with QM requires that the microstates giving rise to this entropy must be accessible at the horizon scale. However, GR coupled to field theory is incapable of supporting this horizon-scale microstructure!  My work has shown that Microstate Geometries (MG’s), based in string theory and higher-dimensional field theory, have all the essential elements for supporting and encoding microstate data:  MG’s are smooth, horizonless solutions in string theory that are identical to black holes on large scales but differ radically from the black holes of GR at the horizon scale.

I propose to launch a new, extensive study of the MG paradigm, focussing on the, as yet, unexplored dynamics of the microstructure in MG’s: (i) How infalling matter is absorbed and diffused into excitations of MG’s; (ii) How the excitations of MG’s, and the MG’s themselves, decay into some form of Hawking radiation that carries the microstructure data to infinity, thereby preserving quantum unitarity; (iii) How the large-scale, collective dynamics of microstructure interacts with matter in the horizon region and, particularly, how microstructure dynamics influences accretion disks and black-hole mergers. Progress will be achieved by analyzing the energy transfer between infalling probes and MG’s, computing the resulting excitations of the MG and the drag on infalling objects. The results will be re-expressed in a hydrodynamic form that can be applied to simulations of astrophysical black holes. 

This proposal will thus solve the information paradox by providing a microscopic description of black-hole entropy at the horizon scale and this should lead to macroscopic, measurable signatures of the horizon-scale microstructure.","2462659","2019-01-01","2023-12-31"
"QCDMAT","Strongly Coupled QCD Matter","Jean-Paul Blaizot","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","This project addresses fundamental issues in the study of nucleus-nucleus collisions at high energy, such as the thermodynamics of matter at extremely high temperature, or the dynamics of the dense system of gluons that constitute most of the wave-function of a nucleus at asymptotically high energy. In either case, one is dealing with strongly interacting systems whose description requires the development of new theoretical tools.

The Relativistic Heavy Ion Collider (RHIC) in the USA has deeply changed our vision of hot and  dense matter, revealing for instance that the quark-gluon plasma produced in heavy ion collisions behaves as a strongly coupled liquid with a relatively small viscosity. Soon, beams of lead nuclei will be accelerated at the Large Hadron Collider (LHC) at CERN, with energies exceeding by more than one order of magnitude those of RHIC. New phenomena are likely to be observed, and one of the goals of the project is to develop the theoretical tools that will be needed to understand these phenomena: by developing new, non perturbative methods of quantum field theory in order to calculate the properties of the quark-gluon plasma and the initial nuclear wavefunctions; by providing the appropriate theoretical frameworks to interpret the data and possibly suggest new measurements.

All members of the proposed research team have made breakthrough contributions to the field. They bring a unique expertise on the various aspects of the project, putting the team in a position to make a groundbreaking contribution. The project has also cross-disciplinary aspects that will be exploited whenever deemed appropriate. This will contribute to broaden the training of the young researchers hired within the project.","1512300","2011-08-01","2016-07-31"
"QCLS","Quantum Computation, Logic, and Security","Bartholomeus Paulus Franciscus Jacobs","STICHTING KATHOLIEKE UNIVERSITEIT","Quantum computing involves a new computational paradigm based on the
laws of quantum mechanics. It uses qubits, which are superpositions of
ordinary bits, and exploits the `strange' quantum phenomena like
entanglement of qubits. It promises new forms of very fast,
distributed computation.  First applications are now appearing in
computer security, based on the manipulation of individual qubits. The
realization of large scale quantum computing, involving multitudes of
qubits is still a technological challenge, beyond the scope of this
proposal.

Quantum computing originated, understandably, in physics.  This
project abstracts from this physical level and will transform and
develop the relevant phenomena at a mathematical level so that they
can be integrated in computational models, logics and formal methods
used in computer science. The project will use the unifying language
and tools of category theory, which are working as a ``Rosetta Stone''
in the multi-disciplinary area of computer science, mathematics and
physics. The project will clarify the subtle but fundamental
difference between quantum computation/logic on the one hand and
probabilistic and non-deterministic computation/logic on the
other. This should result in (programming) logics and models that are
clear and usable for computer scientists, in particular in the area of
(formal methods for) computer security. Overall, the proposal aims to
ensure that the discipline of computer science is well-prepared for
the (approaching) moment when quantum computing becomes a reality.","2500000","2013-05-01","2018-04-30"
"QED-PROTONSIZE","The Proton Size Puzzle: Testing QED at Extreme Wavelengths","Kjeld Sijbrand Eduard EIKEMA","STICHTING VU","A key component of the Standard Model is Quantum Electrodynamics (QED). QED explains e.g. the anomalous magnetic moment of the electron and small energy shifts in the energy structure of atoms and molecules due to vacuum fluctuations. After decades of precision measurements, especially laser spectroscopy in atomic hydrogen, QED is considered the most successful and best-tested theory in physics. However, in 2010 precision spectroscopy in muonic-hydrogen (where the electron is replaced with a muon) has lead to discrepancies in energy level structure that cannot be accounted for. If QED is considered correct, then one way of interpreting the results is that the size of the proton is different in normal (electronic) hydrogen by as much as 4% (a 7 sigma effect) compared to muonic hydrogen. Despite great theoretical and experimental efforts, this 'proton size puzzle' is still unsolved.

I propose to perform precision spectroscopy in the extreme ultraviolet near 30 nm in the helium+ ion, to establish an exciting new platform for QED tests and thereby shed light on the proton-size puzzle. The advantages of helium ions over hydrogen atoms are that they can be trapped (observed longer), QED effects are more than an order of magnitude larger, and the nuclear size of the alpha particle is better known than the proton. Moreover, the CREMA collaboration has recently measured the 2S-2P transition in muonic He+ (both 3He and 4He isotopes) at the Paul Scherrer Institute. Evaluation of the measurements is ongoing, but could lead to an 8 fold (or more) improved alpha-particle radius, so that it is no longer limiting QED theory in normal He+. I will use several ground-breaking methods such as Ramsey-comb spectroscopy in the extreme ultraviolet to measure the 1S-2S transition in trapped normal electronic He+, with (sub) kHz spectroscopic accuracy. This will provide a unique and timely opportunity for a direct comparison of QED in electronic and muonic systems at an unprecedented level.","2497664","2016-09-01","2021-08-31"
"QFPROBA","Quantum Fields and Probability","Antti KUPIAINEN","HELSINGIN YLIOPISTO","Quantum Field Theory (QFT) has become a universal framework in physics to study systems with infinite number of degrees of freedom. 
It has also had in the past significant interaction with Probability starting with Constructive QFT and rigorous statistical mechanics. The goal of this proposal is to bring QFT methods to probabilistic problems and  new ideas from Probability to QFT. It concentrates on two concrete topics: 

(1) Renormalization Group study of rough Stochastic Partial Differential Equations, both their path wise solutions and their space-time correlations and stationary states. These equations are  ubiquitous in non-equilibrium physics and they are mathematically challenging.  

(2) The use of Multiplicative Chaos theory in the rigorous construction and study of the Liouville  Conformal Field Theory. Liouville theory is one of the most studied  Conformal Field Theories in physics due to its connection to scaling limits of random surfaces and string theory. It has   many mathematically puzzling features and its rigorous study is now possible. 

Although the  physical applications of these theories are far apart on the level of mathematical methods they have a  common unity based on renormalization theory that I want to utilize. I think time is ripe for a new fruitful interaction between QFT and Probability.","2463412","2017-10-01","2022-09-30"
"QGBE","Quantum Gases Beyond Equilibrium","Sandro Stringari","UNIVERSITA DEGLI STUDI DI TRENTO","The physics of systems out of equilibrium represents a fascinating chapter of science, with many unsolved problems and unexplored issues.  Quantum gases are particularly well suited to investigate non-equilibrium phenomena, because the key parameters of the problem (scattering length, trapping conditions, etc.) can be varied in a well controlled manner. They also offer unique opportunities to perform experimental measurements with high precision. This project aims to theoretically explore novel dynamic and transport properties of quantum gases at both finite and zero temperature, with special emphasis on the effects of quantum statistics, superfluidity and the role of interactions. The major goal is to achieve a deeper understanding of several fundamental issues: the universal properties exhibited by dilute quantum gases; the role of viscosity in non-uniform configurations; the applicability of quantum Monte Carlo techniques to explore the dynamics and transport properties of Bose and Fermi gases; the concept of superfluidity in systems far from equilibrium; the motion of impurities embedded in quantum baths, their interaction and the consequences on the dynamic behaviour; the collective excitations exhibited by novel quantum phases like binary mixtures of quantum gases and dipolar atomic and molecular gases; the effects of disorder; the condensed matter analogues of gravitational physics; the study of self-trapping and Josephson oscillations in superfluid Bose and  Fermi gases. An important motivation of the project is to identify questions of broad interest which might be relevant also beyond the realm of quantum gases, as well as to develop advanced theoretical approaches to challenging problems of statistical mechanics and many-body physics.","1638560","2011-02-01","2016-01-31"
"QIT4QAD","Photonic Quantum Information Technology and the Foundations of Quantum Physics in Higher Dimensions","Anton Zeilinger","UNIVERSITAT WIEN","One of the most important developments in modern physics was the recent emergence of quantum information science, which by its very nature is broadly multidisciplinary. It was started by investigations of the foundations of quantum mechanics, and fundamental quantum concepts, most notably, entanglement, play a key role. We are now at an historic moment where a major qualitative step, both in developing a new technology and applying it to new fundamental questions, can be made. In this proposal, we aim to combine the investigation of fundamental questions with the development of micro-optics technology to reach a new level of both quantum information experiments and fundamental tests of quantum mechanics.  We propose to utilize the advanced development of micro-optics to build novel integrated quantum optics photonic chips. High quality micro-optics will allow precise control over many properties, including birefringence, dispersion, periodicity, and even absorptive properties. We will combine this with novel highly efficient detectors, hopefully, in the long run, also integrated into the same microchips.  By their very nature, the new multi-mode devices will make new higher-dimensional regions of Hilbert space and new types of multi-photon entanglement accessible to experiment. Such devices will enable many new fundamental investigations of quantum mechanics, such as, to give just one example, exploring quantum complementarity both between different numbers of photons and as a function of Hilbert space dimension with significant mathematical implications. Most importantly, we are convinced that many new ideas will arise throughout the project.  The new integrated quantum optical chips will also be important in quantum computation, specifically with cluster states and similar complex quantum states. With these chips, we will realize multi-qubit procedures and algorithms and demonstrate the feasibility of all-optical quantum computation in realistic scenarios.","1750000","2009-01-01","2013-12-31"
"QMES","Quantum Mesoscopics with Vacuum Trapped Nanoparticles","Lukas Novotny","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","""The objective of this project is to control the dynamics of a nanoscale object with unprecedented precision and to study interactions on the mesoscale, - the grey zone between the discrete atomistic world and the continuous world of macroscopic objects.

A single nanoparticle will be captured  by the gradient force of a focused laser beam in ultrahigh vacuum and its center-of-mass motion will be controlled by optical back-action. To cool the nanoparticle to its quantum ground state we will explore  active parametric feedback cooling in combination with  passive cavity-based  cooling.

A laser-trapped nanoparticle is physically decoupled from its environment, which guarantees extremely long coherence times and quality factors as high as 10^11 in ultrahigh vacuum. Force sensitivities of 10^(-20) Newtons in a bandwidth of 1 Hz can be achieved, which outperforms other measurement techniques by orders of magnitude. In this project, we will use a laser-trapped nanoparticle as a local probe for measuring mesoscopic interactions, such as Casimir forces, vacuum friction, non-equilibrium dynamics and phase transitions, with unprecedented accuracy.

We will also measure the dynamics of nanoparticles in double-well potentials created by two laser beams with closely spaced foci. A pair of trapped nanoparticles defines a highly controllable coupled-oscillator model, which can be used for studying strong coupling, level splitting, and adiabatic energy transfer at the quantum - classical  barrier.

A nanoparticle cooled to its quantum ground state opens up a plethora of fundamental studies, such as the collapse of quantum superposition states under the influence of noise and gravity-induced quantum state reduction. This project will also open up new directions for precision metrology and provide unprecedented control over the dynamics of matter on the nanometer scale.""","2499471","2013-10-01","2018-09-30"
"QOLAPS","Quantum resources: conceptuals and applications","Ryszard Horodecki","UNIWERSYTET GDANSKI","""The studies of quantum resources - entanglement (E) and non-locality (NL) carried out over the last decade have broadened horizons of our conceptual understanding of Nature and at the same time opened unprecedented possibilities for practical applications.
The project aims at taking advantage of the most recent discoveries to understand the ultimate power and find novel applications of these resources. The main objectives are: E) to study novel entanglement-induced non-additivity effects in quantum communication and application of mixed state entanglement to quantum metrology NL) to recognize the influence of information causality on the power of quantum non-locality and verify the power of non-locality, and more generally – contextuality – for quantum computational speed-up. In particular, it is planned: E) to find new non-additivities by providing explicit constructions of bipartite channels, broadcast channels and quantum networks; to demonstrate experimentally non-additivity effects; to provide experimentally friendly entanglement measures in quantum networks; to analyse entanglement-enhanced metrology in presence of decoherence NL) to determine to what extent information-causality reproduces quantum mechanics; to generalize information causality to multipartite systems; to provide new fundamental information-theoretical principles behind quantum mechanics; to quantify and classify contextuality; to design and analyse multiparty non-local systems independently of quantum mechanics; to verify their usefulness for communication and computational tasks.
We shall extensively exploit multiple interrelations between these two aspects of quantum physics. The results of theoretical investigations will be implemented in labs by experimental partners. In particular, we plan pioneering implementations of quantum channel non-additivity effects. The proposed research lines will bring ground-breaking results for quantum information processing.""","1970380","2012-01-01","2016-12-31"
"QON","Quantum optics using nanostructures: from many-body physics to quantum information processing","Atac Imamoglu","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Spins in nanostructures have emerged as a new paradigm for studying quantum optical phenomena in the solid-state. Motivated by potential applications in quantum information processing, the research in this field has focused on isolating a single confined spin from its environment and implementing coherent manipulation. On the other hand, it has been realized that the principal decoherence mechanisms for confined spins, stemming from interactions with nuclear or electron spin reservoirs, are intimately linked to fascinating many-body condensed-matter physics. We propose to use quantum optical techniques to investigate physics of nanostructures in two opposite but equally interesting regimes, where reservoir couplings are either suppressed to facilitate coherent control or enhanced to promote many body effects. The principal focus of our investigation of many-body phenomena will be on the first observation of optical signatures of the Kondo effect arising from exchange coupling between a confined spin and an electron spin reservoir. In addition, we propose to study nonequilibrium dynamics of quantum dot nuclear spins as well as strongly correlated system of interacting polaritons in coupled nano-cavities. To minimize spin decoherence and to implement quantum control, we propose to use nano-cavity assisted optical manipulation of two-electron spin states in double quantum dots; thanks to its resilience against spin decoherence, this system should allow us to realize elementary quantum information tasks such as spin-polarization conversion and spin entanglement. In addition to indium/gallium arsenide based structures, we propose to study semiconducting carbon nanotubes where hyperfine interactions that lead to spin decoherence can be avoided. Our nanotube experiments will focus on understanding the elementary quantum optical properties, with the ultimate goal of demonstrating coherent optical spin manipulation.","2300000","2008-11-01","2013-10-31"
"QORE","Quantum Correlations","Nicolas Gisin","UNIVERSITE DE GENEVE","In all sciences one observes correlations and develops and tests theoretical models describing them. Quantum correlations are produced by measurements on entangled quantum states. Since they may violate some Bell inequality even when the different parties are space-like separated, they can t be described with the usual tools: common causes and communication. Violation of a Bell inequality is the signature of quantumness: all other entanglement witnesses depend on assumptions about the dimension of the relevant Hilbert spaces. The vision of this project is that nonlocal quantum correlations provide new resources without any equivalence in other sciences. The core objectives are to better understand, manipulate and exploit nonlocal quantum correlations. This project covers aspects in theoretical, experimental and applied physics, with an emphasis on fundamental questions. The goal is to improve our understanding of quantum nonlocality as a resource (different from entanglement) and to improve our ability to harness entanglement over long distances, both for intellectual and applied motivations. The tools are the conceptual nonlocal box borrowed from theoretical computer science, quantum optics at telecom wavelengths and rare-earth ion doped crystals as atomic ensemble based quantum memories. The expected outcomes are findings about the minimal resources required to simulate quantum correlations and decisive steps towards a Word Wide Quantum Web.","2049600","2009-01-01","2013-12-31"
"QSpec-NewMat","Quantum Spectroscopy: exploring new states of matter out of equilibrium","Angel RUBIO","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","This project addresses the development of novel theoretical and computational tools that utilize the quantum nature of light to understand and control quantum phenomena in complex systems in and out of equilibrium. Some examples of these processes include exciton-exciton interaction, quantum coherence, assisted energy and charge transport, photochemistry, and new states of matter. 

The present project aims to build up the basic theoretical and computational machinery to allow quantum computations of the electronic and ionic dynamics of atomic, molecular or extended systems coupled to quantised electromagnetic fields and thereby set the stage for a new era in the first-principle computational modelling of light-matter interactions.  To achieve this goal, we will combine the principles of time-dependent density functional theory (TDDFT) and quantum electrodynamics (QED) into a new quantum electrodynamical-DFT approach named as “QEDFT”.

Insight, design and control define the scientific rationale of the project, which will focus on the discovery of the general principles that describe and control systems far from equilibrium and orchestrate the behavior of many electrons and atoms to create new phenomena/states of matter. Besides developing and implementing the new theory of QEDFT, we will  investigate atoms and molecules with quantum optical fields; whether and how selected laser pulses drive molecules and solids into new states of matter that have no equilibrium counterpart. What happens when it enters these coherent states? The objective is to identify the spectroscopic fingerprint of those new states. Which states arise in the strong light-matter coupling regime? e.g. hybridized states such as photon bound states, exciton/plasmon-polariton states, so far still undiscovered states. The long-term goal is to deliver an all-out theoretical and computational toolbox for QED-TDDFT applicable to complex molecular systems (like presently approachable by DFT and by TDDFT).","2492500","2016-10-01","2021-09-30"
"QuaDoPS","Quantum-Dot Plasmonics and Spasers","David James Norris","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","This project will fabricate and study devices known as spasers, which are the plasmonic analog of conventional lasers. In general, plasmonic devices exploit electromagnetic waves known as surface plasmon polaritons (herein shortened to surface plasmons) that propagate at the surface of a metal. Because these waves allow light to be concentrated in nanometer-scale volumes (hot spots), they can lead to enhanced light-matter interactions. Consequently, plasmonic structures have been studied for various photonic applications. However, because surface plasmons dissipate energy in the metal, intrinsic losses can severely limit light-matter interactions and the performance of plasmonic devices. Therefore, simple routes to counteract losses by adding a gain material to rejuvenate the surface plasmons have been sought. Moreover, by adding sufficient optical gain to a plasmonic resonator, a spaser can be created. This can lead to an extremely versatile nanoscale source of surface plasmons, photons, and/or intense electromagnetic fields.  Therefore, spasers can enable fundamental studies on the limits of nanoscale optics as well as various applications.  Recently, the very first spasers have appeared, leading to many open questions. To help address these, the PI will perform fundamental studies on a broad class of plasmonic devices that incorporate gain.  The proposed research will take advantage of his expertise in two areas: (a) highly fluorescent semiconductor nanocrystals (colloidal quantum dots) for the gain material and (b) the fabrication of high-quality low-loss patterned metallic films. By combining these, an ideal route to spasers will be pursued. The project will develop designs and fabrication processes to create quantum-dot-decorated plasmonic resonators, and then investigate their gain, amplification, and spasing behavior. Another objective is to develop new approaches to place individual quantum dots at plasmonic hot spots and study their properties.","2500000","2014-02-01","2019-01-31"
"QUADYNEVOPRO","Quasistatic and Dynamic Evolution Problems in Plasticity and Fracture","Gianni Dal Maso","SCUOLA INTERNAZIONALE SUPERIORE DI STUDI AVANZATI DI TRIESTE","This research project deals with nonlinear evolution problems that arise in the study of the inelastic behaviour of solids, in particular in plasticity and fracture. The project will focus on selected problems, grouped into three main topics, namely:
1. Plasticity with hardening and softening,     2. Quasistatic crack growth,     3. Dynamic fracture mechanics.
The analysis of the models of these mechanical problems leads to deep mathematical questions originated by two common features: the energies are not convex and the solutions exhibit discontinuities both with respect to space and time. In addition, plasticity problems often lead to concentration of the strains, whose mathematical description requires singular measures. Most of these problems have a variational structure and are governed by partial differential equations. Therefore, the construction of consistent models and their analysis need advanced mathematical tools from the calculus of variations, from measure theory and geometric measure theory,  and also from the theory of nonlinear elliptic and parabolic partial differential equations. The models of dynamic crack growth considered in the project also need results from the theory of linear hyperbolic equations.
Our goal is to develop new mathematical tools in these areas for the study of the selected problems. Quasistatic evolution problems in plasticity with hardening and softening will be studied through a vanishing viscosity approach, that has been successfully used by the P.I. in the study of the Cam-Clay model in soil mechanics. Quasistatic models of crack growth will be developed under different assumptions on the elastic response of the material and on the mechanisms of crack formation. For the problem of crack growth in the dynamic regime our aim is to develop a model that predicts the crack path as well as the time evolution of the crack along its path, taking into account all inertial effects.","968500","2012-03-01","2017-02-28"
"QUAGATUA","Quantum Gauge Theories and Ultracold Atoms","Maciej Lewenstein","FUNDACIO INSTITUT DE CIENCIES FOTONIQUES","""This is an interdisciplinary proposal which concerns physics of ultracold atoms and quantum information on one side, and high energy and condensed matter physics on the other. The main objectives are: i) to identify experimentally feasible  ultracold atom systems that may serve as quantum simulators of, or exhibit physics  relevant for  some challenging high energy systems and/or quantum gauge theories; ii) to study these ultracold atom systems, their properties and possibilities of control them for applications in quantum information and quantum metrology that go beyond the high energy physics and quantum simulations; iii) to make very concrete  experimental proposals of preparation, manipulation and detection of such systems. In particular, it is planned  to  investigate: A) ultracold atoms in artificial non-Abelian gauge fields, and non-Abelian integer and fractional Hall effects; B) ultracold atoms with Dirac like dispersion relations (as in relativistic field theories and/or graphene in condensed matter); C) ultracold atoms in polymerized geometries, i.e. in lattices of weakly coupled groups of neighbouring sites (plaquettes); D) ultracold gases in frustrated geometries that can mimic quantum spin liquids, that in turn are often described by Abelian discrete gauge theories; E) ultracold gases with 3- or 4-atom interactions that can serve as simulators of  Abelian lattice gauge theories; F) the ultimate, but rather risky and speculative  objective would be to find the possibilities of realizing quantum simulators of  non-Abelian gauge theories, i.e. to identify the dynamical degrees of freedom of """"gluons"""" and to distinguish them from that of matter fields. Expected results are: i) concrete proposals to simulate quantum gauge theories;, ii) better understanding of quantum gauge theories with ultracold atoms, iii) discovery of novel types of quantum gauge field systems; iv) novel systems for robust quantum information processing and metrology.""","1400000","2008-11-01","2013-12-31"
"QUAMI","The Quantum Microscope","Itzhak Yaron Silberberg","WEIZMANN INSTITUTE OF SCIENCE LTD","We propose to build an optical microscope that will use novel quantum optical concepts in order to break the Rayleigh-Abbe resolution limits of standard optical microscopy. Optical microscopy is still the workhorse of biological and medical research, allowing researchers direct visible view of the microscopic world, and any improvement in the field could have significant impact. Several innovative techniques have been demonstrated in recent years to achieve super resolution, most relate to fluorescence microscopy and requires highly nonlinear excitations and/or novel fluorescence probes, and therefore have more specific applications.
Our goal is to demonstrate a general-purpose machine, that is, a microscope that should be able to inspect general transparent or fluorescent objects, in particular biological and medical specimens, and will include several observation modalities. The high-resolution capabilities of the microscope will come from the application of novel photon-number resolving detectors and non-classical light sources. Our strategy is to build this microscope around a standard laser scanning microscope concept, yet we will achieve sub-diffraction imaging by resolving features within the classical diffraction limited spot of the scanning beam. Fast photon-number resolving detectors will record spatial and temporal distributions of photons at the image plane, enabling quantum correlations for enhanced resolution. We will consider several forms of illuminations   both classical and quantum light   and several microscope modalities, including fluorescence, dark field and differential interference contrast microscopy. We shall also investigate methods to combine quantum microscopy with nonlinear microscopy for further enhancement of resolution. Beyond its immediate goals, this research program will help to determine weather the more novel ideas of quantum metrology are indeed relevant for practical microscopy.","2112146","2011-04-01","2016-03-31"
"QUANTATOP","Quantum Atom Optics
from Entangled Pairs to Strongly Correlated Systems","Alain Aspect","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Ultra cold atoms offer unprecedented possibilities to shed a new light on intriguing quantum phenomenon that were discovered in Photon Quantum Optics (PQO), such as Hanbury Brown and Twiss correlations, Bell’s inequality tests of entanglement, Hong Ou Mandel effect, non classical states of light. It becomes possible to develop a Quantum Atom Optics (QAO), which is more than a simple analogue to PQO. Atoms add two new ingredients to the situations (i) controlled interactions, tunable from zero to giant values; (ii) the possibility to choose between fermions and bosons. The first part of this project aims at revisiting with this new perspective some milestones of Quantum Optics, and to address open questions like possible interaction induced decoherence effects. For this, we will develop single atom detectors and atom-atom correlation measurements techniques, both for metastable Helium and for alkali atoms, and build all optical cooling machines for these species, including a guided atom laser with control of the atomic interactions. We will also consider measurements below the standard quantum limits, to apply them to inertial and gravitational sensors based on atom interferometers.

In the second part of this project, experimental tools and concepts of QAO will be used to address fundamental questions of Condensed Matter Physics (CMP). A 1D horizontally guided Atom Laser will allow us to study transport properties of an interacting Bose gas in the presence of disorder, akin to conductivity measurements in CMP. Atom-atom correlation techniques developed to test Bell inequalities will allow us to investigate non trivial symmetries in paired atomic states BCS-like. Using larger samples of ultra-cold Bose or Fermi atoms, we will investigate the effect of interactions on Anderson localization in 1D, 2D and 3D, as well as other phenomenon beyond the mean field description, e.g. correlations in strongly interacting 1D quantum gases.","2130000","2011-08-01","2016-07-31"
"QuantCom","Ubiquitous Quantum Communications","Lajos HANZO","UNIVERSITY OF SOUTHAMPTON","''It is difficult to make predictions, especially about the future'' - Mark Twain.

Yet, Gordon Moore's predictions - known as Moore's Law - made in 1965 remained valid for half a century!

As a result, semi-conductor technology is approaching nano-scale
integration and on this journey to quantum futures the traveller
enters the world of quantum physics, where many of the phenomena are
rather different from those of classical physics. This proposal
contributes to the 'quantum jig-saw puzzle', with special emphasis on
the enabling techniques of ubiquitous quantum communications,
potentially leading to job- and wealth-creation on a similar scale to
the economic benefits of flawless classic wireless communications.

My ultimate goal as a telecommunications researcher is to build
bridges across the exciting fields of quantum physics, mathematics,
computer science and hardware aspects of quantum communications.
Specifically, the three Key Challenges of Work-Packages 1-3 on the new
concept of Pareto-optimum error control, secret key-distribution,
network coding and entanglement distribution will lead to creating
stepping-stones for the Grand Challenge of Work-Package 4, dedicated
to the support of quantum-communications for aircraft 'above the
clouds'.

Methodology: theoretical performance bounds will be established
 based on the hitherto unexplored Pareto-optimum quantum design
  philosophy using multi-component optimization. Explicitly, the
Pareto-front of optimal solutions will be found off-line, where none
of the conflicting parameters, such as the bit-error ratio, transmit
power, delay and implementation complexity can be improved without
degrading some of the others. A suite of new soft-decision aided
components will be conceived by invoking code-specific quantum
syndrome decoders to be designed for iterative soft-information
exchange without perturbing the fragile quantum states. Finally,
quantum-communications solutions will be created for drones and planes.","2496372","2018-06-01","2023-05-31"
"QUANTIF","Quantitative Multidimensional Imaging of Interfacial Fluxes","Patrick Unwin","THE UNIVERSITY OF WARWICK","Interfacial physicochemical processes are ubiquitous in chemistry, the life sciences and materials science, underpinning some of the most important scientific and technological challenges of the 21st century. The overarching aim of this proposal is to draw together separate strands of interfacial science by creating a unique holistic approach to the investigation of physicochemical processes and developing principles and methods which have cross-disciplinary application. To understand and optimise interfacial physicochemical processes, the major aspiration is to obtain high resolution pictures of chemical fluxes at a scale commensurate with our understanding of structure. The proposed research will address this need and break new ground by: (a) developing a family of innovative imaging methods capable of quantitatively visualising interfacial fluxes with unprecedented resolution that have wide application; and (b) establishing a common framework applicable to different fields of science through the usage of electrochemical principles. Experimental/instrumentation aspects will be supported by advanced modelling of mass transport-chemical reactivity. The research programme will focus on three distinct and important exemplar topics. (i) Electrochemical processes at new forms of carbon, including carbon nanotubes and graphene, where a major challenge is to identify the active sites for electron transfer. (ii) Membrane transport, where the goal is to identify the true factors controlling passive permeation across bilayer lipid membranes, with implications for understanding membrane function. (iii) Crystal growth/dissolution, where there is a major need to bridge kinetic and structural studies so as to understand the relationship between surface features and local flux. The project will allow a team of sufficient critical mass to be constituted to transfer knowledge between each area and establish a new way of addressing and understanding interfacial processes.","2129141","2010-09-01","2015-08-31"
"QUANTUM-N","Quantum Mechanics in the Negative Mass Reference Frame","Eugene Simon Polzik","KOBENHAVNS UNIVERSITET","A fundamental aspect of quantum mechanics is the balance between information and disturbance by the measurement. A textbook example is the measurement of a position of an object which imposes a random perturbation on its momentum. This perturbation, the quantum back action, translates with time into uncertainty of the motion trajectory. The PI has proposed an approach which allows for simultaneous measurements of arbitrary small disturbances in both the position and the momentum. It is based on a measurement performed in a quantum reference frame with an effective negative mass, or frequency for an oscillator. Recently the PI’s group has experimentally demonstrated the first step along this novel path - quantum back action evasion for the measurement of motion in a reference frame of a spin oscillator.

We propose a project which takes detection of motion to a new frontier. We will develop a novel hybrid quantum system involving disparate macroscopic objects, a mechanical oscillator and a reference spin oscillator with the effective negative mass. We will demonstrate quantum entanglement between the two oscillators and entanglement-enhanced sensing of force and acceleration. The technology for high quality mechanical and spin oscillators developed at the PI’s group will be further advanced towards those goals.

We will generate manifestly non-classical states of millimetre size mechanical oscillators and a macroscopic coherent superposition of distant spin and mechanical objects. We will furthermore work towards generation of multi-partite entangled states of spins, macroscopic objects, and photons, thus testing fundamental limits of entanglement and decoherence for large and complex systems. 

Gravitational wave interferometers which have recently detected first gravitational waves are expected to be soon limited in their sensitivity by the quantum back action. The way to overcome this limit using the approach developed within this project will be explored.","2178574","2018-07-01","2023-06-30"
"QUANTUMOPTOELECTR","Quantum Opto-Electronics","Leo Kouwenhoven","TECHNISCHE UNIVERSITEIT DELFT","We propose to develop an opto-electronics interface between single-electron devices and single-photon optics. The ultimate limit in the miniaturization of electronics and photonics is at the nanometer scale. Here the signal level can be controlled at the fundamental level of a single electron for electricity and a single photon for light. These limits are actively being pursued for scientific interest with possible applications in the new area of quantum information science. Yet, these efforts occur separately in the distinct communities of solid state electronics and quantum optics. Here we propose to develop a toolbox for interfacing electronics and optics on the level of single electrons and photons. The basic building block is a nanoscale pn-junction defined in a semiconductor nanowire, which is the most versatile material system for single electron to single photon conversion. We will develop the following technology: (1) growth of complex semiconductor nanowires (2) quantum state transfer for copying the information stored in an electron quantum state onto a photon state (3) single-photon optical-chip with on-chip guiding via single plasmons and on-chip detection with a superconducting detector. Besides being fundamentally interesting by itself, this new toolbox opens a new area of experiments where qubits processed in solid state nano-devices are coupled quantum mechanically over long distances via photons as signal carriers to various kinds of other interesting quantum system (e.g. solid state quantum dots, confined nuclear spins and atomic vapours).","1800000","2009-01-01","2013-10-31"
"QUANTUMPUZZLE","Quantum Criticality - The Puzzle of Multiple Energy Scales","Silke Buehler-Paschen","TECHNISCHE UNIVERSITAET WIEN","Matter at the absolute zero in temperature may reach a highly exotic state: Where two distinctly different ground states are separated by a second order phase transition the system is far from being frozen; it is undecided in which state to be and therefore undergoes strong collective quantum fluctuations. Quantum criticality describes these fluctuations and their extension to finite temperature. Quantum critical behaviour has been reported in systems as distinct as high-temperature superconductors, metamagnets, multilayer $^3$He films, or heavy fermion compounds. The latter have emerged as prototypical systems in the past few years. A major puzzle represents the recent discovery of a new energy scale in one such system, that vanishes at the quantum critical point and is in addition to the second-order phase transition scale. Completely new theoretical approaches are called for to describe this situation.  In this project we want to explore the nature of this new low-lying energy scale by approaches that go significantly beyond the state-of-the-art: apply multiple extreme conditions in temperature, magnetic field, and pressure, use ultra low temperatures in a nuclear demagnetization cryostat, and perform ultra-low energy spectroscopy, to study carefully selected known and newly discovered heavy fermion compounds. Samples of outstanding quality will be prepared and characterized within the project and, in some cases, be obtained from extrenal collaborators. New approaches in the theoretical description of quantum criticality will accompany the experimental investigations. The results are likely to drastically advance not only the fields of heavy fermion systems and quantum criticality but also the current understanding of phase transitions in general which is of great importance far beyond the borders of condensed matter physics.","2100043","2009-06-01","2015-05-31"
"QUANTUMRELAX","Non Equilibrium Dynamics and Relaxation in Many Body Quantum Systems","Hannes Jörg Schmiedmayer","TECHNISCHE UNIVERSITAET WIEN","Relaxation processes in many-body quantum systems arise in many diverse areas of physics ranging from inflation in the early universe to the emergence of classical properties in complex quantum systems. Ultracold atoms provide a unique opportunity for studying non-equilibrium quantum systems in the laboratory. The coherent quantum evolution can be observed on experimentally accessible timescales and the tunability in interaction, temperature and dimensionality allows the realization of a multitude of different relevant physical situations.

Through building specific model systems we propose to study a wide variety of non-equilibrium quantum dynamics under conditions ranging from weakly interacting to strongly correlated, from weakly disturbed to quantum turbulent and search for universal properties in non-equilibrium quantum evolution.

We address questions of de-coherence in a split many-body system and the concomitant emergence of classical properties. We will study the fate of the highly entangled quantum states that are created when a system in its excited state decays. Systems with instabilities and controlled quenches will give us insight into the creation of defects and excitations. We will experiment with bosons, fermions and mixtures, and take advantage of the rich internal structure of the atoms. Our systems will also be observed when interacting with ‘baths’, which can be internal or external with controlled coupling and can be engineered from simple thermal to squeezed, from large to mesoscopic with non-Markovian properties.

Our ultimate goal is insight into the answers to fundamental questions: What does it take for an isolated many-body quantum system with a set of conserved quantities to relax to an equilibrium state? Which universal properties and scaling laws govern its evolution? Can classical physics and thermodynamics emerge from quantum physics through the dynamics of complex many-body systems?","2025400","2013-06-01","2018-05-31"
"QUAREM","Quantitative Reactive Modeling","Thomas A. Henzinger","INSTITUTE OF SCIENCE AND TECHNOLOGYAUSTRIA","The project aims to build and evaluate a theory of quantitative fitness measures for reactive models. Such a theory must strive to obtain quantitative generalizations of the paradigms that have been success stories in qualitative reactive modeling, such as compositionality, property-preserving abstraction, model checking, and synthesis. The theory will be evaluated not only in the context of hardware and software engineering, but also in the context of systems biology. In particular, we hope to use the quantitative reactive models and fitness measures developed in this project for testing hypotheses about the mechanisms behind data from biological experiments.","2326101","2011-05-01","2016-04-30"
"QuDeT","Quantum devices in topological matter: carbon nanotubes, graphene, and novel superfluids","Pertti Hakonen","AALTO KORKEAKOULUSAATIO SR","The project addresses quantum devices in hybrid systems formed using carbon nanotubes, graphene, and 3He superfluid, all with particular topological characteristics. Topological properties of these non-trivial materials can be drastically modified by introducing defects or interfaces into them, like single layer graphene into superfluid helium, boron nitride between graphene sheets, carbon nanotubes in 3He superfluid, or misfit dislocation layers into HOPG graphite. 

We are particularly interested in graphene/3He systems where graphene acts as an interface/substrate of interacting atomic ensembles. The atomic interactions across graphene are expected to provide novel mesoscopic condensates. By studying the topological phases of thin 3He layers and graphene immersed into superfluid 3He, we will investigate pairing across the graphene interface, deduce the origin of supercurrents, and look for excitonic superfluidity in these systems.

Single walled carbon nanotubes provide high-quality nanomechanical resonators with extraordinary properties. By using proximity-induced superconductivity, these objects will be integrated into circuit optomechanics in a way that facilitates strong coupling between the mechanical motion and the microwave cavity. By using adiabatic nuclear refrigeration, these non-linear quantum objects will be cooled below 1 mK, at the temperature of which the quantum ground state is reached. The cooling relies on immersion of the SWNT into superfluid 3He which, in the limit T -> 0, provides a quantum vacuum with unique topological properties. Intriguingly, the characteristics of this vacuum can be probed by ultrasensitive detectors provided by the suspended SWNTs.

Finally, besides non-classical phonon states, e.g. Fock states in the mechanical resonator, reaching the ground state of such an anharmonic oscillator will allow studies of quantum tunnelling of a macroscopic object from its metastable minimum when biased with a large gate voltage.","2398536","2016-01-01","2020-12-31"
"QUENCH","Star formation quenching and feedback in galaxies throughout the cosmic epochs","Roberto MAIOLINO","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","""Throughout the whole life of the universe only 4% of the baryons have been
converted into stars, implying that some physical processes must be responsible
for suppressing star formation in galaxies. Within this context, one of the
most hotly debated open questions is the identification of the process
responsible for quenching star formation in galaxies and transforming them into passive and quiescent (gas poor) systems. Theories of galaxy
formation have proposed various possible mechanisms, such as: gas
removal by powerful outflows or ram pressure stripping, heating
and photoionization of the interstellar medium, turbulent or
gravitational quenching, halting of the gas supply inflow (often referred to as """"strangulation""""). The relevance and
relative role of these mechanisms (as a function of cosmic epoch, 
galaxy properties and environment), especially at high redshift, are not yet understood because the constraints provided
by current observational data have not yet been able to discriminate
between different scenarios.
In the proposed project I will make use of some of the most advanced 
observational facilities that will be available in the coming years
to tackle this major outstanding open issue. More specifically, I will
exploit the James Webb Space Telescope, MOONS (the next generation
multi-object spectrograph at the ESO-VLT) and the Atacama Large Millimeter
Array (ALMA).
Observing programs making use of these unique facilities
will provide an unprecedented amount of information, with unprecedented quality, that will enable 
us to discriminate between various quenching and feedback processes proposed
by theories. More specifically, the aim of this project is to
identify and quantify the dominant quenching and feedback mechanisms 
in galaxies as a function of redshift, as a function of galaxy properties
and as a function of environment. The groundbreaking results of this project will be
a benchmark for any model of galaxy evolution.""","2484531","2016-10-01","2021-09-30"
"QUENOCOBA","Quantum Emitters in non-conventional baths","Juan Ignacio Cirac","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","The coupling of quantum emitters to a common bath gives rise to intriguing quantum optical phenomena, like super-radiance, non-Markovian dynamics, or dipole-dipole interactions. In recent years, new experimental setups are emerging where non-conventional baths, with tailored dispersion relations, can be produced. In particular, atoms (the emitters) can be kept in the proximity of photonic crystals (the bath), whose properties can be engineered thanks to the spectacular progress experienced in nano-fabrication techniques. Low dimensional mate- rials, dispersion relations with Dirac points, or exotic topological features can be designed in photonic crystals, which will dramatically affect the behavior of the emitters. In this scenario, exotic phenomena, unique possibilities for quantum simulation of both coherent and dissipa- tive dynamics, as well as advanced applications are expected to appear. Presently, a new research area is emerging to investigate the physics of emitters coupled to non- conventional baths.

In this project we will: (i) develop the theoretical tools required to investigate this new area; (ii) explore and characterize novel phenomena; and (iii) propose and analyze other physical setups where those phenomena can be observed and exploited (in the context of quantum information and simulation). The research will involve the development of innovative tech- niques to describe new scenarios in quantum optics and many-body physics, as well as re- search on atoms interacting with photonic crystals, in optical lattices, and quantum dots in- teracting with surface acoustic waves.

This interdisciplinary project involves concepts and ideas from quantum optics, many-body quantum physics, and quantum information, research fields where the PI has a vast experi- ence. This, together with a close collaboration with leading experimentalists will provide us with a unique environment for the successful accomplishment of the objectives of the project.","1872969","2017-09-01","2022-08-31"
"QUEST","Quantum Entanglement in Electronic Solid State Devices","Christian Schoenenberger","UNIVERSITAT BASEL","""The quantum world is by far larger than the classical one. It is entanglement, closely linked to non-locality, that spans this larger space manifold. Entanglement plays a central role in emerging quantum technology aiming to harvest quantum space. From the experimentalist’s point of view working in nanoelectronics, there is no instrument on the shelf yet, that would measure the degree of entanglement. This we would like to change with QUEST.

QUEST is a long term project with the goal to experimentally establish a continuous probe of entanglement generation in the electrical signal of quantum devices. It is set up in two parts: the realization of a highly efficient source of spin-entangled electron pairs and the exploration of different correlation measurements providing a measure of entanglement “on the fly”.  During the last decade a wealth of theory proposals have appeared, addressing entanglement in electronic devices. The interaction of particles in solid-state devices provides a natural force for the appearance of entanglement. Examples are correlation between electrons and holes in the emission on a tunnel junction, or the “naturally” occurring Cooper pairs in s-wave superconductors. While first results on the realization of sources of entangled electron pairs have appeared recently, there are no experiments demonstrating entanglement in transport of any of those devices. We aim to change this and propose to implement high-bandwidth current correlation methods up to the forth moment, enabling to test Bell-inequality and quantum state tomo-graphy. Based on our long standing experience in the measurement of second-order correlations in nanodevices, we are well prepared for this very challenging goal.""","1999350","2012-04-01","2017-03-31"
"QUEST","Quantum Algebraic Structures and Models","Roberto Longo","UNIVERSITA DEGLI STUDI DI ROMA TOR VERGATA","This project aims to an innovative deep interplay between Operator Algebras and Quantum Field Theory. On one hand we want both to develop powerful tools to construct Quantum Field Theory models and provide a mathematical-conceptual description of interesting Physical contexts, on the other hand we want to set up and study the emerging mathematical structures, that have their own interest.  
Our first objective aims to an intrinsic description of phase boundaries (defects) in two dimensions, developing mathematical methods needed to this end. The operator-algebraic description of Boundary Conformal Field Theory by the K.-H. Rehren and the PI is the basis to set up the operator-algebraic, Minkowskian description of phase boundary, relating to the tensor categorical, Euclidean description by J. Fröhlich, J. Fuchs, I. Runkel and C. Schweigert. The theory of Subfactors by V. Jones and the PI's notion of Q-system are to be extended to unstudied settings and new basic operations are to be introduced and analyzed. Existing partial classification results will be broadened to more general, physically interesting situations.
A second objective aims to a non-perturbative construction of QFT models that relies on recent ideas, based on algebraic deformation, by E. Witten and the PI (in a massless context) and by G. Lechner (in a massive context), and further developed by other researchers. We aim at a unifying framework and new constructive methods.
A third objective plans to construct, and analyze, new classes of models of local Conformal Nets of von Neumann Algebras by means of Vertex Operator Algebras; among them the “Shorter Moonshine Net”.
A further objective points to understand known effects in Information Theory within the Noncommutative Geometrical viewpoint provided by a QFT index theorem proposed by the PI.","1587500","2015-12-01","2020-11-30"
"QUOMP","Quantum optics with microwave photons building a tool-box based on superconducting technology","Per Erik Delsing","CHALMERS TEKNISKA HOEGSKOLA AB","The research proposed in this application has grown out of the research on solid-state qubits, where a superconducting circuit including Josephson junctions can be made into a quantum-coherent, two-level system, an artificial atom. It has recently been shown that these artificial atoms can be integrated with microwave cavities in such a way that the states of the &quot;atom&quot; can communicate in a quantum coherent way with individual photons in the cavity. This opens up an opportunity to engineer quantum system utilizing both the atom and the photon degree of freedom. There are three essential features in this proposal, circuit-QED, tunable Josephson elements and the possibility to integrate many qubits and many cavities on the same chip. The overall objective of this proposal is to build a toolbox based on circuit-QED and tunable superconducting elements, to enable on-chip integrated quantum optics. Our vision is to move quantum optics experiments from large optical tables and integrate them on chip, with a substantially increased level of integration. Working in the microwave domain, we have the following specific objectives: &quot; An on-demand single photon source &quot; A number resolving single photon click detector &quot; A single photon router &quot; A single photon sluice &quot; A linear quantum limited parametric amplifier &quot; Demonstration of the dynamical Casimir effect","2500000","2010-03-01","2015-02-28"
"QUOWSS","Quantum Optics in Wavelength Scale Structures","John Rarity","UNIVERSITY OF BRISTOL","In this project I will investigate the interaction between quantum light and matter in optical structures that are at or below the wavelength scale. Such devices could provide unprecedented performance in the storage of data, the switching of light and the generation of light of tailored properties. I will address this topic through the study of 2-level systems (quantum dots, N-V centres, atoms), and non-linear materials (Ç(2) ,Ç(3) ) in various nanoscale dielectric and conducting structures. This will include: &quot; quasi 1D systems such as pillar microcavities, &quot; 2D systems such as microstructured fibres and suspended waveguide photonic bandgap cavities &quot; 3D systems such as single particle assembled 3D nano-cavities I will design suitable systems using the wide suite of electromagnetic modelling softwares available in my group. This will also involve the inclusion of the allowed modes and their interaction with single two-level quantum systems and non-linear materials.","2500000","2010-02-01","2015-01-31"
"QuReM","Cavity Quantum Optomechanics: Exploring mechanical oscillators in the quantum regime","Tobias Jan August Kippenberg","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","In recent years the development of cavity otomechanical coupling has enabled to achieve the quantum regime of mechanical systems in an experimental setting. The present proposal building on these developments and demonstrate mechanical oscillators ability as a quantum technology. It seeks to investigate and utilize coherent coupling of ground state cooled mechanical oscillators for storage of single photons in mechanical oscillators, explore backaction evading measurements, explore quantum dissipation due to two level systems, demonstrate novel transducers and explore an microwave to optical quantum link based on effective optomechanical interactions. The applicant has laid the basis for these objectives within the framework of an ERC Starting Grant (call 2007) that demonstated the methods to achieve the quantum regime of mechanical oscillator. The present proposal seeks to use the ERC Advanced Grant to directly build on the advances of this project.","2496000","2013-07-01","2018-06-30"
"QWORK","Quantum Chromodynamics at Work","Petrus Johannes Gerardus Mulders","STICHTING VU","Quantum Chromodynamics (QCD) is one of the cornerstones of the Standard Model of particle physics. It describes the world of quarks, anti-quarks and gluons (partons) making up the protons and neutrons and therewith the ordinary matter in our universe. Collisions of protons and heavy nuclei at unprecedented energies in the Large Hadron Collider (LHC) at CERN enable experiments that will uncover mechanisms and symmetries underlying the Standard Model. In experiments at the LHC, as in many other high-energy physics experiments, QCD plays a crucial role as a toolbox. It employs the property that at very high energies, or equivalently very short distances, the transition of protons to partons is a long distance phenomenon that can be encoded through parton probabilities and decay functions, which incorporate the complex structure of the proton itself.

Earlier I have revealed a new element in QCD: specific momentum-spin correlations can also be encoded in terms of (polarized) parton probabilities, collectively known as transverse momentum dependent (TMD) distribution and fragmentation functions. Experimental results have confirmed the applicability and the necessity of including the novel correlations in QCD in order to cope with current and upcoming experimental results. This proposal outlines my ambitions to develop the next generation of the QCD toolbox.

I want to break with the restrictions of the collinear approximation for partons in high-energy processes and develop the full QCD dynamics underlying the novel correlations, study their universality and make them into workable tools that enable a full manipulation of spins and momenta of the partons for understanding experimental results at all frontiers, energy and precision. The results of this enterprise will affect the entire field of high-energy/nuclear physics and open up new windows to reveal the fundaments of the Standard Model through dedicated experiments at present-day and future facilities.","2068946","2013-05-01","2018-10-31"
"R3S3","Research on Really Reliable and Secure Systems Software","Andrew Stuart Tanenbaum","STICHTING VU","Current operating systems have poor reliability and security. Computers crash regularly whereas other electronic devices such as televisions and mobile phones never crash. Furthermore, practically every week one reads about another security hole in Windows. As computers become more essential for all aspects of society this situation is unacceptable. The goal of my proposed research is to conceive, design, implement, and test an operating system that is as reliable and secure as is humanly possible. The job will be finished when the average user has never experienced a crash in his lifetime and RESET buttons on computers have passed into history, like 5¼ -inch floppy disks.       The basic concept I want to use to achieve a reliable, secure operating system is the POLA The Principle of Least Authority. The operating system will be moved out the kernel (where it has unrestricted access to all of memory, critical machine instructions the I/O devices) into a set of multiple, tightly constrained user processes. Each process (e.g., a file server) will be given exactly the authority it needs to do its job and no more. This mechanism ensures that problems in one process cannot spill over into other ones. While this goal has floated around for years, no one really knows how to do it, so research is needed. Furthermore, I also want to make the system fault tolerant and self healing so it can continue to run even in the presence of hardware and software errors. Recovery should be done automatically without affecting running programs.      Designing and building a new operating system that runs counter to 50 years of experience is extremely ground-breaking and ambitious. But the current road we are on with millions of lines of code in the kernel and growing all the time cannot be sustained. We need research that will lead to much better reliability and security. I have 30 years experience in the field and think I have a chance to pull it off.","2448420","2008-11-01","2014-04-30"
"RACE","Reasoning About Computational Economies","Michael John Wooldridge","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The aim of this project is to develop, apply, and evaluate RACE: a robust and practical model checking tool for use in the formal specification, verification, and analysis of computational economies – computer systems in which system components are assumed to have their own goals/preferences about the overall behaviour of the system, and where these system components are assumed to behave selfishly and strategically in the furtherance of their goals/preferences. The key deliverables of the project will of course include the RACE system itself, and in addition: new formalisms for representing and reasoning about computational economies, suitable for use in the RACE system and elsewhere; theoretical results (e.g., complexity analyses, axiom systems, . . . ) relating to the use of these formalisms; new algorithms and data structures for the verification and analysis of computational economies; and a library of case studies, demonstrating the use of RACE in a variety of settings. The project is both timely and essential. It is timely because computer networks populated by multiple self-interested computational entities are increasingly the reality of computing in the 21st century, and as a consequence, research in this area has witnessed a huge explosion of interest recently. It is essential because current formalisms, tools, and techniques for the specification, analysis, and verification of systems were not intended for, and are not appropriate for, this emerging reality. The project will build on two decades of enormously influential research by the PI, who is among the most highly cited researchers in computer science and artificial intelligence today.","2129617","2012-06-01","2018-05-31"
"RadioLife","Exploiting new radio telescopes to understand 
the role of AGN in galaxy evolution","Raffaella Morganti","STICHTING ASTRON, NETHERLANDS INSTITUTE FOR RADIO ASTRONOMY","An intricate interplay between various, and sometimes, competing processes, rules the formation of galaxies. Accretion of gas leads to the formation of new stars and the growth of the central massive black hole. In return, the energy released by these processes can drive strong outflows of gas, profoundly affecting the gaseous medium in and around a galaxy. This mechanism of energy feedback is now a key ingredient in models of structure formation. However, its implementation is still largely ad hoc and far from realistic..
Observations of radio AGN suggest that this type of active nucleus can play an important role in feedback. Their large jets and lobes can efficiently couple with the ISM/IGM and transport energy to large distances, preventing gas from cooling and, therefore, providing a self-regulating mechanism for gas accretion.
This proposal requests funding to set up a science team dedicated at explore the full impact of radio AGN on galaxy evolution. The possibility of making a major breakthrough in this field has become possible thanks to two new and revolutionary radio facilities becoming available in Europe: the LOw Frequency ARray (LOFAR), now in its commissioning phase, and the focal-plane array system Apertif to be installed starting 2013 on the Westerbork Synthesis Radio Telescope (WSRT). Both instruments use state-of-the-art technology and innovative approaches in all aspects of observing, calibration, processing and in the exploitation of radio data. They are among the most exciting pathfinders geared towards the realization of Square Kilometer Array (SKA).
The scientific goals will be achieved by understanding the duty cycle of radio sources, i.e. how often the radio-loud phase appears in the life of a galaxy, and by quantifying their role in feedback effects by tracing massive gaseous outflows resulting from the impact of radio jets and lobes on the ISM/IGM of the host galaxy.","2441923","2013-04-01","2018-03-31"
"RandomZeroSets","Zero sets of random functions","Mikhail SODIN","TEL AVIV UNIVERSITY","""The proposed research is focused on zero sets of random functions. 
This is a rapidly growing area that lies at the crossroads of analysis, 
probability theory and mathematical physics. Various instances of zero 
sets of random functions have been used to model different phenomena 
in quantum chaos, complex analysis, real algebraic geometry, and 
theory of random point processes.

The proposal consists of three parts. The first one deals with asymptotic 
topology of zero sets of smooth random functions of several real variables. 
This can be viewed as a statistical counterpart of the first half of Hilbert's 16th 
problem. At the same time, it is closely related to percolation theory.

In the second and third parts, we turn to zero sets of random analytic functions 
of one complex variable. The zero sets studied in the second  part provide one 
of few natural instances of a homogeneous point process with suppressed 
fluctuations and strong short-range interactions. These point processes have 
many features, which are in striking contrast with the ones of the Poisson point 
process. One of these features is the coexistence of different Gaussian scaling 
limits for different linear statistics.

The third part deals with zeroes of Taylor series with random and pseudo-random 
coefficients. Studying these zero sets should shed light on the relation between 
the distribution of coefficients of a Taylor series and the distribution of its zeroes, 
which is still """"terra incognita'' of classical complex analysis.""","1658750","2016-10-01","2021-09-30"
"RANMAT","""Random matrices, universality and disordered quantum systems""","Laszlo Erdös","INSTITUTE OF SCIENCE AND TECHNOLOGYAUSTRIA","""Large complex systems tend to develop universal patterns that often represent their essential characteristics. A pioneering vision of E. Wigner was that the distribution of the gaps between energy levels of complicated quantum systems depends only on the basic symmetry of the model and is otherwise independent of the physical details. This thesis has never been rigorously proved
for any realistic physical system but experimental data and extensive numerics leave no doubt as to its correctness. Wigner also discovered that the statistics of gaps can be modelled by eigenvalues of large random matrices. Thus the natural questions, “How do energy levels behave?” and “What do eigenvalues of a typical large matrix look like?”, have surprisingly the same answer! This project will develop new tools to respond to the two main challenges that Wigner’s vision poses for mathematics.
First, prove that a large class of natural systems exhibits universality. The simplest model is the
random matrix itself, for which the original conjecture, posed almost fifty years ago, has recently been solved by the PI and coworkers. This breakthrough opens up the route to the universality for more realistic physical systems such as random band matrices, matrices with correlated entries and random Schrödinger operators. Second, eigenvalue statistics will be used to detect the basic dichotomy of disordered quantum systems, the Anderson metal-insulator transition. Third, describe the properties of the strongly correlated eigenvalues viewed as a point process.
Although this process appears as ubiquitous in Nature as the Poisson process or the Brownian motion, we still know only very little about it. Due to the very strong correlations, the standard toolboxes of probability theory and statistical mechanics are not applicable. The main impact of the
project is a conceptual understanding of spectral universality and the development of robust analytical tools to study strongly correlated systems.""","1754717","2014-03-01","2019-02-28"
"RATIONAL POINTS","Fundamental groups, etale and motivic, local systems, Hodge theory  and rational points","Hélène Esnault","FREIE UNIVERSITAET BERLIN","From the viewpoint of geometric classification, there are two extreme cases of smooth varieties X defined over an algebraically closed field: those which are hyperbolic, and those which are rationally connected.  If k is no longer algebraically closed, a central question of Algebraic Arithmetic Geometry is what properties of k force  X to have a rational point in those two opposed cases.   It is conjectured (Lang-Manin, extended by Kollár), that rationally connected varieties have a rational point over a C1 field. It has been shown for function fields by Graber-Harris-Starr  and by myself over a finite field. There is no relation between their geometric proof relying on the geometry of the moduli of punctured curves and my proof relying on motivic analogies between Hodge level and slopes in l-adic cohomology. The study of the case of the maximal unramified extension of the p-adic numbers might provide a bridge through the use of the inertia.  Very little is known on Grothendieck's section conjecture, which predicts that sections of the Galois group of k, assumed to be a finite type over Q,  into the arithmetic fundamental group of X, are given by rational points. Our hope goes in two directions, arithmetic and geometric on one side, motivic on the other.  With Wittenberg, we hope to use Beilinson's geometric description of the nilpotent completion of the fundamental group, and with Levine, we wish to characterize sections of the motivic groups of mixed Tate motives over k and X and relate this to the section conjecture.","894000","2009-01-01","2014-12-31"
"ReactionBarriometry","Towards a chemically accurate description of reactions on metal surfaces","Gerardus Johannes Kroes","UNIVERSITEIT LEIDEN","This proposal attacks the four major challenges facing theorists who aim to make accurate predictions for reactions of molecules on metal surfaces. The research is curiosity driven, but also of practical importance to an accurate description of heterogeneous catalysis, which enables the production of > 90% of man made chemicals. The central goal is to enable the ab initio computation of chemically accurate barrier heights for reactions with metal surfaces of catalytic interest.
In the first challenge addressed, to establish the accuracy of a new electronic structure method we will test whether specific reaction parameter density functional theory (SRP-DFT) can describe reactions like dissociation of N2 on Ru(0001), CH4 on Pt(111), and H2 on Pt containing surfaces of catalytic interest with chemical accuracy. We will try to put SRP-DFT on a ab initio basis by fitting SRP density functionals to single point Quantum Monte-Carlo calculations. Second, we aim to achieve an accurate description of the effect of surface phonons on reaction through implementing Ab Initio Molecular Dynamics calculations on systems like CHD3 + Pt(111), CH4 + Pt(533), and N2 + Ru(0001). Third, we additionally aim to achieve an accurate description of the effect of electron-hole pair excitation on reaction in systems like N2 + Ru(0001) by implementing a new method called Ab Initio Molecular Dynamics with Electronic Friction, using a novel and efficient way to accurately compute the required friction coefficients. The fourth goal is to achieve an accurate quantum dynamical description of the reaction of hydrogen containing polyatomic molecules at surfaces at incidence energies of catalytic interest (in the quantum regime). The quantum dynamics calculations will treat all molecular degrees of freedom and one surface mode, which will ultimately enable detailed interpretations of recently observed mode-selectivity, bond-selectivity, and steric effects in the reaction of methane with metal surfaces.","2499995","2014-01-01","2018-12-31"
"READI","Reaction-Diffusion Equations, Propagation and Modelling","Henri Berestycki","ECOLE DES HAUTES ETUDES EN SCIENCES SOCIALES","Our goal is to accomplish a leap forward in the knowledge on propagation phenomena in reaction-diffusion equations, in heterogeneous media and/or non standard diffusion, systems as well as non local interactions. This proposal deals both with the general theory of nonlinear PDE’s of elliptic and parabolic type as well as with the development and study of some specific models. These range from ecology, medicine, mathematical economics  and social sciences.

Reaction-diffusion models, especially in ecology (for instance those describing biological invasions), feature long range interactions and heterogeneities, whose understanding is a current outstanding challenge. Models in theoretical medicine couple multi-scale phenomena to complex geometries and mixtures of local and nonlocal interactions. Economy is a constant source of new
and  nonstandard free boundary problems. We therefore propose to bring our expertise in propagation phenomena for reaction-diffusion, calculus of variations and free boundary problems, to treat a large class of these new models. The level of both generality and precision we are aiming at has not, to our knowledge, been reached before.
The project is especially timely: on the one hand, the international activity in reaction-diffusion equations and all related topics is intense. On the other hand, the modelling activity in theoretical biology, ecology, medicine and social sciences is experiencing a considerable growth. The  PI of this proposal being at the leading edge of both fields, there is a unique occasion to give a new impulse to a domain that is important both to mathematical analysis and to its potential applications.","1542055","2013-02-01","2019-01-31"
"REALM","Re-inventing Ecosystem And Land-surface Models","Iain Colin PRENTICE","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","Terrestrial ecosystems respond to changes in climate and the atmospheric environment, which they in turn help to regulate. As global change has become an international concern, high expectations have been laid on Earth system models with embedded ecosystem and biophysical land-surface components to deliver reliable, quantitative predictions of large-scale changes in ecosystems and their feedbacks to the climate system. But the lack of established quantitative theory for many fundamental processes – such as the long-term effects of temperature on primary production and carbon allocation, the sustainability and nutrient requirements of CO2 ‘fertilization’, and the regulation of green vegetation cover and its water use – has made such expectations impossible to fulfil. As a result, numerical models of land ecosystem processes continue stubbornly to disagree both with one another, and with benchmark data sets.

This impasse can be overcome, but not without re-thinking modelling practice. Theory must be re-instated as the required link between observations and models. Multidisciplinary data resources now available should be used far more extensively and creatively. Observational and experimental results should be integral to model development, not merely used for ‘end-of-pipe’ testing of complex, poorly constrained models. I propose to develop a comprehensive, next-generation vegetation model using eco-evolutionary optimality hypotheses to generate testable predictions, and multiple data sources to provide tests. Initial results have demonstrated the remarkable power of this ‘strong inference’ approach to explain patterns seen in nature. The project will transform the practice of global vegetation and land-surface modelling and in doing so, establish the foundations of a more robust, quantitative understanding of the role of terrestrial ecosystems in Earth System dynamics.","2499615","2018-10-01","2023-09-30"
"REALTIME","Real Time Computational Mechanics Techniques for Multi-Fluid Problems","Sergio Idelsohn","CENTRE INTERNACIONAL DE METODES NUMERICS EN ENGINYERIA","The simultaneous presence of several different fluids in external or internal flows is found in daily life, environment and numerous industrial processes. These types of flows are termed multi-fluid flows. Examples are gas-liquid transport, crude oil recovery, spray cans, sediment transport in rivers, pollutant transport in the atmosphere, cloud formation, fuel injection in engines, bubble column reactors and spray driers for food processing, to name only a few. Real time computational mechanics (RTCM) aims to developing computational systems to solve problems which must produce their results within short time intervals. Examples of real-time systems include flight control programs, patient monitoring; nuclear plants controls, industrial processing and prevention of risk. RTCM systems are having an ever increasing impact on the quality of human life. The objective of the project is to develop new formal approaches and computational methods based on innovative RTCM procedures for building accurate and robust quasi-real time computer codes applicable to solve multi-fluid engineering problems. In the project we will develop and validate new computational fluid dynamic techniques based on innovative particle methods, new time integration schemes allowing large time steps, reduction methods, GPUs and parallel processing to reach acceptable results for multi-fluid problems in quasi-real time. The main outcome of the REALTIME project will be a collection of methods and codes to predict and control in quasi-real time different problems involving natural and human induced hazards such as risky industrial processes, fire spread, critical atmospheric situations or patients monitoring situations in which the security or human life depends of a response in real time.","2478000","2009-12-01","2014-11-30"
"ReCaP","Regeneration of Articular Cartilage using Advanced Biomaterials and Printing Technology","Fergal O'BRIEN","ROYAL COLLEGE OF SURGEONS IN IRELAND","Adult articular cartilage has a limited capacity for repair and when damaged or injured, experiences a loss of function which leads to joint degeneration and ultimately osteoarthritis.  Biomaterials-based treatments have had very limited success due to the complex zonal structure of the articular joint,  problems with biomaterial retention at the joint surface and achieving integration with the host tissue while also maintaining load bearing capacity.  Stem cell therapies have also failed to live up to significant hype for a number of reasons including the challenges with achieving formation of stable hyaline cartilage which does not undergo hypertrophy.   Building on a wealth of experience in the area, we propose a solution. ReCaP will initially overcome the problems with traditional biomaterials approaches by utilising recent advances in the area of advanced manufacturing and 3D printing to develop a 3D printed multi-layered scaffold with pore architecture, mechanical properties and bioactive composition tailored to regenerate articular cartilage, intermediate calcified cartilage and subchondral bone.  Following this, and building on internationally recognised pioneering research in the applicant’s lab on scaffold-mediated nanomedicine delivery, this system will be functionalised for the controlled non-viral delivery of nucleic acids (including plasmid DNA and microRNAs) to direct host stem cells to produce stable hyaline cartilage at the joint surface and encourage the rapid formation of vascularised bone in the subchondral region. A new paradigm-shifting surgical procedure will then be applied to allow this system to be anchored to the joint surface while directing host cell infiltration and tissue repair, thus promoting restoration of even large regions of the damaged joint through a joint surfacing approach.  The proposed ReCaP platform is thus a paradigm shifting disruptive technology that will revolutionise the way joint injuries are treated.","2999410","2018-08-01","2023-07-31"
"RECGLYCANMR","Breaking the limits in glycan recognition by NMR","Jesús JIMÉNEZ-BARBERO","ASOCIACION CENTRO DE INVESTIGACION COOPERATIVA EN BIOCIENCIAS","Carbohydrates (glycans, sugars) play key roles in virtually all biological events. Given their chemical complexity, understanding their roles in nature requires a multidisciplinary approach. Research in the field is growing, since advances in the area could be part of the solution to many health issues. However, we lack full knowledge on the role of most glycan-mediated events especially at the resolution required from a chemical perspective to manipulate them and create new probes and eventually drugs. Understanding sugar recognition remains a major challenge in science. Although X-ray diffraction has been employed to study sugar/protein complexes, a recent report has highlighted that most sugar conformers deposited in the Protein Data Bank are incorrect. Flexible glycans are handled poorly in X-ray: errors reflect incorrect refinement of sugars, with huge implications when interpreted in the biocontext.I propose to address glycan recognition by using a multidisciplinary approach, combining synthesis, molecular biology and biophysics, with a prominent role for NMR. In RECGLYCANMR I will develop new NMR protocols to decipher key glycan recognition aspects beyond current knowledge: the role of presentation and dynamics and understanding the mechanisms behind the exquisite receptor and ligand selectivity. Importantly, till now, sugar recognition NMR studies have been exclusively limited to in vitro. RECGLYCANMR will break the limits of NMR, studying the interactions in-cell, a crowded ambient where viscosity is doubled respect to water. I am in a unique position to approach this project due to my wide expertise in NMR and the network of collaborators I have established for years, enabling me to access a large variety of synthetic sugars. Discovering the molecular bases of in-cell interactions will provide groundbreaking information on sugar chemical biology and will open unexplored avenues for approaching sugar-associated diseases, as inflammation and viral infections.","2499980","2018-09-01","2023-08-31"
"REE-CYCLE","Rare Earth Element reCYCLing  with Low harmful Emissions","Thomas Nicolas Zemb","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","""It is a matter of strategic independence for Europe to urgently find processes taking into account environmental and economic issues, when mining and recycling rare earths. Currently THERE ARE NO SUCH INDUSTRIAL PROCESS AVAILABLE and 0% WASTE RECYCLING of RARE EARTH ELEMENTS (REE). Plus,  97% of the mining operations are performed in China, hence representing a major Sword of Damoclès for the rest of the world’s economy.

We propose to develop a new, cost effective and environmentally friendly REE recycling process. We will achieve this: (i) by enabling, for the first time ever, the fast measurement of free energy of mass transfer between complex fluids; hence it will now be possible to explore an extensive number of process formulations and phase diagrams (such a study usually takes years but will then be performed in a matter of days); (ii)  develop predictive models of ion separation  including the effect of  long-range interactions between metal cations and micelles; (iii) by using the experimental results and prediction  tools developed, to design an advanced & environmentally friendly process formulations and pilot plant; (iv) by enhancing the extraction kinetics and selectivity, by implementing a new, innovative and selective triggering cation exchange process step (ca. the exchange kinetics of a cation will be greatly enhance when compared to another one). This will represent a major breakthrough in the field of transfer methods between complex fluids.

An expected direct consequence of REE-CYCLE will be that acids’ volumes and other harmful process wastes, will be reduced by one to two orders of magnitude. Furthermore, this new understanding of mechanisms involved in selective ion transfer should open new recycling possibilities and pave the way to economical recovery of metals from a very rapidly growing “mine”, i.e. the diverse metal containing “wastes” generated by used Li-ion batteries, super-capacitors, supported catalysts and fuel cells.""","2255515","2013-07-01","2018-06-30"
"REGENKNEE","Re engineering and regenerating the knee","John Fisher","UNIVERSITY OF LEEDS","Over fifteen percent of the western population suffer from osteoarthritis. For severe disability, total joint replacement remains the onlty treatment. The ageing population is growing and their expectations for an active, high quality lifestyle is increasing. The number of knee joint replacements is predicted to increase five time by 2025. While technology for knee joint replacements has advanced considerably over the last decade,  knee joint replacements do yet restore the natural function or provide the longevity expected for a population expecting - fifty more active years after fifty. An alternative is needed.
We wil address the potential of regenerative biological scaffolds to re-engineer and regenerate the knee in early stage degenerative disease. We will build upon our successful research and clinical trials in the application of regenerative biological scaffolds in the cardiovascular system.. We will research and develop bioprocesses to produce a portfolio of tissue specific regenerative biological scaffolds to replace disrupted and degenerative tissues in the knee, including ligaments, meniscus, bone and cartilage. and evaluate their biomechanical, biotribological and biological function. We will investigate the regenerative potential of these novel biological scaffolds individually and collectively in the knee as a bioengineering system.
This research will open the opportunity for a paradigm shift for treatment of degenerative disease in the knee and offer the potential for substantially delaying the need for a joint replacement in hundreds of thousands of patients every year.","2495134","2011-04-01","2016-03-31"
"REGULARITY","Regularity and Irregularity in Combinatorics and Number Theory","Endre Szemeredi","MAGYAR TUDOMANYOS AKADEMIA RENYI ALFRED MATEMATIKAI KUTATOINTEZET","""Regularity and irregularity plays a central role in mathematics. In the present research proposal we will select problems from combinatorics and number theory (including additive combinatorics), where regularity and irregularity appear. In some cases we have to deal, e.g., with arbitrary finite or infinite subsets of natural numbers, where the only information we have is  their cardinality, namely, that they are of positive (lower asymptotic) density within the set of all natural numbers or within the interval [1,N] for a large N. In other cases we consider an arbitrary distribution of n points within the unit square, where all we know is the density of our point set. The goal is often to show that certain configurations appear within the arbitrary set of numbers or points. These configurations definitely appear in a random set of numbers or points, but we have to show this for an arbitrary set of numbers or points with certain general properties. In order to reach our goal one can use two well-known methods. The first one is deterministic, often some kind of greedy algorithm. The second is the probabilistic method of Erdős, which shows that almost all arrangements of the given points or numbers (or graphs) fulfill the wanted property. A third method, the so called pseudorandom method, was initiated by the PI (together with M. Ajtai and J. Komlós), uses a combination of these. In other cases we have a deterministic set of numbers with certain quasi-random properties, for example, the primes. Randomness was the key idea in the recent breakthrough of Green and Tao, in proving that primes contain arbitrarily long arithmetic progressions. We will deal with 6 groups of problems: (i) finite or infinite sequences of integers, (ii) difference sets and Fourier analysis, (iii) graph and hypergraph embedding theorems, (iv) Ramsey theory, (v) distribution of points in the plane and in the unit square, (vi) regularities and irregularities in the distribution of primes.""","1776000","2013-03-01","2018-02-28"
"REMOCEAN","Remotely sensed biogeochemical cycles in the ocean","Hervé Claustre","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","In the context of global change and ocean response to climatic and anthropogenic forcing, it is critical to improve our understanding of biologically mediated carbon fluxes and to reduce the uncertainties in their estimates. At the root of much of the present uncertainty in carbon budget is the scarcity of data. Based on state-of-the-art remotely-operated techniques of observation (profiling floats and satellite) and bio-optical modelling, REMOCEAN aims at addressing the causes of variability in the so-called biological oceanic pump in key oceanic areas: (1) the North Atlantic (especially Labrador Sea, Irminger Sea, Iceland basin), which despite representing only 1.4% of the ocean s area, accounts for about 20% of the global ocean carbon sink; (2) the sub-tropical gyres of the Atlantic and Pacific for which, although they represent ~ 60% of the ocean surface, the contribution to oceanic carbon cycle is still a matter of debate. The scientific objectives of REMOCEAN will be implemented along four main activities. &quot; Development of profiling floats to measure oceanic variables essential for the characterization of phytoplankton dynamics and related carbon fluxes. &quot; Deployment of these floats in the four sub-tropical gyres of the Pacific and Atlantic Oceans and in the North Atlantic to conduct a totally automated investigation of biogeochemical cycles in these areas over a continuum of temporal scale and over a period of 3-4 years. &quot; Development of parameterisations linking surface biogeochemical properties to their vertical distribution in the ocean interior, and ultimately development of 3D fields of these properties by combining float and satellite data. &quot; Estimation of carbon fluxes by combining these fields with bio-optical modelling including retrospective analyses thanks to satellite data archives. A large part (~2 M¬) of the requested budget will be dedicated to the development, the acquisition and the functioning of the floats.","3322000","2010-06-01","2016-05-31"
"ReSuNiCo","Inverted Reactive Spray Processes for Sulphide/Nitride High Surface Area Electrode Coatings","Lutz MÄDLER","UNIVERSITAET BREMEN","Highly pure, binary and ternaty metal sulphides/nitrides are increasingly important materials for energy storage, electrocatalysis, optoelectronics and battery materials. To fully use their potential, radical new technologies that allow the synthesis of complex, and multicomponent crystalline materials with specific size and morphology are required. While the reactive spray technology is already a key element for the scalable and economic synthesis of metal oxides, we will fundamentally advance the strength of the reactive spray processes by generating a knowledge-base for sulphide/nitride materials through our ReSuNiCo project. We will achieve this goal with a fast, safe, versatile, time and resource efficient high throughput single droplet combustion screening that identifies complete new reaction schemes and processes as we highlighted in Nature news. The method is highly flexible and adaptable to a large variety of reactive liquids and gas atmospheres that readily comply with the safety requirements via small volumes, small liquid streams and gas flows. We will establish in-situ process diagnostics in order to identify droplet reactions, particle formation pathways and product characteristics. We will use this knowledge to build standard and inverted (fuels and sulfidizers/nitridizers are exchanged in the reactive spray) lab-scale reactors that serve as demonstrators to transfer the first material samples into performance evaluations in specific applications. The objectives and work packages of ReSuNiCo reach far beyond the state of the art materials synthesis exploration and calls for new process innovations in reactive spraying technologies, aerosol and gas phase characterizations, process model formulations and particle synthesis. The implemented know-how in in-situ high surface area coatings on electrodes/substrates offers unique opportunities to take the existing knowledge to the next level.","2361130","2019-01-01","2023-12-31"
"RG.BIO","Renormalization group approach to the collective behaviour of strongly correlated biological systems","Andrea CAVAGNA","CONSIGLIO NAZIONALE DELLE RICERCHE","Biological systems displaying collective behaviour are characterized by strong spatio-temporal correlations, which partly transcend the multiform diversity of their microscopic details, much as it happens in statistical physics systems close to a critical point. Recent experiments show that collective biological systems conform to another hallmark of statistical physics, namely scaling. Scaling laws have been found to be valid both at the static and at the dynamic level, although with critical exponents unlike any known model.  Building on the experimental evidence of strong correlations and scaling laws, I will develop a novel renormalization group (RG) approach to strongly correlated biological systems, with the purpose of classifying into new universality classes the collective phenomena of life. The key theoretical idea of the method is to perform an expansion around an equilibrium field theory in which the interaction network between the individuals in the group is fixed, and to assess the stability of the resulting RG fixed points with respect to weak off-equilibrium effects, complementing the RG flow with the standard loop expansion around the upper critical dimension. The renormalization group will allow us to go beyond the hydrodynamic regime of small fluctuations, thus pushing theoretical prediction into the critical region of the parameters, where the correlation length scales with the system's size, a region experiments show to be inhabited by several biological systems. To define the starting field theory in the most economic way I will study the role of effective symmetries in giving rise to weakly conserved quantities and non-dissipative terms, and assess their stability under the RG flow. Finally, my lab will conduct new experimental campaigns on flocks (starlings, swifts), swarms (midges, mosquitoes) and cell colonies, to explore and test the new universality classes identified by the renormalization group approach.","2301250","2018-10-01","2023-09-30"
"RGDD","Rigidity and global deformations in dynamics","Sebastian Van Strien","IMPERIAL COLLEGE OF SCIENCE TECHNOLOGY AND MEDICINE","""Proposal : While there have been tremendous advances in one-dimensional dynamics, higher-dimensional systems are far less well understood. This novel programme of work will take paradigms from one-dimensional dynamics to apply to the higher-dimensional case. In so doing, it will answer a number of long-standing questions of major significance and open up the field for sustained future investigation. The current emphasis on conformal (complex-analytic) techniques will be supplemented in the higher-dimensional setting by drawing on a combination of techniques from dynamics, geometry and analysis.
•	Take observations generated by a (possibly chaotic) dynamical system. Do their averages converge? Outcome A will show that for ‘typical’ one-dimensional systems this is indeed the case. This will solve a famous conjecture of Palis and that such maps are stochastically stable.
•	Outcome B will rule out certain pathologies in higher-dimensional systems with sufficient regularity. To do this, I will use the one-dimensional paradigm of rigidity associated to smoothness and build on work in progress to deal with the higher dimensionality.
•	Specifically, Poincare ́ asked whether recurrent orbits can be shadowed by periodic orbits for a system with nearby parameters. Outcome C will answer his question in a particular setting, using the one- dimensional paradigm of global deformations and higher-dimensional techniques.
•	Outcome D will give insight into dynamical systems associated to learning models in economics and game theory – concentrating on models that are either piecewise-affine, have time averages which are essentially piecewise affine, or can be viewed as stochastically perturbed systems. Systems associated to random graphs and coupled networks will also be investigated.
The mathematical methods in these objectives are interlinked, and straddle pure and applied dynamics. This combined approach will greatly re-energise the field.""","2228829","2014-03-01","2019-02-28"
"RHEOLITH","Rheology of the continental lithosphere, a geological, experimental and numerical approach","Laurent Jolivet","UNIVERSITE D'ORLEANS","A better comprehension of the rheology of the lithosphere is required to relate long and short term deformation regimes and describe the succession of events leading to earthquakes. But our vision of the rheology is blurred because gaps exist between visions of geologists, experimentalists and modellers. Geologists describe the evolution of a structure at regional-scale within geological durations. Specialists of experimental rheology control most parameters, but laboratory time constants are short and they often work on simple synthetic rocks. Specialists of modelling can choose any time- and space-scales and introduce in the model any parameter, but the resolution of their models is low compared to natural observations, and mixing short-term and long-term processes is uneasy. It seems now clear that there is not one rheological model applicable to all contexts and that rheological parameters should be adapted to each situation. We will work on exhumed crustal-scale shear zones and describe them in their complexity, focussing on strain localisation and high strain structures that can lead to fast slip events. A number of objects will be studied, starting from geological description (3D geometry, P-T-fluids estimates and dating), experimental studies of rheological properties of natural sampled rocks and numerical modelling. We will set an Argon-dating lab to work on dense sampling for dating along strain gradients in order to overcome local artefacts and quantify rates of strain localisation. We will deform in the lab natural rocks taken from the studied objects to retrieve adapted rheological parameters. We will model processes at various scales, from the lab to the lithosphere in order to ensure a clean transfer of rheological parameters from one scale to another.","2645000","2012-08-01","2017-07-31"
"RheoMan","MULTISCALE MODELLING OF THE RHEOLOGY OF MANTLE MINERALS","Patrick Cordier","UNIVERSITE DES SCIENCES ET TECHNOLOGIES DE LILLE - LILLE I","Understanding mantle convection is essential to understand the thermal and chemical evolution of the Earth and to constrain the forces driving plate tectonics. The rheological properties of the mantle are traditionally inverted from surface geophysical data. Radial profiles of the viscosity are thus available but a lot of uncertainties remain.

A more detailed model of mantle rheology could be obtained from the knowledge of the constitutive flow laws of mantle phases. A lot of progresses have been achieved to extend the P, T range accessible to rheological studies. However, constitutive flow laws are only available so far for minerals from the upper mantle. More severe is the timescale issue since phenomenological flow laws must be extrapolated over several orders of magnitude to be applied to mantle convection.

Recently, a new field has emerged in materials science called multiscale modelling. It allows to link our understanding of a few elementary mechanisms (usually at the microscopic scale) with a behaviour observed at the macroscopic scale. I consider that this offers a ground-breaking opportunity to set a microphysics-based model of the rheology of mantle phases. Much progress has recently been obtained by my group in this direction. A multiscale model of plastic flow consist in modeling:
a)	the defects responsible for plastic shear at the atomic scale (dislocations);
b)	their mobility under the influence of stress and temperature;
c)	their collective behaviour resulting in plastic flow.
I propose to build upon those accomplishments and to model the plastic flow of some key phases of the Earth’s mantle: wadsleyite, ringwoodite, MgSiO3 perovskite and post-perovskite to constrain:
i)	the viscosity contrast between the transition zone and the lower mantle;
ii)	the viscosity profile of the lower mantle (and understand the origin of the peak of viscosity at mid-mantle);
iii)	the rheology at the thermal boundary with the core.","2166407","2012-05-01","2018-04-30"
"RicciBounds","Metric measure spaces and Ricci curvature — analytic, geometric, and probabilistic challenges","Karl-Theodor STURM","RHEINISCHE FRIEDRICH-WILHELMS-UNIVERSITAT BONN","The project is devoted to innovative directions of research on metric measure spaces (‚mm-spaces’) and synthetic bounds for the Ricci curvature. 

It aims to bring together two - currently unrelated - areas of mathematics which both have seen an impressive development in the last decade: i) the study of ,static‘ mm-spaces with synthetic Ricci bounds and ii) the study of Ricci flows for ,smooth‘ Riemannian manifolds. A new ansatz - based on the concept of dynamical convexity - will enable to merge these two cutting-edge developments and will lead to the very first approach to Ricci flows on singular spaces. 

The project also aims to break up the limitations for the study of (generalized) Ricci curvature for mm-spaces, until now
being restricted exclusively to spaces with uniform lower bounds for this curvature. For the first time ever, mm-spaces with
(signed) measure-valued lower bounds for the Ricci curvature will be studied - the absolutely continuous, non-constant case being highly innovative as well. Besides Ricci bounds also Ricci tensors will be defined and utilized for novel insights and
sharp estimates. 

Furthermore, the project aims to initiate the development of stochastic calculus on mm-spaces and, in particular, to provide pathwise insights into the effect of (singular) Ricci curvature. The focus will be on pathwise optimal coupling, stochastic parallel transport, and derivative formulas. Both the static and the dynamic case are of interest. Methods from optimal transport and from stochastic calculus will be combined to push forward the analysis on path and loop spaces.

Each of these aims is important and worth in its own. Only in combination, however, they produce the dynamics, synergy effects, and cross-fertilization requested for maximum success. The anticipated breakthroughs of the project depend on exceeding classical borders of mathematical disciplines and on merging together topical developments from different fields.","2430000","2016-09-01","2021-08-31"
"RIGIDITY","Rigidity: Groups, Geometry and Cohomology","Nicolas Monod","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","""Our proposal has three components:

1. Unitarizable representations.

2. Spaces and groups of non-positive curvature.

3. Bounds for characteristic classes.

The three parts are independent and each one is justified by major well-known conjectures and/or ambitious goals. Nevertheless, there is a unifying theme: Group Theory and its relations to Geometry, Dynamics and Analysis.

In the first part, we study the Dixmier Unitarizability Problem. Even though it has remained open for 60 years, it has witnessed deep results in the last 10 years. More recently, the PI and co-authors have obtained new progress. Related questions include the Kadison Conjecture. Our methods are as varied as ergodic theory, random graphs, L2-invariants.

In the second part, we study CAT(0) spaces and groups. The first motivation is that this framework encompasses classical objects such as S-arithmetic groups and algebraic groups; indeed, the PI obtained new extensions of Margulis' superrigidity and arithmeticity theorems. We are undertaking an in-depth study of the subject, notably with Caprace, aiming at constructing the full """"semi-simple theory"""" in the most general setting. This has many new consequences even for the most classical objects such as matrix groups, and we propose several conjectures as well as the likely methods to attack them.

In the last part, we study bounded characteristic classes. One motivation is the outstanding Chern Conjecture, according to which closed affine manifolds have zero Euler characteristic. We propose a strategy using a range of techniques in order to either attack the problem or at least obtain new results on simplicial volumes.""","1332710","2011-03-01","2016-02-29"
"RINEC","River networks as ecological corridors for biodiversity, populations and waterborne disease (RINEC)","Andrea Rinaldo","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The proposal hinges on the noteworthy scientific perspectives provided by ecohydrological studies  of river basins,seen as a natural laboratory for complex system perspectives integrating hydrologic, ecological and geomorphological dynamics. Moving from morphological and functional analyses of dendritic geometries  observed in Nature over a wide range of scales,my claim is that essential processes sustaining human life and societies taking place along  dendritic structures can be predicted.  Population migrations and human settlements historically proceeded along river networks to follow water supply routes. Riparian systems,critically important ecosystems positioned along streams and rivers, play crucial roles in their watersheds,including nutrient filtering, biogeochemical processing,shade and resource provisioning, and stream bank stabilization. Devastating water-borne disease,such as cholera, and invading foreign species spread through water bodies linked by river networks. Although the dynamics of such systems has been extensively studied, existing approaches were mostly within the framework of mean-field or two-dimensional landscapes that ignore directionality of dispersal implied by the network acting as environmental matrix. How does connectivity within a a river network affect the emergent spreading of water-borne infections?  Does the river basin act as a template for biodiversity? Are there hydrologic controls on the spreading of water-borne disease? To answer such questions, the present proposal addresses the  study of  biodiversity in the river basin (freshwater fish and riparian vegetation); cholera dynamics  and zebra mussel invasions along river networks. Observational data and theoretical models, in a comparative mode,will be analyzed within  a unified theoretical framework.This  is intended to prove of crucial interest for understanding the functioning of river basins as a whole,including its ecosystem structure and function.","1146200","2009-01-01","2013-12-31"
"RIVAL","Risk and Valuation of Financial Assets: A Robust Approach","Walter Schachermayer","UNIVERSITAT WIEN","The recent financial crisis has brought to light the importance of correctly evaluating financial assets and their underlying risk. Any such valuation should be robust, i.e., should not be overly sensitive to the modelling assumptions. According to the Black--Scholes theory, which lies at the heart of most current valuation methods, the risk involved by a financial asset can be perfectly eliminated by pursuing a proper dynamic hedging strategy. Unfortunately, although formally elegant, this theory is too much of an idealization of the real world situation. The underlying model fails to be robust in two ways: the prices follow geometric Brownian motion, and transaction costs must be zero. The use of alternative models, e.g. based on fractional Brownian motion, was proposed more than 45 years ago by B.~Mandelbrot. The empirical findings give support to the use of such alternative models. Nevertheless, up to now these models could not be used to value financial assets, as they are not free of arbitrage. We propose an approach which makes it possible to value financial assets in an arbitrage free way, even in the framework of fractal models, by properly taking transaction costs into account. Our approach is based on utility theory. We also propose to control the risk of the related hedging strategies by imposing bounds in terms of risk measures. This allows for more realistic financial modelling with special emphasis on the aspect of the residual risk, remaining after hedging. From a mathematical point of view, our approach is based on the duality theory of infinite-dimensional optimization.","1266000","2010-04-01","2015-03-31"
"RLUCIM","Resilient large unit cell inorganic materials","Matthew Rosseinsky","THE UNIVERSITY OF LIVERPOOL","A grand challenge in science is the controlled assembly of atoms and molecules into novel forms as the basis for new physical phenomena and next-generation technologies. This programme will focus on excellence in synthesis of advanced inorganic functional materials through the development of fundamental capabilities for control of structure and composition in crystalline materials with large unit cells that are resilient to the introduction of multiple functionality. The targeted synthesis of these materials is addressed by developing a  smart intuition  approach in which detailed chemical appreciation of the structure-composition-property relationships is focused by predictive computation. The task is structured as a computation and growth-led Theme 1, where nanodeposition tools and tightly controlled sub-structure selection focus the computational task, and a synthesis- and measurement lead Theme 2 where compositional and structural features controlling complex properties are identified to initially define target materials selection. Theme 1 initially addresses the assembly of modular thin film and bulk materials to permit the required methodology development. Theme 2 will identify a toolkit of chemical components by synthesis and measurement (encompassing both average and local structure and dynamics) that will then be analysed predictively by computation to identify specific compositions for synthesis. The developed capability will integrate computation as a focused tool in the synthesis of complex materials, rather than devising an approach capable of surveying all possible compositions. It will permit the isolation of specific structures within a focused space of components, identified by structure-property-composition analysis in bulk materials (specifically those where structural frustration is imposed by competing interactions between multiple sublattices) or by selection of modules for unit cell by unit cell assembly of thin films.","2291997","2009-04-01","2014-03-31"
"RMAST","Random Models in Arithmetic and Spectral Theory","Zeev Rudnick","TEL AVIV UNIVERSITY","The proposal studies deterministic problems in the spectral theory of the Laplacian and in analytic number theory by using random models. I propose two projects in spectral theory on this theme, both with a strong arithmetic ingredient, the first about minimal gaps between the eigenvalues of the Laplacian, where I seek a fit with the corresponding quantities for the various conjectured universality classes (Poisson/GUE/GOE), and the second about curvature measures of nodal lines of eigenfunctions of the Laplacian, where I seek to determine the size of the curvature measures for the large eigenvalue limit. The third project originates in analytic number theory, on angular distribution of prime ideals in number fields, function field analogues and connections with Random Matrix Theory, where I raise new conjectures and problems on a very classical subject, and aim to resolve them at least in the function field setting.","1840625","2019-02-01","2024-01-31"
"RobOT","Robust Organic Tectonics","Andrew Ian Cooper","THE UNIVERSITY OF LIVERPOOL","This proposal will transform the area of organic crystal engineering by introducing a new level of ‘designability’ into functional molecular crystals. In the last 20 years, extended frameworks, and particularly metal-organic frameworks, have changed the perception of what is possible in terms of purposeful crystal engineering. This is because these frameworks comprise strong and directional extended bonding. By contrast, molecular crystals are not usually dominated by a single, directional motif. It remains highly challenging, therefore, to predict structure in molecular organic crystals, despite their enormous potential for synthetic diversity and function. If crystal structure is not predictable then ‘design’ of function is impossible. We will develop ‘robust organic tectons’—that is, organic molecules that assemble in a modular and predictable way without forming intermolecular coordination or covalent bonds.

Our ambitious end goal, which goes beyond the state-of-the-art, is to predict physical properties for organic molecules a priori, based only on chemical formulae, thus guiding the synthetic programme. We will target solids with unprecedented properties—for example, chiral porous organic crystals that combine both shape selectivity and site-isolated, ‘orthogonal’ functionality, inspired by enzymes. To take a longer view, modular and computationally-led engineering of organic crystals could underpin future applications that are conceptual at present, such as molecular computing.

The proposal comprises an integrated blend of chemical synthesis, supramolecular synthesis, characterization (e.g., PXRD), and computation (e.g., crystal structure prediction and molecular dynamics). Overall, we would summarize this as materials chemistry, but underpinned by physical chemistry and computation.","2496371","2013-04-01","2018-03-31"
"ROBOTGENSKILL","Generalizing human-demonstrated robot skills","Joris DE SCHUTTER","KATHOLIEKE UNIVERSITEIT LEUVEN","Future robots are expected to perform a multitude of complex tasks with high variability, in close collaboration or even physical contact with humans, and in industrial as well as in non-industrial settings. Both human-robot interaction and task variability are major challenges. A lot of progress is needed so that: (1) robots recognize the intention of the human and react with human-like motions; (2) robot end-users, such as operators on the factory floor or people at home, are able to deploy robots for new tasks or new situations in an intuitive way, for example by just demonstrating the task to the robot. 

The fundamental challenge addressed in this proposal is: how can a robot generalize a skill that has been demonstrated in a particular situation and apply it to new situations? This project focuses on skills involving rigid objects manipulated by a robot or a human and follows a model-based approach consisting of: (1) conversion of the demonstrated data to an innovative invariant representation of motion and interaction forces; (2) generalization of this representation to a new situation by solving an optimal control problem in which similarity with the invariant representation is maintained while complying with the constraints imposed by the new context. Additional knowledge about the task can be added in the constraints.
 
Major breakthroughs are that the required number of demonstrations and hence the training effort decrease drastically, similarity with the demonstration is maintained in view of preserving the human-like nature, and task knowledge is easily included.

The methodology is applied to program robot skills involving motion in free space (e.g. human-robot hand over tasks) as well as advanced manipulation skills involving contact (e.g. assembly, cleaning), aiming at impact in industrial and non-industrial settings.  

Application of the invariant motion representation in the neighbouring field of biomechanics will further leverage impact.","2494971","2018-10-01","2023-09-30"
"ROCOCO","CONFORMATIONAL COMMUNICATION AND CONTROL","Jonathan Clayden","UNIVERSITY OF BRISTOL","We aim to offer to science a molecular scale mechanism for communication and control.  Using stereochemical information and conformational control as the mechanism by which that information is transmitted and processed, we  take inspiration from the phenomenon of allostery in biology, and will put to dynamic use a set of conformationally controlled foldamer structures. We will use these structures to convey information over multi-nanometre distances, allowing control of chemical function from a remote site. By embedding the foldamers into membranes, we will control chemistry (eg catalytic activity) within an artificial vesicle by communicating information through the chemically impermeable phospholipid bilayer.
To achieve our aim we will synthesise oligomeric and polymeric compounds with well-defined helical conformations, and use a stereochemical influence located at one terminus to induce a conformational preference (for the left or the right handed form of the helix) which is relayed to a site many nanometres distant.  Precedent suggests that by employing polymeric structures we will achieve control even over micrometre scales.  Simple but powerful new techniques will quantify the remote (on a molecular scale) transmission of information by NMR, circular dichroism spectrophotometry and/or fluorescence. The result of the information relay will be a detectable change in chemical reactivity or binding properties   and one aim will be to vastly increase, by orders of magnitude, the distance over which remote stereochemical control is possible (from the current 2.5 nm to the order of >100 nm).
The feature which distinguishes biology from chemistry is information, and in particular the ability to encode and process information using molecular interactions.  Our project will take a step towards the development of designed chemical structures which can mimic, using far simpler molecules, the function of biological communication systems.","2426106","2011-04-01","2016-03-31"
"RODYMAN","Robotic Dynamic Manipulation","Bruno Siciliano","C.R.E.A.T.E. CONSORZIO DI RICERCA PER L'ENERGIA L AUTOMAZIONE E LE TECNOLOGIE DELL'ELETTROMAGNETISMO","The goal of the RoDyMan project is the derivation of a unified framework for dynamic manipulation where the mobile nature of the robotic system and the manipulation of non-prehensile non-rigid or deformable objects will explicitly be taken into account. Novel techniques for 3D object perception, dynamic manipulation control and reactive planning will be proposed. An innovative mobile platform with a torso, two lightweight arms and multifingered hands, and a sensorized head will be developed for effective execution of complex manipulation tasks, also in the presence of humans. Dynamic manipulation will be tested on an advanced demonstrator which is currently unfeasible with the prototypes available in the labs, where the application scenario is conceived to emulate the human ability to carry out a challenging robotic task. The research results to be achieved in RoDyMan will contribute to paving the way towards enhancing autonomy and operational capabilities of service robots, with the ambitious goal of bridging the gap between robotic and human task execution capability.","2496600","2013-06-01","2019-05-31"
"RoFiRM","Mathematical Methods for Robust Financial Risk Management","Nizar Touzi","ECOLE POLYTECHNIQUE","Reliable techniques in finance should take into account the unavoidable modelling error. This is the main focus of this project that we intend to address from two viewpoints raising new questions in applied mathematics.
Our first research direction is to device robust risk management methods which use the market observations and the no-arbitrage principle. A classical result in financial mathematics essentially states that, in idealized frictionless financial markets, the price processes of tradable securities must be a martingale under some equivalent probability measure. We propose to adopt a conservative viewpoint by deriving the bounds over all possible choices of martingales. By accounting for the rich information corresponding to the prices of European call options, we arrive naturally to a new optimal transportation problem. We intend to analyze several questions: clarify the connection with the Skorohod embedding problem, understand better the duality, develop the corresponding numerical techniques, explore the robust portfolio optimization problems under such constraints, and understand their impact on the risk measurement.
The second direction of research proposed in this project concerns the recent theory of Mean Field Games, recently introduced by Lasry and Lions. Our intention is to address this theory from the probabilistic point of view. The main observation is that the MFG equations, consisting of a coupled system of a Fokker-Planck equation and a semilinear Hamilton-Jacobi-Bellman equation, can be viewed as an extension of the theory of forward-backward stochastic differential equations (FBSDE) with mean-field dependence. This theory provides a simple modelling of the interactions which may be used to explain important phenomena on financial markets as the contagion effect and the systemic risk. In particular, the connection with FBSDEs opens the door to probabilistic numerical methods.","1871400","2013-05-01","2018-04-30"
"RoMoL","Riding on Moore's Law","Mateo Valero Cortes","BARCELONA SUPERCOMPUTING CENTER - CENTRO NACIONAL DE SUPERCOMPUTACION","The most common interpretation of Moore's Law is that the number of components on a chip and accordingly the computer performance doubles every two years. At the end of the 20th century, when clock frequencies stagnated at ~3 GHz, and instruction level parallelism reached the phase of diminishing returns, industry turned towards multiprocessors, and thread level parallelism. However, too much of the technological complexity of multicore architectures is exposed to the programmers, leading to a software development nightmare.
We propose a radically new concept of parallel computer architectures, using a higher level of abstraction, Instead of expressing algorithms as a sequence of instruction, we will group instructions into higher-level tasks that will be automatically managed by the architecture, much in the same way superscalar processors managed instruction level parallelism.
We envision a holistic approach where the parallel architecture is partially implemented as a software runtime, and the reminder in hardware. The hardware gains the freedom to deliver performance at the expense of additional complexity, as long as it provides the required support primitives for the runtime software to hide complexity from the programmer. Moreover, it offers a single solution that could solve most of the problems we encounter in the current approaches: handling parallelism, the memory wall, the power wall, and the reliability wall in a wide range of application domains from mobile up to supercomputers .
We will focus our research on a most efficient form of multicore architecture coupled with vector accelerators for exploiting both thread and data level parallelism.
All together, this novel approach toward future parallel architectures is the way to ensure continued performance improvements, getting us out of the technological mess that computers have turned into, once more riding on Moore's Law.","2356467","2013-04-01","2018-03-31"
"ROMY","ROtational Motions: a new observable for seismologY","Heiner Igel","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","When the ground shakes from earthquakes, the oceans, or the atmosphere, it not only translates (up-down, sideways), but also undergoes rotational motions. To fully characterize seismic sources and wave fields theoreticians have insisted for decades that these motions should also be measured. However, this was hampered by the substantial technical difficulties in observing rotational motions with the necessary sensitivity. This implies that the observation of the complete ground motion is still an unsolved problem. Based on promising pilot studies in the past years, we aim at breaking new ground with an innovative instrumentation strategy that would allow solving this outstanding problem. If the strategy is successful the new observable is expected to have an impact in a wide range of fields making it a classical high-risk high-gain situation. We aim at the establishment of the first-of-its-kind six-component seismic observatory based on ring-laser technology, the field deployment of portable fibre-optic based rotation sensors,  the integration of the  new observations with the global seismological data infrastructure, and the reporting of discoveries in a variety of fields based on the new observable. At the end of the project we expect to have substantially pushed forward the emerging field field of rotational seismology with new standards, and recommendations for the use of rotation sensors in Earth sciences and engineering. We expect advances in understanding 1) the dynamics of volcano’s interior, 2) the origin of the Earth’s ocean generated noise field, 3) and seismic inverse problems for structure and source using the new rotational observables. This will impact several pressing problems of societal relevance in particular in association with natural hazards such as volcanic eruptions and earthquake strong ground motion as well as geophysical exploration.","2488907","2014-03-01","2019-02-28"
"ROSE","Robust Sensor Array Processing","Alex Gershman","TECHNISCHE UNIVERSITAT DARMSTADT","Sensor array signal processing has emerged a key technology for future radar, wireless communication, sonar, and microphone array systems. Sensor array processing methods are commonly used in multi-sensor systems to estimate signal parameters (such as source directions-of-arrival and waveforms), to extract signals-of-interest received on the background of multiple interferers and sensor noise, and to transmit signals to multiple locations in a spatially selective way.  Current sensor array processing techniques are known to degrade severely if some of the exploited assumptions on the environment, sources, or antenna array become wrong or imprecise. In particular, the existing signal parameter estimation and beamforming methods are known to be very sensitive to any type of array manifold or multi-antenna channel mismodeling which may be caused by environmental non-stationarities, signal fading and scattering effects, array shape distortions, imperfect array calibration, unknown ambient colored noise, etc. As a result, advanced sensor array processing techniques can suffer a severe performance degradation in situations with even rather slight mismatches between the actual and presumed array responses. Therefore, robust sensor array processing techniques are currently of great demand. The main objective of the proposed research will be to develop new robust and computationally efficient sensor array signal processing methods for signal parameter estimation and receive and transmit adaptive beamforming in complicated environments with imperfectly known array and propagation channel parameters. The robust adaptive array processing and signal parameter estimation techniques developed during this project will be based on an explicit modeling of uncertainties in the array response and robust performance optimization. The proposed methods will provide substantial improvements of the robustness and computational complexity relative to the current array processing techniques.","1531628","2008-11-01","2012-10-31"
"ROSEPOT","Revolutionising Organic Synthesis: Efficient One-Pot Synthesis of Complex Organic Molecules for Non-Experts","Varinder Kumar Aggarwal","UNIVERSITY OF BRISTOL","The creation of new molecular entities and subsequent exploitation of their properties is central to a broad spectrum of research disciplines from medicine to materials. However, despite substantial progress, the problems and difficulties associated with chemical syntheses severely limits the development of these disciplines. In order to meet the emerging challenges across new disciplinary boundaries in a rapidly changing scientific landscape we require a step-change in the development of more rapid and robust techniques in organic synthesis. Our plan is to develop new reactions and strategies which enable us to essentially grow a carbon chain with complete control over its shape (stereochemistry) and functionality (which groups are incorporated). Specifically, we propose to create a family of chiral carbanions with good leaving groups attached which can be inserted into C-B bonds sequentially and in one pot so that at the end of the operation a complex natural product, pharmaceutical or material will be produced. Our ultimate vision is to render complex organic synthesis as easy as peptide/oligonucleotide synthesis is now by attaching the boronic esters to a solid support and automating the steps. This would enable complex organic molecules to be accessible even by non-experts in synthesis. Furthermore, by variation of the reagents and their stereochemistry any compound and any stereoisomer will be accessible thus allowing diversity elements to be introduced without additional cost. The impact for organic synthesis and the wider scientific community is immense as it will allow the properties and potential function of complex molecules (from pharmaceuticals to materials) to be studied and then exploited. It is enabling science.","1579277","2010-04-01","2015-03-31"
"RotaNut","Rotation and Nutation of a wobbly Earth","Veronique Dehant","KONINKLIJKE STERRENWACHT VAN BELGIE","The rotation of the Earth has long been used as a measure of time, and the stars as reference points to determine travellers’ whereabouts on the globe. Today, precise timescales are provided using atomic clocks and precise positioning is determined using geodetic techniques such as GPS grounded on two reference frames: the terrestrial frame, fixed relative to the Earth and rotating synchronously with the planet, and the celestial frame, which is immobile in space, where the artificial satellites such as those of GPS are moving. The relationship between these frames is complicated by the fact that the rotation and orientation of the Earth is subject to irregularities induced by global mass redistributions with time and external forcing such as the gravitational pull of the Sun and the Moon. With the advance of observation precision, the causes of Earth orientation changes are progressively being identified by geodesists and geophysicists. The term ‘precession’ describes the long-term trend of the orientation of the axis of spin, while ‘nutation’ is the name given to shorter-term periodic variations, which are the prime focus of the present project. The rotation axis of the Earth is moving in space at the level of 1.5km/year due to precession and has periodic variations at the level of 600 meters as seen from space in a plane tangent to the pole. The present observations allow scientists to measure these at the sub-centimetre level enabling them to identify further physics of the Earth’s interior to be taken into account in the Earth orientation models such as the coupling mechanisms at the boundary between the liquid core and the viscoelastic mantle, as well as many other factors (sometimes not yet definitely identified). The proposed research will address many of these and will result in the development of improved global orientation of the Earth with an unprecedented accuracy - at the sub-centimetre level.","2500000","2015-09-01","2020-08-31"
"RTCO","Reductive Transformations of Carbon Oxides","Frederick Geoffrey Nethersole Cloke","THE UNIVERSITY OF SUSSEX","The development of new approaches to the activation and functionalisation of carbon monoxide and carbon dioxide is a highly topical and significant challenge for chemistry. The use of biomass and coal derived carbon monoxide as a fundamental building block for simple organic molecules is a key objective in energy research, and the latter, coupled with global warming considerations, dictate that new ways to both activate and derivatise carbon dioxide must also be found. The programme of work described in this proposal tackles these problems through experience and excellence in organometallic chemistry and small molecule activation. It is centred around three closely integrated and synergistic strands. The synthetic work in Strand 1 focuses on reductive assembly of carbon monoxide and carbon dioxide on low oxidation state metal centres to build simple organic molecules and rings stoichiometrically. The work in this strand will also be driven by developing a detailed understanding of the mechanisms of these reactions through experiment. A key feature of this proposal is the computational work, which is the subject of Strand 2, in which theory and modelling will be used in an iterative way to predict and inform the experimental synthetic and mechanistic work in Strand 1. Strand 3 will establish the underlying redox properties of these systems using electrochemical techniques, in order to ultimately to generate systems which will catalytically generate organic molecules from carbon monoxide and carbon dioxide. The program of work described in this proposal, which will deliver stoichiometric and catalytic recycling of carbon oxides, is at the frontier of future sustainable chemical technology. It is therefore of high risk, but ultimately extremely high reward.","1775222","2010-02-01","2015-01-31"
"RULE","Rule-Based Modelling","Vincent Nicolas Julien Danos","THE UNIVERSITY OF EDINBURGH","The purpose of this research programme is to contribute to the solution of a major problem in today’s systems biology: namely, the difficulty to bring mechanistic modelling to bear on full scale systems. Statistical, and experimental techniques have scaled up considerably in the last two decades. Mechanistic modelling, on the other hand, is still confined in much smaller scales. Witness the painstakingly slow scaling up of cellular signalling models, despite their central role in cell response. To address the problem, we build on a new modelling methodology, called the rule-based approach (RB), pioneered by the PI of this proposal, and hailed (in a recent Nature Methods article) as the “harbinger of an entirely new way of representing and studying cellular networks”. By exploiting the modularity of biological agents, RB breaks through the combinatorial challenge of describing and simulating signalling systems. But with the possibility of writing and running larger models, new questions come to the fore. To bring mechanistic modelling to the next level requires: innovative knowledge representation techniques to anchor modelling in the data-side of systems bi- ology; new means to tame the complexity of, and reason about, the parameter space of models; new concepts to identify meaningful observables in the highly stochastic behaviour of large and combinatorial models; and clean and structured languages to comprehend spatial aspects of the biological phenomenology. The realism accrued by working at larger scales gets one closer to the bottom-up reconstruction of behaviours at the heart of systems biology, and to an understanding of the computational architecture of complex biological networks. This research programme, firmly grounded in the mathematics of programming language semantics and formal methods, extends the RB approach so as to address all of the above needs, and deliver an integrated modelling framework where full scale mechanistic modelling is achievable.","2084316","2013-02-01","2018-01-31"
"RWPERCRI","Random Walks, Percolation and Random Interlacements","Alain-Sol Sznitman","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","The general theme of this research proposal involves random walks and percolation theory. The research proposal aims at exploring in depth the surprising links between on the one hand a class of problems directly pertaining to random walks (in particular related to disconnection, or the creation of large vacant components and separating interfaces), and on the other hand questions pertaining to a non-conventional model of percolation based on random interlacements. Traditional methods of percolation typically do not apply to this model. The present research program if successfull ought to uncover new paradigms and lead to the development of new methods.","583092","2010-04-01","2014-03-31"
"SAEMPL","Scattering and absorption of electromagnetic waves in particulate media","Karri Olavi Muinonen","HELSINGIN YLIOPISTO","""The canonical problem of electromagnetic scattering in complex particulate media is solved numerically using multiple-scattering theory based on the Maxwell equations, with an exact treatment of the leading ladder and cyclical interaction diagrams. The numerical methods are validated using a nanotechnology-based scattering experiment that, simultaneously with the measurement of the full scattering matrix at arbitrary illumination and observation geometries, allows for a detailed physical characterization of the scattering object using Atomic Force Microscopy. The numerical and experimental methods will have a major impact on how knowledge is accrued on objects in our Solar System based on their scattering characteristics, with wavelengths spanning from the ultraviolet to radio, using both space-based and ground-based observing programs. The methods will have immediate applications in Earth observation, including remote sensing of the atmosphere, land, and sea.""","2749532","2013-04-01","2018-03-31"
"SAFECON","New Computational Methods for Predicting the Safety of Constructions to Water Hazards accounting for Fluid-Soil-Structure Interactions","Eugenio Oñate Ibañez De Navarra","CENTRE INTERNACIONAL DE METODES NUMERICS EN ENGINYERIA","The objective of this research project is the development and experimental validation of a new generation of mathematical and computational methods allowing the solution of practical fluid-solid structure interaction (FSSI) problems of interest for predictive safety of civil constructions to water-induced hazards. These constructions include: buildings, bridges, harbours, dams, dykes, breakwaters, and similar infrastructures in water hazard scenarios such as flooding, large sea waves, tsunamis and water spills due to the collapse of dams, dykes and reservoirs, among others.
The specific research aims of the SAFECON project are: a) development, integration and validation of a next generation of predictive methods based on new mathematical models and efficient computational procedures integrating a new particle-based method, the discrete element method and the finite element method for estimating accurately the dynamics of three dimensional (3D) free surface multiscale heterogeneous flows and their interaction with constructions accounting for FSSI effects. b) Extension and validation of the new particle-discrete-finite element method (PDFEM) for solving 3D FSSI problems allowing for failure mechanisms in the structure and the soil, and c) application of the new computational method (the PDFEM) for predicting the risk of failure in selected civil constructions under the effect of water forces.
The ultimate outputs of SAFECON will be: a) new mathematical models and numerical techniques for analysis of multiscale free surface heterogeneous flows and their interaction with soils and structures and b) new validated computational methods and software for enhanced design and risk assessment of engineering constructions to protect human populations and civil infrastructure in presence of water-induced hazards.","2487734","2011-01-01","2015-12-31"
"SAFERVIS","Uncertainty Visualization for Reliable Data Discovery","Rüdiger Heinrich Westermann","TECHNISCHE UNIVERSITAET MUENCHEN","Visualization has significantly changed the way humans analyze large amounts of multi-dimensional data. However, current visualization techniques can give only little or no guarantees regarding the confidence in the displayed information. Since this information is always affected by uncertainties in the data generation and visualization processes, the user can be lead to misclassifications, misinterpretations, and false assumptions.
This proposal challenges the status quo in visual data analysis with innovative ideas for next-generation technology that provides uncertainty visualization as a core methodology. We will develop a visual language for the communication of the variability of features due to uncertainties in the data generation and visualization processes. Our research aims at modeling the uncertainty stochastically and deriving probability distributions for the occurrence of features. Especially in 3D, finding meaningful visualizations of the effect of uncertainty is extremely demanding and requires going far beyond existing approaches.
Besides radically changing the way visual data exploration is performed, our research has the potential to strongly influence the way scientific measurements and computer simulations are carried out. The precise knowledge of uncertainties enables to discover quantitatively the data generation process, and it can, therefore, be used to quantify the sensitivity of a process and the generated results to the choice of parameterization over the input parameters. Hence, it is our second vision to position uncertainty visualization as a future technology for guiding research towards the most reliable data generation process within a given uncertainty tolerance.","2296800","2012-03-01","2017-02-28"
"SAHR","Skill Acquisition in Humans and Robots","Aude Gemma BILLARD IJSPEERT","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Society is rapidly opening its doors to robots in our daily life with autonomous vehicles, rehabilitation devices and autonomous appliances. These robots will face unexpected changes in their environment, to which they will have to react immediately and appropriately. Even though robots exceed largely humans’ precision and speed of computation, they are far from matching humans’ capacity to adapt rapidly to unexpected changes. In the past decades, robotics has made leaps forward in the design of increasingly complex robotic platforms to meet these challenges. In this endeavour, it has benefited from advances in optimization for solving high-dimensional constrained problems and in machine learning (ML) to analyse vast amounts of data. These methods are powerful for planning in slow-paced tasks and when the environment is known. This project addresses a growing need for methods that show fast and on-line reactivity. 
We design controllers that can plan at run time and adapt to new environmental constraints. We offer a novel approach to robot learning that follows stages of skill acquisition in humans. To inform modelling, we conduct a longitudinal study of the acquisition of dexterous bimanual skills in craftsmanship. We study how humans exploit task uncertainty to overcome their sensory-motor noise, and how humans learn bimanual synergies to reduce the control variables. This study informs the design of novel learning strategies for robots that exploit failures as much as successes. We combine planning and ML to learn feasible control laws, retrievable at run time with no need for further optimization. We exploit properties of dynamical systems (DS), which have received little attention in robot control, and use ML to identify characteristics of DS, in ways that were not explored to date. The approach is assessed in live demonstrations of coordinated adaptation of a multi-arm/hand robotic system engaged in a fast-paced industrial task, in the presence of humans.","2492036","2017-10-01","2022-09-30"
"SCALE","Scalable Quantum Photonic Networks","Peter Lodahl","KOBENHAVNS UNIVERSITET","It is an outstanding challenge in quantum physics of today to scale small proof-of-concept experimental demonstrations into larger quantum networks. In the last decade, solid-state photonic systems have matured significantly, and an ambitious research project on such scaling seems viable. With the present proposal we intend to take up this challenge and exploit single quantum dots in photonic-crystal nanostructures as a deterministic photon-emitter interface for scalable quantum architectures. 

The project objectives are threefold. We will explore: 1) Deterministic single-photon sources for quantum simulations, 2) A giant photon nonlinearity for quantum-information processing, 3) The deterministic interfacing of multiple quantum dots.

In 1) we will exploit our recently developed deterministic single-photon source to produce a spatially multiplexed array of single photons (prospectively of 10 photons or more). This source will be used for quantum simulations. Area 2) exploits a single quantum dot in a photonic-crystal waveguide as a giant nonlinearity. The quantum dot will be operated either as a passive nonlinear scatterer or actively controlled. The nonlinearity will enable constructing a deterministic CNOT gate for photons or a single-photon transistor. Finally, 3) concerns the coupling of two or more quantum dots by an extended dipole-dipole interaction that is mediated by the photonic-crystal waveguide. The fundamental limits for the size and complexity of such a quantum photonic network will be explored.

The present project focus on overcoming the fundamental obstacles that photonic quantum-information processing applications have been suffering from, i.e., probabilistic single-photon emission and weak nonlinearities. The successful accomplishment of the project could elevate quantum photonics to a frontrunner technology for scalable quantum-information processing.","2499981","2015-12-01","2020-11-30"
"Scale-FreeBack","Scale-Free Control for Complex Physical Network Systems","Carlos Canudas de Wit","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Technology achievements were typically built upon fundamental theoretical findings, but nowadays technology seems to be evolving faster than our ability to develop new concepts and theories. Intelligent traffic systems benefit from many technical innovations, for example. Mobile phones, radars, cameras and magnetometers can be used to measure traffic evolution and provide large sets of valuable data. Vehicles can communicate with the network infrastructure, as well as each other. However, these huge technological advances have not been used to the full so far. Traffic lights are far from functioning optimally and traffic management systems do not always prevent the occurrence of congestions.
So what is missing? Such systems affect our daily life; why aren’t them on pace with technology advances? Possible because they have become far more complex than the analytical tools available for managing them. Systems have many components, communicate with each other, have self-decision-making mechanisms, share an enormous amount of information, and form networks. Research in control systems has challenged some of these features, but not in a very concerted way. There is a lack of “glue” relating the solutions to each other.
In the Scale-FreeBack project, it is proposed to approach this problem with a new holistic vision. Scale-FreeBack will first investigate appropriate scale-free dynamic modeling approaches breaking down system’s complexity, and then develop control and observation algorithms which are specifically tailored for such models. Scale-FreeBack will also investigate new resilient issues in control which are urgently required because of the increasing connectivity between systems and the external world. Road traffic networks will be used in proof-of-concept studies based on field tests performed at our Grenoble Traffic Lab (GTL) and in a large-scale microscopic simulator.","2873601","2016-09-01","2021-08-31"
"SCALMS","Engineering of Supported Catalytically Active Liquid Metal Solutions","Peter WASSERSCHEID","FRIEDRICH-ALEXANDER-UNIVERSITAET ERLANGEN NUERNBERG","This project deals with a disruptive innovation for engineering heterogeneous catalysts. Materials technologies that promise improved catalytic performance are of utmost interest for a more sustainable chemical industry. Very recently, the applicant and his collaborators have introduced a new paradigm in heterogeneous catalysis, namely the use of Supported Catalytically Active Liquid Metal Solutions (SCALMS) (Nature Chemistry, 2017, DOI:10.1038/nchem.2822). The first account of this new class of catalytic materials demonstrated remarkable reactivity of liquid mixtures of gallium and palladium supported on porous glass, outperforming commercial catalysts in the dehydrogenation of butane with unprecedented high resistance against coke formation. 
The project aims at developing these seminal findings into a general methodology for technical heterogeneous catalysis. The applicant and his team are convinced that SCALMS represent a step-change toward catalytic materials with a higher degree of surface uniformity, structural definition, reactivity and robustness. We are fascinated by the fact that the catalytic reaction in SCALMS does not proceed at the surface of solid metal nanoparticles (with their unavoidable irregularities on technical-scale production) but presumably at homogeneously distributed metal atoms in a highly dynamic liquid metal surface. From this fundamental difference, drastically altered electronic and steric properties are expected and may lead to outstanding catalytic performance. To leverage the full potential of this approach, we aim to explore all relevant effects of interface formation, reactant adsorption, and surface reactivity by a combination of synthetic, analytic, reaction engineering and material processing methodologies. We will focus on selected base and precious metals in liquid Ga supported on porous supports and aim to study these materials for alkane dehydrogenation and alkene conversion reactions.","2493650","2018-09-01","2023-08-31"
"SCAPDE","Semi-Classical Analysis and Partial Differential Equations","Gilles Lebeau","UNIVERSITE DE NICE SOPHIA ANTIPOLIS","""Semi-classical  analysis  started to be developed about 50 years ago by the works of Sato and Hormander on micro-local analysis.
Nowadays, it has reached great achievement with many applications to different topics in analysis including spectral theory, scattering theory, control theory, and some aspects in non linear equations, by the use of dispersive estimates and paraproduct techniques .

The objective of our proposal is to develop new tools and applications in two directions : boundary value problems and connections between probability and semi-classical analysis. We expect to solve  basic remaining open problems in the analysis of boundary problems, and to make  contributions to develop new links between probability and analysis of partial differential equations.

We will focus on four topics :
- 1) Dispersive and Strichartz estimates for wave or Schrödinger equations in domains. Applications to the Cauchy problem for non linear waves in domains.
- 2) Theoretical analysis of the optimal control operator in control theory.
- 3) Analysis of  Markov Chain Monte Carlo algorithm of Metropolis type via PDE's tools.
- 4) Applications of  probabilistic tools to  the analysis of  PDE.

Topics 1) and 2) are strongly connected to progress in the analysis of boundary value problems.
Topic 3) involves a generalization of the classical pseudo-differential calculus. The purpose of topic 4) is to develop a new field of research for deterministic PDE's (and therefore is not in the area of stochastic PDE's).

All topics involve geometric analysis in the phase space.""","1705750","2013-05-01","2018-04-30"
"SCEON","Scanning Electron Optical Nanoscopy","Albert POLMAN","STICHTING NEDERLANDSE WETENSCHAPPELIJK ONDERZOEK INSTITUTEN","Novel developments in optical technology increasingly depend on control of light at the nanoscale. To study light at this small length scale it is essential to employ techniques that can excite and image light at the nanoscale. In recent years, my group has explored cathodoluminescence (CL) spectroscopy for this purpose. Based on the exciting potential of this technique, I propose to design and construct a new time- and angle-resolved CL microscope that exploits the primary electron beam as a coherent optical excitation source with deep-subwavelength spatial resolution. We will use the new instrument to address four challenges that will provide new insight in the behaviour of light at the nanoscale. Specifically, we will:
(1) use CL microscopy to excite and characterize ultra-short wavelength-plasmons on graphene. We will create 3D tomographic reconstructions of the local optical density of states in resonant plasmonic and dielectric nanostructures. 
(2) determine 2D and 3D spatially-resolved ultrafast carrier recombination processes in resonant semiconductor photovoltaic nanostructures and reveal the radiative properties of single quantum emitters. 
(3) develop CL momentum spectroscopy to reveal embedded eigenstates in dielectric photonic crystals and topological photonic protection in complex three-dimensional architectures. 
(4) develop CL polarimetry in combination with phase-resolved CL detection to study electric and magnetic polarizabilities in nanoscale light emitters and to control the orbital angular momentum of light.
The proposed program will firmly establish time- and angle-resolved CL imaging spectroscopy as a key deep-subwavelength nanoscopy tool to investigate the interplay of electric and magnetic fields that constitute light at the nano scale, and will enable applications in photovoltaics, solid-state lighting, photonic and optoelectronic integrated circuits, quantum communication, sensing and metrology.","2495625","2016-07-01","2021-06-30"
"SCIENCEFORE","The Science of Forecasting: Probabilistic Foundations, Statistical Methodology and Applications","Tilmann Joachim Gneiting","HITS GGMBH","The future being uncertain, forecasts ought to be probabilistic in nature, taking the form of probability distributions over future quantities or events.  Accordingly, a transdisciplinary transition from point forecasts to probabilistic forecasts is well under way.  The ScienceFore project seeks to provide guidance and leadership in this transition, by developing the theoretical foundations of the science of forecasting, as well as cutting-edge statistical methodology, along with applications in meteorology and economics.

Theoretically, we will focus on the study of aggregation methods for the combination of multiple probabilistic forecasts for the same quantity or event, and on the design and structure of performance measures that encourage truthful predictions, including but not limited to proper scoring rules.  In applications, we will develop statistical postprocessing techniques for the THORPEX Interactive Grand Global Ensemble (TIGGE), which comprises the world's leading global numerical weather prediction models.  The key challenge is to retain physically realistic and coherent joint dependence structures across meteorological variables, continents and oceans, and look-ahead times.  Furthermore, we will investigate the use of statistical postprocessing techniques in macroeconomic surveys, and aim to resolve a long-standing puzzle in the evaluation of economic and financial forecasts.

Theory and applications will intertwine closely, to result in a project that constitutes much more than the sum of its parts.  For example, the study of the properties of aggregation methods will inform the development of postprocessing methods for ensemble weather forecasts, and decision theoretically principled approaches to the design of performance measures call for a change of paradigms in the practice of the generation and evaluation of point forecasts, to be demonstrated in case studies.","1726794","2012-06-01","2018-02-28"
"SCIFRI","Science Friction","Joost Wilhelmus Maria Frenken","STICHTING NEDERLANDSE WETENSCHAPPELIJK ONDERZOEK INSTITUTEN","""There is no fundamental law that dictates the necessity of losing energy in a sliding contact. In spite of its apparent simplicity, we have a relatively poor understanding of the mechanisms that determine how energy is lost when two bodies are forced to slide over one another. The SciFri project will launch a research attack on friction that will not only deepen our fundamental insight into this important phenomenon but also involves the development of several strategies to significantly lower or ‘lose’ friction under practical circumstances.
"""" We will address in detail how energy is really dissipated on the atomic scale when sliding objects slip over a single lattice spacing.
"""" We will bring friction to a halt by employing two mechanisms that we have explored recently on the atomic scale: superlubricity and thermolubricity.
"""" Scaling up the two friction-lowering effects to the macro-scale will be attempted by a combination of two completely novel approaches. One is the use of special coatings, namely single monolayers of graphene or hexagonal boron nitride. The other involves a specific nanopatterning of the contacting surfaces.
"""" In our friction experiments and modeling we will cover the full range of length scales, from the atomic regime all the way to the practical scale of so-called MEMS devices. This will prove to be an essential element in the extrapolation of nanoscale behavior to friction on a practical level. We will further compare measurements in uncompromised (ultrahigh) vacuum with observations under controlled ambient conditions, in order to explore the role of the atmosphere.""","2494000","2011-07-01","2016-06-30"
"SCIPORE","A new paradigm in modelling flow and transport in porous media: revisiting foundations of porous media science","Seyed Majid Hassanizadeh","UNIVERSITEIT UTRECHT","Our models of fluid-filled porous media are based on ad-hoc extensions of relatively simple equations. But, in almost all cases, they have failed to provide acceptable descriptions of complex porous media. In petroleum engineering, we are not able to predict the true reservoir behaviour; our predictions must be continuously revised through “history matching”. In soil physics, we find that persistent pesticides do reach deep groundwater resources contrary to our model predictions. Almost all models fail to predict the outcome of soil and groundwater remediation operations. Predictions of performance of industrial systems such as fuel cells and fluid absorbents are poor at the best. We are confronted with major challenges related to the prediction of performance and safety of subsurface CO2 sequestration, and questions related to threats and opportunities associated with methane gas hydrates under the oceans. One major shortcoming of our porous media models is the lack of consideration for the fact that fluids can fill up the pores in many different configurations, even for the same degree of fluid saturations. Each configuration results in different rates of fluids flow and in different mass and heat transport behaviours. Another major defect is the fact that our current models apply to continuous phases. But, in many applications, we have discontinuous fluid phases. The general aim of the proposed research is to establish a new paradigm for modelling flow and transport in porous media. We shall perform integrated experimental and computational research in order to establish advanced physically-based theories for description of porous media processes. We shall construct a host of micromodels for physical experiments on flow and transport and perform sandbox experiments on multiphase flow to study processes that occur in gas hydrates and fuel cells. In the course of this project, a first-class integrated experimental/computational laboratory will be established.","2237200","2013-09-01","2019-08-31"
"SDMODELS","Structured Discrete Models as a basis for studies in Geometry, Numerical Analysis, Topology, and Visualization","Günter Matthias Ziegler","FREIE UNIVERSITAET BERLIN","""Discrete structures appear throughout mathematics not only as approximations to continuous objects, but also as mathematical objects of their own right. The """"right"""" discrete models should have analogous theory to the continuous limit, but often more transparent, more interesting structure, it """"tells you more"""". The proposed project has the agenda to connect, and make substantial progress in, a number of interesting, but rather diverse instances for this, including
- Convex Polytopes as models for linear, semi-definite and non-linear optimization problems,
- Polyhedral Surfaces as models for differential geometry, including questions of (discrete) integrability,
- Structured meshes as the """"right"""" discrete structures for solving systems of partial differential equations with quality guarantees,
- Triangulation models as they appear as models for space in quantum gravity.
In this simultaneous treatment of these topics we hope to capture connections and identify analogous and parallel structures in different parts of mathematics. This is a theory proposal, but a number of the core topics are suggested by applied research, as done e.g. in the framework of the Berlin DFG Research Center MATHEON in Berlin. It will connect to, and rely on, other major structured research groups in Berlin, such as the """"Polyhedral Surfaces"""" DFG Research Group, and the Research Training Group led by the PI. In collaboration between individuals and groups with diverse mathematical expertise in Berlin, throughout Europe and beyond we are set to establish an additional """"theory backbone""""; for applied research in Berlin.""","1854400","2010-07-01","2015-06-30"
"SEACHANGE","Sea-level change due to climate change","Jonathan Michael Gregory","THE UNIVERSITY OF READING","Sea-level rise as a consequence of climate change will have severe impacts on coastal populations and ecosystems. However, owing to the incomplete state of scientific knowledge, there is a large range of uncertainty in the predictions, of a factor of more than two for any given emissions scenario during the 21st century, and much larger for subsequent centuries. This is a serious obstacle to the assessment of impacts. Sea-level change is a diagnostic of a complex combination of Earth system processes operating over a wide range of timescales. Explaining the record of past sea-level change and predicting the future is therefore a fascinating interdisciplinary scientific challenge as well as one with practical implications. The aim of this project is to improve quantitative understanding and hence reduce uncertainty in predictive models of the two main climate-related contributions to sea-level change, namely ocean density change and changes in ice-sheets. The former is the most important term on timescales of years to centuries, and the latter is the principal influence over timescales of centuries to millennia. In both parts of the proposal, our focus is on analysis of changes simulated by 3D atmosphere-ocean climate models, which we compare with observational data. For ocean density change, the objectives are to analyse the physical processes responsible for global-mean and local sea-level change due to density change, in order to quantify and constrain the uncertainties in predictions. For ice-sheets, the objectives are to increase our understanding of and ability to model the coupled evolution of ice-sheets and climate on multi-millennial timescales, in particular regarding the last glacial cycle and the long-term future of the Greenland ice-sheet.","1441719","2010-04-01","2016-03-31"
"SECO","Search Computing","Stefano Ceri","POLITECNICO DI MILANO","""Who are the strongest European competitors on software ideas? Who is the best doctor to cure insomnia in a nearby hospital? Where can I attend an interesting conference in my field close to a sunny beach?  This information is available on the Web, but no software system can accept such queries nor compute the answer. We hereby propose search computing as the new multi-disciplinary science which will provide the abstractions, foundations, methods, and tools required to answer these and many similar questions. The emerging paradigm of service computing has so far been neutral to the presence of search services, which are equal """"inter pares"""". This proposal brings about a simple yet revolutionary idea: service computing evolves into search computing, a new paradigm where ranking is the dominant factor for composing services. While state-of-art search systems answer generic or domain-specific queries, search computing enables answering questions via a constellation of dynamically selected, cooperating search services. The idea is simple, yet pervasive. New foundational theories are needed, rooted into formal disciplines such as mathematics, statistics, and optimization theory. New language and description paradigms are required for expressing queries and for discovering services. New interfaces and protocols help capturing ranking preferences and enabling their refinement. Semantic domain knowledge helps enriching terminological knowledge about objects being searched. Ranking is always relative to individuals and context, thus the study of personal and social behaviour is also essential. Economical and legal implications of search computing must be understood and mastered. In summary, search computing is a multi-disciplinary effort which requires adding to sound software principles contributions from other sciences such as mathematics, operations research, psychology, sociology, knowledge representation, human-computer interfaces, economical and legal sciences.""","2500000","2008-11-01","2013-10-31"
"SEED","Seizing Electron Energies and Dynamics: a seed for the future","Lucia Anna Reining","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Electronic correlation causes a wide range of interesting phenomena,  such as superconductivity or the fractional quantum hall effect. It strongly impacts our surroundings – think about defect creation through a self-trapped exciton, or, in the animal world, the adhesion of a gecko on a surface (through the van der Waals attraction). Although the underlying Coulomb interaction  is « simple » and well understood, a unifying framework is still missing that would allow us to describe, analyze, understand and predict all those phenomena on the same footing. In this project we will introduce and establish a completely new method for the calculation of properties of correlated electron systems including ground state total energies, excitation spectra, electron-phonon coupling and non-equilibrium dynamics. The method is based on a non-perturbative solution of a multidimensional functional differential equation. This equation is the SEED from which distinct sub-lines of research will be grown.
Based on my widely recognized experience in the field of many-body physics and starting from recent results of an exploratory study, the project will encircle the problem working on different levels of approximation, each of them introducing new physics. Thus every step along the project will allow us to tackle challenging questions, such as: “Does strong coupling in a material lead to new or exotic elementary excitations?” or “What can we say about multi - exciton generation, and how could it be tuned?”. These questions and our theoretical answers will be embedded in a tangible context through the study of emerging topics including Mott insulators and materials for photovoltaic applications. Each of these theoretical steps and planned applications carries the potential for breakthrough; together, they promise a seismic shift in our understanding of correlated processes and in our capability to predict new materials properties.","1700000","2013-02-01","2018-01-31"
"SEISMAZE","Data-intensive analysis of seismic tremors and long period events: a new paradigm for understanding transient deformation processes in active geological systems","NIKOLAI CHAPIRO","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Seismic tremors form a broad class of signals generated by internal sources that are different from regular earthquakes. Volcanic tremors have been known for a long time, and tectonic tremors associated with seismogenic fault zones have been described more recently. While the physical origin of seismic tremors remains to be fully understood, they are related to slow transient energy release processes that occur in active geological systems during the accumulation of mechanical energy that is then released during catastrophic events, such as strong earthquakes or volcanic eruptions. Therefore, seismic tremors represent a unique source of information that can be used to understand the physics of these ‘preparation’ processes and to design new monitoring and forecasting approaches.

Modern digital seismological networks record huge numbers of tremors in different active regions, and breakthroughs can be achieved with systematic exploration of these observations that includes data analysis and physical modeling. My goal is to undertake such an effort via the development of a new unified framework for the study of seismic tremors. I plan to combine advanced methods for data mining, signal processing, and numerical simulations of the generating processes, to apply these to different large datasets of volcanic and tectonic tremors.

I will develop an innovative and holistic approach based on massive analysis of observations that requires high performance computing and will be combined with advanced physical modeling of the generating dynamical processes. This will produce the new framework that can be used on the one hand for an understanding of the physical tremor-generating mechanisms, and on other hand for the development of new adaptive methods for monitoring volcanoes and seismic faults. The implementation of these will involve machine learning approaches to gain information from continuous fluxes of data from dense seismological networks.","2490000","2019-01-01","2023-12-31"
"SeismoSun","Magnetohydrodynamic Wave Diagnostics of the Solar Atmosphere in the Era of Transformative High-Resolution Observations","Valeri Nakariakov","THE UNIVERSITY OF WARWICK","Funding is sought for a 5-year project supporting two postdoctoral research assistants and two 4-year PhD studentships to carry out a research programme on qualitative advances of our understanding of basic physical processes operating in the atmosphere of the Sun. This will be accomplished by the use of the unprecedented high-resolution data obtained with the new generation of solar space missions, which allow us to fully implement the novel technique for plasma diagnostics, based upon magnetohydrodynamic waves. The project aims to reveal the mechanisms behind the most intriguing and disturbing solar phenomena: the heating of the corona and the physics of solar flares and mass ejections. The specific objectives are to develop and apply innovative techniques for  determining the plasma’s heating function and the characteristic spatial scale of fine structuring; the method for instant diagnostics of plasma parameters by propagating wave trains; the technique for the determination of microphysical processes by macroscopic observables; to discriminate observationally between the theoretically predicted regimes of powerful releases of magnetic energy; to create a theoretical and observational foundation for stellar coronal seismology; and to implement novel methods for the analysis of wave phenomena in spatially-resolved observational data. The project is based upon the synthesis of analytical and numerical theoretical studies and forward modelling of observables, accounting for both magnetohydrodynamic and kinetic effects, and the analysis of multi-wavelength data obtained with space missions SDO, Hinode and STEREO and the Nobeyama radioheliograph. The knowledge to be gained from the project – robust and detailed diagnostics of key parameters and fundamental processes operating in the solar atmosphere – is of vital importance for the creation of models with a predictive value, and for further progression of solar, space and astro plasma physics.","1590035","2013-04-01","2018-03-31"
"SELFCOMPLETION","UV-Completion through Bose-Einstein Condensation: A Quantum Model of Black Holes","Georgi Dvali","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","The project addresses the two greatest unresolved problems in quantum field theory and gravity.
The question of UV-completion  beyond the Planck length  and  the mysteries of black holes.  It is grounded on a recent program of research where we have put forward a fundamentally different unifying approach  to both of these problems.
This approach is based on modeling Black Holes as self-sustained Bose-Einstein condensates of long wave-length gravitons with the very peculiar property of being stuck at the critical point of a quantum phase transition. This quantum model of black holes is the outcome of understanding the UV-completion of gravity not taking place at the expense of some new dynamics of very short-wavelength degrees of freedom but rather at the expense of long-wavelength collective excitations of the above graviton Bose-Einstein condensate.
Apart of being of undoubted theoretical value for our understanding of black hole physics and its role in the UV-completion,  the new framework has important implications for astrophysics, for LHC searches of micro black holes and for studies of alternative UV-completions of the Standard Model, as well as for making connections between gravity and condensed matter physics, both theoretical and experimental.  Our project is fully devoted to the exploration of this new framework with special emphasis in the development of concrete experimental predictions.","1167183","2014-02-01","2019-01-31"
"SELFORG","Self Organization in Cytoskeletal Systems","Andreas Bausch","TECHNISCHE UNIVERSITAET MUENCHEN","The requirements on the eukaryotic cytoskeleton are not only of high complexity, but include demands that are actually contradictory in the first place: While the dynamic character of cytoskeletal structures is essential for the motility of cells, their ability for morphological reorganisations and cell division, the structural integrity of cells relies on the stability of cytoskeletal structures. From a biophysical point of view, this dynamic structure formation and stabilization stems from a self-organisation process that is tightly controlled by the simultaneous and competing function of a plethora of actin binding proteins (ABPs). The ABPs can be classified to regulate (1) the polymerisation and depolymerisation kinetics of the individual filaments, (2) the structure of the network by crosslinking and bundling the filaments and (3) the dynamic reorganisation and contraction mediated by molecular motors.
The major goal is to obtain a sound physical understanding of cellular self organizing principles, by successively increase the complexity of the experimental system in a bottom up approach. Three work packages are defined: (i) the microscopic mechanism of the interaction of ABPs with actin filaments on the treadmilling behaviour of actin. (ii) the effect of confinement, hydrodynamic flow and crosslinking proteins in a 2D high density motility assay and (iii) developing a reconstituted 3D active gel composed of actin filaments and crosslinking molecules. To this end, we will introduce and combine established and new biophysical and biochemical methods and interdisciplinary approaches, with a major emphasis on quantitative imaging and analysis techniques. The outcome of the proposed research will have important consequences and impact in fields ranging way beyond biophysics. Direct implications in rheology, polymer physics, material sciences and cell biology are evident, but also new aspects for the construction of novel biomaterials can be expected.","2285532","2012-04-01","2017-12-31"
"SELFPHOS","Design and Self-Assembly of Organometallic-Based Polypnictogen Materials and Discrete Nano-sized Supramolecules","Manfred Scheer","UNIVERSITAET REGENSBURG","In view of current developments in the fields of porous materials and discrete nano-sized molecules and aggregates the lack of organometallic-based compounds acting as nodes together with functionalized organic linkers in such materials and as linkers and building blocks for nano-sized spheres and aggregates is obvious. By using organometallic polyphosphorus compounds it was possible to synthesize unprecedented prototypes of such materials and molecular nano-sized superspheres. These ground-breaking discoveries will be subsequently further developed to excess a qualitatively novel level of research by using polypnictogen starting materials. Key targets will be the generation of rigid 3D organometallic-based materials, discrete supramolecular nano-sized aggregates (charged moiety approach) and novel fullerene-like supramolecules as nano-spheres, nano-capsules and nano-wheels (neutral moiety approach). Especially the latter approach will generate unprecedented spheres and molecules which are extreme in size and function as there are multifunctional binding sites; multi-magnetic properties; tuning templates in size; generating, encapsulating and releasing highly reactive intermediates and reaction components. Finally, the work will move beyond our knowledge of known structurally characterized fullerenes by the development of non-carbon based alternatives within and beyond the fullerene topology.","2499853","2014-02-01","2019-01-31"
"SEPON","Search for emergent phenomena in oxide nanostructures","Falko Netzer","UNIVERSITAET GRAZ","Oxide nanostructures in low dimensions on well-defined metal surfaces form novel hybrid systems with tremendous potential and impact in fundamental research and for the emerging nanotechnologies. The focus of the project is on the fabrication of two-, quasi-one-, and quasi-zero-dimensional oxide nanostructure model systems suitable for elucidation of their emergent properties in terms of structure, electronics, magnetism, and catalytic chemistry. This will be achieved by controlled self-assembly in ultrahigh vacuum, with atomic-scale precision, and in-situ characterisation employing the full palette of modern surface science methodology. Established kinetic preparation routes as well as a new approach to steer the self-assembly via external fields will be applied to the growth of a variety of transition metal oxides on suitable substrate surface templates. The stabilisation mechanism of polar oxide surfaces in nanoscale oxide objects, the catalytic chemistry of a nanoscale  inverse model catalyst  consisting of oxide nanowires coupled to an array of one-dimensional metal step atoms, and the magnetic properties of a surface-supported oxide quantum dot superlattice will be among the emergent phenomena to be probed in this project. Such fundamental questions will be addressed in a close collaboration between state-of-the-art experimental and theoretical techniques. The possibility to separate dimensionality from nanoscale effects made possible by the model systems created here will add an extra dimension in the understanding of oxide nanophase systems.","2026800","2008-12-01","2013-11-30"
"SEQCLAS","A Sequence Classification Framework for Human Language Technology","Hermann Josef NEY","RHEINISCH-WESTFAELISCHE TECHNISCHE HOCHSCHULE AACHEN","This project will develop a unifying framework of novel methods for sequence classification and thus make a major break-through in automatic speech recognition and machine translation, advancing these areas of human language technology (HLT) beyond state-of-the-art. Despite the huge progress made in the field, the specific aspect of sequence classification has not been addressed adequately in the past research in these disciplines and remains a big challenge. The proposed project will provide a novel framework under consistent consideration of the leading aspect of sequence classification. It will break the ground for a deeper, more comprehensive foundation for sequence classification and pave the way for a new generation of algorithms that will put human language technology on a more solid basis and that will accelerate progress in the field across several disciplines. 
The leading research objectives are: 1. A novel theoretical framework for sequence classification. 2. Consistent sequence modeling across training and testing, which is specifically lacking in machine translation. 3. Adequate sequence-level performance-aware training criteria to learn the free parameters of the models. 4. Investigation of (true) unsupervised training for HLT sequence classification: its principles, its prerequisites, its limitations and its practical usage. The study of these four problems will provide key enabling techniques for HLT sequence classification in general that will carry over to and create high impact on the areas of speech recognition, machine translation and handwritten text recognition. Using our top-ranking research prototype systems, we will verify the validity and effectiveness or our research on public international benchmarks.","2500000","2016-08-01","2021-07-31"
"SFN","Soft Matter Nanotechnology to Create Life-Like Machines","Owe Orwar","CHALMERS TEKNISKA HOEGSKOLA AB","We propose several new soft-matter nanotechnology-bases where the underlying principles of operation as well as the materials used are derived or inspired by biological systems. Specifically, in one aspect we wish to develop nanoscale biomimetic components, some of them with complex capacities (such as memory functions) that can perform a task in a useful device setup and that are mainly KT-driven (thermal energy-no external power source needed). A central issue relates to remote control of nanoscale soft-matter devices using microfluidics integration. Specifically, control of reactor shape, volume, connectivity, reactant concentrations etc are proposed. In another aspect, we wish to address central questions in biochemistry, biophysics and cell biology pertaining to among other things nanotube-mediated transport in cell-and cell-hybrid networks, and reaction dynamics in geometrically fluctuating nanoenvironments.","2500000","2009-01-01","2013-12-31"
"SHADOKS","Active nanofluidics towards ionic machines","Lydéric BOCQUET","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","Filtering and water purification rely traditionnally on the concept of passive sieving across properly decorated nanopores. Such basic separation principle contrasts with the highly advanced membrane processes existing in Nature, which harness the full subtleties of active transport across channels. This involves advanced functions like ionic pumps, ultra-high selective channels, or voltage-gated nanopores, which all play a key role in many vital needs and neuronal functions.
The Shadoks project aims at developing the concept of artificial ionic machines, based on active nanofluidic transport.  This is an experimental project targeting a fundamental proof of concept. It moreover involves a strong theoretical counterpart, essential to experimental advances and prototyping. I will investigate a wealth of strongly non-equilibrium transport phenomena occurring at the nanoscales, taking advantage of our unique know-how in building nanofluidic heterostructures, in particular made of carbon and boron-nitride. I target ionic Coulomb blockade, on/off voltage-gated nanopore, ionic pumps, dynamical osmosis. These processes allow to tune ionic fluxes against the gradients and induce out-of-equilibrium charge separation, hereby conceiving active sieving as a novel route for separation and desalination. Those new building blocks will subsequently be assembled to create advanced bio-inspired membrane functionalities. We will use ionic pumps to store and deliver charge carriers on demand, akin to the triggered electric shock of the electric eel. Furthermore we use the active nanofluidics building blocks to mimic a basic machinery of neuronal processes. I target in particular to build an artificial dendritic spine, as an ionic information transmitter. As an ultimate goal, this is a route towards elementary neuronal computational processes based on the artificial ionic machines.","2431000","2018-07-01","2023-06-30"
"SHRINE","Seamless Human Robot Interaction in Dynamic Environments","Martin Buss","TECHNISCHE UNIVERSITAET MUENCHEN","A major barrier exists preventing today's and future interactive robots from smoothly acting in joint workspaces in real-world and in an efficient and socially compatible manner on human terms. Robots lack the ability to plan actions in a timely manner in order to match dynamics of environment and humans. Environment and human motion dynamics have to be known and represented in a way that sufficient prediction and planning quality is provided even in complex dynamic scenarios with many interaction partners accounting for social aspects. Robots need to be aware of human communication principles in order to estimate intentions and to predict future behavior.
SHRINE aims at breaking this barrier in order to provide future robots with interactive capabilities similar to those of humans and to facilitate joint action of humans and robots sharing one world. SHRINE targets an integrated approach towards interaction in complex dynamically changing human-centered environments in terms of a hierarchical framework. SHRINE will investigate dynamic systems approaches to haptic and affective communication in human-robot interaction to consider intention and affect within an integrated robotic control and action planning framework. SHRINE seeks to extend the conventional, purely physics-driven motion prediction approach by estimating intentions of humans and merging these estimates with the dynamic physical environment model. SHRINE will integrate findings from psychology and sociology into the latest path planning and navigation methods.
The approach of SHRINE is highly innovative with pioneering character in the integrated approach as well as in the various sub-fields, strongly interdisciplinary bridging engineering approaches and human sciences, and constitutes visionary high-risk research with high-impact on future technologies in the fields of personal assistant and care robots, human-robot collaboration in manufacturing, and autonomous robots in hum-centered environments.","2490000","2011-05-01","2016-04-30"
"SILAMPS","Silicon integrated lasers and optical amplifiers","Kevin Peter Homewood","UNIVERSITY OF SURREY","""This project is a six year programme of work to develop fully integrated optical emitters, lasers and optical amplifiers in silicon.  Recent years have seen tremendous advances in the development of silicon photonic devices.  However, the last hurdle to full silicon photonic systems and optical data transfer on and between integrated circuits are electrically pumped optical amplifiers and lasers in silicon using a CMOS compatible technology.  Consequently, there have been massive efforts worldwide to search for efficient light emission from silicon.  Our team made a major initial breakthrough producing the first LED in bulk silicon - published in NATURE (1997).  Although a world first, this device only operated efficiently at low temperatures.  This problem was solved using a new nanotechnology - dislocation engineering - reported in NATURE (2001) - and crucially uses only conventional CMOS technology.  The development of this into a silicon injection laser and optical amplifiers is the essential next step for high technology high value applications. We have recently made a further breakthrough by obtaining extraordinary optical gain in erbium doped silicon that now offers a realistic route to this goal. Currently the incorporation of lasers and amplifiers on silicon platforms can only be achieved hybridizations of active devices based on III-V materials """"pasted"""" on to silicon waveguides and cavities.  Gain has been reported using four-wave-mixing and Intel has recently demonstrated a Raman laser in silicon but both rely on purely optical-to-optical transitions and are fundamentally unable to be electrically pumped.  We believe we have the only route that has the potential to produce electrically pumped amplifiers and lasers with room and higher temperature operation and that is capable of genuinely being fully integrated into silicon using standard silicon process technology.""","1928021","2009-01-01","2014-12-31"
"SILVER","Silver Isotopes and the Rise of Money","Francis ALBAREDE","ECOLE NORMALE SUPERIEURE DE LYON","Silver was the primary metal of economic exchange and military finances in ancient Mediterranean and Near-Eastern societies. Silver isotopes will help quantify monetization of these societies by identifying Ag mineral sources, monetary sinks, and its major transfer routes. High-precision stable Ag isotope analysis initiated in Lyon has shed new light on the provenance of silver coinage. This is because Ag isotopes are distinctive of coinage’s intrinsic value in contrast to traditionally-used Pb and Cu isotopes, which may characterize impurities or additives. 
The common belief that PbS (galena) ores accounted most of the silver mined in the antique world will be tested. We will extract Ag from ores around the Mediterranean and test PbS prevalence over As and Sb sulfosalts and low-temperature ores with Ag, Cu, and Pb isotopes and trace elements. 
Our work will address major questions: (i) understand the sources of unminted silver as a precursor to coinage; (ii) use Ag isotope fingerprinting of the earliest coinages of Athens to identify the contributions of Greek mines to the development of the world’s first democracy; (iii) map the Greek and Persian mines which sourced the treasure captured by Alexander the Great, and investigate the spread of its silver; (iv) study the causes of the monetary reform of the Roman Republic in 211 BC; and (v) model the silver cycle from mines to coinage and artefacts in its economic context. 
In the short term this project represents radical scientific innovation, which will pave the way for a global and quantitative understanding of the history of monetary development in the ancient Mediterranean. In the long term, it will contribute to the emergence of a community of analysts, numismatists and economic historians with shared expertise about the monetization of ancient societies and their management of precious metal resources.","2496243","2017-10-01","2022-09-30"
"SIMCOFE","Simulating correlated fermions","Matthias Troyer","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","""This proposal concerns simulations for correlated fermionic quantum systems where strong quantum effects give rise to a plethora of fascinating phenomena and new methodological developments are needed for their understanding. Ultracold Fermi gases in optical lattices provide a unique opportunity:  being simpler and more controlled and tuneable than condensed matter material they are not only ideal experimental realizations of correlated fermions but also provide an excellent testing ground for numerical simulation methods. Over the past years we have developed new algorithmic approaches, including continuous time quantum Monte Carlo (QMC) methods and diagrammatic Monte Carlo methods for fermionic simulations. These new methods provide performance improvements of many orders of magnitude compared to previous state of the art methods. In this project we will further develop these algorithms and implement them on new massively parallel petaflop supercomputers. This will enable reliable simulations of correlated fermionic quantum systems, such as single and multi-band Hubbard models first in cold atomic gases, and later in realistic models for materials. A second line of research will be the development of Kohn-Sham density-functional theory (DFT) for ultracold atomic gases. DFT based on density functional for the electron gas is the main workhorse for materials simulation, but it is challenging to apply it to the strongly correlated regime. With a DFT method for atomic gases we will, on the one hand, be able to solve challenging problems in ultracold gases. On the other hand and maybe even more important will be the ability to use cold gases to improve hybrid functionals for the strongly correlated regime. This will form a strong link between ultracold gases and materials science, stronger than the Hubbard model that is the focus of attention at the moment.""","2023980","2012-05-01","2017-04-30"
"simDNA","Advanced multiscale simulation of DNA","Modesto Orozco López","","The availability of new computers and software is making possible the theoretical representation of DNA, increasing then our knowledge on the behavior of one of the most relevant biological macromolecules. Unfortunately, current simulation procedures present two major problems, which handicapped their use: i) classical force-fields present well known biases, which limit their accuracy; ii) current atomistic procedures are limited to study systems in the range of 100 base pairs (around 34 nm long), while the DNA of the simplest prokaryotic organisms is one billion times larger. The main objective of this proposal is the development of a multiscale simulation technology for the study of DNA, which will cover, with different levels of resolution, but with the same physical roots, the entire range of DNA scales, from nucleobase (Ǻ-scale) to the human genome (m-scale). Our roadmap will start for the development of a polarized force-field which will be parametrized against a variety of experimental and theoretical data. In a second stage, we will analyze a very large number of DNA sequences in different epigenetic and packing states and we will create a MoDEL-like database of DNA trajectories. In a third stage we will derive coarse grained and essential dynamic-based strategies for ultra-fast accurate simulations for medium to long segments of DNA.  In the last stage of this project we will develop a new mesoscopic model, which will go beyond the harmonic nearest-neighbors model, accounting for multi-modality, for neutralization-induced deformations, and for changes in DNA properties related to epigenetic changes. Using these models we expect to analyze fine details of (human) genome structure and regulation, trying to reach the connection point between physical properties of DNA, chromatine structure, epigenetic signatures and gene regulation","1961400","2012-07-01","2017-06-30"
"SIMOSOMA","Single molecules in soft matter: dynamical heterogeneity in supercooled liquids and glasses","Michel Orrit","UNIVERSITEIT LEIDEN","Single-molecule optical microscopy provides average-free, dynamical and structural information about condensed matter at molecular scales. Single fluorescent molecules can now be located and tracked with a spatial resolution as high as a few tens of nanometers, even at depths as large as several microns. These capabilities are ideal to link the macroscopic physical properties of soft condensed matter with the structure, organization and dynamics of the constituent molecules. Perhaps the most surprising conclusion drawn from single-molecule observations is the unsuspected heterogeneity of molecular assemblies, both in time and space, which had remained largely hidden in conventional ensemble experiments. The structural glass transition is said to be one of the hardest open problems in condensed matter science. Although most agree on the crucial part played by heterogeneity in this process, the guesses vary wildly as to the scale and relaxation times of the inhomogeneities. Our recent discovery of glassy rheology in supercooled glass formers, following earlier observations of heterogeneity, has been received with much interest in the complex liquids community. I am convinced that single-molecule studies have the potential to radically change our view of supercooled liquids and glasses. In a broader sense, molecular insight from chemical physics complements the general ideas developed by statistical physicists. I believe it is the missing link toward a molecular control of the physical properties of soft materials. I propose to perform a broad range of novel single-molecule experiments using a micro-rheological cell to apply mechanical stress, strains and/or temperature jumps. In particular, we will perform mechanical studies of solid-solid friction, and temperature-jump studies of single proteins and single protein complexes.","1836000","2009-04-01","2014-03-31"
"SINGLEION","Spectroscopy and microscopy of single ions in the solid state","Vahid Sandoghdar","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","The progress in optical spectroscopy has made it is possible to study individual quantum emitters. However, only a few select “bright” emitters have been detected so far, leaving a large gap in the choice of critical parameters such as wavelength, coherence time, and energy level schemes. In this project, we develop methods for the detection of single emitters with long fluorescence lifetimes. In particular, we concentrate on rare earth ions embedded in crystals, which are of great technological and fundamental interest. To achieve this goal, we exploit methods from ultrahigh resolution microscopy, laser spectroscopy, scanning probe technology, cavity quantum electrodynamics, and plasmonics.
The first approach to the detection of single ions at cryogenic temperatures will be to perform direct fluorescence excitation as well as absorption spectroscopy to address single Pr3+ ions spectrally within the inhomogeneous line of the sample. Here, we will develop a tunable laser system with sub-kHz linewidth for probing the narrow transitions of the ions.  We expect a signal-to-noise ratio of about 10 in this first step. In order to improve this, we will enhance the emission of ions by pursuing two strategies. In the first case, we shall embed doped crystalline films in monolithic Bragg microcavities. In the second approach, we use plasmonic nanoantennas to reduce the radiative lifetime of the ions in the near field. The well-defined energy levels of ions provide ways for the preparation of long-lived coherent states for use in quantum information processing. Furthermore, access to the homogeneous spectra of ions at different temperatures and doping concentrations will shed light on fundamental open questions regarding their interaction with their matrices.","1925673","2011-08-01","2016-07-31"
"SIREN","Stability Islands: Performance Revolution in Machining","Gábor Stépán","BUDAPESTI MUSZAKI ES GAZDASAGTUDOMANYI EGYETEM","""Cutting went through a revolution in the 1990s when high-speed milling (HSM) was introduced: the sculpture-like workpieces produced with high precision and efficiency resulted in one order of magnitude less parts in cars/aircrafts, which kept this traditional technology competitive at the turn of the century. This has been followed by an incremental development when not just the cutting speeds, but depths of cut and feed rates are pushed to limits, too.
The limits are where harmful vibrations occur. Cutting is subject to a special one called chatter, which is originated in a time delay: the cutting edge interferes with its own past oscillation recorded on the wavy surface cut of the workpiece. In 1907, the 3rd president of ASME, Taylor wrote: “Chatter is the most obscure and delicate of all problems facing the machinist”.
In spite of the development of the theory of delay-differential equations and nonlinear dynamics, Taylor’s statement remained valid 100 years later when HSM appeared together with a new kind of chatter. The applicant has been among those leading researchers who predicted these phenomena; the experimental/numerical techniques developed in his group are widely used to find parameters, e.g. where milling tools with serrated edges and/or with varying helix angles are advantageous.
The SIREN project aims to find isolated parameter islands with 3-5 times increased cutting efficiency. The work-packages correspond to points of high risk: (1) validated, delay-based nonlinear modelling of the dynamic contact problem between chip and tool; (2) fixation of the tool that is compatible with a dynamically reliable mathematical model of the contact between tool and tool-holder; (3) up-to-date dynamic modelling of the spindle at varying speeds.
High risk originates in the attempt of using distributed delay models, but high gain is expected with robust use of parameter islands where technology reaches a breakthrough in cutting efficiency for the 21st century.""","2573000","2014-03-01","2019-02-28"
"SISYPHE","Species Identity and SYmbiosis Formally and Experimentally explored","Marie-France Sagot","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","Symbiosis is described as a close relationship between different biological species. It is a pervasive phenomenon, often of a long term nature. It has been estimated that 50% of all known species are parasites, i.e. maintain a symbiotic relation with another species from which they benefit while the partner in the relation is harmed, and that close to a 100% of all plants and animals are parasitised as individuals. Indeed, there are thought to be 10 times more bacterial cells in a human body than human cells. There is growing recognition that symbiosis has a profound impact on the origin and maintenance of the biome and of its ecosystems, on the health of living organisms, and even on sex! Symbiosis thus appears essential to understand some of the most fundamental evolutionary and functional questions related to living organisms. Nevertheless, although symbiotic relationships have been studied by biologists since the early 19th century, they remain little explored by computational biologists. Yet, investigating the enormous variety of such relationships raises formidable mathematical and computational issues. By a highly pluri-disciplinary approach that blends mathematics, algorithmics and wet-lab experiments, we propose to do an intensive, large-scale exploration of the huge variety of genomic and biochemical landscapes observed in the symbiont world, at the interface between symbionts and hosts, and of both with their environment. Our objective is to arrive at a clear view of the importance of symbiosis. This could have far-fetched theoretical and practical implications, notably on our notions of health, our relation with our environment, and our idea of what is species identity, including our idea of what is an individual .","2333272","2010-04-01","2015-03-31"
"SIZEFFECTS","Size effects in fracture and plasticity","Stefano Zapperi","UNIVERSITA DEGLI STUDI DI MILANO","Understanding how materials respond to external mechanical perturbation is a central problem of science and engineering. While for most practical purposes it is useful to idealize the mechanical response of a material as a deterministic function of the externally applied perturbation, disorder and fluctuations are unavoidable, leading to sample-to-sample variations and non-trivial size effects. The size dependence of strength is a well known but still unresolved issue in the fracture of materials and structures. The difficulty in addressing this problem stems from the complex interplay between microstructual heterogeneity and long-range elastic interactions. Furthermore, in micro and nanoscale samples, the plastic yield strength displays size effects and strain bursts, features that are not present in macroscopic samples where plasticity is a smooth process. Large fluctuations both in fracture processes and in microscale plasticity make the use of conventional continuum mechanics problematic and calls instead for a statistically based approach.  These problems are becoming particularly important in the current miniaturization trend towards nanoscale devices, since the relative amplitude of fluctuations grows as the sample size is reduced. In this project, concepts and tools of statistical mechanics  are used to address size effects and fluctuations in the irreversible deformation and failure of materials. The general objective is to provide a quantitative theory that can be used as base for setting reliable safety factors. The theory will be based on the renormalization group and will be guided and validated by large scale numerical simulations such as molecular dynamics, discrete dislocation dynamics and disordered network models. Finally, we will analyze experimental data present in the literature.","2500000","2012-03-01","2018-02-28"
"SM-GRAV","Gravity, Holography and The Standard Model","Ilias Kyritsis","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","""The main thrust of this proposal is to investigate implications of a recent correspondence (string theory (ST) vs. gauge theory) to the physics beyond the Standard Model (SM) and its coupling to gravity. Instead of relying on the string picture of the unification of all interactions with gravity, I propose to look at its dual version: 4d quantum field
theories (QFT). The different perspective is expected to provide 3 distinct results: 
(a) A QFT view of the SM embedding in string theory 
(b) Novel phenomena and properties that are hard to see in the string theory picture. 
(c) A """"dual"""" view that would be valid in non-stringy regimes. 
The key idea is that gravity, as observed in nature, is emergent: it is the avatar of a (hidden) large-N (near)
CFT that is interacting with the SM at high energy (the Planck scale). Such an approach provides an appealing UV completion to the SM+gravity: a UV complete four-dimensional QFT. There are, however, many questions that need to be
addressed in order for this setup to be a viable physical theory: 
1. Why is the gravitational force four-dimensional (instead of higher-dimensional as suggested by standard holography)? 
2. Why does the coupling of the gravitational force to the SM satisfy the equivalence principle to such a high accuracy? 
3. What are other universal interactions with the SM model implied in this picture? What are their phenomenological consequences? 
4. How can one construct, precise and controllable models for this setup? 
5. How is Cosmology emerging in this picture? How do the important problems associated with it get resolved?
 SM-GRAV will address all of the above questions using the tools of QFT, of string theory and the AdS-CFT correspondence. The outcome of the proposed research is expected to be a concrete and quantitative model/scenario for the emergence and coupling of the """"gravitational sector fields"""" to the SM model and  the novel phenomenological implications for particle physics and cosmology.""","1649238","2016-01-01","2020-12-31"
"SMALLOSTERY","Single-molecule spectroscopy of coordinated motions in allosteric proteins","Gilad HARAN","WEIZMANN INSTITUTE OF SCIENCE LTD","Critical for the function of many proteins, allosteric communication involves transmission of the effect of binding at one site of a protein to another through conformational changes. Yet the structural and dynamic basis for allostery remains poorly understood. In particular, there is no method to follow coordinated large-scale motions of domains and subunits in proteins as they occur. Since the subunits of allosteric proteins often contain multiple domains, any such method entails probing the dynamics along several intra-protein distances simultaneously.
This proposal aims at ameliorating this deficiency by creating the experimental framework for exploring time-dependent coordination of allosteric transitions of multiple units within proteins. Our methodology will rely on single-molecule FRET spectroscopy with multiple labels on the same protein and advanced analysis. We will explore fundamental issues in protein dynamics: relative motions of domains within subunits, propagation of conformational change between subunits, and synchronization of these motions by effector molecules.
To investigate these issues, we have carefully selected three model systems, each representing an important scenario of allosteric regulation. While the homo-oligomeric protein-folder GroEL conserves symmetry in a concerted transition between major structural states, the symmetry of the homo-oligomeric disaggregating machine ClpB is broken via a sequential transition. Symmetry is attained only after binding to DNA and ligands in the third system, the family of RXR heterodimers. 
This exciting project will provide the very first catalogue of coordinated and time-ordered motions within and between subunits of allosteric proteins and the first measurement of the time scale of the conformational spread through a large protein. It will enhance dramatically our understanding of how allostery contributes to protein function, influencing future efforts to design drugs for allosteric proteins.","2484722","2017-05-01","2022-04-30"
"SMART-POM","Artificial-Intelligence Driven Discovery and Synthesis of Polyoxometalate Clusters","Leroy Cronin","UNIVERSITY OF GLASGOW","We outline a 5 year programme that introduces a new platform for the preparation, understanding, and exploitation of precisely defined nano-molecules / materials based upon the assembly of molecular metal oxide precursors (polyoxometalates) under non-equilibrium conditions with well-defined physical properties using automated intelligent feedback. We will elucidate the mechanism of assembly of these gigantic molecules and  devise a set of rules similar to the magic numbers found in gold nanoclusters, using these to break the 10 nm size barrier for a single molecule. Targeted properties include photochemical and electrochemical sensors, bistable molecules, doped traditional oxides with polyoxometalates, and new catalysts including water oxidation via a Universal Building Block (UBB) approach that links properties of the building blocks with emergent properties of the resulting clusters and materials for the first time. The new approach includes the conversion of batch to flow synthesis not only for automation, but to understand fundamental mechanistic aspects, and to use artificial intelligence algorithms to help move through the myriad of possible combinations (without needing to synthesise every possible molecule).  The SMART-POM approach is therefore not merely automation of one-pot chemistry, but an entirely new paradigm building on our recent developments and will allow us to move through a vast combinatorial space effectively only locating areas of novelty via feedback control. This feedback will be used to discover, design, and develop complex, adaptive and functional metal oxide-based materials based upon sensory feedback from the physical properties measurements. Thus SMART-POM will open up a whole new synthetic space, give mechanistic understanding, and allow the discovery of molecules with potential real-world applications. Finally, we will aim to extend the SMART-POM paradigm to other areas of chemistry which will benefit from the search for novelty.","2464532","2015-11-01","2020-10-31"
"SmartCast","Smart casting of concrete structures by active control of rheology","Geert De schutter","UNIVERSITEIT GENT","Concrete production processes do not take full advantage of the rheological potential of fresh cementitious materials, and are still largely labour-driven and sensitive to the human factor. SmartCast proposes a new concrete casting concept to transform the concrete industry into a highly automated technological industry. Currently, the rheological properties of the concrete are defined by mix design and mixing procedure without any further active adjustment during casting. The goal of this proposal is the active control of concrete rheology during casting, and the active triggering of early stiffening of the concrete as soon as it is put in place. The ground-breaking idea to achieve this goal, is to develop concrete with actively controllable rheology by adding admixtures responsive to externally activated electromagnetic frequencies. Inter-disciplinary insights are important to achieve these goals, including inputs from concrete technology, polymer science, electrochemistry, rheology and computational fluid dynamics.

We will develop 4 new experimental test set-ups allowing to study active rheology control during different phases of the casting process: 1)concrete pumping (control of slip layer), 2)while flowing in the formwork (bulk control of rheology), 3)while flowing through formwork joints (control of formwork tightness), and 4)once the concrete is in its final position (trigger stiffening). Well-designed polymers with the desired response to the applied activation will be added to the concrete during mixing. The experiments will be analysed by advanced computational flow modelling based on fundamental rheological laws. Special attention will be paid to the compatibility of all responsive polymers selected for the different control phases. SmartCast will mean a paradigm shift for formwork-based concrete casting. The developed active rheology control will provide a fundamental basis for the development of future-proof 3D printing techniques in concrete industry","2498750","2016-10-01","2021-09-30"
"SMARTGATE","""Smart Gates for the """"Green"""" Transistor""","Athanasios Dimoulas","""NATIONAL CENTER FOR SCIENTIFIC RESEARCH """"DEMOKRITOS""""""","Ultra-low voltage/power operation is expected to be an important requirement for future nanoelectronics allowing more dense and fast circuits on one hand and enabling the operation of energy efficient intelligent autonomous systems on the other. In present day devices quite a lot of power is consumed during switching since it requires a minimum bias of 60 mV on the gate to overcome a potential barrier and increase the transistor current by a decade, a process which is fundamentally limited by thermal Boltzmann statistics. We propose the development of novel negative capacitance “smart” gates with a positive feedback and internal amplification to overcome the “Boltzmann tyranny” and obtain steeper slope “green” transistors capable of operating at very low voltage. Metallic systems with a low density of states could provide the required dominant negative contributions to the capacitance due to strong carrier correlation effects. Such metallic systems made of 2D Dirac fermions with linear dispersion bands are supported in graphene and on the surface of the newly discovered topological insulators having the very interesting property that they offer a nearly zero density of states at the band crossing near the charge neutral point. We propose here the graphene and Bi2Se3-based topological insulators as the key components of the targeted “smart” gates. We aim at developing complex gate structures facing the challenges of growth of high purity and high crystalline quality graphene and Bi2Se3 thin films in combination with conventional dielectrics and metals on Si semiconductor in an effort to obtain the required properties and ensure their robust functionality at room temperature. Possible negative capacitance effects will be investigated in terms of generic capacitor electrical characterization, while transistor devices with optimum smart gates will be fabricated to prove the principle of steep slope switching.","1221611","2012-01-01","2016-07-31"
"SMARTMET","Adaptive nanostructures in next generation metallic materials: Converting mechanically unstable structures into smart engineering alloys","Dierk Rolf Raabe","MAX PLANCK INSTITUT FUR EISENFORSCHUNG GMBH","The design of advanced high strength and damage tolerant metallic materials for energy, mobility, and health applications forms the engineering and manufacturing backbone of Europe's industry. Examples are creep-resistant Ni-alloys in power plants and plane turbines; ultrahigh strength steels, Al- and Mg-alloys for light-weight mobility and aerospace design; or Ti-implants in aging societies.
Since the Bronze Age the design of metallic alloys rooted in trial and error, owing to the complexity of the physical and chemical mechanisms involved and the engineering conditions imposed during manufacturing. This traditional approach has two shortcomings. First, current alloys are not developed via systematic design rules but via empirical methods. This approach is time consuming and inefficient. Second, the increase in strength via traditional hardening mechanisms always causes a dramatic decrease in ductility, i.e., making the material brittle and susceptible to failure.
SMARTMET aims at solving this inverse strength-ductility problem: The joint use of advanced synthesis and atomic characterization (expertise of PI) together with ab initio modeling (expertise of Co-PI) opens a new path to the design of next generation metallic alloys. The objective is to use these methods to identify and utilize strengthening mechanisms that allow to overcome the inverse relationship between strength and ductility. The key idea is to incorporate phases into alloys that are close or beyond their mechanical and thermodynamic stability limit. They undergo transformations under load acting as self-organized repair mechanism. SMARTMET contains risks and gains: (i) Mechanical stability through unstable phases includes the risk of material weakening but it may break the inverse strength-ductility principle. (ii) New metallurgical alloys (PI) designed via quantum mechanics (Co-PI) is risky owing to the complexity of metallic nanostructures but allows alloy tailoring based on first principles.","2920000","2012-02-01","2017-01-31"
"Smartphon","Small - and nano - scale soft phononics","Georgios FYTAS","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Colloid and polymer science allows the engineering of acoustic and optical material functionalities of hierarchical structures on various length scales commensurate with and well below the characteristic length scales of phonons and photons. Periodic structures act as both hypersonic phononic and visible light photonic crystals (phoxonics). We recently extended the decade-old field to hypersonic phononics. Many important questions in this young field are just being raised and require new conceptual and technical approaches to address them.
Powerful synthesis and assembly methods are able to create novel structures to host unconventional properties of flexibility and multi-functionality, locally resonant hypersonic soft metamaterials and topological phononic insulators. To complement our best world-wide Brillouin spectroscopy for retrieving the dispersion relations in transparent structures, two new experimental techniques based on laser-induced high frequency phonons and tapered fiber optomechanics will be implemented to engineer strong wave-matter interactions. Band structure calculations will be used as tools to model and predict the acoustic wave propagation in composite structures of varying symmetry, architecture and topology of the building components. Our novel approach, together with intricate methods of processing such materials at a large scale, shows the outline of the emerging field of polymer-and colloid-based phononics. 
Promising applications range from tunable responsive filters and one way phonon waveguides to compact acousto-optic devices and sensors and from hypersonic imaging to materials and devices, which allow for directed heat flow and recovery. To access such fundamental concepts a detailed understanding of phonon propagation in nanostructured media is a precondition. This proposal ensures that we will hear much more about currently unknown and unexpected properties and functions of soft phononics and will open up many new lines of research.","2181250","2016-09-01","2021-08-31"
"SMel","Electric field imaging of single molecular charges by a quantum sensor","Joerg WRACHTRUP","UNIVERSITAET STUTTGART","Precision measurements are a key application of quantum technology. They have brought record precision in time measurement or outstanding sensitivity in measuring a whole wealth of quantities. Bringing such measurements to the nanoscale is new. If those quantum sensor function under a variety of environments, including ambient or physiological condition they promise unprecedented application.   

SMel aims to exploit the outstanding nanoscale quantum sensing capabilities of spin defects to achieve highly sensitive charge detection down to single elementary charges under ambient conditions. On top of that the project aims to detect and image molecular polarizability with sub-molecular resolution. This will allow to use spin defects as quantum sensors to measure resonant infrared excitation with sub-molecular resolution. It is the visionary aim of SMel to refine these methods to such an extent, that charge transfer inside a single biomolecular photosynthetic reaction centre can be detected and imaged.","2422543","2017-08-01","2022-07-31"
"SMSCOM","Self-Managing Situated Computing","Carlo Ghezzi","POLITECNICO DI MILANO","SMScom will develop a consistent, integrated, and homogeneous set of methods and tools for the design, validation, and operation of dependable self-managing situational software, i.e., software that addresses a particular situation, problem, or challenge, and behaves according to the evolving situation in which it operates. Examples of situations are: the type of user (its preferences, its knowledge, ...), the physical environment (the current location of the executing environment), the time at which the application is executed, the device on which it runs, environmental conditions (like temperature, light, ...). We assume that situational changes are frequent, or even continuous, and require adaptation as the application is running and offering a service. This requires the software to be able to self-manage itself via a control loop that propagates the sensed information about situation to changes in the software structure. Several examples of situational applications already exist, and several research directions are already being explored. We argue, however, that at this stage most efforts are ad-hoc, and we miss a coherent global picture. We know how to deal with specific problems, but we lack general methods.  SMScom is a holistic approach to self-managing situational computing. The term  computing  stresses the fact that our work will be rooted into an underlying theory of the field, upon which we wish to develop unified engineering methods and tools to develop dependable self-managing situational software applications. SMScom is a far-looking software engineering research project, which aims at providing a sound and systematic approach to developing practical and useful products. As any far-looking engineering research, it is expected to provide radical innovations. Radical innovations come in two flavors: by pushing some emerging requirements to their extreme and by revisiting through them in a radical manner the way we today develop and operate software.","2544156","2008-12-01","2013-11-30"
"SNDUST","Supernova dust: production and survival rates","Michael BARLOW","UNIVERSITY COLLEGE LONDON","The dust content of galaxies is dominated by silicate and carbon grains, whose origin is the subject of much debate - are the dust grains provided mainly by red giant stars, by supernovae from massive stars, or can they grow in the interstellar medium itself from stardust seeds? My team's recent observations with Herschel of three supernova remnants, Cas A, SN 1987A and the Crab Nebula, have provided direct evidence that supernovae from massive stars can form dust masses in the range of 0.1-0.8 solar masses per event, a level at which dust evolution models for high and low redshift galaxies predict that supernovae can become the dominant contributors of dust. With both O-rich and C-rich shells, core-collapse supernovae can make both silicate and carbon particles, as observed. Most of SN 1987A's current dust mass of 0.6-0.8 solar masses appears to have been grown between 3 and 25 years after outburst, a period that is currently poorly observed for other remnants. To build on and to extend these results beyond our initial sample of three core-collapse objects, dust masses will be measured for a much larger sample of late-epoch (3-50 yrs post-outburst) supernova remnants. This will be done by using a new Monte Carlo line transfer code to model red-blue line profile asymmetries observed in 8m telescope optical spectra to derive dust masses at a range of epochs, and via JWST mid-infrared observations of SN dust emission as the dust cools. We will extend our dust and gas emission modelling code to include dust heating not just by radiation but also by particle impacts, in order to determine accurate dust masses for collisionally ionized supernova remnants covered by Herschel surveys of the Magellanic Clouds and Milky Way. The theory programme will also determine grain lifetimes against destruction by supernova remnant reverse shocks, accounting for shielding in clumps, as well as destruction lifetimes for dust in circumstellar shells impacted by supernova blast waves.","2498535","2016-06-01","2021-05-31"
"SNICC","Studying Secondary Nucleation for the Intensification of Continuous Crystallization","Marco MAZZOTTI","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Many products in the chemical, food and pharmaceutical industry are produced as powders through a crystallization process. Continuous crystallization has been for decades the process of choice for large-scale production, for instance of sugar, table salt, adipic acid, and has become more recently a key component of the new paradigm, i.e. the continuous
manufacturing of active pharmaceutical ingredients.
All product crystals of continuous crystallization processes are formed through secondary nucleation, which is both ubiquitous and elusive. Research has left several key questions unanswered, such as about where and how secondary nuclei are formed, either from the parent crystal due to  collisions or from the solution layer around it due to fluid shear; or
about the rate at which secondary nuclei are generated, with which properties, and under which conditions; or, also, about the coupling between the parent crystal microscale and the collective behavior of the ensemble of crystals suspended in a specific continuous crystallizer.
The project SNICC intends to unveil the microscale mechanisms of secondary nucleation, and to bring its scientific understanding to a level where it can be exploited to model, design, operate, optimize and control continuous crystallization processes at any desired scale. It aims at creating a comprehensive knowledge about secondary nucleation at the microscale, applicable to any type of compound, and at establishing guidelines for the intensification of continuous crystallization, based on the use of newly developed full models of different types of continuous crystallizers. This will have a major impact on both the science of crystallization and the related industrial processes. Building on the recognized experimental and theoretical expertise on crystallization at all relevant scales of the PI and his lab, a team consisting of 4 PhD students and 2 postdocs will work for five years on this challenging, interdisciplinary project.","2500000","2018-07-01","2023-06-30"
"SNLSID","Data Driven Structured Modelling of Nonlinear Dynamic Systems","Joannes Schoukens","VRIJE UNIVERSITEIT BRUSSEL","Today’s state-of-the art methods for system and control design are model based. The ever increasing demand for higher performance and efficiency pushes the systems in a nonlinear operation mode so that nonlinear models are required for their design and control. The model quality and the model building cost are becoming limiting factors for further technological developments.

To close the gap between the designers and the modellers we propose a fundamentally new approach to deliver highly structured nonlinear models meeting the designer’s needs. From a theoretical point of view, the major contribution is the development of a new nonlinear structured system identification framework. From practical point of view, the new nonlinear modelling paradigm will become an enabling technology to further push the performance and efficiency of system and control design.

We follow a three step strategy to identify structured nonlinear models:
- A top down approach in which we develop data driven structure revealing methods starting from initial unstructured nonlinear state space models.
- A bottom up approach that identifies complex block oriented models, including parallel and feedback structures starting from the best linear approximation of the nonlinear system. These models are highly structured from the start.
- An new dedicated experiment design strategy will be developed to retrieve the “best” models with the least experimental cost.

Solving these problems is far beyond the actual abilities of the system identification community.  However, our long standing recognized experience in frequency domain system identification in the presence of nonlinear distortions, and recent work by the PI guarantee the feasibility of the project.

Structured nonlinear model building has applications in traditional industrial and emerging new high technological applications, including biomechanical and biomedical applications.","2499040","2013-02-01","2018-01-31"
"SOCATHES","Solid State/Cold Atom Hybrid Quantum Devices","Reinhold Kleiner","EBERHARD KARLS UNIVERSITAET TUEBINGEN","Solid state physics and atomic physics have developed in a way that the combination of the two fields will produce massive synergetic effects and new physics. Thin film structures can be patterned and controlled down to the atomic level. Mesoscopic structures are used to create well defined two level systems and are presently explored in terms of their capability of being the basis of a quantum computer. The quantum dynamics of single electrons on a quantum dot or single Cooper pairs or flux quanta in case of superconductors can be controlled very well at temperatures in the Millikelvin range. Complementary, atomic physics has learned to control atoms and molecules almost perfectly and has turned to large ensembles of cold atoms forming e. g. Bose Einstein Condensates at low temperatures. In the present proposal we aim on the realization of such coupled solid state - atomic objects, starting with superconducting structures on the solid state side and with Rubidium atoms on the quantum optics side. We plan to investigate their fundamental properties and explore possible applications. In a second stage we consider including mechanical systems - nanoresonators - into our investigations. The heart of the experiments will be a ultra high vacuum millikelvin environment realized by a properly designed 3He-4He dilution refrigerator combined with a cold atom/BEC system. In terms of fundamental physics we will investigate the quantum nature of systems consisting of a macroscopic object like a flux quantum coherently coupled to a microscopic object like an atom. Combining solid state devices with atoms could lead to novel architectures in the field of quantum devices. In a similar spirit, ultrasensitive solid state detectors could be combined with atomic detection schemes, allowing for novel high precision measurement systems. We thus envision enormous potential for precision measurements and quantum engineered devices.","2344800","2009-01-01","2013-12-31"
"SOFT HANDS","A Theory of Soft Synergies for a New Generation of Artificial Hands","Antonio Bicchi","FONDAZIONE ISTITUTO ITALIANO DI TECNOLOGIA","""Although many advances have been made in the mechatronics and
computational hardware of artificial hands, the state of the art
appears to be only marginally closer to a satisfactory functional
approximation of the human hand than it was twenty years ago. In
my analysis, the main reasons for this are not merely techni-cal,
but invest some fundamental issues in the understanding of the
organization and control of hands, and ultimately the lack of a
theory to guide us in the search for a principled approach to
taming the complexity of hands.  In this project, I propose to
contribute to the development of the fundamental elements of such
a theory, and bring them to fruition in functional engineered
devices. I expect to be able to break through the rather slowly
moving front of the state of the art because of the combination
of two crucial, recent innovations. The first pillar, and the
prime theoretical enabler for this program, is an approach to the
description of the organi-zation of the hand sensorimotor system
in terms of geometric constraints, or synergies: correlations in
redun-dant hand mobility (motor synergies), correlations in
redundant cutaneous and kinaesthetic receptor readings (multi-cue
integration), and overall sensorimotor control
synergies. Elements of such theories have emerged recently in
neurosciences, but their exploitation in the sciences of the
artificial is an enormous potential barely touched upon till
now. The second pillar, providing the new technology needed to
build simpler and more effective artificial hands, is the
understanding of the role of variable impedance actuation in
embodying intelligent grasping and manipulation behaviours in
humans, and the availability of a new generation of “robot
muscles”, i.e. actuators capable of tuning their impedance to
adapt to the environment and the task. These ideas will be
pursued in close collaboration with specialists in related
domains of neuroscience and robotics.""","2279600","2012-06-01","2017-05-31"
"Soft-Map","Stretching soft matter performance: From conformable electronics and soft machines to renewable energy","Siegfried Günter Bauer","UNIVERSITAT LINZ","This project aims at exploring the unique properties of soft matter, inimitable with conventional solid materials, to extend basic knowledge and to trigger novel applications. Three core objectives going far beyond the state of the art are tackled: in conformable electronics stretchable ferroelectrets and rechargeable batteries promise novel electronic items. The groundbreaking approach of this proposal is the development of stand-alone stretchable electronic items including sensing capabilities for touch and other environmental stimuli, information distribution, as well as power generation and distribution. Such devices can be used in soft robots, artificial limbs and morph designs. The second objective on soft machines addresses electromechanical instabilities in soft transducers, leading to giant stroke actuators, enabling programmable Braille readers and tactile feedback in consumer electronics. Stretchable nonlinear optical materials allow for efficient phase-matched operation, opening new research fronts in adaptive photonics. The third objective on renewable energy concerns mechanical energy scavenging from environmental sources. We focus on water as electrode in deformable capacitors and as gap filling dielectric in variable capacitors. Efficient operation cycles maximizing energy of conversion are developed. The wide range of objectives, but also the possible applications require expertise in thermodynamics of soft matter, in electrical and mechanical engineering and in device implementation. The challenge of the overall approach is to base research on materials from renewable sources. We expand knowledge in applying soft materials in conformable electronics, soft machines and energy harvesting and contribute to general challenges in sustainability and energy supply. This creative project establishes new research reflected in publications, patents and devices but will also expand our fundamental understanding in stretching soft matter performance.","2494800","2012-01-01","2016-12-31"
"SoilLife","The Hidden Frontier: Quantitative Exploration of Physical and Ecological Origins of Microbial Diversity in Soil","Dani Or","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","By some accounts exploring the origins of soil microbial diversity represents a scientific frontier similar to that of space exploration in its scope, described by Curtis and Sloan (2004) as ”an immense and unexplored frontier in science of astronomical dimensions and of astonishing complexity”. The complexity is attributed to soil ecological heterogeneity reflecting interplay of spatio-temporal, physical, and nutritional variables delineating spheres of influence that define microbial habitats and function. Key to microbial life in soil is a flickering aqueous network that defines nutrient diffusional pathways and shapes microbial dispersion and interactions. We propose to develop an individual-based and spatially resolved modeling platform that explicitly considers soil pore structure and aqueous phase configuration and associated biophysical processes forming a virtual soil microcosm. The assembly of these complex ingredients into a computational platform will enable systematic hypotheses testing concerning central questions in microbial ecology that are neither addressed by present ecological theories nor emerge from standard continuum models. Specifically, the project will provide quantitative insights into effects of hydration extremes on survival strategies, the roles of space and diffusional heterogeneity, aspects of dispersion and trophic interactions, self-organization of consortia, and emergence of temporal niches in soil. The research will transform quantitative understanding of soil biophysical processes, a gap that presently limits coherent interpretation of the rapidly growing molecular-based estimates of soil biodiversity, and is essential for guiding future data collection. The research lies at the interface between environmental microbiology and soil physics cutting across disciplinary boundaries and addressing broad issues impacting soil and water quality, the functioning of global bio-geochemical cycles, and the fate of anthropogenic pollutants.","2196632","2013-02-01","2018-01-31"
"SOLCRIMET","Solvometallurgy for critical metals","Koen Binnemans","KATHOLIEKE UNIVERSITEIT LEUVEN","The recent “rare-earth crisis” has brought about the widespread realisation that the long-term availability and cost stability of many materials – not just the rare earths – can no longer be guaranteed. Increasing the levels of critical metal recycling from pre-consumer, manufacturing waste and complex, multicomponent end-of-life consumer products is considered as arguably the most important and realistic mitigation strategy. However, extracting a critical metal from complex waste is a very different challenge to that faced when attempting to produce a pure metal from a primary ore deposit. SOLCRIMET therefore develops a ground-breaking, novel approach called “solvometallurgy”, a new branch within metallurgy, next to conventional hydro- and pyrometallurgy. SOLCRIMET’s aim is to successfully apply this approach to the extraction of specific critical metals, i.e. rare earths, tantalum, niobium, cobalt, indium, gallium, germanium and antimony. As these critical metals are essential components for clean-tech and high-tech applications, they are key enablers of the required transition to a low-carbon, circular economy. The approach involves the discovery of non-aqueous solvent pairs that are immiscible and allow the extraction of metal complexes at moderate temperatures, leading to high-purity recycled metals. The idea is certainly high risk, but the preliminary results already obtained are highly encouraging. The main outcomes of the project will be lab-scale demonstrators that show the enhanced efficiency, utility and applicability of the new solvometallurgical process, with respect to conventional hydro- and pyrometallurgy. SOLCRIMET’s impact on chemistry, chemical technology, metallurgy and materials engineering science will be game-changing. The possibility to recycle critical metals with energy-efficient, low-cost processes could have a significant impact on the global recycling rates of these metals.","2496250","2016-09-01","2021-08-31"
"SOLLIQ","Mathematics of solid and liquid crystals","John Macleod Ball","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The project combines two closely related themes in which nonlinear analysis  addresses central issues of material behaviour. The first is the  prediction and analysis of microstructure arising from solid phase transformations in alloys.     Such microstructure largely determines the macroscopic properties of the material, but the prediction of its morphology remains poorly understood, and is related to deep unsolved problems in the calculus of variations. The aim is to make advances in this area using appropriate static and dynamic continuum models of nonlinear elasticity type, thus helping to create a predictive theory.  The second theme is to develop the mathematical theory of  the Landau - de Gennes theory of liquid crystals, in which the distribution of molecular orientations is described by a matrix order parameter. Regarded by physicists as a theory of choice for liquid crystals, the Landau - de Gennes model has been little studied by mathematicians. The aim is to understand more about its validity and properties of solutions, with potential gains for the prediction of the behaviour of new generations of liquid crystal displays.

Linking and underpinning the two themes are common mathematical and conceptual challenges, such as understanding the existence and singularities of minimizers in the multi-dimensional calculus of variations, the approach to equilibrium of thermomechanical systems, and the passage from atomic and molecular to continuum descriptions of materials. An expectation of the project is that  the simultaneous study of problems from the two themes will lead both to new understanding of these fundamental scientific questions  and to beneficial cross-fertilization between the themes.","2006998","2012-04-01","2018-03-31"
"SOLMAG","Solar magnetic field and its influence on solar variability and activity","Sami K. SOLANKI","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","For life on Earth, the Sun is the most important astrophysical object in the universe. For astrophysicists, the atmosphere of the Sun presents an intriguing, complex and extremely varied environment generated by continuous dynamic, small-scale interactions between plasma and intricately structured magnetic fields.
 
The purpose of this proposal is to elucidate the physics underlying the structure and dynamics of the solar magnetic field that is responsible for the Sun’s varied activity and its variability. This goal is to be achieved by following an integral approach combining new observational facilities, novel instruments developed in the group of the PI, the next generation of inversion techniques for data analysis and state-of-the-art magnetohydrodynamic simulations. This wide range of expertise present in the group of the PI is unique and well suited to such an approach.
 
The research proposed here will provide measurements of the Sun’s magnetic field at high spatial and temporal resolution at unprecedented sensitivity to Zeeman splitting and to magnetic flux. Also, the use of a novel polarimetric hyperspectral imager, combined with the next generation of inversion techniques will allow following the 3D structure of the magnetic field and of other physical parameters in time through a sequence of snapshots. This will enable following the build-up of magnetic tension and of waves following the field lines and will set important constraints on the heating mechanism of the solar chromosphere and corona. The proposed work, in particular the comparison of measurements with simulations, will also set constraints on the presence and properties of a small-scale turbulent dynamo as well as other fundamental physical processes taking place in the solar atmosphere. The techniques introduced here will enable reliable and robust measurements of chromospheric magnetic fields, shedding new light on this enigmatic but centrally important layer of the solar atmosphere.","2418750","2016-10-01","2021-09-30"
"SOMBOT","Soft Micro Robotics","Bradley James NELSON","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","The field of Micro and Nano Robotics has made impressive strides over the past decade as researchers have created a variety of small devices capable of locomotion within liquid environments. Robust fabrication techniques have been developed, some devices have been functionalized for potential applications, and therapies are being actively considered. While excitement remains high for this field, we are facing a number of significant challenges that must be addressed head-on if continued progress towards clinical relevance is to be made. This project will address what we consider to be primary roadblocks to be overcome. This includes the development of bioerodable and non-cytotoxic microrobots, development of autonomous devices capable of self-directed targeting, catheter-based delivery of microrobots near the target, tracking and control of swarms of devices in vivo, and the pursuit of clinically relevant therapies. 

As we consider these advances, it becomes clear that the field of micro and nanorobotics is moving away from hard microfabricated structures and towards soft, polymeric structures capable of shape modification induced by environmental conditions and other “smart” behaviors. Just as the field of robotics witnessed the emergence of “soft robotics” in which soft and deformable materials are used as primary structural components, the field of microrobotics is beginning to experience a move towards “soft microrobots.” Soft microrobots are made of soft, deformable materials capable of sensing and actuation and have the potential to exhibit behavioral response. As we develop more complex soft microrobots, we are poised to realize intelligent microrobots that autonomously respond to their environment to perform more complex tasks.  This project will develop a number of fundamental technologies required for the fabrication of intelligent soft microrobots suitable for in vivo applications. Animal trials and preclinical studies will be performed.","2500000","2017-10-01","2022-09-30"
"SOMEF","Critical State Soil Mechanics Revisited: Fabric Effects","Ioannis Dafalias","NATIONAL TECHNICAL UNIVERSITY OF ATHENS - NTUA","The theory of Critical State Soil Mechanics (CSSM) has become a paradigm within the framework of which elastoplastic soil constitutive models have been developed for the last 50 years. The present project will constructively challenge this paradigm from a missing fundamental perspective, namely the effect of soil fabric on the premises of CSSM.

The current CSSM postulates that at critical state the stress and void ratios reach critical values with no reference to orientational aspects of the soil fabric, such as particles long axes, contact normals or void vectors statistical orientations. Thus, several soil mechanical response characteristics associated with fabric anisotropy cannot be adequately or even correctly described within the existing theory. The hypothesis that an evolving soil fabric tensor must also acquire a critical value for critical state to occur will be investigated by theoretical, numerical and experimental means, including continuum and discrete elements methods (DEM) of analysis for cohesive and cohesionless soils, X-ray Computed Tomography studies on real soil samples, and triaxial, biaxial and hollow cylinder experiments. The results of this investigation will be used to propose a new enhanced CSSM theory with fabric playing a distinct role. Particular tasks will include the derivation of an objective rate equation of evolution of the fabric tensor, the formulation of classes of constitutive models for sands and clays within the new fabric-enhanced framework of CSSM, and the Finite Elements analysis of selected geomechanics boundary value problems illustrating the effect of soil fabric by comparing the results with and without fabric effects.

Successful completion of this project will change the way Soil Mechanics is taught at Universities and applied in advanced analysis of geomechanics problems, a field of increasing social impact in regards to hazard mitigations related to earthquakes and landslides.","1924000","2012-03-01","2018-02-28"
"SORBET","Spin Orbitronics for Electronic Technologies","Stuart Parkin","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Spintronics is a vibrant field of research that involves the intimate interaction of magnetic structure on the atomic scale with spin currents and spin-polarized charge currents. SORBET is focussed on an emerging sub-field of spintronics, namely that of spin orbitronics. Recent discoveries in this field concern the interplay of several distinct spin orbit coupling derived phenomena that, together, allow for the highly efficient current
induced motion of domain walls (DWs) in magnetic nanowires. It is proposed to explore two classes of domain-wall device concepts: a novel two terminal single-domain wall device composed of a spin-valve based structure that is deposited on a vertical wall or other 3D structure; and a 3D racetrack memory that involves multiple domain walls. The main objectives of the project involve the exploration of atomically engineered thin film magnetic nano-structures that could enable these revolutionary devices, and to unravel and exploit the new physics of this emerging field of research. To achieve these objectives fundamental breakthroughs are needed both in the thin film materials themselves and in the physics that determines the material properties and controls the motion of the DWs. These devices are innately three-dimensional and thus can overcome challenges that limit the scaling of existing two-dimensional electronic technologies.
Novel methods to fabricate these devices will be explored, especially, the use of atomic layer deposition and 3D printing techniques. An important objective will be to understand the origin of the spin orbit torques that
drive domain walls in nanowires and the detailed relationship of these torques to the DW structure; it is anticipated that this will enable even more complex 3D spin textures to be realized that have, for example, much lower threshold currents for motion than is currently possible, and that exhibit topological transport phenomena that could even be used to generate or detect domain walls.","2750000","2015-11-01","2020-10-31"
"SOS","Smooth dynamics via Operators, with Singularities","Viviane BALADI","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The ergodic theory of smooth dynamical systems enjoying some form of hyperbolicity has undergone important progress since the beginning of the twenty first century, in part due to the development of a new technical tool: anisotropic Banach or Hilbert spaces, on which transfer operators have good spectral properties. Very recently, such tools have yielded exponential mixing for dispersing (Sinai) billiard flows (i.e. the 2D periodic Lorentz gas), which are the archetypal smooth systems with singularities.  

We will study other challenging natural systems, mostly with singularities, by using functional analytical tools, in particular transfer operators acting on anisotropic spaces (including the new """"ultimate'"""" space introduced recently, which combines desirable features of several existing spaces), and revisiting the Milnor-Thurston kneading theory to obtain nuclear decompositions in low regularity. 

 Goals of the project include:

-Thermodynamic formalism for the Sinai billiard maps and flows (2D periodic Lorentz gas), in particular existence and statistical properties of the measure of maximal entropy.

-Intrinsic resonances of Sinai billiard maps and flows (2D periodic Lorentz gas) via the dynamical zeta function.

-Fine statistical properties of (infinite measure) semi-dispersing billiards with  non compact cusps.

-Growth of dynamical determinants and zeta functions of differentiable (non analytic) geodesic flows, with applications to the global Gutzwiller formula.

-Fractional response and fractional susceptibility function for transversal families of smooth nonuniformly hyperbolic maps (including the logistic family).","1830070","2018-09-01","2023-08-31"
"SouLMan","Sound-Light Manipulation in the Terahertz","Alessandro Tredicucci","UNIVERSITA DI PISA","The interaction of electromagnetic radiation with the mechanical vibrations of solids affects and determines many different physical phenomena. At the microscopic level, scattering of light with phonon excitations is a well known process exploited in semiconductor devices like Raman amplifiers and acousto-optic modulators. At the macroscopic scale, the interaction is mediated by the radiation pressure and is raising considerable interest as a way to excite and control mechanical oscillators, allowing, for instance, the refrigeration of a macroscopic object near the quantum limit.
This rich physics has been mostly developed in the visible or near-infrared spectral ranges. The progress of quantum cascade technologies now offers a new device platform where to explore concepts for the reciprocal manipulation of light and vibrations with unprecedented possibilities. The accessible THz spectrum is in fact particularly intriguing. The wavevector of the electromagnetic field can be tuned to that of the vibration, be it a phonon or the oscillating mode of a macroscopic object, enhancing selectivity and strength of the interaction. The low radiation frequency then makes it technically feasible to optically couple mechanical elements at distances much smaller than the wavelength, allowing, for instance, to exploit optical forces between surface plasmon modes atop metallic membranes. Lastly, it is foreseeable  to include mechanical oscillators within the laser cavities, thereby creating new laser dynamics driven by the radiation pressure, and developing opto-mechanical effects in an active device where they can be studied, and eventually controlled, through the laser emission.
SouL Man aims at establishing the field of THz opto-mechanics, relying on quantum cascade lasers for investigating phenomena and concepts available in this spectral range and in optically active systems, as wells as at using this knowledge to implement innovative device functionalities and applications.","2180306","2013-04-01","2018-03-31"
"SOURCE","Self Organization in Competition and Diversity","Kim SNEPPEN","KOBENHAVNS UNIVERSITET","""At all scales, biology presents an astounding diversity of discrete states or species that coexist with each other long times.
At the sub-cellular scale, molecular competition and positive feedback maintain cells in specialized epigenetic states over very long time, allowing for embryonic development of complex multicellular organisms. On larger length scales, stable yet dynamic ecosystems emerge from competition between different species. This general pattern calls for research with a focus on diversity, and how competition can act as its ""engine"". Statistical mechanics of complex systems provides a framework for studying universality of collective and cooperative phenomena, usually through repeated action of identical agents. I want to extend this methodology by allowing these agents to diversify. And I want to focus on the emerging diversity as a main measure of complexity. I will explore the origin and sustainability of diversity, using model systems from biology. The hypothesis is that competition is the main driver of diversity, with randomness and cooperation playing auxiliary roles.

The research will be guided by agent based models of biologically relevant examples of competition and diversity:  
- Patterns of Competitive Regulation
- Competition as an Engine of Ecosystem Diversity 
- Diseases and host Immunity.
These categories will include computational aspects of gene regulation, sustainable structures of ecosystems, mechanism of speciation facilitated by competition and defense systems including the interplay between diseases and host immune systems. My aim is both multidisciplinary and ambitious: To create a new paradigm that explicitly measures self organization in terms of its resulting diversity.""","2211015","2017-09-01","2022-08-31"
"SOX","SOX: Short distance neutrino Oscillations with BoreXino","Marco Pallavicini","ISTITUTO NAZIONALE DI FISICA NUCLEARE","""We propose to realize an experiment sensitive to a large fraction of the parameter space for short distance neutrino flavor oscillations into sterile components.
The experiment aims at the clear and unambiguous discovery, or at the definitive disproof, of the so called neutrino anomalies, a set of circumstantial evidences of electron neutrino disappearance at short distance from the source observed by several experiments. The interpretation of the anomalies as oscillations into sterile neutrino components is also supported by cosmological data, which consistently indicate that the total number of neutrinos might be larger than three.
If successful, we will demonstrate the existence of sterile neutrinos, opening a brand new era in fundamental particle physics and in cosmology. A solid signal would mean the discovery of the first particles beyond the Standard Electroweak Model and would have profound implications in our understanding of the Universe.
In case of a negative result, we would close a long standing debate about the reality of the neutrino anomalies, probe the existence of new physics in low energy neutrino interactions, provide a measurement or a limit of neutrino magnetic moment, and give Borexino a superb energy calibration, very beneficial for high-precision solar neutrino measurements.
The experiment will be done by placing a well designed artificial neutrino (or antineutrino) source close or inside the Borexino solar neutrino detector at the Laboratori Nazionali del Gran Sasso. The superb Borexino sensitivity, its large size, and its very low radioactive background will be the key elements of the experiment.
The expected sensitivity, calculated with high precision Monte Carlo simulations which implements the deep knowledge of the detector response developed by the collaboration and by the P.I. in particular, is sufficient to guarantee either a clear discovery or the complete exclusion of sterile neutrinos as an explanation of the neutrino anomalies.""","3451600","2013-06-01","2018-05-31"
"SPARSAM","Sparse Sampling: Theory, Algorithms and Applications","Martin Vetterli","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","Signal representations with Fourier and wavelet bases are central to signal processing and communications. Non-linear approximation methods in such bases are key for problems like denoising, compression and inverse problems. Recently, the idea that signals that are sparse in some domain can be acquired at low sampling density has generated strong interest, under various names like compressed sensing, compressive sampling and sparse sampling. We aim to study the central problem of acquiring continuous-time signals for discrete-time processing and reconstruction using the methods of sparse sampling. Solving this involves developing theory and algorithms for sparse sampling, both in continuous and discrete time. In addition, in order to acquire physical signals, we plan to develop a sampling theory for signals obeying physical laws, like the wave and diffusion equation, and light fields. Together, this will lead to a sparse sampling theory and framework for signal processing and communications, with applications from analog-to-digital conversion to new compression methods, to super-resolution data acquisition and to inverse problems in imaging. In sum, we aim to develop the theory and algorithms for sparse signal processing, with impact on a broad range of applications.","1839174","2010-05-01","2015-04-30"
"SPARSE","Next Generation Sparsity-Based Signal Modeling","Michael Elad","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","One could not imagine the vast progress made in signal and image processing in the past 50 years without the central contribution of data models. A model imposes a structure on the data, enabling numerous applications. Due to their importance, a considerable research attention has been devoted to the design and use of signal models. Through the past several decades, an evolution of contributions led to a series of constantly improving modeling ideas, and better performance in applications as a consequence. In that respect, the past decade has been certainly the era of sparse and redundant representations, a popular and highly effective model for describing signals.

Despite the huge attractiveness and success that this and other signal models have had so far, this field is still at its infancy, with many unanswered questions and major shortcomings, all pointing to unexplored avenues of future research. The overall objective of this proposal is to bring sparsity-based signal modeling to new frontiers by revolutionizing the way these models are defined and practiced.

More specifically, this proposal outlines several key research directions that will enable us to overcome existing modeling flaws. These include a thorough investigation of the co-sparse analysis model, one of the next fascinating phases of the field of sparse and redundant representations. This new model suggests an alternative rational and has the potential to outperform earlier models. Other directions to be explored in this project are a super-model built as a tree-constellation of sparsity-based models in an attempt to carve better the signal space, a migration from a union-of-subspaces to a union-of-sets, a systematic study of modeling errors in general, and more. The advances that we aim to make will have a marked impact and open the way towards the next generation of signal models and their use in practice.","2269554","2013-01-01","2018-12-31"
"SPARSEASTRO","Sparse Representation of Multivalued Images: Application in Astrophysics","Jean-Luc Starck","COMMISSARIAT A L ENERGIE ATOMIQUE ET AUX ENERGIES ALTERNATIVES","Modern imaging instruments generally provide multi-value datasets, which means that for a given image pixel, we do not only have one measurement (such as the intensity) but a set of ancillary data. This could be information on color, but in general this extra data set can also be much more complex. The proposed interdisciplinary project intends to develop the next generation of sparse representation methods for complex multi-valued astronomical data in order  to probe the fine structure and extract information in high dimensional astronomical data sets.  Objectives: Our project will have three main scientific directions: i) Find new decompositions for sparse representation of complex  multi-valued data, ii) develop new scheme for data restoration, component separation, data compression using the new sparse representations and iii) apply the new developed techniques on astronomical data.  Originality: New sparse representations for such data set is essential for fundamental progress in a wide range of problem areas  where traditional multiscale methods have now run their course.  Expected results: Our effort will result in three main deliverables. Theory: Coherent, comprehensive knowledge, showing what can and cannot be accomplished with sparse representation. Tools: A wide range of practical algorithms and a unified, publicly available software environment -- SparseAstro-Lab -- deploying them. Applications: Our main initial focus will be on the analysis of data in astronomy, such as those coming soon from the satellites PLANCK, HERSCHELL or GLAST.","2268600","2009-10-01","2014-09-30"
"SPDMETALS","Using Severe Plastic Deformation for the Processing of Bulk Nanostructured Metals","Terence George Langdon","UNIVERSITY OF SOUTHAMPTON","The processing of metals through the application of severe plastic deformation (SPD) provides an opportunity for achieving exceptional grain refinement to the submicrometer or the nanometer range.  This grain refinement cannot be achieved by conventional methods and it introduces both significant strength and a potential superplastic forming capability.  The two most important SPD techniques are Equal-Channel Angular Pressing (ECAP) and High-Pressure Torsion (HPT).  In both procedures, very high strains are imposed without introducing any significant changes in the overall dimensions of the samples.  This proposal outlines a research programme based on these techniques.

Although processing by ECAP and HPT has attracted considerable attention, there have been few attempts to make use of this technology for the fabrication of commercial products.  There are several reasons for this omission.  First, both techniques produce materials having high strength but limited ductility.  Second, ECAP is a labour-intensive process that must be adapted to provide a continuous processing capability.  Third, the flow processes are not understood and recent experiments using HPT show remarkable similarities to fluid flow and plasma turbulence.  We will take advantage of the ERC Advanced Grant to investigate these areas with three overall objectives.  First, to provide an understanding of the flow processes and the microstructural evolution occurring in metals when using these techniques.  Second, to develop procedures for the successful utilisation of these techniques in manufacturing.  Third, to evaluate the potential for producing super-strong solids by combining HPT with a surface mechanical attrition treatment to introduce a hard surface layer of nanostructured grains.","2300000","2011-05-01","2017-04-30"
"SPECMATE","Specification Mining and Testing","Andreas Zeller","UNIVERSITAT DES SAARLANDES","""In the past decade, automated validation of software systems has made spectacular progresses. On the testing side, it is now possible to automatically generate test cases that effectively explore the entire program structure; on the verification side, we can now formally prove properties for software as complex as operating systems. To push validation further, however, we need specifications of what the software actually should do. But writing such specifications has always been hard—and so far significantly inhibited the deployment of rigorous development methods.

The SPECMATE methodology automatically extracts such specifications from existing systems, effectively leveraging the knowledge encoded into billions of code lines. SPECMATE starts with just an executable program and automatically produces an incremental specification, starting with the most relevant properties; and a set of test cases fully covering the specification.  Such specifications are ideal starting points for compositional modeling and verification, enabling the rigorous construction and derivation of new, safe, dependable software systems; they also widely automate development activities such as testing, defect detection, and program maintenance.

To achieve these goals, SPECMATE employs a combination of specification mining (extracting specifications from executions), test case generation (generating additional runs to explore execution space) and mutation analysis (seeding synthetic defects to assess test quality). The proposed techniques all scale to industrial-sized programs; all we need is the ability to execute individual functions.""","2260000","2012-01-01","2016-12-31"
"SPEED","Single Pore Engineering for Membrane Development","Ian Metcalfe","UNIVERSITY OF NEWCASTLE UPON TYNE","Mankind needs to innovate to deliver more efficient, environmentally-friendly and increasingly intensified processes.  The development of highly selective, high temperature, inorganic membranes is critical for the introduction of the novel membrane processes that will promote the transition to a low carbon economy and result in cleaner, more efficient and safer chemical conversions.  However, high temperature membranes are difficult to study because of problems associated with sealing and determining the relatively low fluxes that are present in most laboratory systems (fluxes are conventionally determined by gas analysis of the permeate stream). Characterisation is difficult because of complex membrane microstructures.

I will avoid these problems by adopting an entirely new approach to membrane materials selection and kinetic testing through a pioneering study of permeation in single pores of model membranes.  Firstly, model single pore systems will be designed and fabricated; appropriate micro-analytical techniques to follow permeation will be developed.  Secondly, these model systems will be used to screen novel combinations of materials for hybrid membranes and to determine kinetics with a degree of control not previously available in this field.  Thirdly, I will use our improved understanding of membrane kinetics to guide real membrane design and fabrication.  Real membrane performance will be compared to model predictions and I will investigate how the new membranes can impact on process design.

If successful, an entirely new approach to membrane science will be developed and demonstrated.  New membranes will be developed facilitating the adoption of new processes addressing timely challenges such as the production of high purity hydrogen from low-grade reducing gases, carbon dioxide capture and the removal of oxides of nitrogen from oxygen-containing exhaust streams.","2080000","2013-02-01","2019-01-31"
"SPEQUACHIRAL2","Spectroscopy, Quantum Dynamics and Electroweak Parity Violation in Chiral Molecules","Martin Paul Werner Quack","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","The PI and his group have in theoretical and preparatory experimental efforts approached one of the most fundamental questions of molecular physics, concerning the role of parity violation in chiral molecules, with consequences for physical stereochemistry. The traditional point of view assigns exactly equal ground state energies to the enantiomers of chiral molecules. However, with the discovery of parity violation in physics we expect a small “parity violating” energy difference D, corresponding to an in principle measurable reaction enthalpy. Because D is expected typically in the subfemto eV range these energies were in the past considered to be immeasurably small. Recent theoretical and experimental progress initiated by our group has led to order of magnitude larger predicted values for D and has also made rovibrational line assignment of optical spectra of chiral molecules possible (Quack, Stohner, Willeke, Ann. Revs. Phys. Chem. 2008). Thus the outlook to carry out successful experiments on D is now good, following a scheme previously published by us. The present proposal describes in detail several logical steps for these experiments.
1. Theoretical analysis and high resolution spectroscopy of selected chiral molecules.
2. Quantum tunneling dynamics from spectroscopy to identify appropriate quantum states for the experiment.
3. Preparation of superposition states of otherwise stable R and S enantiomer quantum levels to generate “parity isomer” states (never before prepared)
4. Spectroscopic observation of the time dependent change of the parity isomer spectra resulting in a first determination of the parity violating D in chiral molecules.
We also discuss the feasibility and the outlook to gain fundamentally new knowledge.","1579600","2011-12-01","2015-11-30"
"SPICY","Simulating 2d Spin Lattices with Ion Crystals","Christian ROOS","OESTERREICHISCHE AKADEMIE DER WISSENSCHAFTEN","The objective of this project is to experimentally realize a 100-particle quantum simulator with complete quantum control at the single-particle level that will be used for investigating models of interacting spins in two dimensions. 

The experimental platform is a two-dimensional crystal of laser-cooled ions held in a radio-frequency trap. In this approach, the quantum state of a spin is encoded in two electronic levels of an ion. Effective spin-spin interactions are induced by laser fields coupling the ions’ electronic levels to excitations of the crystal lattice. Single-particle quantum control will be achieved by manipulating individual ions with a strongly focused steerable laser beam. Single-shot quantum measurements with near-unit detection efficiency will enable measurements of arbitrary spin correlation functions.

The main goals of SPICY are:

1. Trapping and laser-cooling of two-dimensional ion crystals to millikelvin temperatures in a radio-frequency trap. 
2. Realization of quantum spin models with particle numbers for which the simulation becomes intractable by numerical techniques.
3. Development of methods for validating quantum simulators 
4. Investigation of various models with spin-frustration in two-dimensional geometries.

SPICY builds on my experience with small-scale one-dimensional trapped-ion simulators. The exploration of two-dimensional lattice geometries will overcome difficulties in scaling up one-dimensional trapped-ion systems and enable the experimental investigation of the rich physics of two-dimensional spin models.","2496525","2017-09-01","2022-08-31"
"SPIDI","Star-Planet-Inner Disk Interactions (SPIDI): unveiling the formation and evolution of inner planetary systems","Jerome BOUVIER","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","With more than 2,000 confirmed exoplanets discovered to date, and about 4,000 additional candidates, it is now widely accepted that nearly every star in the Galaxy hosts a planetary system. These systems greatly differs from our Solar System: a vast majority of exoplanets revolves at a distance less than the Earth’s orbit (1 astronomical unit, 1 AU), and many orbit very close to their parent star indeed (<0.1 AU). These inner planets, with an orbital period less than 100 days, are quite diverse, ranging from Earth-like to Jupiter-like. How do they form or migrate close their star is still an open issue. ALMA and VLT/SPHERE recently released spectacular images of circumstellar disks around young stars, which exhibit large-scale structures (>10 AU), including rings, gaps, and spiral arms that presumably are the signposts of planet formation. Yet, as powerful as they are, imaging techniques are yet unable to probe the inner disk region. The goal of the SPIDI project is to investigate the origin and evolution of inner planetary systems. Specifically, we will develop dynamical models of inner planets embedded in the accretion disk of young stars to investigate the physical processes that govern the star-disk-planet interactions from 1 AU down to the stellar surface. From these models, we will then predict the observational signatures of disk-embedded inner planetary systems, and devise and implement observations that will allow us to detect them. This can only be done indirectly through simultaneous time domain photometry, spectroscopy, spectropolarimetry, and interferometry. Combined with current results obtained on larger scales, the SPIDI project will thus yield a synthetic view of nascent planetary systems, down to the inner edge of protoplanetary disks. It will bring clues to the origin of our own inner Solar System, and more generally, address the formation process and ubiquity of inner planetary systems throughout the Galaxy.","2362230","2018-01-01","2022-12-31"
"SPINMOL","Magnetic Molecules and Hybrid Materials for Molecular Spintronics","Eugenio Coronado","UNIVERSITAT DE VALENCIA","In this project we intend to design new magnetic molecules and new classes of magnetic molecular materials which, conveniently nanostructured, can be of interest in molecular spintronics, quantum computing and, in general, in nanomagnetism. The project pretends to cover either the development of molecule-based materials with interesting spintronic properties (molecule-based spintronics), as well as the design and study of magnetic molecules of interest in unimolecular spintronics and quantum computing. The objectives will be the following: - Use of molecule-based magnets for the preparation of multilayered spintronic structures (molecular spin valves) - Design of molecule-based magnetic materials exhibiting multifunctional properties (ferromagnetic superconductors, magnetic multilayers and magnetic/conducting multilayers) - Nanopatterning of magnetic nanostructures on surfaces via a molecular approach. - Chemical control of quantum spin dynamics and decoherence in single-molecule magnets based on magnetic polyoxometalates with the aim of developing qu-bits based on these inorganic molecules. - Positioning and addressing magnetic polyoxometalates on surfaces. An unconventional strategy of this project is the use of purely inorganic building blocks, as well as of inorganic magnetic molecules to design these magnetic materials, instead of using metal-organic molecular systems. This purely inorganic molecular building-block approach will benefit from the robustness of this kind of molecules and materials. Another characteristic feature of this project is the combination of top-down and bottom-up approaches for the processing of the molecules / materials. Thus, the project will exploit the advantage of using lithographic techniques (high throughput, easy scalability, etc.) in combination with the chemical bottom-up design of the molecular system, for the nanopatterning of the materials and the positioning of the molecules on surfaces with nanoscale accuracy.","1679700","2010-03-01","2015-02-28"
"SPLE","String Phenomenology in the LHC Era","Luis Enrique Ibañez Santiago","UNIVERSIDAD AUTONOMA DE MADRID","String Theory is the leading candidate for a theory of quantum gravity including Particle Physics. In the last ten years important progress has been made in the construction of string theory solutions resembling the Standard Model (SM) of Particle Physics. String compac- tifications giving rise to the gauge group of the SM and three generations of quarks and leptons have been constructed. It has also recently been found that quantized fluxes of the anti-symmetric fields present in the theory  provide for a solution for the long standing problem of moduli stabilization in String Theory.  On the other hand a new era for Particle Physics  has  begun  with the starting into operation of the LHC accelerator in 2010. The new data is expected to reveal the origin of the masses of all particles and what is the physics underlying it. New forces or symmetries like supersymmetry, extra dimensions etc. could be found. Whatever is found it will provide stringent tests of models for physics beyond the SM and also string theory compactifications. The purpose of this project is to make progress in the construction of string semi-realistic solutions and try to obtain information from the LHC data in order to constraint the underlying theory. In order to do that a general study of different classes of string compactifications will be made. If e.g. supersymmetry is found at LHC, we will be able to compare the structure of soft SUSY-breaking soft terms in large classes of string models with experimental data, giving important information about the underlying string theory.  One of the  objectives of the project  is to answer the question: What is LHC telling us about string Theory?","1496000","2013-04-01","2018-09-30"
"SQH","Superconducting quantum heat engines and refrigerators","Jukka Pekka Pekola","AALTO KORKEAKOULUSAATIO SR","The aim of the proposed work is to realize experimentally the first genuinely quantum mechanical refrigerator/heat engine in the solid state, and test whether one can boost its performance by information/feedback, optimized control, and merely by exploiting the quantum coherences vs the classical dynamics. To achieve this goal, we will investigate experimentally and theoretically the thermodynamics of open quantum systems. For the experimental realization, we will develop calorimetry with superior energy and time resolution and build competitive quantum circuits based on superconducting quantum bits (qubits). In order to achieve the ultimate goal, we will, for the first time, implement a test for quantum fluctuation relations in a truly open quantum system, and demonstrate an implementation of the so-called quantum Maxwell's Demon by controlling a qubit in an optimal way. In our studies we will utilize the state-of-the-art nanofabrication and measurement facilities of the national OtaNano research infrastructure that I coordinate.
This project presents a serious effort to investigate experimentally open quantum systems from the point of view of thermodynamics. It brings the classical field of research, thermodynamics, to the quantum regime, where experiments are in their infancy. Beyond the direct fundamental significance of this endeavor, the outcome of this project will technologically benefit the performance of both current and novel devices that is often limited by our present understanding of fluctuation relations and the characteristics of open quantum systems.","2418002","2017-10-01","2022-09-30"
"SQMS","Synthetic Quantum Many-Body Systems","Tilman Holger Esslinger","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","This proposal shows a new path to explore frontiers in quantum many-body physics using degenerate atomic gases. We will address fundamental open questions, create novel quantum-many body systems and seek applications beyond the realm of quantum gases. A two-component Fermi gas in an optical lattice is a unique realisation of the Fermi-Hubbard model and it is intimately linked to elementary concepts and open questions in many-body physics. We will develop novel tools for continuous cooling and detection of fermionic atoms in optical lattices. This will enable us to enter the anti-ferromagnetic phase and to study fundamental questions concerning the interplay between localization, coherence and spin-ordering in quantum many-body systems. An intriguing direction towards the creation of novel quantum many-body systems is the coupling of a strongly correlated quantum gas to an optical cavity. Here the cavity creates an effective long-range interaction with global character. This will bring together the physics of strongly-correlated systems and non-linear phenomena using a microscopically accessible system. In this highly explorative field we envisage, as a first experiment, a study of cavity-driven self-organization which may allow us to identify a novel form of a supersolid phase. Rather than investigating or manipulating the quantum gas using light we will also invert this approach and study the light after the interaction with a quantum gas inside a cavity. Using cavity opto-mechanical effects and a van der-Waals blockade by Rydberg atoms excited inside the cavity we will explore squeezing of the light and a novel photon blockade.","2000000","2010-03-01","2015-02-28"
"SQUTEC","Solid State Quantum Technology and Metrology Using Spins","Joerg Wrachtrup","UNIVERSITAET STUTTGART","The development of quantum devices by nanoscopic control of constituents is expected to be one of the largest and most intriguing challenges of modern solid state physics. Among the degrees of freedom which allow exploiting most quantum phenomena are spins. Their long coherence and relaxation times make them of interest to any kind of quantum spin”tronics” from spin memories to quantum computing and ultrasensitive sensors. In well chosen and engineered environments spin state control and readout can be easy and robust even under ambient conditions. It is the aim of the present proposal to develop complex single spin systems from diamond defects and other dopant/host systems to a degree known hitherto only in atomic physics in terms of controllability and isolation from their environment. These systems should be used to investigate fundamental physical properties, e.g. the quantumness of solid state spins as well as their utilization in sensory devices.
To this end spin defects should be implanted into ultrapure diamond materials with a spatial precision below 10nm. Their relaxation properties should be optimized to the ultimate (spin phonon interaction) limit and individual spin quantum states should be read out in a quantum non demolition-type measurement with highest possible fidelities. This on the one hand will allow the set up of versatile quantum arrays to e.g. study quantum many body physics. On the other hand such structures will yield sensors for magnetic and electric fields with unprecedented sensitivity and scale spatial resolution. Such devices might have revolutionary impact on imaging applications in various fields from materials’ investigation to bio sciences.","2308000","2011-03-01","2016-02-29"
"srEDM","Search for electric dipole moments using storage rings","Hans STRÖHER","FORSCHUNGSZENTRUM JULICH GMBH","One of the great mysteries in the natural sciences is the dominance of matter over antimatter in the universe. According to our present understanding, the early universe contained the same amount of matter and antimatter. If the universe had behaved symmetrically as it developed, every particle would have been annihilated by one of its antiparticles. We therefore
owe our very existence to mechanisms that have led to a world where something that we call matter remains. We propose to study such mechanisms by searching for electric dipole moments (EDMs) of charged hadrons in a new class of precision storage rings. Our project will lay the foundations for a new European flagship research infrastructure. The breaking of the combined charge conjugation and parity symmetries (CP-violation) in the Standard Model is not strong enough to explain the observed excess of matter and further sources of CP-violation must be sought. These sources could manifest themselves in Electric Dipole Moments of elementary particles, which occur when the centroids of positive and negative charges are mutually and permanently displaced. The observation of an electric dipole moment will elucidate the mechanisms which led to the matter that dominates the universe. Although the measurement principle, the time development of the polarization vector subject to a perpendicular electric field, is simple, the smallness of the effect makes this an enormously challenging project. This can only be mastered through the common effort of an international team of accelerator and particle physicists, working closely with engineers. The proponents of this design study and the research environment at the Forschungszentrum Jülich (Germany), including the conventional storage ring COSY, provide the optimal basis for one of the most spectacular possibilities in modern science: finding an EDM as a signal for new physics beyond the Standard Model and perhaps explaining the puzzle of our existence.","2379276","2016-10-01","2021-09-30"
"SSRR","Smart Structured Rotating Reactors","Jacob Cornelis Schouten","TECHNISCHE UNIVERSITEIT EINDHOVEN","This ERC Grant proposal aims at internationally highly recognized scientific and technological breakthroughs in the understanding, design and operation of Smart Structured Rotating Reactors for the production of high-grade chemicals. Novel reactor types as the multiple spinning disks reactor and the rotating foam reactor will be developed.  The use of smart designs of rotating structured (catalytic) packings in multiphase reactors will be an extremely innovative and exciting new research area in the chemical engineering sciences. It is my firm believe that technologically advanced, compact and highly efficient new reactor types will origin from the approach as outlined in this proposal. The research will also result in original, fundamental knowledge about the interaction of multiphase flows and physical transport processes in these rotating reaction and separation systems.  The quiescent fluid flow in conventional reactors and separators does not permit full control of the transport processes involved. Consequently, product yield, separation efficiency, and energy consumption are not optimal. Conventional equipment is also mostly gravitation force driven, which further limits the operation window. New apparatus incorporate rotating motion which induces high shear in the flow and mimics high gravitation conditions through the centrifugal force. Heat and mass transport are optimally integrated with catalytic activity by using innovative structured (catalyst) packings and smart materials and designs. One example is the proposed rotating chemical plant in which functionalities as compression, mixing, heating, reaction, extraction, and distillation are integrated on interconnected sets of rotating disks.  These new technologies provide highly promising perspectives for future  green  production plants. Chemicals processing will be more flexible using small-sized units and takes place just-in-time and close to the raw materials source or at the location of use.","1999833","2009-04-01","2014-03-31"
"StabilityDTCluster","Stability conditions, Donaldson-Thomas invariants and cluster varieties","Thomas Andrew Bridgeland","THE UNIVERSITY OF SHEFFIELD","This proposal is concerned with the homological properties of Calabi-Yau threefolds, the  geometric structures which play a crucial role in string theory. Rather than working directly with categories of sheaves, we focus on a closely-related class of models defined using quivers with potentials, which have themselves been the subject of intensive research over the last decade.

Associated to a quiver with potential are two complex manifolds: the space of stability conditions  and the cluster variety.  Recent work by physicists Gaiotto, Moore and Neitzke suggests that there is a remarkable geometric relationship between these spaces, involving Donaldson-Thomas invariants and the Kontsevich-Soibelman wall-crossing formula. Work by the PI and others over the last couple of years has paved the way for a rigorous mathematical understanding of this relationship. This has the potential to open up new vistas in algebra and geometry,  as well as greatly enhancing our  understanding of the mathematics of quantum field theory.

Our proposal combines powerful general constructions with specific computable examples. We will work initially with a class of examples related to triangulated surfaces; here the relevant spaces can be identified with familiar objects in the topology of surfaces, including moduli spaces of quadratic differentials, projective structures and local systems. These examples already involve  deep mathematics, and are closely related to quantum field theories of current interest in theoretical physics.

This proposal involves an unusually wide range of  mathematics.  Our ambition is to assemble a team of 4 research assistants having a sufficiently broad expertise to make progress on this exciting multi-disciplinary project. The PI is in a perfect position to lead such a team: he invented stability conditions, carried out important work on Donaldson-Thomas invariants, and proved a major theorem which forms one of the starting points of the proposal.","1556550","2015-10-01","2020-09-30"
"STACKSRTFPERIODS","Stacks in Representation Theory, Relative Trace Formula and analysis of  Automorphic Periods","Joseph Bernstein","TEL AVIV UNIVERSITY","""One of the central problems in number theory is to prove some
reasonably sharp bounds for values of $L$-functions and of
corresponding automorphic periods (they are usually called """"subconvexity
bounds""""). In this project I propose to study two  new tools
in representation theory of reductive groups over local fields
-- Densities on Stcks and Mirolocalization of representtions.

I am going to use these tools  to obtain very strong subconvexity bounds
for periods of automorphic representations. This is an extension
of my recent work with A. Reznikov where we established these
bounds in some special case.

These tools will also have many other applications to  problems
in Representation theory and the theory of automorphic forms
(e.g. Langlands' functoriality conjecture).""","1179000","2012-03-01","2017-08-31"
"STAHDPDE","Sparse Tensor Approximations of High-Dimensional and stochastic Partial Differential Equations","Christoph Buchs-Schwab","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","The present project addresses numerical analysis and algorithmic realization of sparse, adaptive tensor product discretizations of partial differential equations (PDEs) in high dimensions with stochastic data. The aim of the project is to develop mathematically founded adaptive algorithms which are based on sparse tensorization of hierarchic Riesz bases or frames. These will be hierarchic multilevel bases in the physical domain, either Finite Element wavelet type bases or hierarchical, multilevel bases. In the parameter domains corresponding either to random inputs or to phase spaces in transport problems, spectral type representations of ``polynomial chaos&apos;&apos; type shall be employed. Mathematical aim is to analyzed for a classes of elliptic and parabolic PDEs on high or possibly infinite dimensional parameter spaces adaptive, deterministic and dimension independent solution methods with convergence rates superior to those afforded by Monte Carlo Methods, in terms of accuracy vs. complexity. Algorithmic work will address design of data structures with minimal overhead for the efficient realization of the sparse tensor approximations. Applications include space-time adaptive solvers for elliptic, parabolic and certain parametric hyperbolic PDEs, nonlinear approximate spectral representations of nonstationary random fields, scale-resolving solvers of elliptic and parabolic problems with multiple scales with complexity independent of the number of scales, and sparse, adaptive numerical solvers for parametric transport problems. The project will be in collaboration with coworkers in France, Germany, UK, The Netherlands. The project involves mentoring postdocs and predocs who will be actively involved in all aspects of the research, as well as a teaching component.","1349564","2010-01-01","2014-12-31"
"STAMP","Separation Technology for A Million Peaks","Petrus Johannes SCHOENMAKERS","UNIVERSITEIT VAN AMSTERDAM","Extremely high separation powers are required to fully characterize complex mixtures that are of crucial importance in many fields, such as life science (including systems biology), food science, renewable energy sources and feedstocks, and high-tech materials. The STAMP project is aimed at obtaining a peak capacity of one million in liquid-phase analytical separations. Spatial three-dimensional liquid chromatography will be used to achieve this goal. The major advantage of this technique is that all second-dimension separations and – in a next step – all third-dimension separations are performed in parallel. This allows high-resolution separations to be performed in each dimension, while the total analysis time remains reasonable. Optical and mass-spectrometric imaging techniques are envisaged as detection methods after printing (STAMPing) the effluent from the 3D separation body on a suitable substrate. The STAMP project also has a number of sub-targets that will bring additional significant benefits to all the above application fields.  

The target and the sub-targets of the STAMP project may be summarized as follows.
• Separations with a peak capacity of 1,000,000 (through the use of spatial 3D-LC)
• Fast and efficient spatial 2D-LC separations
• Devices for spatial 2D-LC and 3D-LC
• Detection principles for spatial 2D-LC and 3D-LC
• Suitable stationary-phase materials and mechanisms for orthogonal 2D and 3D separations
• Relevant applications of all of the above in various fields of science.","2499780","2016-09-01","2021-08-31"
"STAMP","Stratified turbulence and mixing processes","Paul LINDEN","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","We propose to study turbulence and mixing in stably stratified fluid. Mixing is central to a wide range of questions from the heat uptake in the global ocean, the transport and dilution of pollutants in the atmosphere, the efficient cooling of buildings, to the homogenising of products in the food industry. However, the mechanisms that are responsible and their physical and dynamical aspects are largely unknown, and it is not possible to predict mixing rates from a knowledge of the overall flow and density fields.

We have invented a new laboratory experiment that produces a maintained stratified shear flow in parameter ranges directly applicable to the situations described above. The experiment, consisting of a two-layer counterflow in a stratified inclined duct, is easy to use and highly flexible. A rich variety of flows from transitional, to spatial and temporal intermittent flow, to fully turbulent flow are obtained, and can be maintained for long times to explore the life-cycles of the turbulence.

We have also developed a unique capability to make near-instantaneous, highly spatially resolved, measurements of all three components of velocity and the density field over a volume. This capability allows, for the first time in a laboratory experiment, measurements of all the quantities of interest over a three-dimensional region.

In addition we have a computational code with which we will carry out direct numerical simulations (DNS) of the experiments over a limited region of parameter space. We will use data from the experiments as initial conditions for the DNS, and compare the time evolution of the flow in the computations and the experiments. We will then use the experiments to extrapolate the results to the full scale.

This study, using the new experiment and diagnostics and state-of-the-art computations, will provide new insights into the dynamics of stratified turbulence and set the standard for future studies of this problem.","2283955","2017-10-01","2022-09-30"
"STANPAS","Statistical and Nonlinear Physics of Amorphous Solids","Itamar Procaccia","WEIZMANN INSTITUTE OF SCIENCE LTD","I propose an extensive and ambitious program to greatly increase our understanding of the properties of amorphous solids, focusing mainly on the mechanical and  magnetic properties of these fascinating materials, including their modes of failure via plastic flow, shear banding and fracture. Amorphous solids are important in many modern engineering applications, including as important examples structural glasses, metallic glasses and polymeric glasses. Our work combines a careful analysis of  computer simulations of model-glasses with analytic theory in which we introduce to material science methods from statistical and nonlinear physics, both of which are subjects of expertise in our group. We challenge some present approaches that try to connect linear elasticity with some objects that carry plasticity; we claim that nonlinear elasticity is crucial, as its signature appears much before plastic failure. Similarly, we break away from current theories that assume  that  plastic events are spatially localized. We show that in athermal conditions the opposite is true, and we discover  very interesting sub-extensive scaling phenomena characterized by a host of scaling exponents that need to be understood. The peculiarities of amorphous solids, in particular their memory of past deformation, call for the identification of new 'order parameters' that are sorely missing in present theories. Understanding the dependence on system size, temperature, external loading rates etc. calls for introducing new approaches and methods from statistical and nonlinear physics.   In the body of the proposal we present a number of preliminary results that point towards a radically new way of thinking that we propose to develop to a new theory over the next five years.","1792858","2011-04-01","2016-03-31"
"STARLIGHT","Formation of the First Stars","Ralf Stephan Klessen","RUPRECHT-KARLS-UNIVERSITAET HEIDELBERG","""The appearance of the first stars marked a primary transition in cosmic history. Their light ended the so-called “dark ages”, and they played a key role in the metal enrichment and the reionization of the Universe, thereby shaping the galaxies we see today. Understanding high-redshift star formation is central to many areas of modern astrophysics. However, studying stellar birth in the early Universe is a relatively young field of science, and so still little is known about the origin and observable
characteristics of the first stellar populations. Shedding light on the physical processes that govern
the formation of stars in the early Universe requires a concerted, multi-facetted approach that
combines a range of complementary expertise and innovative techniques. Using novel, high-resolution computer simulations we will (1) identify the physical phenomena that led to the formation of the first and second generations of stars in a systematic and quantitative way, (2) determine their mass distribution, which is the key parameter setting their lifetimes, luminosities, and chemical yields, (3) study the influence of the first stars on their surrounding environment, and (4) by doing so learn more about the subsequent cosmic evolution. We will set up a comprehensive theoretical and computational framework that enables us for the first time to make clear predictions and to compare our results with observational data from the high-redshift Universe as well as from the oldest stellar population in the Milky Way.""","2464896","2014-02-01","2019-01-31"
"STELLARPROP","The origins of stellar properties","Matthew Bate","THE UNIVERSITY OF EXETER","""Less than a century ago it was realised that stars are still forming in our Galaxy today.  Over the decades since, the questions of what physical processes dominate the star formation process and how the statistical properties of stars are determined have been some of the key questions in astrophysics.  Recently, I have advanced numerical simulations of star formation to the point that, for the first time, we can reproduce a wide range of the observed statistical properties of stars and brown dwarfs.
Here I propose an ambitious project that will make a step change in star formation theory and produce a truly predictive theory of star formation, as opposed to the past state of the field where we have been constantly searching for a mixture of initial conditions and physical processes that can reproduce the stellar properties that we observe.  The project will involve substantial numerical code development, culminating in a fluid dynamical code that incorporates all of the major physical processes thought to influence star formation, including radiative transfer, non-ideal magnetohydrodynamics, dust, and chemistry.  The scientific outputs will be the determination of how each physical process affects the star formation process, and a wide range of predictions of how stellar properties should vary in different environments and with different initial conditions.  These predictions will give direction to, and be tested by, the next generation of observational surveys of star-forming regions and stellar systems, while at the same time may be employed to improve our understanding of how star formation affects galaxy formation and evolution and how the variation in stellar properties impacts the diversity of planetary systems.""","1706418","2014-03-01","2019-02-28"
"STEMS","Spatiotemporal multimode complex optical systems","Stefan WABNITZ","UNIVERSITA DEGLI STUDI DI BRESCIA","The STEMS project is about exploiting the new concept that has been recently introduced by the PI and his co-workers, namely the self-control of the spatial coherence of optical beams in multimode nonlinear optical fibers. This concept will enable a breakthrough technology, capable of delivering high-energy optical pulses with high-average powers and much higher beam quality from fiber lasers than what is possible today. High-power fiber lasers are largely limited by transverse mode instabilities, and the loss of spatial coherence in delivery fibers. Optical fibers provide the backbone of today’s internet communication networks, and enable compact, low cost light sources for a variety of industrial and biomedical applications. In most of these applications, single-mode fibers are used. Replacing single-mode fibers with multimode fibers leads to a dramatic growth of transmission capacity, and a substantial increase of average power and pulse energy from fiber lasers. However, because of spatial dispersion and resulting mode interference, multimode fibers suffer from an inherent randomization of the spatial transverse beam profile, leading to a loss of spatial coherence. My approach is to exploit the intensity dependent refractive index, or Kerr nonlinearity, of glass fibers to recover the spatial coherence of a multimode wave, and compensate for temporal modal dispersion.
First, I propose to develop methods to control fiber nonlinearity, to compensate for temporal and spatial dispersion, thus preventing information spreading in the temporal domain, and coherence loss in the spatial domain. Second, by adding rare-earth dopants to multimode fibers, I will demonstrate self-control of modal dispersion and beam quality in active multimode fibers. Third, via the spatio-temporal control of beam propagation, I will introduce a new fast saturable absorber mechanism for the mode-locking of high-power fiber lasers, analogous to Kerr-lens mode-locking with bulk crystals.","2084181","2017-11-01","2022-10-31"
"STOCHEXTHOMOG","Stochasticity in Spatially Extended Deterministic Systems and via Homogenization of Deterministic Fast-Slow Systems","Ian Melbourne","THE UNIVERSITY OF WARWICK","Ergodic theory is the analysis of probabilistic or statistical aspects of deterministic systems.   Roughly speaking, deterministic systems are those that evolve without any randomness.   Nevertheless, the probabilistic approach is appropriate since specific trajectories are unpredictable in “chaotic” systems.  At the other extreme, stochastic systems evolve in a random manner by assumption.

One of the main topics of this proposal is to investigate how separation of time scales can cause a fast-slow deterministic system to converge to a stochastic differential equation (SDE).   This is called homogenization; the fast variables are averaged out and the limiting SDE is generally of much lower dimension than the original system.   The focus is mainly on situations where the SDE limit is driven by Brownian motion, but SDEs driven by stable Lévy processes are also of interest.   Homogenization is reasonably well-understood when the underlying fast-slow system is itself stochastic.  However there are very few results for deterministic fast-slow systems.   The aim is to make homogenization rigorous in a very general setting, and as a byproduct to determine how the stochastic integrals in the SDE are to be interpreted.

A second main topic is to explore the idea that anomalous diffusion in the form of a superdiffusive Lévy process arises naturally in odd dimensions but not in even dimensions.   The context is pattern formation in spatially extended systems with Euclidean symmetry, and this dichotomy can be seen as an extension of the classical Huygens principle that sound waves propagate in odd but not even dimensions.  For anisotropic systems (where there are translation symmetries only), the situation is simpler: chaotic dynamics leads to Brownian motion and weakly chaotic dynamics (of intermittent type) leads to a Lévy process.  However in the isotropic case (rotations and translations), anomalous diffusion is suppressed in even dimensions in favour of Brownian motion.","1577880","2013-06-01","2018-09-30"
"STRATUS","Structure and dynamics of biomolecules by two-dimensional ultraviolet spectroscopy","Giulio Cerullo","POLITECNICO DI MILANO","""Two-dimensional (2D) nuclear magnetic resonance is a diagnostic technique that has revolutionized structural biology. A wealth of spectroscopic information can be obtained by extrapolating 2D techniques to the optical frequency domain, using ultrashort light pulses. 2D electronic spectroscopy (2DES) allows fundamentally new insights into the structure and dynamics of multi-chromophore systems, by measuring how the electronic states of chromophores interact with one another and transfer electronic excitations. Due to technical difficulties, 2DES has been limited so far to the visible range, while most biomolecules absorb in the ultraviolet (UV). This project aims at extending 2DES to the challenging and still uncharted UV-domain and applying it to the study of the photophysics of genomic systems and of the secondary structure of proteins.
Nature has engineered DNA molecules to be photostable, so that harmful photochemical processes are minimized. 2DES will unravel the molecular mechanisms of the photoinduced electronic intra/inter-chromophore events in DNA, exposing the energy dissipation pathways which are responsible for its photoprotection.
2DES will be also established as a new diagnostic tool for structural studies of polypeptides and proteins, relying on the UV absorbing peptide bonds and aromatic residues, the latter acting as native local structural probes. 2DES will provide sensitive information on the misfolding/aggregation processes responsible for a wide class of diseases, with the speed of standard optical techniques but with a much greater information content. This will bridge the experimental gap between crude estimates of protein unfolding and full structure determination, enabling rapid assessment of which variants are worth of deeper structural studies.
To realize the full power of 2DES, experiments will be combined with simulations and electronic calculations that are necessary to correlate the data with molecular states and structures.""","2493000","2012-04-01","2017-03-31"
"STREAM","Structural evolution at the nano- and mesoscale","Stephan Förster","UNIVERSITAET BAYREUTH","""This proposal aims to establish a novel type of kinetic experiment by combining microfluidics with micro-x-ray technology to develop a fundamental understanding of nucleation and growth of organic and inorganic nanoparticles, thus aiming to help producing these particles more efficiently in times of constraint materials resources.
The methodology maps particle growth kinetics form the time- to the length scale. The proposed combination with microbeam x-ray diffraction extends the temporal resolution, determined by the spot-size of the microbeam, into the microsecond regime. This enables to elucidate nanoparticle nucleation and growth from early nucleation states to late growth states during which the shape of the particles is decided, thus opening pathways to new particle morphologies and improving existing synthetic procedures.
The method is applied to the investigation of amphiphile self-assembly kinetics, inorganic nanocrystal growth and ultrafast polymer nanoparticle formation, where any improvement in the understanding of the growth mechanism is expected to directly lead to a more rational design of the synthesis, extending the range of morphologies and applications.
That way, it is expected that STREAM can clarify particle nucleation and growth to expand the possibilities of nanoparticle synthesis to provide new and better materials for energy, information and medical technology.""","2407400","2012-05-01","2017-04-30"
"STRING","Properties and Applications of the Gauge/Gravity Correspondence","Michael Green","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","This proposal is concerned with developing and exploiting the remarkable connections between many relativistic and nonrelativistic matter systems on the one hand, and gravitating systems on the other. These connections are implied by the correspondence between gauge quantum field theory and quantum gravity (the gauge/gravity correspondence ) that is a characteristic feature of string theory, which unifies gravity and Yang-Mills gauge theory in a fundamental manner. There are several interrelated objectives: (1) Strongly coupled non-gravitational systems. The aim is to make use of the gauge/gravity equivalence to further our understanding of particularly interesting matter systems that are intrinsically strongly coupled and for which there are therefore few other calculational tools available. Systems of this type include high energy collisions of heavy ions, systems exhibiting quantum criticality, such as high temperature and heavy fermion superconductors, and quarks confined within hadrons by the strong force. (2) Strongly coupled gravitational systems. The aim here is to make use of the gauge/gravity equivalence to deduce properties of gravitational systems at high curvature, starting from weakly coupled gauge theory. The aim is to study: (a) cosmological singularities and the evolution of the early universe in the big-bang era; {b) quantum properties of black holes. (3) Developing the structure of the gauge/gravity correspondence. The projects outlined above rely on further developing the geometrical structure of string theory. The proposal focuses on two crucial aspects: (a) The exact integrability of string theory in anti de Sitter space and the corresponding Yang-Mills gauge quantum field theory, which leads to powerful exact results. (b) The detailed manner in which string theory extends Einstein&apos;s general theory of relativity at ultrashort distances, which is crucial for realizing non-perturbative symmetries.","1475000","2010-01-01","2015-12-31"
"STRINGS&GRAVITY","Fundamental Aspects of Strings and Gravity","Dieter Lüst","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","The proposed ERC Advanced Grant “Strings&Gravity” is centered around string theory as a consistent theory of quantum gravity in the context of particle physics and cosmology. We are addressing several ground breaking and highly innovative questions about the structure of space and time and the nature of fundamental interactions at very short distances. In particular we are planning to investigate various new aspects concerning the fundamental relation between string theory and gravity. These contain the intriguing properties of string scattering amplitudes and its so far not understood relations to the new concept of classicalization as well as the relation between black hole production and the production of heavy string Regge excitations. This part of the project will be also relevant for the search of extra dimensions and for mini black holes at the LHC. Furthermore we are planning to investigate stringy background geometries, which generalize the concept of Riemannian manifolds. We are aiming to get new insights from string theory and from non-geometric string backgrounds into the problem of dark energy, which is one of the most mysterious parts of Einstein relativity in relation to Standard cosmology.
This part of the project will be related to derive possible inflationary scenarios from new non-geometric compactifications without and with D-branes and background fluxes. The final aim of string theory will be to understand if the quantum structure of gravity and of space and time can emerge from an underlying and so far unknown dynamical principle, a problem which is equally important for the understanding of the Big Bang as for the physics of Black Holes.","959400","2013-04-01","2019-03-31"
"SUMO","Supramolecular Motive Power","Bengt Nordén","CHALMERS TEKNISKA HOEGSKOLA AB","Many important biological systems have the ability of transferring mechanical energy within individual molecules across distances of 1-10 nm. The mechanisms behind such energy transfer are poorly understood. Increased knowledge about them may not only explain fundamental processes in biology, but may also enable novel approaches to energy-related problems in general and new applications in supramolecular nanotechnology in particular. We propose to use physico-chemical methods to study how chemical, electrical and photochemical energy is converted into mechanical energy in supramolecular systems as models for the biological systems. We will concentrate on the energy conversion in proteins: ATP synthase, ion channel KvAP and Rad51, which each exemplifies a different solution to intra-molecular energy transfer. To enhance our mechanistic understanding, we will use model systems and methods that have previously been developed in the laboratory. We intend to build on our extensive expertise in spectroscopic methodology and exploit and develop further  site-selected linear dichroism by molecular replacement  (SSLD-MR) for studying structure and dynamics of the systems and their components. The studies here described constitute a new direction of research and a unique approach to addressing fundamental questions on energy conversion in biological systems. The results may give insights into important events in biology and new methodologies that enable us for the first time to study structural details of membrane proteins in membrane environment.","1742145","2009-01-01","2013-12-31"
"SUNFUELS","SOLAR THERMOCHEMICAL PRODUCTION OF FUELS","Aldo Ernesto Steinfeld","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","""The research is aimed at the efficient production of solar fuels from H2O and CO2. Solar thermochemical approaches using concentrating solar energy inherently operate at high temperatures and utilize the entire solar spectrum, and as such provide thermodynamic favorable paths to efficient solar fuel production. The targeted solar fuel is syngas: a mixture of mainly H2 and CO that can be further processed to liquid hydrocarbon fuels (e.g. diesel, kerosene), which offer high energy densities and are most convenient for the transportation sector without changes in the current global infrastructure. The strategy for the efficient production of solar syngas from H2O and CO2 involves research on a 2-step thermochemical redox cycle, encompassing: 1st step) the solar-driven endothermic reduction of a metal oxide; and 2nd step) the non-solar exothermic oxidation of the reduced metal oxide with H2O/CO2, yielding syngas together with the initial metal oxide.  Two redox pairs have been identified as most promising: the volatile ZnO/Zn and non-volatile CeO2/CeO2-δ.  Novel materials, structures, and solar reactor concepts will be developed for enhanced heat and mass transport, fast reaction rates, and high specific yields of fuel generation. Thermodynamic and kinetic analyses of the pertinent redox reactions will enable screening dopants. Solar reactor modeling will incorporate fundamental transport phenomena coupled to reaction kinetics by applying advanced numerical methods (e.g. Monte Carlo coupled to CFD at the pore scale). Solar reactor prototypes for 5 kW solar radiative power input will experimentally demonstrate the efficient production of solar syngas and their suitability for large-scale industrial implementation. The proposed research contributes to the development of technically viable and cost effective technologies for sustainable transportation fuels, and thus addresses one of the most pressing challenges that modern society is facing at the global level.""","2187650","2013-03-01","2018-02-28"
"SUPERFIELDS","SUPERSYMMETRY, QUANTUM GRAVITY AND GAUGE FIELDS","Sergio Ferrara","EUROPEAN ORGANIZATION FOR NUCLEAR RESEARCH","This project aims at investigating some crucial issues in globally supersymmetric and Supergravity theories. Firstly, it focuses on perturbative and non-perturbative sources of Supersymmetry Breaking in the low-energy effective Supergravity description of Superstrings and M-theory. These include Gaugings and Fluxes in compactifications from higher dimensions, Gaugino Condensation and other non-perturbative effects generated by (unoriented) D-brane instantons. Secondly, it explores the physics of extremal  Black Holes by means of the Attractor Mechanism, that relates their Entropy to the extrema of an Effective Potential. The tantalizing analogy with moduli stabilization in flux compactifications is considered in detail. Moreover, the deep connection between the Entropy-Formula and certain topological string partition functions is exploited to improve the connection between macroscopic and microscopic interpretations. The holographic (AdS/CFT) correspondence conjectured by Maldacena between (super)conformal Yang-Mills theories and certain (super)gravity theories in Anti De Sitter spaces is analyzed in view of the attractive nature of universal horizon geometries and in relation to Higher-Spin Symmetries, that may be associated with bulk duals of certain gauge-invariant composite operators on the boundary. The project also addresses the possible link between higher-spin theories and an unbroken phase of Superstring or M-theory. The project will be carried out through the abilities and the skills of the PI and of the team members, with their complementary expertise on different but interrelated topics in the holographic approach to modern theories of quantum gravity. Supersymmetry and gauge principles will serve as basic tools for their research.","1700000","2009-06-01","2014-12-31"
"SuperMagnonics","Supercurrents of Magnon Condensates for Advanced Magnonics","Burkard Hillebrands","TECHNISCHE UNIVERSITAET KAISERSLAUTERN","I propose the opening of the new research field of room-temperature supercurrents formed in condensates of magnons. These supercurrents represent a novel type of macroscopic quantum phenomenon analogous to the low-temperature effects of superconductivity and superfluidity. They constitute the transport of angular momentum, which is driven by a phase gradient in the magnon-condensate wavefunction. The results I envision possess the potential to completely revolutionize information processing with minimum dissipation and in ambient conditions.

Magnons are the quanta of spin waves, the dynamic eigen-excitations of a magnetically ordered body. Condensates of magnons relate to Bose-Einstein condensates, and they spontaneously form a spatially extended coherent ground state, which can be established independently of the magnon excitation mechanism and, most importantly, can be realized at room temperature. 

Magnon condensates and supercurrents will offer unprecedented opportunities to address novel, emergent, fundamental perspectives for the investigation of macroscopic quantum phenomena and their potential applications. SUPERMAGNONICS will pioneer the generation, processing and detection of magnonic supercurrents. I will specifically address the realization of magnonic Josephson junctions and the magnon version of the Aharonov-Casher effect where the phase of a magnon condensate and, thus, a persistent supercurrent, is controlled by an electric field. This approach will allow for fundamentally new means of magnon control. 

Experiments will be carried out using the unique technique of space-, phase- and time-resolved Brillouin Light Scattering spectroscopy for the imaging of the wavefunction of the condensates allowing for direct access to the supercurrent phenomena. In order to show the high potential for applications, I will demonstrate the functionality of a logic gate based on supercurrent wavefunction manipulation.","2443438","2016-10-01","2021-09-30"
"SuperQuNet","Superconducting Quantum Networks","Andreas Joachim Wallraff","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","Today superconducting electronic circuits are one of the prime physical systems to explore both foundations and technological applications of quantum mechanics.The concept of processing information more efficiently using quantum mechanics has stimulated enormous progress in control and measurement of quantum electronic circuits. Now such circuits are one of the prime contenders for realizing a viable quantum information processor. Similarly, the realization of strong coherent interactions between superconducting quantum bits and individual photons has stimulated a wide range of research exploring quantum optics in these systems. In this project we plan to investigate quantum communication using superconducting circuits, an area altogether unexplored in this domain. For this purpose we will develop both hardware and experimental techniques to realize superconducting quantum networks across distances of tens of meters. In contrast to existing experiments in which quantum information is distributed over millimeter distances only, realizing such networks will allow us to address both fundamental and practical questions. In particular, we will create and test networking architectures for superconducting quantum information processors, we will create entanglement over distances on meter length-scales and perform Bell-tests of space-like separated objects with high detection efficiency. We also plan to realize and test elements for quantum repeaters and  to explore ideas of blind quantum computation. The remarkable progress in quantum technologies based on superconducting circuits, including more than 5 orders of magnitude improvement in coherence over the last 13 years, contributes to the great potential of these systems for applications. The challenging realization of quantum networks covering larger distances will contribute to expand the range of fundamental questions addressable and applications conceivable in superconducting quantum technologies.","3242977","2014-09-01","2019-08-31"
"SUPERSOLID","THE ENIGMA OF SUPERSOLIDITY","Sébastien Pierre Balibar","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","A « supersolid » is a quantum solid inside which a fraction of the mass is superfluid. The coexistence of solid and superfluid properties is paradoxical and its understanding is a challenge attracting many research groups since its discovery in helium by Kim and Chan in 2004. The existence of anomalies in the rotational inertia of solid helium around 100 mK has been confirmed. They are accompanied by a peak in the specific heat and by astonishing anomalies in the elastic properties. It is now generally accepted that supersolidity is due to the presence of disorder in the samples: dislocations, grain boundaries, helium 3 impurities &amp;However, in our opinion, nobody understands how which kind of disorder is responsible for the observed supersolidity. Although the characterization of disorder in solid helium samples is a difficult task, we have a long experience of crystal growth in helium and a dilution refrigerator with optical access which allows direct observation of the crystals during their growth, an essential control of their quality. We know how to prepare either polycrystals , disordered single crystals or very high quality single crystals. We propose to study the elastic properties and the rotational inertia of solid samples with known disorder. In most previous experiments one had to guess what was the crystal quality. With dc-flow experiments and ion mobility measurements, we also propose to investigate the predicted superfluidity of grain boundaries and dislocation networks. After clarifying the respective role of defects and impurities in solid helium, and thanks to the international collaboration we propose, we should be able to progress in the understanding of supersolidity, hopefully to discover its nature.","1815150","2010-01-01","2014-09-30"
"SUPERSPIN","Triplet supercurrents and superconducting spintronics","Mark Giffard Blamire","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","""In almost all superconductors the pairs of electrons which carry the charge are in the so-called “singlet” state in which the quantum spin of the two electrons is antiparallel. There are only a few known compounds which show so-called p-wave superconductivity in which the electron spins within a pair are parallel and hence in a “triplet” state.

During the past five years there has been increasing evidence that proximity coupling between singlet superconductors and ferromagnets can sometimes generate triplet pairs within the ferromagnet - the evidence being that supercurrents can be passed through ferromagnetic materials over length scales which are simply too large for singlet pairs to survive. Earlier this year, in parallel with two other international groups, we made a breakthrough in demonstrating how this triplet state can be created in a controlled way. Together, the results have opened the way for a rich new field of triplet superconductivity in which the potential ability of a supercurrent to carry spin can be allied with standard spin electronics (""""spintronics"""").

In this project we will  build on our lead in this field and to explore how triplet currents can be controlled by magnetic elements within a device so that the spin supercurrent can be directly measured. As well as demonstrating superconducting spintronic devices, this project also aims to investigate the potential of creating artificial p-wave superconductors by exploiting materials which are predicted to have a favourable p-wave coupling but which are not themselves superconductors. The results from this programme will inevitably stimulate the broader scientific community interested in unconventional superconductivity and spintronics and pave the way for important new research fields.""","1822084","2012-03-01","2017-02-28"
"SUPOCOSYS","From Supramolecular Polymers to Compartmentalized Systems","Egbert Willem Meijer","TECHNISCHE UNIVERSITEIT EINDHOVEN","This ERC Grant proposal aims to explore the many challenges offered by non-covalent synthesis of functional supramolecular systems. This proposal will use the many possibilities of supramolecular polymers and how we envisage the construction of supramolecular compartmentalized systems based on specific secondary interactions. By studying the mechanisms of the formation of supramolecular polymers, new entrees are foreseen to limit the degree of supramolecular polymers by anti-cooperative mechanisms and to control both the depolymerization and polymerization aiming at supramolecular polymerization processes out of equilibrium. These insights will be used to design, synthesize and self-assembly materials that dynamically adapt their properties to cells that are brought in contact with these biomaterials. With these materials, parts of a bioartificial kidney will be made. With all the knowledge obtained through the years, we have recently introduced a concept to stepwise create folded macromolecules making use of our well-known supramolecular units. These single chain nanoparticles with internal structure are now proposed to be the starting point for making compartmentalized three-dimensional systems that possess functionality similar to proteins. Therefore, also novel techniques to synthesize well-defined polymers are introduced.","1947937","2010-04-01","2015-03-31"
"SUPRABIOTICS","Supramolecular Protective Groups Enabling Antibiotics and Bioimaging","Andreas HERRMANN","RIJKSUNIVERSITEIT GRONINGEN","The pharmaceutical sector has a huge demand for new active compounds including natural products to fill the drug pipelines and to stop the global decline in novel approved active pharmaceutical ingredients. Therefore, developing new tools to fabricate complex molecular structures in a fast and reliable way is paramount. This holds especially true for the field of antibiotics. Multidrug resistant (MDR) pathogens evolve at a terrifying rate and confer resistance to all presently available antibacterial treatments and therefore WHO has identified MDR bacteria as major threat to human health.

In this ERC Advanced Grant, I propose a radically new approach to fabricate very complex molecules with minimal synthetic effort. The technology is based on nucleic acid binders (aptamers), which are evolved in a selection protocol and block several functional groups within a target molecule while allowing other functionalities not in contact with the aptamer to be selectively modified in a single reaction step. Here, we aim to establish this groundbreaking aptameric protective group (APG) method as a novel tool that gives access to compounds that would otherwise be too difficult to obtain by multistep synthesis.  Toward this end, the specific objectives are:

•	To develop reagents and reactions that are compatible with aptamer-mediated reactions
•	To control the site of chemical modification within complex molecules by APGs
•	To establish APGs as a general paradigm in natural product derivatization to modify several kinds of substrates
•	To achieve site selective modification of proteins by aptamers
•	To synthesize novel antibiotics that kill MDR bacteria
•	To fabricate “image-and-activate” antibiotics by the APG technology
•	To employ the aptamer-target complexes for live-cell imaging of RNA

The outcomes will enable future advances in drug discovery and drug design, bioimaging technologies, and the site-specific modification of therapeutic proteins.","2500000","2016-09-01","2021-08-31"
"SUPRADAPT","Frontiers in Supramolecular Chemistry
Towards Adaptive Chemistry","Jean-Marie Lehn","CENTRE INTERNATIONAL DE RECHERCHE AUX FRONTIERES DE LA CHIMIE FONDATION","""Supramolecular chemistry is intrinsically a dynamic chemistry in view of the lability of the interactions connecting the molecular components of a supramolecular entity and the resulting ability of supramolecular species to incorporate, decorporate and exchange their components. On the other hand, such ability to undergo constitutional variation, in response to physical stimuli or chemical effectors, may be conferred upon molecules themselves by introduction of covalent bonds formed by reversible chemical reactions. Together, these features define a Constitutional Dynamic Chemistry (CDC) at both molecular and supramolecular levels. CDC takes advantage of constitutional dynamics to allow variation and selection, so as to achieve adaptation.
The overall objective of the present proposal is to further explore the scope of CDC through:
1) establishment of novel reversible covalent reactions, (in particular the synthetically important Diels-Alder and C=C bond forming reactions) without or with catalysis, and the physicochemical studies of the CDC processes;
2) investigation of the modulation of dynamic libraries by either physical stimuli or chemical effectors and their adaptation to these sollicitations;
3) development of constitutional dynamic materials, of either technological or biological interest, in particular dynamic polymers such as dynamic polyamides and dynamic biomolecules;
4) elaboration of multiple dynamic systems, combining dynamic processes of different types, and their implementation in allosterically switchable synthetic reactions and in dynamic information processing devices.
CDC represents a paradigm shift with respect to constitutionally static chemistry. In the process of reaching higher levels of complexity, it gives access to the generation of constitutional dynamic networks of interconverting constituents, that provide an approach to the chemistry of adaptive and evolutive systems.""","1842400","2012-04-01","2017-03-31"
"SUPRAIMAGINGMACHINES","Ditopic Imaging Agents, Interlocked Sensors and Machines","Paul Derek Beer","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","The development of new innovative approaches for personalised healthcare, and the recognition and sensing of environmentally important pollutants resulting from anthropogenic activities, are extremely important areas in which chemical understanding can contribute enormously to the improved well-being of humanity. In particular, the recognition of anions in aqueous media remains a significant challenge. This project will exploit cation-anion interactions augmented by supramolecular chemistry in the preparation of novel heteroditopic receptor molecules for lanthanide cation- anion ion pair recognition. These have the potential to revolutionise magnetic resonance imaging for personalised healthcare, and will provide interlocked host systems capable of sensing and analyte induced molecular motion. Particular emphasis will be given to the construction of heteroditopic macrocyclic and interlocked host systems designed to recognise lanthanide cation-fluoride anion ion pairs. The stimulus for recognising the fluoride anion stems from its duplicitous nature, where for example high levels in drinking water is causing dental and skeletal fluorosis. In stark contrast, importantly for personalised healthcare, fluoride anion recognition offers the potential development of novel 19F MRI and 18-fluoride PET imaging agents. The programme of work in this proposal centres around three closely integrated and synergistic strands. The common theme is to exploit lanthanide- fluoride ion pair recognition in multimodal imaging (Strand 1), to construct interlocked host structures for fluoride recognition, sensing and molecular machine-like induced switchable behaviour (Strand 2), and to assemble interlocked host systems onto transducing surfaces for analyte induced molecular switching (Strand 3).","2488849","2011-03-01","2017-02-28"
"SUPRANANO","From metal nanocrystal to supracrystal:
crystallinity at nanometer and micrometer scales","Marie-Paule Pileni","UNIVERSITE PIERRE ET MARIE CURIE - PARIS 6","The Applicant has an outstanding record of achievement and an international reputation for independent research in many areas of physical chemistry and more specifically over the last 25 years in nanosciences. This large expertise makes it possible, through this project, to come to a decisive turning point in her career. This high-impact and challenging proposal brings together innovative ideas in nanomaterials within a single inter- and multi-disciplinary project to open up new horizons across materials science. The challenging and innovating issue of this project consists in authenticating and detailing the emergence of new chemical and physical properties directly related to the ordering of atoms in nanocrystals (nanocrystallinity) and the ordering of nanocrystals in supracrystals (supracrystallinity). Au, Ag, and Co nanocrystals with different nanocrystallinities (single domain, multiply-twinned and polycrystalline particles) will be synthesized by new methods. Nanocrystals will be used to produce supracrystals of these metals with different supracrystallinities (fcc, hcp, or bcc). The influence of nanocrystallinity on the diffusivity of different atoms within Ag and Co nanocrystals will be investigated. Physical properties of both nanocrystals and supracrystals such as the vibrational, electronic and mechanical properties and their dependence on crystallinity will be explored. From the data thus obtained it should be possible to point out analogies between the properties of atoms in nanocrystals or in the bulk phase and those of nanocrystals ordered in supracrystals. Moreover, correlations between the studied properties could emerge. This research will result in important scientific knowledge and may ultimately open new technological applications.","2133990","2011-01-01","2016-06-30"
"SUPRAWOC","Supramolecular Architectures for Ruthenium Water Oxidation Catalysis","Frank WUERTHNER","JULIUS-MAXIMILIANS-UNIVERSITAT WURZBURG","Ruthenium complexes with 2,2'-bipyridine-6,6'-dicarboxylate (bda) as equatorial ligand and pyridines as axial ligands are currently the most favored class of efficient water oxidation catalysts (WOCs) and thus a great hope for achieving practical artificial photosynthesis. Based on the outstanding WOC performance of our recently reported macrocycles bearing three [Ru(bda)] units, this proposal aims to explore a wider variety of multinuclear metallosupramolecular architectures including more diverse polygons, polyhedra and coordination polymers. Precise control of structure and size will be achieved through a directional bonding approach with suitable vertices and edges, e.g. for cubic, tetrahedral, or dodecahedral architectures, and new ring-opening living supramolecular polymerization protocols with specially-tailored [Ru(bda)] precursors and multitopic azaaromatic initiators towards unprecedented polymer topologies. 
Whereas the synthesis and isolation of these metallosupramolecular structures will take advantage of rapid axial ligand exchange at elevated temperatures and the charge neutrality in the Ru(II) oxidation state, water networks will form in the internal cavities of the polygons, polyhedra and coordination networks for the catalytically active Ru(IV/V) species. These networks facilitate substrate water binding and proton-coupled electron transfer processes, both of which accelerate the challenging oxidative half reaction of (photo-)catalytic water splitting. Taking advantage of the accumulation of positive charge in the envisioned metallosupramolecular scaffolds, negatively charged photosensitizers will be embedded into host-guest complexes to accelerate solar light-driven WOC. Accordingly, this proposal will establish a new family of metallosupramolecular structures with outstanding functionality based on innovative synthetic concepts and important principles found in natural photosynthesis.","2490934","2018-07-01","2023-06-30"
"SuPro","Superamphiphobic surfaces for chemical processing","Hans-Jürgen Butt","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Superhydrophobic surfaces hold enormous promise as future self-cleaning or anti-fouling coatings. Their widespread use was, however, limited by contamination with oils and dissolved substances and insufficient mechanical stability. Superamphiphobic surfaces prevent contamination. They not only repel water but also non-polar liquids, surfactant and protein solutions. We recently developed a concept to fabricate transparent, robust superamphiphobic coatings, that is potentially upscalable for industrial mass production. The almost contact-free interface will open up new opportunities in membrane technology, solvent-free production of microspheres, in microfluidics, and in preventing biofilm formation. With targeted experiments and simulation we relate the microscopic structure of superamphiphobic layers to their impalement pressure, roll-off angle, mechanical strength and hydrodynamic drag. Based on these insight, improved and adapted designs will be developed. This project will make it possible to determine the potential of superamphiphobic layers in novel approaches to microchemical processing including improved transport, synthesis and characterization.","2497800","2014-03-01","2019-02-28"
"SUREPIRL","Picosecond Infrared Laser for Scarfree Surgery
with Preservation of the Tissue Structure and Recognition of Tissue Type and Boundaries","R. J. Dwayne Miller","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","""The proposed research is based on a recent breakthrough in directly observing atomic motions during structural changes and in studying energy redistribution channels in liquid water on the femtosecond time scale. These studies provided the key insights that drove the development of a new laser concept for direct-drive laser ablation. By judicious choice of laser pulse parameters, it is now possible to selectively excite water molecules to act as a propellant to drive molecules into the gas phase faster than any other energy exchange mechanism or growth of nucleation sites to cause shock wave damage or other deleterious effects. With the resulting Picosecond Infrared Laser (PIRL) surgical tool, proof-of-principle studies have shown that it is possible to produce wound sizes at the fundamental limit of a single cell - with virtually no scar tissue formation. More than an order of magnitude reduction in the wound healing zone relative to the conventional scalpel has been achieved with complete healing. Equally important, the laser ablated molecules remain completely intact for determination of the tissue type. This is the first time it has been possible to drive intact proteins into the gas phase, in a matrix independent manner, for ultrasensitive detection using mass spectroscopy. The prospects for advancing both surgery and biodiagnostics to their respective fundamental limits of single cell precision and single molecule detection is within reach. This proposal represents a well coordinated effort involving medical researchers across a broad spectrum of disciplines to fully explore the applications of this new technology, from minimally invasive surgery with molecular signatures for feedback and on the fly guidance to prevent cutting critical structures, to in situ pathology and cancerous tissue identification, to detection of disease at the earliest possible stage from a single drop of body fluids. Such is the promise of this new technology.""","2499600","2012-12-01","2017-11-30"
"SURFUND","Fundaments and Principles for Measurement and Characterization of 21st Century Science and Engineering Surfaces","Xiangqian Jiang","THE UNIVERSITY OF HUDDERSFIELD","This proposed project explores fundaments and principles for surface measurement and characterization for ultra/nano-precision non-Euclidean geometry and deterministic surfaces, which are vital for making possible key areas of 21st century science - pure and applied, engineering and bio-engineering.  The research will explore an original integrated measurement and characterization system with two major aspects: (1) Characterization: to develop radical new thinking as to what are the fundamental building blocks of a texture-characterization system and apply that thinking to non-Euclidean and deterministic surfaces. It will explore necessary and sufficient mathematical operations and principles, surface decomposition models, distortion-free representation of texture etc. (2) Measurement: to investigate principles and enabling optical methods to on-line/in-line measurement for ultra/nano-precision non-Euclidean geometry and deterministic surfaces.  The fruits of this research project will significantly facilitate surface-manufacturing control and functional performance of surfaces applied in 21st Century Science and Engineering over a wide set of sectors. Examples are surfaces used in optics and target shells in high-power laser-energy systems, optics in new earth/space-based large telescopes (e.g. the 42 m E-ELT telescope), interfaces in fluid-dynamics (energy-efficient jet engines, aircraft fuselages and wings), long-life human-joint implants, microelectronics and MEMS/NEMS devices in nanotechnology applications.  The capability to perform surface quantitative measurements and characterization on the above key components does not exist today. This confronts the state-of-the-art in surface-measurement science with regard to new surface characteristics (structured or patterned surfaces), extremity of size (1 m - 2 m), ultra precision (1 in 10^9), quality, complexity of shape (non-Euclidean geometry), or combinations of these aspects.","1895152","2009-01-01","2014-12-31"
"SUSCAT","New Directions in Sustainable Catalysis by Metal Complexes","David Milstein","WEIZMANN INSTITUTE OF SCIENCE LTD","The discovery of novel sustainable catalytic reactions is a major current goal. Based on recent discoveries in our group, we plan to develop unprecedented sustainable catalytic reactions with special emphasis on reactions catalyzed by complexes of earth-abundant metals.
 
We have recently discovered an intriguing reaction, namely the oxidation of organic compounds using water, with no added oxidant, evolving H2. This simple, selective reaction, offers now a novel, conceptually new, environmentally benign approach in the field of oxidation of organic compounds, which we will explore. 

We recently discovered a new mode of activation of multiple bonds by metal-ligand cooperation, including activation of CO2 and nitrile triple bonds, in which reversible C-C bond formation with the ligand is involved. Based on that, activation of nitriles has resulted in unprecedented C-C bond formation involving addition of simple aliphatic nitriles to various α,β-unsaturated carbonyl compounds. This mode of multiple bond activation may open a new approach to catalysis, “template catalysis”, which we plan to explore. 
In addition, the highly desirable, catalytic activation of the kinetically very stable, potent greenhouse gas N2O for the (so far elusive), efficient oxygen transfer to organic compounds, will be pursued.
 
The use of CO2 in organic synthesis is an important timely topic. Based on its activation by metal ligand cooperation, new catalytic reactions of CO2 will be pursued, including unprecedented  carbonylation of non-activated C-H bonds.
 
Most reactions catalysed by metal complexes involve noble metals. Development of sustainable catalysis based on complexes of earth-abundant metals is of great interest. In all topics described above, catalysis by complexes of such metals will be emphasized. Moreover, based on recent results in our group, we plan to develop an unprecedented family of complexes of earth-abundant metals, and pursue novel sustainable catalysis, based on it.","2497975","2016-07-01","2021-06-30"
"SUSY","SUPERSYMMETRY: a window to non-perturbative physics","Bernardus Quirinus Petrus Joseph De Wit","STICHTING NEDERLANDSE WETENSCHAPPELIJK ONDERZOEK INSTITUTEN","Supersymmetry provides an invaluable tool for quantitatively exploring a large variety of non-perturbative phenomena arising in gauge theories and gravitation. This proposal intends to exploit this fact to make significant progress on three important topics in theoretical physics, namely, black holes, strongly-coupled gauge fields, and instantons and supersymmetry breaking. Besides supersymmetry, there is a variety of cross-links between these topics, as well as joint applications. The specific objectives of the proposal are as follows. The first objective concerns the determination of supersymmetric black hole entropy for finite electric and magnetic charges, improving our understanding of critical aspects of the field-theoretic description of the entropy, in direct confrontation with the results based on the counting of microscopic states. The second objective is a construction of the exact spectrum of quantum strings moving in an anti-de Sitter space-time, which, according to the gauge-string correspondence, yields the spectrum of a corresponding dual supersymmetric gauge theory. Deforming the anti-de Sitter space will then lead to stringy descriptions of non-perturbative phenomena in a generic gauge theory with a confining phase. The third objective pertains to instantons and their implications for phenomenologically viable string compactifications on spaces with generalized geometries, which include background electric and magnetic fields. An instanton calculus will be developed to improve the understanding of non-perturbative string theory and its implication for moduli stabilization and supersymmetry breaking.","1910093","2010-09-01","2016-08-31"
"SWITCH2STICK","Engineering of biomimetic surfaces: Switchable micropatterns for controlled adhesion and touch","Eduard Arzt","LEIBNIZ-INSTITUT FUER NEUE MATERIALIEN GEMEINNUETZIGE GMBH","""Nature has, in the course of evolution, found many fascinating solutions to “engineering problems”. The proposed work aims at three-dimensional (3D) surface structures inspired by insects, spiders and geckoes. Based on the PI’s earlier work on passive structures, the new challenge addressed by this interdisciplinary project is to design and investigate active, switchable 3D micropatterns, whose adhesion and touch can be tuned at will and modified on demand. The resulting features will bend or tilt in response to external stimuli (especially temperature, electric field and stress) and thereby create a responsive surface structure. Theoretical modelling and simulation of the relevant mechanics will be a major effort to establish “structure-property relationships” for switchable patterned surfaces, to guide the choice of structure parameters and to establish new multifunctional design rules for targeted applications. Emphasis will be placed on the novel aspect of interaction with soft, compliant objects, with a view to creating future opportunities for interaction with soft matter and skin. Talented junior scientists – with both experimental and theoretical background - will be heavily involved as an opportunity to promote their career opportunities in this modern field of materials research. A final objective will be the exploration of the transferability of patterning techniques to larger-scale areas. Overall, such switchable micropatterns will likely open up revolutionary new possibilities in various technologies: robotic grippers with careful, benign “touch” of delicate objects, medical adhesives that become sticky on reaching body temperature, and active devices that can respond and send signals to touching fingers. The successful project will thus lay the scientific foundations for innovative devices and solutions that will improve our competitiveness and the living conditions of an ageing society.""","2482800","2014-02-01","2019-01-31"
"switchlet","A multi-resolution theory for systems and control across scales","Rodolphe Sepulchre","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The multi-resolution approach to science and engineering is key to tackle the complexity of systems that span across many temporal and spatial scales. This approach has revolutionized signal processing over the last two decades, most notably through wavelet theory, which builds upon the elementary concept of zooming in and out a mother signal localized in time and frequency. A similar revolution is needed in systems and control to address the most pressing engineering challenges of the 21st century, particularly in the field of medical neuroscience.

Our proposal is to lay the mathematical foundations of a multi-resolution behavioral theory. Multi-resolution behaviors are behaviors that can be modeled, analyzed, controlled, and designed at different resolutions. Our approach is based on the core novel idea that an elementary feedback principle regulates localization. Analogously to the wavelet in signal processing, we introduce the switchlet as an elementary nonlinear feedback system statically localized in range, dynamically localized in space and time. Analogously to filter banks in signal processing, our proposed interconnection theory of switchlets provides specific zooming in and out principles relying on synchronization principles. 

The theory of our proposal is entirely inspired, steered, and benchmarked by the specific application of understanding the  robustness and modulation principles of neuronal behaviors, in collaboration with experimental neuroscientists.  We propose that the multi-resolution organizing principles that we have learned by studying neuronal behaviors provide entirely novel design principles for the control of natural and artificial behaviors across scales. The objective of our proposal is to demonstrate the potential impact of such principles in the emerging age of distributed sensing and actuating technology.","2497111","2015-10-01","2020-09-30"
"SYDUGRAM","Symmetries and Dualities in Gravity and M-theory","Marc André Marie Albert Henneaux","UNIVERSITE LIBRE DE BRUXELLES","Despite its considerable success, Einstein theory of gravity is an unfinished revolution: it has limitations both at the microscopic scales and at the macroscopic scales.  The objective of this proposal is to provide a better understanding of the gravitational interaction beyond Einstein. This will be done by analyzing, with the aim of identifying it,  the symmetry structure underlying the searched-for fundamental formulation of gravity, relying on and exploring further the intriguing and fascinating infinite-dimensional algebras uncovered recently in the study of supergravities and M-theory. One of the motivations of the project is to make progress in the development of quantum gravity, with the goal of providing new insight into black holes and cosmological singularities.","1511556","2011-01-01","2015-12-31"
"SYMPTOPODYNQUANT","Symplectic topology and its interactions: from dynamics to quantization","Leonid Polterovich","TEL AVIV UNIVERSITY","""The proposed research belongs to symplectic topology, a rapidly developing
field of mathematics which originally appeared as a geometric tool for problems of classical mechanics. Since the 1980ies, new powerful methods such as theory of pseudo-holomorphic curves, Morse-Floer theory on loop spaces, symplectic field theory and mirror symmetry changed the face of the field and put it at the crossroads of several mathematical disciplines. In this proposal I develop function theory on symplectic manifolds, a recently emerged subject providing new tools and an alternative intuition in the field. With these tools, I explore footprints of symplectic rigidity in quantum mechanics, a brand new playground for applications of ``hard"""" symplectic methods. This enterprise  should bring novel insights into both fields. Other proposed applications  of function theory on symplectic manifolds include Hamiltonian dynamics and Lagrangian knots. Function theory on symplectic manifolds is fruitfully interacting with geometry and algebra of groups of symplectic and contact transformations, which form another objective of this proposal.  I focus on distortion of cyclic subgroups, quasi-morphisms and restrictions on finitely generated subgroups including the symplectic and contact versions of the Zimmer program. In the contact case, this subject is making nowadays its very first steps and is essentially unexplored. The progress in this direction will shed new light on the structure of these transformation groups playing a fundamental role in geometry, topology and dynamics.""","1787200","2013-10-01","2019-09-30"
"SynCatMatch","MATching zeolite SYNthesis with CATalytic activity","Avelino Corma","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","Solid catalysts are key components of many industrial processes, offering advantages such as reuse and ease recovery. This research programme aims to develop a new concept and methodology for the synthesis of zeolite catalysts. Zeolites are solid, porous catalysts that have wide ranging industrial applications for gas adsorption, separation and catalysis. While a relatively large number of zeolites have been synthesised, zeolite selection as catalyst for a particular reaction still retains a large element of trial and error. Our objective is to design a zeolite synthesis methodology that creates pores and cavities in the resulting zeolite that approach a “molecular recognition” pattern to catalyze the desired reaction. The approach focuses on the study of the reactions transition states since it is accepted that the most efficient catalysts are those that lower the transition state energy in the reaction, boosting the catalytic activity and efficiency. We will study the transition state of the desired reaction and design and synthesise transition state mimics to be used as Organic Structure Directing Agents for the synthesis of zeolites. By maximizing the host-guest interactions between zeolite and transition state, the efficiency of the catalyst will be increased. We aim to obtain tailor made catalysts for a chosen spectrum of reactions that also have clear industrial applications. Synthesis of highly efficient catalysts for particular industrially relevant applications will lead to lower energy consumption, fewer by-products and lower consumption of reactants. In summary, this will result in more sustainable processes. In many industrial applications a 1% increase in selectivity can represent more than 3000Tm/year additional manufacturing in a single unit.","2470519","2015-06-01","2020-05-31"
"SynDiv","A nanophysics approach to synthetic cell division","Cornelis Dekker","TECHNISCHE UNIVERSITEIT DELFT","Imagine building a living cell from basic components, a vesicle filled with biomolecules that can sustain itself and reproduce into similar offspring. Can this be done?
This proposal addresses the most tantalizing aspect: synthetic cell division. We aim to build liposomes (lipid vesicles enclosing an aqueous solution with proteins and DNA) that can spontaneously divide through a contractile protein ring at the vesicle perimeter. To realize this, we employ an experimental biophysics approach that addresses both the actual division and the prerequisite spatial control, with:
1. Cells in nanofabricated shapes. We will study cell-division proteins and DNA in live E.coli bacteria that are molded into user-defined arbitrary shapes and sizes. Clarifying the effects of cell shape will elucidate the guiding principles for the spatiotemporal organization of the cell-division machinery.
2. Proteins and DNA in nanofabricated chambers. We will use a bottom up approach to study the basic divisome components in vitro exploiting the full control provided by nanochambers. This will resolve the spatial organization of the fascinating patterns of Min proteins and chromatin that dictate the localization of the division ring.
3. Liposomes on chip. We will develop a chip-based technology to generate liposomes for exploring synthetic cell division. We will use both microfluidic constrictions and a biomimetic approach (encapsulation of divisome proteins such as FtsZ) to induce autonomous liposome splitting, thus enabling a simplified but tightly controlled form of synthetic cell division.
To our knowledge, this nanofabrication-based approach to synthetic division is unique. We expect to be able to make important contributions to understanding cell division, and anticipate that on a 5-year scale we indeed can master synthetic division. We believe that our mix of nanophysics and synthetic biology is bound to yield deep insight into the biophysical underpinnings of cellular reproduction.","2500000","2015-07-01","2020-06-30"
"SYNMAT","Synthesis of Functional Multi-Component Supramolecular Systems and Materials","Egbert Willem Meijer","TECHNISCHE UNIVERSITEIT EINDHOVEN","This ERC proposal targets novel synthetic strategies to arrive at functional multi-component systems and materials. They possess architectures of such high complexity that it is hard to imagine access to these systems by self-assembly or self-organization only. We will explore tools required to introduce multi-step non-covalent synthesis. We have taken inspiration from Nature, but more importantly we propose to mimic the impressive progress in the field of covalent organic and polymer synthesis. Three connected approaches are defined:
   The first section focusses on supramolecular polymers and how their polymerization can be compared to that of traditional covalent polymerization; unprecedented similarities are foreseen. Unexplored areas like controlled co-polymerization and achieving “tacticity” in asymmetrically modified building blocks are proposed, with special attention to kinetics and structure characterization. 
    The second section is aiming at synthesizing a multi-component hydrogel leading to a system that is able to recruit receptors in a dynamic and reversible fashion. This work is proposed to culminate in new insights for preparing an artificial extracellular matrix for stem cell to organoid growth. The latter is proposed using a double supramolecular network.  
   The final section takes inspiration from the recent finding that chirality can control spin-selective chemistry. Novel chiral structures with control over pitch and diameter are proposed by two-step synthetic processes. In a three-step non-covalent synthetic procedure, a space-controlled arrangement of chiral quartets on surfaces is proposed using discrete block co-oligomers.
   Since molecules only have structures and properties, their functions can only be expressed when they are part of complex molecular systems. Hence, if chemists want to synthesize functions in lifelike materials, they have to introduce new approaches and technologies, a game changer is proposed in this ERC proposal.","2499929","2018-10-01","2023-09-30"
"Synth","Synthesising Inductive Data Models","Luc DE RAEDT","KATHOLIEKE UNIVERSITEIT LEUVEN","Inspired by recent successes towards automating highly complex jobs like programming and scientific experimentation, the ultimate goal of this project is to automate the task of the data scientist when developing intelligent systems, which is to extract knowledge from data in the form of models. More specifically, this project wants to develop the foundations of a theory and methodology for automatically synthesising inductive data models.
An inductive data model (IDM) consists of 1) a data model (DM) that specifies an adequate data structure for the dataset (just like a database), and 2) a set of inductive models (IMs), that is, a set of patterns and models that have been discovered in the data. While the DM can be used to retrieve information about the dataset and to answer questions about specific data points, the IMs can be used to make predictions, propose values for missing data, find inconsistencies and redundancies, etc. The task addressed in this project is to automatically synthesise such IMs from past data and to use these to support the user when making decisions.
It will be assumed that the data set consists of a set of tables, that the end-user interacts with the IDM via a visual interface, and the data scientist via a unifying IDM language offering a number of core IMs and learning algorithms.
The key challenges to be tackled in SYNTH are: 1) the synthesis system must ”learn the learning task”, that is, it should identify the right learning tasks and learn appropriate IMs for each of these; 2) the system may need to restructure the data set before IM synthesis can start; and 3) a unifying IDM language for a set of core patterns and models must be developed.
The approach will be implemented in open source software and evaluated on two challenging application areas: rostering and sports analytics.","2458656","2016-09-01","2021-08-31"
"SYSDYNET","Data-driven Modelling in Dynamic Networks","Paul VAN DEN HOF","TECHNISCHE UNIVERSITEIT EINDHOVEN","Dynamic models play a key role in many branches of science. In engineering they have a paramount role in model-based simulation, monitoring, control and optimization. The accuracy of the models is key to their subsequent use in model-based operations. With the growing spatial complexity of engineering systems, e.g., in power networks, transportation networks and industrial production systems, also referred to as cyber-physical systems of systems, there is a strong need for effective modelling tools for dynamic networks, being considered as interconnected dynamic systems, whose spatial topology may change over time. 

Data-driven modelling and statistical parameter estimation are established fields for estimating models of dynamical systems on the basis of measurement data from dedicated experiments. The currently available methods, however, are limited to relatively simple structures, as open-loop or closed-loop (controlled) system configurations.

In this project I will make the fundamental step towards data-driven modelling (identification) methods for dynamic networks by developing a comprehensive theory with the target to identify local dynamical models as well as the interconnection structure of the network. I will incorporate the selection of sensing and excitation locations, data synchronization, and the optimal accuracy of estimated models in view of their use for distributed control. 
Solving these problems is by far beyond the current abilities of the existing identification frameworks in the systems and control community. My internationally recognized expertise in the field of system identification and model-based control, together with recent work on dynamic networks, warrants the feasibility of the project.
Identification methods for dynamic networks will become essential tools in the high-level future ICT environment for monitoring, control and optimization of these cyber-physical systems of systems, as well as in many other domains of science.","2499690","2016-09-01","2021-08-31"
"SYSTEAM","Systems and Signals Tools for Estimation and Analysis of Mathematical Models in Endocrinology and Neurology","Peter Stoica","UPPSALA UNIVERSITET","This proposal envisages a research program in the field of systems and signals that will lead to innovative and agile mathematical modeling as well as model-based signal processing and control tools for applications in biology and medicine. The project&apos;s goal is to bridge the gap between systems biology, on one hand, and medical signal processing and control engineering, on the other. Mathematical models of systems biology will be used to devise algorithms for biological data processing and computerized medical interventions. Experimental biological and clinical data will be utilized to estimate and characterize the parameters of mathematical models derived for biological phenomena and mechanisms. An extensive collaboration network of medical researchers from Sweden and abroad will provide the project team with necessary experimental data as well as with access to medical competence. The envisaged tools are expected to be applicable more generally but their efficacy will be demonstrated in two main application areas. These areas are endocrinology and neurology for which the use of formal control engineering and signal processing methods is currently deemed to be most promising. The proposed program will result in novel systems and signals tools for medical research and health care enabling multi-input multi-output modeling and analysis of endocrine regulations and providing model-based algorithms for individualized drug dose titration.","2379000","2010-04-01","2015-03-31"
"T-FORCES","Tropical forests in the changing earth system","Oliver Phillips","UNIVERSITY OF LEEDS","The ambition of this proposal is to determine, from the ground up, the changing role of tropical forests in the global carbon cycle. The scientific objectives are: (1) Determine the trajectory of change in remaining tropical forests, (2) Unravel the drivers of change, (3) Gauge the sensitivity of forests to the climate change threat, and (4) Scale the findings to the present and future earth system.

The 5-yr interdisciplinary research project led by Oliver Phillips will construct a Pan-Tropical Observatory of Forest Function, using this to reveal the transient and long-term forest response to global change. In Asia, Africa, Australia, and South America, T-FORCES will direct long-term on-the-ground observations of forest dynamics, and integrate them with complementary approaches including high frequency measurement of the climate-sensitivities of different components of the carbon cycle, and full analysis of forest climate, landscape, and ecology. The huge scale of the tropical forest biome will be used to develop a series of natural experiments:
*Within continents, gradients of climate, soil, and disturbance will be sampled to explore current, transient responses and to scale-up in space and time.
*Six elevational gradients will provide replicates to reveal the equilibrium sensitivities of tropical carbon cycle processes to temperature.
*By working across four tropical continents each provides independent tests to distinguish between change drivers that may be regional (eg Amazon drought) or global (eg CO2 fertilisation) in nature.

T-FORCES will transform scientific understanding of tropical forests in the global carbon cycle, by revealing the key patterns, impacts and processes so far, the threat to forests from global change, and the risk that forests will magnify those changes. The Pan-Tropical Observatory of Forest Function will provide the global science community with the baseline and framework to investigate the processes involved throughout our century.","2500000","2012-07-01","2018-06-30"
"TADMICAMT","""Topological, Algebraic, Differential Methods in Classification and Moduli Theory""","Fabrizio Catanese","UNIVERSITAET BAYREUTH","""Moduli of curves with symmetries:determine the stable irreducible components of the moduli space of curves of  genus g with an action of a finite group G, using a new homological invariant. Stable means: for g sufficiently large, or for sufficiently large numerical branching function. Higher homological stabilization for these moduli spaces. Faithful actions of the absolute Galois group on moduli spaces of marked varieties, triangle curves, varieties isogenous to a product, Beauville surfaces. Change of fundamental group. Fields of definitions of triangle curves and the scheme representing triangle curves.
Uniformization: characterization of proj. var. whose universal cover is a given bounded symmetric domain (Catanese-Di Scala did the case of tube domains). Orbifold Uniformization: where we have a quotient of a non free action, or a noncompact such  quotient. Classification of surfaces with genus 0 having the bidisk as universal cover. Symmetric differentials and fundamental groups of some ball quotients.
Topological methods in Moduli Theory: strong, weak and pseudo rigidity for the Inoue type varieties of Bauer and Catanese (free quotients of ample divisors on projective varieties  which are K(\pi, 1)). With Lonne and Wajnryb, using methods by Auroux and Katzarkov: study canonical symplectic structures and  deformation types of some simply connected algebraic surfaces, determining  braid group factorizations associated to subcanonical projections. More general bicoloured braid factorizations associated to general projections. Teichmueller space of certain algebraic surfaces.
Classification and Moduli of surfaces with low invariants. Surfaces of geometric genus 0: new construction techniques, structure of fundamental groups, moduli spaces, existence questions for surfaces with certain invariants, for homotopy quadrics, structure of fake quadrics.""","1725420","2014-04-01","2019-03-31"
"Tailor Graphene","Tailoring Graphene to Withstand Large Deformations","Constantine Galiotis","FOUNDATION FOR RESEARCH AND TECHNOLOGY HELLAS","This proposal aims via a comprehensive and interdisciplinary programme of research to determine the full response of monolayer (atomic thickness) graphene to extreme axial tensional deformation up to failure and to measure directly its tensile strength, stiffness, strain-to-failure and, most importantly, the effect of orthogonal buckling to its overall tensile properties. Already our recent results have shown that graphene buckling of any form can be suppressed by embedding the flakes into polymer matrices. We have indeed quantified this effect for any flake geometry and have produced master curves relating geometrical aspects to compression strain-to-failure. In the proposed work, we will make good use of this finding by altering the geometry of the flakes and thus design graphene strips (micro-ribbons) of specific dimensions which when embedded to polymer matrices can be stretched to large deformation and even failure without simultaneous buckling in the other direction. This is indeed the only route possible for the exploitation of the potential of graphene as an efficient reinforcement in composites. Since orthogonal buckling during stretching is expected to alter- among other things- the Dirac spectrum and consequently the electronic properties of graphene, we intend to use the technique of Raman spectroscopy to produce stress/ strain maps in two dimensions in order to quantify fully this effect from the mechanical standpoint. Finally, another option for ironing out the wrinkles is to apply a simultaneous thermal field during tensile loading. This will give rise to a biaxial stretching of graphene which presents another interesting field of study particularly for already envisaged applications of graphene in flexible displays and coatings.","2025600","2013-06-01","2018-05-31"
"TAMING","Taming non convexity?","Jean-Bernard Lasserre","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","In many important areas and applications of science one has to solve non convex optimization problems and  ideally and ultimately one would like to find the global optimum. However in most cases one is faced with NP-hard problems and therefore in practice one has been often satisfied with only a local optimum obtained  with some ad-hoc (local) optimization algorithm. 

TAMING intends to provide a systematic methodology for solving hard non convex polynomial optimization problems in all areas of science. Indeed the last decade has witnessed the emergence of Polynomial Optimization as a new field in which powerful positivity certificates from real algebraic geometry have permitted to develop an original and systematic approach to solve (at global optimality) optimization problems with polynomial (and even semi-algebraic) data.  The backbone of this powerful methodology is the « moment-SOS » approach also known as « Lasserre hierarchy » which has attracted a lot of attention in many areas (e.g., optimization, applied mathematics, quantum computing, engineering, theoretical computer science) with important potential applications. It is now a basic tool for analyzing hardness of approximation in combinatorial optimization and the best candidate algorithm to prove/disprove the famous Unique Games Conjecture. Recently it has also become a promising new method for solving the important Optimal Power Flow Problem in the strategic domain of Energy Networks (as the only method that could solve to optimality certain types of such problems). 

However in its present form this promising methodology inherits a high computational cost and a (too) severe problem size limitation which precludes from its application many important real life problems of significant size. Proving that indeed this methodology can fulfill its promises and solve important practical problems in various areas poses major theoretical & practical challenges.","1450625","2015-09-01","2019-08-31"
"TEC1p","Big-Data Analytics for the Thermal and Electrical Conductivity of Materials from First Principles","Matthias SCHEFFLER","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","""Thermal conductivity (TC) is a key characteristic of many materials, particularly those used in the energy and environment sectors (thermoelectrics, thermal-barrier coatings, catalysis, etc.). However, TC is largely unknown − of the 225,000 identified inorganic semiconductor and insulator crystals, only 100 have any TC data available.

By combining a novel ab initio molecular dynamics TC theory  and big-data analytics (machine learning, compressed sensing, subgroup discovery), we will generate quantitative values and understanding of TC (and electrical conductivity (EC)) for most of these 225,000 materials, as well as for materials not yet discovered.

TEC1p will develop and deploy five key approaches. Individually these are already novel for materials science, but their combination in TEC1p enables a true breakthrough. 
These five components are: 
     1) Ab initio theory of TC 3 (advanced density-functional theory, seamlessly linked to size- and time-converged statistical mechanics).
     2) Ab initio theory of EC (advanced …; see #1).
     3) Compressed sensing to identify a set of physical parameters that describe the TC and EC behaviour and to derive predictive equations that work for all materials.5 
     4) Active learning to build a systematic big-data database of materials, their TCs and ECs.
     5) Subgroup discovery to recognise trends and anomalies in the big data, enhance the active learning, and elucidate the underlying physical mechanisms.

In analogy to Mendeleev’s table of the elements, we will build maps that arrange existing and predicted materials according to their TC and EC properties.
 
The methods that we will develop and the extensive calculations that we will execute are both innovative and timely. They will greatly progress scientific knowledge of the physical properties of materials. The impact of the concepts, methodology, and results will be far reaching for materials science, novel materials discovery, engineering,""","2048183","2017-10-01","2022-09-30"
"TERAUNIVERSE","Exploring the Terauniverse with the LHC, Astrophysics and Cosmology","Jonathan Richard Ellis","EUROPEAN ORGANIZATION FOR NUCLEAR RESEARCH","The visible matter in the Universe is well described by the Standard Model, but this leaves open major questions in both particle physics and cosmology that may be answered by new physics at the Tera-electron-Volt range: the Terascale. The Large Hadron Collider (LHC) will soon open a new stage in humanity’s direct exploration of the fundamental physical laws at Terascale energies, which governed the evolution of the Universe a fraction of a second after the Big Bang, and are essential for understanding high-energy astrophysics. In addition to unraveling the intimate structure of matter at the Terascale, e.g., by discovering the source of particle masses and exploring matter-antimatter asymmetry, the LHC will address key cosmological issues such as how dark and conventional matter originated, which may well have been at the Terascale, and the nature of the primordial plasma that filled the Universe. This proposal will lead the understanding whatever new physics the LHC may reveal, incorporating insights from cosmology, high-energy astrophysics and speculative ideas such as string theory. This interdisciplinary approach will also facilitate the application of knowledge acquired from the LHC to fundamental cosmological and astrophysical problems, as well as illuminate future collider priorities, e.g., for LHC upgrades and/or a linear collider. This proposal will bring together particle theorists, experimentalists, astroparticle physicists and experts on field and string theory in the framework of a new ‘London Centre for Terauniverse Studies’. This will provide new opportunities for students and other young researchers to get directly involved in making LHC discoveries and exploring their implications for the Universe, and provide a mechanism for transferring to them interdisciplinary skills.","1928700","2011-04-01","2016-03-31"
"TESTDE","Testing the Dark Energy Paradigm and Measuring Neutrino Mass with the Dark Energy Survey","Ofer Lahav","UNIVERSITY COLLEGE LONDON","One of the greatest mysteries in the whole of science is that 75% of the Universe appears to be made of an enigmatic ‘Dark Energy’. A further 21% of the Universe is made from invisible ‘Cold Dark Matter’ that can only be detected through its gravitational effects, with the ordinary atomic matter making up only 4% of the total cosmic budget. These discoveries require a shift in our perception. I play leadership roles in several large surveys, in particular the $40M international Dark Energy Survey (DES), where I coordinate the entire science programme, with 200 scientists from 5 countries. DES will have its first light in October 2011, with observing from 2012 to 2017. I propose three Themes, which are interlinked: (1) Modelling the cross-talk of DES probes and a feasibility study for a spectroscopic follow-up (DESpec) which will allow testing modified General Relativity models as alternatives to Dark Energy; (2) Attempting for the first time to measure the as yet unknown neutrino mass from DES, including novel modelling of the non-linear power spectrum; and (3) Follow up of an intriguing excess in galaxy clustering on large scales we recently detected in our home-grown MegaZ-LRG sample, and developing new approaches to photometric redshifts. The research is interdisciplinary since it is connected to statistical methods as well as to High Energy Physics. The techniques developed will also be used for many other projects, including the ongoing Hubble Space Telescope CLASH survey of clusters and the planned ESA Euclid space mission. At this stage of my career, after founding the Cosmology area at University College London and playing a key role in setting up DES, I wish to focus on creative research to exploit DES with the help of four Post-docs. I believe this work will significantly influence the next paradigm shift in Cosmology, and it will make a major contribution to Cosmology in Europe.","2416388","2012-05-01","2018-04-30"
"TGF-MEPPA","Terrestrial Gamma Flashes-the Most Energetic Photon Phenomenon in our Atmosphere","Nikolai Østgaard","UNIVERSITETET I BERGEN","""Only 20 years after the discovery of Cosmic Gamma-ray Bursts from the universe another completely unknown phenomenon involving gamma-rays was discovered by coincidence the BATSE instrument on the Compton Gamma-Ray Observatory. Short-lived (~1 ms) and very energetic photon emissions (>1 MeV and later: >40 MeV) were found to originate from the Earth’s atmosphere and were named Terrestrial Gamma Flashes (TGFs). These flashes are the most energetic natural photon phenomenon that is known to exist on Earth, in which also anti-matter is produced. Based on the few datasets available to date we believe that TGFs are related to electric discharges in thunderstorm systems and that electrons accelerated to relativistic energies are involved to produce bremsstrahlung of such high energies. However, it is not known how frequent TGFs are, the altitude range and the spatial extent of their source region, to what kind of thunderstorms and lightning they are related or the implications of relativistic electrons and positrons ejected into space. There is no consensus on how TGFs are produced. All these questions need to be answered before we understand how important they are and how they may affect the Earth’s electrical circuit and atmosphere.

The goal of the TGF-MEPPA project is to attack these questions by combining modelling of electron acceleration in thunderstorm electric fields, X- and gamma-ray production and propagation, lightning development with unprecedented measurements of TGFs from three different altitudes: 350 km, 30 km  and 20 km to obtain the most comprehensive and detailed dataset needed to make significant advances in the TGF research. I will also perform electric discharge experiments in the laboratory. The goal is to establish a consistent model for the TGF-production and answer the question ‘How common are TGFs?’ to determine their implications for the Earth’s electrical circuit, atmosphere and outer space.""","2492811","2013-03-01","2018-02-28"
"TGRES","The Greenhouse Earth System","Richard David Pancost","UNIVERSITY OF BRISTOL","""Human activity is fundamentally changing the chemical composition of the atmosphere and warming the Earth. However, the impact of these changes, especially on continental precipitation patterns and biogeochemical cycles, remains poorly understood. The study of ancient climates allows a mechanistic exploration of the Earth system and the opportunity to evaluate new generations of climate models. My proposed research will focus on three inter-related paleoclimatic themes, applied to the very warm climates of the Early Eocene, one of the most fascinating intervals in Earth history. First, I will generate new records of continental temperature using bacterial membrane lipid based proxies that have been recalibrated and critically evaluated for wetland environments. Second, I will assess how the global hydrological cycle responded to both transient and long-term warmth, including evaluating precipitation change and its impact on erosional and weathering regimes; this will entail the development of compound-specific hydrogen isotopic tools in modern contexts, doubling the number of such deep time records, and interpreting those data in the context of isotope-enabled climate models.  Third, I will generate the first Paleogene records of terrestrial methane cycling using lipids derived from methanotrophs and methanogens, calibrated in modern settings and applied to Eocene lignites.  These objectives are intrinsically linked via the feedbacks between pCO2, temperature, hydrology and carbon cycling.  Each objective will comprise: the development of the proxies in modern settings in collaboration with world-leading biogeochemists; creation of unprecedented and globally widespread geochemical records for the Eocene; and quantitative interpretation of our findings using climate/biogeochemical models.  Collectively, the work will exploit very recent discoveries to develop or create new proxies and apply them to a major challenge in understanding Earth history.""","2500000","2014-01-01","2018-12-31"
"ThermoQuantumImage","Thermal imaging of nano and atomic-scale dissipation in quantum states of matter","Elia ZELDOV","WEIZMANN INSTITUTE OF SCIENCE LTD","Energy dissipation is a fundamental process governing the dynamics of physical, chemical and biological systems and is of major importance in condensed matter physics where scattering, loss of quantum information, and even breakdown of topological protection are deeply linked to intricate details of how and where the dissipation occurs. But despite its vital importance, dissipation is currently not a readily measurable microscopic quantity. The aim of this proposal is to launch a new discipline of nanoscale dissipation imaging and spectroscopy and to apply it to study of quantum systems and novel states of matter. The proposed scanning thermal microscopy will be revolutionary in three aspects: the first-ever cryogenic thermal imaging; improvement of thermal sensitivity by five orders of magnitude over the state of the art; and imaging and spectroscopy of dissipation of single atomic defects. We will develop a superconducting quantum interference nano-thermometer on the apex of a sharp tip which will provide non-contact non-invasive low-temperature scanning thermal microscopy with unprecedented target sensitivity of 100 nK/Hz1/2 at 4 K. These advances will enable hitherto impossible direct thermal imaging of the most elemental processes such as phonon emission from a single atomic defect due to inelastic electron scattering, relaxation mechanisms in topological surface and edge states, and variation in dissipation in individual quantum dots due to single electron changes in their occupation. We will utilize this trailblazing tool to uncover nanoscale processes that lead to energy dissipation in novel systems including resonant quasi-bound edge states in graphene, helical surface states in topological insulators, and chiral anomaly in Weyl semimetals, and to provide groundbreaking insight into nonlocal dissipation and transport properties in mesoscopic systems and in 2D topological states of matter including quantum Hall, quantum anomalous Hall, and quantum spin Hall.","3075000","2018-06-01","2023-05-31"
"THINKBIG","""Patterns in Big Data: Methods, Applications and Implications""","Nello Cristianini","UNIVERSITY OF BRISTOL","""The availability of huge amounts of data has revolutionized many sectors of society, enabling engineers to bypass complex modeling steps, scientists to find shortcuts to new knowledge, and businesses to explore novel business models. For all its success, this field is still very young, and in need of systematic attention. Both risks and opportunities are very significant at this stage. They can be organized into three interconnected areas, which need to be addressed in a coordinated way: methods, applications and implications. By this we mean the interconnected needs to 1) develop new technology to take advantage of this resource; 2) explore the domains where this technology can make a significant impact; and 3) develop a set of cultural, legal and technical tools to reduce the risks associated with the application of these technologies to science and society. This project is about understanding, exploiting and managing the paradigm shift that is under way. It will address these three areas at the same time, 1) developing new types of algorithms and software architectures to take full advantage of this opportunity; 2) exploring new areas of opportunity for big-data to make an impact, with particular attention to the growing field of computational social sciences; and 3) investigating the ethical and epistemological challenges that arise from the transition towards a data-driven way of running society, business and science. We build on a strong track record in each of these areas. We have secured access to a valuable resource for historians, the collection of all UK newspapers from the past 200 years, which we will analyze with our tools, and we will greatly expand our current work on social media mining, working closely with colleagues from other disciplines. It is our intention to impact the social sciences, the general public and the law makers, besides our field of engineering.""","2112798","2014-03-01","2019-02-28"
"THIRDWAVEHCI","Third Wave HCI: Methods, Domains and Concepts","William Gaver","GOLDSMITHS' COLLEGE","This proposal is for interdisciplinary research that will help bring to maturity the emerging paradigm of  third-wave HCI , which addresses interaction as situated meaning-making in everyday life. With my established interdisciplinary research team, I will design prototypes that show how third-wave thinking is relevant for domains of recognised importance to help bring this paradigm to the centre of HCI. We will develop an integrated set of tactics and orienting concepts based on our practice to elucidate and support research and design in third-wave HCI. Crucially, we will develop a new methodology for this research, based on the deployment and study of 50   100 batch-produced prototypes in real-world situations. This will mark a significant leap forward, allowing prototype technologies to be studied using social scientific and design-led methods in field trials several orders of magnitude larger than normal   a development from which third-wave HCI, with its commitment to multiple, local appropriations, will benefit enormously.  The project will be centred around two Case Studies in which we will develop robust and highly finished prototypes, batch produce them in large numbers, deploy them in large-scale field studies with members of the general public as well as specialist commentators, and use a variety of traditional and experimental methods to capture their experiences.  The first Case Study will produce a suite of electromechanically extended sensors that provide resources for environmental awareness in the home without being judgmental or didactic. The second Case Study will develop mobile devices that display readymade, location-based information to provide a  behind the scenes  view of local neighbourhoods.  When dozens of these prototypes are in use simultaneously, we will be able to observe as communities of practice form, and a hundred different stories emerge, leading to a transformative coming-of-age for third-wave HCI.","2439757","2009-04-01","2014-12-31"
"THUNDERR","Detection, simulation, modelling and loading of thunderstorm outflows to design wind-safer and cost-efficient structures","Giovanni SOLARI","UNIVERSITA DEGLI STUDI DI GENOVA","Wind actions are crucial for the safety and cost of structures.
The wind climate of Europe and many parts of the world is dominated by synoptic extra-tropical cyclones and mesoscale thunderstorm outflows. Thunderstorms are frequent events causing wind speeds often higher than cyclones.
In spite of an impressive amount of research, there is not yet a model for thunderstorm outflows and their actions on structures like that for cyclones. Thus, thunderstorm actions on structures are still determined using the cyclone model developed half a century ago and engineering practice often leads to unsafe or too expensive construction.
This happens because the complexity of thunderstorms makes it difficult to develop realistic and simple models. Their short duration and small size make the available data very poor. There is a great gap between research in wind engineering and atmospheric sciences.
The realization of an unprecedented wind monitoring network, the role of the PI and his novel vision, a project team leader in wind engineering with interdisciplinary skill in atmospheric sciences, the chance to simulate large-scale thunderstorms in a new unique laboratory, the recent advance in CFD simulations, the success and synergy of previous and on-going projects, and a top host institution represent extraordinary conditions to overcome these shortcomings.
THUNDERR is an acronym of THUNDERstorm that points out the ground-breaking Roar of this project. It aims to detect novel thunderstorm measurements, to create a huge dataset of field acquisitions and a new interpretation of their weather scenarios, to conduct unique wind tunnel tests and CFD analyses, to formulate a thunderstorm model that is physically correct and suitable to develop a loading scheme easily transferable to engineering and codification, to radically change the existing wind loading format and the engineering practice, to design safer and cost-efficient structures producing a deep social and economic impact.","2396644","2017-09-01","2022-08-31"
"THZCALORIMETRY","Time Resolved THz Calorimetry explores Molecular Recognition Processes","Martina HAVENITH","RUHR-UNIVERSITAET BOCHUM","Time-resolved THz-Calorimetry Explores Molecular Recognition Processes

Definition: “THz-calorimetry is the science of measuring low frequency density of states kinetics for the purpose of deriving the entropy changes associated with biological processes in real time.”

Goal: Our scientific vision is to introduce time-resolved “THz-Calorimetry” to access the entropy changes of the protein and the solvent accompanying enzymatic reactions or more general molecular recognition processes, i.e. under non-equilibrium conditions with envisioned time resolutions of nanoseconds. 
Advances in THz technology will make it possible to develop strong, ultrafast THz sources in the frequency range up to 10W that allow the detection of frequency-resolved transient low frequency spectra of biological samples with μsec or even nsec time resolution. We will correlate the transient THz spectra with changes in the entropy and enthalpy, both experimentally and theoretically. Ultrafast THz-calorimetry is an innovative tool to access to the specific transient entropy changes of molecular recognition processes under non-equilibrium conditions.

Objectives of the proposal:
1.	Monitor transient low frequency spectra (0-300cm-1) during molecular recognition processes.  
2.	Develop cutting edge experimental methods which allow the separation of protein and solvent modes under ambient conditions. Determine Delta S(protein(t)), Delta S(ligand(t)), Delta S(solvent(t)), thus specify the role of changes in the solvent entropy (hydrophobic effect) for biomolecular recognition. 
3.	Apply ultrafast THz-calorimetry: Explore the entropy changes of the enzyme, the ligand and the solvent for a family of MMP enzymes and substrates and correlate these results with biological function. Deduce underlying molecular mechanisms.
4.	Develop new models for molecular recognition processes which are quantitative and predictive.","2500000","2016-10-01","2021-09-30"
"TICAL","TICAL: 4D total absorptionTime Imaging CALorimeter","Paul Lecoq","EUROPEAN ORGANIZATION FOR NUCLEAR RESEARCH","This 4-year project proposes a breakthrough in particle detectors by developing a highly granular calorimeter with high-resolution timing information, thus providing precise information of the space-time development of electromagnetic and hadronic showers.
The objective of this project is to develop a completely new imaging calorimeter that uses light encoding methods, and thus simultaneously records:
- the total energy deposited in the calorimeter cells with a time tag in the 10 picosecond (ps) range;
- the high-precision spatial distribution of the energy deposition in the calorimetric volume both for low energy (photo-electric and Compton) and high energy (shower components);
- the time structure of the signals corresponding to the different components of the shower.
The key point in this novel approach is to introduce light production, collection and detection techniques that are now accessible due to spectacular technological advancements in this field, in which the PI is directly involved.  Examples are:
- new crystal production technologies (micro-pulling-down (μ-PD,) ceramics, nano-crystals);
- photonic crystals, plasmonic resonances and nano-optics;
- single-photon-counting silicon photomultipliers, both digital(d-SiPM) and analogue (a-SiPM).
The use of precise time information in the tens of picosecond range in calorimetric techniques will have a large impact on different applications in many domains:
- High Energy Physic (HEP), in particular at new high energy and high collision rate colliders;
- Medical imaging in Time of Flight Positron Emission Tomography (TOF-PET);
- Spectrometry of low energy γ- quanta;
- Homeland security: crystals of higher sensitivity always be in demand;
- Space applications.","2258000","2014-02-01","2018-12-31"
"TimeMan","RHEOLOGY OF EARTH MATERIALS: CLOSING THE GAP BETWEEN TIMESCALES IN THE LABORATORY AND IN THE MANTLE","Patrick CORDIER","UNIVERSITE DE LILLE","Most large-scale geological process such as plate tectonics or mantle convection involve plastic deformation of rocks. With most recent developments, constraining their rheological properties at natural strain-rates is something we can really achieve in the decade to come.  
Presently, these theological properties are described with empirical equations which are fitted on macroscopic, average properties, obtained in laboratory experiments performed at human timescales. Their extrapolation to Earth’s conditions over several orders of magnitude is highly questionable as demonstrated by recent comparison with surface geophysical observables.
Strain rates couple space and time. We cannot expand time, but we can now reduce length scales. By using the new generation of nanomechanical testing machines in transmission electron microscopes, we can have access to elementary deformation mechanisms and, more importantly, we can measure the key physical parameters which control their dynamics. At this scale, we can have access to very slow mechanisms which were previously out of reach. This approach can be complemented by numerical modelling. By using the recent developments in modelling the so-called “rare events”, we will be able to model mechanisms in the same timescales as nanomechanical testing.  
By combining, nanomechanical testing and advanced numerical modelling of elementary processes I propose to elaborate a new generation of rheological laws, based on the physics of deformation, which will explicitly involve time (i.e. strain rate) and will require no extrapolation to be applied to natural processes.
Applied to olivine, the main constituent of the upper mantle, this will provide the first robust, physics-based rheological laws for the lithospheric and asthenospheric mantle to be compared with surface observables and incorporated in geophysical convection models.","2499400","2019-03-01","2024-02-29"
"TITAN","Transition into the Anthropocene: learning about the climate system from the 19th and early 20th century","Gabriele Clarissa Hegerl","THE UNIVERSITY OF EDINBURGH","The research proposed here will use a novel and fully-integrated data-modelling approach to provide a step-change in our understanding of the nature and drivers of climate variability and change on the societally-relevant timescales of decades to centuries.  Identifying the causes of observed changes in climate requires an understanding of the natural, internally generated variability of the climate system, and of the response of climate to external influences. Our present knowledge is heavily weighted towards changes observed over the recent few decades. This proposal focuses on the early Anthropocene, namely the 19th through to the early 20th century. This period covers the emergence from an anomalously cold period, the so-called ‘Little Ice Age’, and shows periods of warming including the still enigmatic early 20th century warming. Newly available observational data now make it possible to analyze this period in detail. ‘Fingerprints’ for climate changes in response to external drivers, such as changes in atmospheric composition, solar radiation, and volcanism will be used to estimate the contribution by these factors to observed changes over the 19th and early 20th century. These fingerprints will be based on a large, multi-model ensemble of climate model simulations that is presently becoming available. Changes in observed temperature, sea ice variations, and precipitation will be linked to the state of the atmospheric circulation.  Targeted model simulations will help to determine the role of sea surface temperature patterns and atmospheric and oceanic circulation in setting temperature records in the 1930s and 1940s. The result will be a synthesis of the causes of climate change over the early Anthropocene, an improved estimate of the natural variability of climate, probabilistic estimates of the climate’s transient sensitivity to greenhouse gas increases, and improved understanding of the response of sea ice, precipitation, and temperature extremes.","2445546","2013-02-01","2019-01-31"
"TMSS","Topology of Moduli Spaces and Strings","Ib Henning Madsen","KOBENHAVNS UNIVERSITET","This project is on the crossroad of three interrelated mathematical subjects, namely (i) The topology of moduli spaces of Riemann surfaces (in a background) (ii) Algebraic K-theory and Topological Cyclic homology with special emphasis to the symplectic algebraic K-theory and its relation to the moduli space of principal polarized abelian varieties (iii) String topology and Homological Conformal Field Theory. It is an important aspect of the project to study the implications of the Madsen-Weiss theorem, about the topological structure of the stable Riemann moduli space, to Gromov-Witten theory in algebraic geometry and to the compactified Jacobian The project will take place at University of Copenhagen in close contact with the newly established research group in Topology","1650720","2009-01-01","2013-12-31"
"ToDL","Systems Chemistry: Steps Towards De-Novo Life","Sijbren OTTO","RIJKSUNIVERSITEIT GRONINGEN","Can we synthesise life de-novo in the lab? This is one of the Grand Challenges of contemporary Science. Overall objective of this project is to set important steps in turning chemistry into biology by building fully synthetic chemical systems that contain and integrate some of the essential elements of life: replication, metabolism and compartmentalisation. Functional coupling of any of life’s essential elements has not been achieved, at least not without making use of biomolecules. We now aim to achieve such coupling and develop fully chemical systems to become increasingly life-like. Specific aims are:
1. Achieve and explore Darwinian evolution of a fully synthetic system of peptide-based self-replicating molecules.
2. Develop self-replicating molecules that are capable of catalysing not only their own formation, but also other chemical reactions. We will specifically target chemical reactions that result in the production of building blocks which the replicators can utilize to replicate, thereby integrating replication with a rudimentary form of metabolism.
3. Achieve self-reproducing compartments and develop ways to couple replication inside compartments with compartment division. Three parallel approaches will be explored, based on (i) vesicle-type compartments made from self-replicating molecules; (ii) coascervates and (iii) compartments made by surfactants that are produced by catalytically active self-replicators.
4. Extend replication from peptide-based building blocks to ones containing nucleobases. We also plan to investigate reaction networks made from mixtures of peptide- and nucleobase-containing building blocks).
5. Develop kinetic modelling tools that allow an efficient exploration of multi-parameter space of the reaction networks developed in 1-4. Through stochastic computational modelling we will address mechanistic issues that are experimentally intractable. Furthermore, modelling will allow a more efficient exploration of multi-parameter space.","2499975","2017-09-01","2022-08-31"
"TODYRIC","Topological Dynamics of Rings and C*-algebras","Joachim Johannes Richard Cuntz","WESTFAELISCHE WILHELMS-UNIVERSITAET MUENSTER","This project is concerned with problems in several areas. A starting point is the new concept of a ring C*-algebra associated with a countable ring without zero divisors. For special rings this C*-algebra has a very rich and surprising structure. A particularly interesting case is the ring of algebraic integers in a global field. In this context the algebra contains well known topological dynamical systems. We plan to use the analysis of ring algebras as an organizing  principle for the study of many questions in C*-algebra theory, K-theory, ergodic theory and number theory. Some of these questions are well known and very difficult.","1179880","2011-02-01","2016-01-31"
"TOFU","Toward a new generation of multi-dimensional stellar evolution models: the TOol of the FUture","Isabelle Baraffe","THE UNIVERSITY OF EXETER","""Modelling stars and planets in three dimensions is the holy grail of stellar and planetary astrophysics and is now the necessary step forward in stellar evolution. It is the only way to proceed in order to correctly understand the various physical processes which rule the life of a star or a planet. The heart of the TOFU project is the development of an unique numerical tool, namely a three dimensional, time implicit and fully compressible hydrodynamical code, that allows the multi-dimensional description of a complete star (or planet) interior during timescales relevant to the study of various stellar/planetary evolutionary phases.
The major scientific motivation is to improve the description of key stellar/planetary physical processes and to solve long standing problems characterising the life of stars and planets. The development of such a tool is challenging and innovative, and has never been addressed to date. The time implicit approach provides the major innovation and breakthrough of this project as it will allow the exploration of physical processes on spatial and temporal scales presently out of reach. The TOFU project proposes a new vision of stellar/planetary evolution  based on a multi-D description  of anisotropic and non linear processes related to key problems in stellar evolution, like transport of angular momentum, disk accretion, pulsations and turbulent convection. The project will focus on three typical astrophysical examples at the heart of key investigations in stellar formation, evolution and asteroseismology, whose study requires multi-dimensional time implicit simulations.  Beyond these three main astrophysical applications, the project will open new avenues to explore a vast variety of problems in stellar/planetary physics. TOFU  will thus deliver an unique numerical tool which will lay the foundation of modern stellar and planetary astrophysics.""","2499980","2013-05-01","2018-04-30"
"TOMCAT","Theory of Mantle, Core and Technological Materials","Ronald Cohen","LUDWIG-MAXIMILIANS-UNIVERSITAET MUENCHEN","I propose to work full time on this ERC Advanced project to predict properties of Earth and technological materials using fundamental physics, spending half of my time at University College London, building a research group, and educating students and post-docs. I compute properties of minerals and melts to better understand them, and estimate properties when data are unavailable. The latter has been important in the Earth sciences, starting with my prediction of the elastic constants of silicate perovskite, the most common mineral in the Earth, before they were measured. This project will constrain properties of major problematic minerals, melts, and aqueous fluids crucial to modelling of the Earth, including equations of state, phase transitions, electrical and thermal conductivity, chemical diffusivity, and viscosity and rheology.  This project will concentrate on iron and other transition metal bearing Earth materials, for which conventional electronic structure methods are inadequate. Whereas properties of closed-shell atomic systems and simple met-als can now be computed to accuracy limited only by available computing time, open-shelled systems have been problematic. Standard methods give FeO as a metal, but it is an insulator. My recent work on FeO showing that it becomes metallic under lower mantle conditions is requiring reconsideration of many areas in geophysics, ranging from heat flow from the core to the mantle, and fluctuations in the length of the day. We will apply dynamical mean field theory (DMFT) within an accurate framework and ground state Quantum Monte Carlo (QMC DMC) to compute properties of iron-bearing minerals and melts and their impact on the Earth’s behaviour. We will simulate C-H-O fluids to understand slab dewatering and mantle fluids. We will study useful technological materials and design new ones exploiting their synergy with Earth materials. These projects will boost our understanding of the Earth and develop new useful materials.","2720000","2013-09-01","2018-08-31"
"TOPCHEM","Towards Perfect Chemical Reactors:
Engineering the Enhanced Control of Reaction Pathways at Molecular Level via Fundamental Concepts of Process Intensification","Andrzej Ignacy Stankiewicz","TECHNISCHE UNIVERSITEIT DELFT","Molecular-level control of chemical reactions presents undoubtedly the most important scientific challenge on the way to fully sustainable processes. Factors responsible for the effectiveness of a reaction include number/frequency of molecular collisions, orientation of molecules at the moment of collisions and their energy. Current reactors offer a very limited control of the above factors. Reactions usually take place in random geometries and the energy is brought to molecules by conductive heating which is non-selective and thermodynamically inefficient.

A groundbreaking solution here can only be achieved by creating a “perfect” reaction environment, in which the geometry of molecular collisions is fully controlled while energy is transferred selectively from the source to the required molecules in the required form, in the required amount, at the required moment, and at the required position. The current proposal aims at the first of its kind development of structured reactors using electric and electromagnetic fields for alignment, orientation and selective activation of targeted molecules. To engineer such “perfect” reaction environment the fundamental concepts of Process Intensification are applied. We build here on the Nobel Prize-awarded fundamental works in the area of the reaction dynamics and molecular reaction control that were not considered in chemical engineering thus far. Chemistries studied are mono- and bi-molecular reactions using CO2, CH4 and H2O, which are of paramount importance for clean fuel production and carbon dioxide management.

The proposal bridges chemical physics and chemical engineering incorporating the knowledge domains of chemistry, catalysis, materials science, electronics, computer modelling and micromechanical engineering.","2298789","2011-05-01","2016-04-30"
"TOPFIT","Topological Spin Solitons for Information Technology","Christian Pfleiderer","TECHNISCHE UNIVERSITAET MUENCHEN","Present day limitations of information technology based on magnetic materials may be traced to the notion, that all magnetic materials known until recently exhibit conventional, i.e., topologically trivial, forms of magnetic order. Only two years ago the first example of an entirely new form of magnetic order has been discovered, which is composed of topologically stable spin solitons (so called skyrmions) caused by chiral spin interactions. These skyrmions display several exceptional properties, e.g., great stability against perturbations and spin torque effects at ultra-low current densities. Because the underlying chiral interactions exist, in principle, in a very wide range of different settings a comprehensive search for similar spin solitons in other bulk compounds, thin films and especially at interfaces promises major break-throughs for information technology. For instance, the spin transfer torques at ultra-low current densities open an unexpected, new route to high-speed data processing. Further, the topological stability of the solitons may be exploited in non-volatile high-density data storage devices. Finally, the topological Hall effect caused by spin solitons may be used to build a new class of field sensors.

The objectives of this proposal are a systematic search for new forms of magnetic order composed of topologically protected (particle-like) spin solitons in bulk materials driven by chiral interactions. This will establish a new field of magnetic phenomena. We further propose the development of concepts how to exploit specifically the topological aspects of these spin solitons in information technology. The proposed research program comprises state-of-the art materials preparation, in-depth studies of the materials properties using bulk and microscopic probes and advanced theoretical modelling. While the proposed project represents a high-risk effort, it promises a fundamentally new approach to information technology","2200000","2012-03-01","2017-02-28"
"TOPMAT","Topological Materials: New Fermions, Realization of Single Crystals and their Physical Properties","Claudia FELSER","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","The topologies of the electronic and magnetic structure in reciprocal and real space underlies much of condensed matter physics. Moreover, the properties of single crystals with particular topological electronic structures can mimic phenomena found in high energy physics and cosmology. New classes of quantum materials are found in insulators and semimetals that exhibit non-trivial topologies: they display a plethora of novel phenomena including: topological surface states; new Fermions such as Weyl, Dirac or Majorana; and non-collinear spin textures such as Skyrmions. A hallmark of many of these new quantum properties that are derived from fundamental symmetries of the bulk, is that they are topologically protected. Just recently a general scheme to identify novel Fermions was proposed that is based on the symmetries and the Wyckhoff positions of relevant space groups. These new types of Fermions are a groundbreaking concept beyond the known Dirac and Weyl and have no high-energy counterparts. The translation of these theoretical concepts into realizable materials is one focus of this proposal. The next step is to apply this approach to magnetic space groups so as to identify new magnetic Fermions. We will engineer these topological materials via synthesising high quality single crystals and by applying for example high magnetic fields and high pressure, to tune topological phase transitions, electrical transport properties and surface states. Particularly high-pressure Hall measurements will be developed. Hall measurements allow the investigation of the fundamental electrical transport properties of topological materials such as their carrier densities, oscillation frequencies, mobilities, and anomalous and topological Hall effects. The PI and her team have already synthesized more than 50 different topological materials as single crystals. To boost topological science in Europe even further a single crystal platform will be established within the proposed project.","2070000","2017-07-01","2022-06-30"
"Topo Ins Laser","Topological Insulator Laser","Mordechay SEGEV","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","Triggered by condensed matter, a new frontier recently emerged: Photonic Topological Insulators (PTIs). These are photonic structures where the transport of light is topologically protected: light propagates in a unidirectional manner without reflection, even in the presence of corners, defects, or disorder. The first step toward PTIs was the electromagnetic analogue of the quantum Hall effect, employing magnetic fields in gyrooptic media. Bringing the concepts of topological insulators into photonics required fundamentally different effects, eluding researchers until in 2013 we demonstrated the first PTI. That, along with experiments in silicon photonics and pioneering theory work, launched the field of Topological Photonics.   

This proposal aims to explore the possibility of the “next big thing”, a fundamentally new concept, never suggested before in any context, with high potential impact on fundamentals and on lasers technology: we will explore the idea of the Topological Insulator Laser.  

Topological Insulator Lasers are lasers where the lasing mode is topologically protected: light propagates around the cavity unaffected by disorder and defects. Based on our preliminary studies, we envision that by lasing in a topological mode, the interplay between the topology and gain will lead to a highly efficient laser, robust to defects and disorder, that lases in a single mode even at high gain values. 

The road to achieve this goes against current knowledge: topological insulators are linear Hermitian closed systems, whereas the topological insulator laser is a non-Hermitian, highly nonlinear, open system. 

Our study will be theoretical and experimental, starting at the fundamentals of topological transport in systems with gain, and we will take it all the way to experimentally demonstrate the concepts in several different platforms. 

The idea of the Topological Insulator Laser is unique: success will mark a new milestone in optics and topological physics.","1864000","2018-06-01","2023-05-31"
"TopSupra","Engineered Topological Superconductivity in van der Waals Heterostructures","Christian Schönenberger","UNIVERSITAT BASEL","Topological matter is a new research focus with great perspectives. These are insulators with an inverted “negative” bandgap and a conducting surface state. While the surface state in a topological insulator (TI) is composed of chiral fermions carrying charge and spin, in topological superconductors it is pinned to zero energy due to particle-hole symmetry and composed of fermions that carry neither charge nor spin. In-stead, they are non-abelian fermions, Majorana and parafermions (MF/PF), that have been proposed for topological quantum computing. Evidence for MFs have been found in nanowires. However, the scaling-up challenge requires a platform in which networks of MFs can be realized. Here, we propose to use graphene-based van der Waals heterostructure for this purpose. The unprecedented versatility is enabled by combining high-mobility graphene with other layered materials, such as transition-metal dichalcogenide, few-layer ferromagnets and superconductors (SCs). This allows to design topological systems, e.g. the quantum spin, anomalous and valley Hall effect, by combining Zeeman energy, spin-orbit and pairing interaction. We will design 2D quantum matter using different approaches, including strain tuning and the dressing of the bandstructure by photon-fields (Floquet TI), and couple it to SCs to induce topological superconductivity. We will use our expertise from studies of Cooper-pair splitters to not only add pairing in a single edge-state, but also between different edge-states, beneficial in obtaining MFs and more exotic quasiparticles. We will apply advanced high-frequency techniques, e.g. emission and noise - in addition to local tunneling spectroscopy - to characterize the in-gap states and to prove their topological nature. We will deliver a versatile technology with which new states of matter can be obtained in a platform which can be engineered in a top-down manner into networks allowing for quantum-state manipulation of MFs and PFs.","2497577","2018-07-01","2023-06-30"
"TOPVAC","From Topological Matter to Relativistic Quantum Vacuum","Grigory VOLOVIK","AALTO KORKEAKOULUSAATIO SR","The structure of relativistic quantum vacuum (RQV) in our Universe is one of the main challenges in modern physics. We plan to advance our understanding of the vacuum structure and on this basis treat the most important unsolved problems in physics, such as the cosmological constant problem (why the measured vacuum energy is 120 orders of magnitude smaller than estimates from the zero point motion) and the hierarchy problem (why the masses of the known particles in the Standard Model (SM) of particle physics are much smaller than the Planck energy). The quantum vacuum shares many common properties with topological matter. The goal of the proposal is to concentrate both theoretical and experimental efforts in the investigation of connections between the topological quantum matter and RQV, to enhance understanding of topological condensed-matter systems especially in the ultra-low-temperature regime, and to apply this experience to solution of problems in SM & cosmology. As a condensed-matter system we shall use superfluid phases of liquid 3He – unique topological materials, which are the most close to SM and gravity: Superfluid 3He-A, where the low-energy excitations are topologically protected Weyl fermions, gauge bosons, and gravitons, is similar to the vacuum of SM above the electroweak transition. The fully gapped topological superfluid 3He-B with its Higgs bosons is the counterpart of SM vacuum in its broken symmetry phase. In particular, theory of relaxation of dark energy will be accompanied by experimental study of resonant decay of coherent states of non-equilibrium superfluid vacuum. Determination of the topological classes of the quantum vacua of SM including the vacua with Majorana fermions will be accompanied by experimental studies of Majorana fermions on the boundaries of topological superfluids and in cores of quantized vortices. Theory of extra Higgs bosons in SM will be accompanied by experimental study of light and heavy Higgs modes in 3He-B.","2159191","2016-10-01","2021-09-30"
"TORCH","A large-area detector for precision time-of-flight measurements","Neville Harnew","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","""We propose to construct and prototype an innovative high-precision time-of-flight system suitable for large areas, up to tens of square metres. The TORCH (Time Of internally Reflected CHerenkov light) detector provides a time-of-flight measurement from the imaging of photons emitted in a 1 cm thick quartz radiator, based on the Cherenkov principle. The photons propagate by total internal reflection to the edge of the quartz plane and are then focused onto an array of Micro-Channel Plate (MCP) photon detectors at the periphery of the detector. A timing resolution of 15 ps per particle can be achieved over a flight distance of 10 m. This will allow particle identification in the challenging intermediate momentum region, up to 20 GeV/c. The TORCH detector is highly compact, and the technique will have wide-ranging use in particle and nuclear physics experiments, and especially those where space is at a premium.
The work involves a number of ground-breaking and challenging techniques. We will develop ultra-fast Micro-Channel Plate (MCP) photon detectors that can survive for several years in high radiation environments. The MCPs will be procured in industry to our specification with customized active area and granularity. We will also develop state-of-the-art electronics to read out the MCPs with picosecond precision. Photon imaging will be achieved with milliradian resolution over the large optical volume of the TORCH.
Whilst the TORCH detector has its primary application in the field of particle physics, the MCP and optical developments will also have applications in space physics, nuclear physics, as well as medical imaging.""","2696243","2012-06-01","2017-05-31"
"TORMCJ","Thermal, optical and redox processes in molecular conduction junctions","Abraham Nitzan","TEL AVIV UNIVERSITY","Much of the current intense study of molecular conduction junctions is motivated by their possible technological applications, however this research focuses on fundamental questions associated with the properties and operation of such systems. Junctions based on redox molecules often show non-linear conduction behavior as function of imposed bias. Optical interactions in molecular junctions pertain to junction characterization and control. Issues of heating and thermal stability require a proper definition of thermal states (effective temperature) and the understanding of heat production and thermal conduction in non-equilibrium junctions. This proposal focuses on theoretical problems pertaining to these phenomena with the following goals: (a) Develop theoretical methodologies for treating non-equilibrium molecular systems under the combined driving of electrical bias, thermal gradients and optical fields; (b) provide theoretical tools needed for the understanding and interpretation of new and ongoing experimental efforts involving thermal, optical and redox (charging) phenomena in molecular junctions, and (c) use the acquired insight to suggest new methods for characterization, functionality, control and stability of molecular junctions.","842420","2008-12-01","2014-05-31"
"TOSCA","Terahertz Optoelectronics - from the Science of Cascades to Applications","Edmund Harold Linfield","UNIVERSITY OF LEEDS","Over the last 10 years, research in the terahertz (THz) frequency region of the electromagnetic spectrum has grown dramatically. The most significant development has been the demonstration of the first THz frequency quantum cascade laser (QCL) in 2002 by my EC FP-V consortium, WANTED. These advances have been accompanied by an equally important industrial applications-pull, with exploitation envisaged in the pharmaceutical and security sectors, for medical imaging and atmospheric sensing, and for high frequency electronics and communications. Yet, the enormous potential of the THz range has still to be unlocked, principally as there remains a lack of versatile, compact THz systems. My vision here is to address this, creating a step-change in the exploitation of THz technology. I will develop the patterning of periodic and aperiodic grating structures both lithographically, and for the first time, electronically, to engineer the photonic properties of THz QCLs. I will demonstrate the use of surface acoustic waves to modulate QCLs piezoelectrically, creating dynamically tunable sources. A continuous wave system-on-a-chip based on a QCL source, waveguide and integrated solid state detectors will be developed, together with an on-chip continuous-wave THz interferometer, and proven in the study of low-dimensional, nanostructured systems. I will develop a compact fibre-coupled broadband THz system, based on 1.55µm fs-laser excitation of photoconductive antennae. Investigations into the fundamental science underlying THz QCLs will include magnetic field gain measurements of THz QCLs to probe the role of non-Markovian transport in superlattice optoelectronic structures. This programme, comprising the symbiotic development of THz engineering and science, will be unique internationally and will open new opportunities and directions in the study and exploitation of THz frequency electronics and photonics.","2491989","2010-04-01","2015-03-31"
"TOTALPHOTON","A Total Photon Camera for Molecular Imaging of Live Cells","Robert Kerr Henderson","THE UNIVERSITY OF EDINBURGH","""How can we construct a high-resolution camera capable of imaging the time-of-arrival, polarisation and wavelength of each of the maximal 10Gphoton/s emitted from a labelled, biological cell?
Such a measurement would capture the complete information available in the optical signal, and significantly enhance our ability to observe the organisation, movement and interactions of cellular components at molecular scales. Advances in single molecule light microscopy are steadily improving our understanding of the processes underlying normal cellular function, and their alteration in disease states. However, these technologies are unable to reach their full potential due to their reliance on pre-existing, suboptimal detectors. A dedicated camera technology is now required to permit simultaneous, multidimensional measurements of large cohorts of molecules at high temporal and spatial (sub-diffraction limit) scales through total imaging of the photon flux.
Today’s digital cameras capture photons in packets of 10-100 thousand and provide them for external display or recording at fraction of second intervals. In order to process photons individually rather than as packets we must develop a camera operating 10-100 thousand times faster. Each pixel must be capable of capturing single photon parameters without compromising the high resolution and sensitivity achieved by current technology.  The """"total photon"""" camera will be realised in nanoscale CMOS technology, based on recent breakthroughs in ultra-miniature single-photon detectors. We will combine these with novel approaches to pixel circuits, image processing and high-speed readout electronics to provide a fundamental research tool for the emerging area of computational microscopy. We will provide access to the full record of photon emission from live cells, and hence the clearest possible visualization of dynamic cellular processes in a single device capable of wide-field molecular spectroscopy and superresolution imaging.""","2280232","2014-02-01","2019-01-31"
"TRAJECTORY","Coherent trajectories through symmetry breaking transitions","Dragan Mihailovic","INSTITUT JOZEF STEFAN","We propose to investigate the coherent trajectories of many-body systems undergoing symmetry-breaking transitions (SBTs) in real time, where trajectories are meant here in a mathematical sense used to describe the dynamics of nonlinear systems. The key idea which makes this project possible is the development of a specific femtosecond laser spectroscopy technique which allows us to distinguish the order parameter dynamics in complex matter systems from hot-electron energy relaxation, quasiparticle recombination processes, damping and dephasing of coherent phonon oscillations. This allows real-time high resolution investigations of the critical system trajectories through SBTs, beyond the capabilities of current state of the art time-resolved techniques. We will investigate coherent collective field oscillations and the fundamentals of topological defect creation by the Kibble-Zurek mechanism including a study of their annihilation in the aftermath of SBTs. We will aim to control the coherent trajectories at bifurcation points by laser pulses and external fields. We will address fundamental questions on the effect of symmetry and fundamental interactions of underlying microscopic vacua on global behaviour. Systems included in our study belong to a number of different universality classes and include the study of nontrivial transitions to newly discovered hidden states of matter. In the general framework of reductionism, we expect our findings to have fundamental bearing on our understanding of SBTs revealing predictive tell-tale signatures of critical events of relevance in areas beyond many-body condensed matter physics, in elementary particle physics, primordial cosmological events and tipping points in nonlinear systems. Transition trajectories to and from hidden states are of particular interest for practical applications in new femtosecond state change memory devices.","1503600","2013-05-01","2018-04-30"
"TRAMAN21","Traffic Management for the 21st Century","Markos Papageorgiou","POLYTECHNEIO KRITIS","Traffic congestion on motorways is a serious threat for the economic and social life of modern societies and for the environment, which calls for drastic and radical solutions. Conventional traffic management faces limitations. During the last decade, there has been an enormous effort to develop a variety of Vehicle Automation and Communication Systems (VACS) that are expected to revolutionise the features and capabilities of individual vehicles. VACS are typically developed to benefit the individual vehicle, without a clear view for the implications, advantages and disadvantages they may have for the accordingly modified traffic characteristics. Thus, the introduction of VACS brings along the necessity and growing opportunities for adapted or utterly new traffic management.

It is the main objective of TRAMAN21 to develop the foundations and first steps that will pave the way towards a new era of motorway traffic management research and practice, which is indispensable for exploiting the evolving VACS deployment. TRAMAN21 assesses the relevance of VACS for improved traffic flow and develops specific options for a sensible upgrade of the traffic conditions, particularly at the network’s weak points, i.e. at bottlenecks and incident locations. The proposed work comprises the development of new traffic flow modelling and control approaches on the basis of appropriate methods from many-particle Physics, Automatic Control and Optimisation. A field trial is included, aiming at a preliminary testing and demonstration of the developed concepts.

TRAMAN21 risk stems from the uncertainty in the VACS evolution, which is a challenge for the required modelling and control developments. But, if successful, TRAMAN21 will contribute to a substantial reduction of the estimated annual European traffic congestion cost of 120 billion € and related environmental pollution and will trigger further innovative developments and a new era of traffic flow modelling and control research.","1496880","2013-03-01","2018-02-28"
"TRANSATLANTICILAB","Trans-Atlantic Imaging of Lithosphere Asthenosphere Boundary","Satish Chandra Singh","INSTITUT DE PHYSIQUE DU GLOBE DE PARIS","The Plate Tectonics Theory is the most important discovery in all of earth sciences. It is based on the concept of plates (lithosphere) that float over the asthenosphere. Although the lithosphere is a basic building block of the plate tectonics theory, its nature, its thickness, its boundary with the asthenosphere (LAB) are still matter of heated debates. Here we propose to image the LAB and internal structure of the lithosphere at a very high-resolution using a combination of different geophysical methods in a systematic manner across the Atlantic Ocean (Trans-Atlantic) for a lithosphere of 0-100 Ma age. Along with using seismological and magnetotelluric methods, we propose to use a technology newly developed for the oil and gas exploration that is capable of providing a seismic reflection image down to 120 km depth with a few hundred metres resolution, resolving the controversy on the formation and evolution of the oceanic lithosphere once and for all, filling the gap between seismological and seismic reflection methods, opening up a new frontier of research, and creating synergy between academic and industrial research to address fundamental scientific problems. These new seismic data should also provide images of melt lenses in the mantle beneath the spreading centre axis, if present, which will help us to build a new model of melt generation and migration from the mantle. We should also be able to image deep penetrating faults that might have been generated due to the cooling of the lithosphere as it moved away from the spreading centre, allowing the development of a new model of hydration of the oceanic lithosphere, which would be extremely valuable for the understanding of the earthquake process at subduction zones. The imaging of the structure down to 120 km in an oceanic environment would be a major breakthrough, and likely to open up new horizons for deep seismic imaging.","3499900","2014-11-01","2019-10-31"
"TRANSEP","Flow physics and interaction of laminar-turbulent transition and flow separation studied by direct numerical simulations","Dan Henningson","KUNGLIGA TEKNISKA HOEGSKOLAN","The vision spelled out in this proposal is to overcome the failure of Computational Fluid Dynamics to tackle one of the central unsolved fluid physics problems, namely predicting the sensitive flow physics associated with laminar-turbulent transition and flow separation. A recent, highly influential report by NASA (Slotnick et al., 2014) clearly states that the major shortcoming of CFD is its “… inability to accurately and reliably predict turbulent flows with significant regions of separation”, most often associated with laminar-turbulent transition.   

The research proposed here will address this shortcoming and develop and utilize computational methods that are able to predict, understand and control the sensitive interplay between laminar-turbulent transition and flow separation in boundary layers on wings and other aerodynamic bodies. 

We will be able to understand enigmas such as the recent results from the experiments of Saric et al. at the Texas A&M Univeristy where the laminar area of a wing grows after a smooth surface have been painted (increased roughness), or the drastic changes of laminar-turbulent transition and separation locations on unsteady wings, or the notoriously difficult interaction of multiple separation and transition regions on high-lift wing configurations. For such flows there have been little understanding of flow physics and few computational prediction capabilities. Here we will perform simulations that give completely new possibilities to visualize, understand and control the flow around such wings and aerodynamic bodies, including the possibility to compute and harness the flow sensitivities. 

We will tackle these outstanding flow and turbulence problem using the new possibilities enabled by multi-peta scale computing.","2097520","2016-09-01","2021-08-31"
"TRANSFORM OPTICS","Transformation optics: cloaking, perfect imaging and horizons","Ulf Leonhardt","WEIZMANN INSTITUTE OF SCIENCE LTD","Transformation optics grew out of ideas for invisibility cloaking devices and exploits connections between electromagnetism in media and in geometries. Invisibility has turned from fiction into science since 2006, but is far from being practical yet. Advances in the theory of transformation optical are the key for bringing invisibility closer to practicality. Probably the most important practical application of connections between media and geometries is perfect imaging, the ability to optically transfer images with a resolution not limited by the wavelength. This is because imaging lies at the heart of photolithography, the key technology used for making electronic chips. On the other hand, probably the intellectually most important application of connections between media and geometries lies in the quantum physics of the event horizon, which, for the first time, could be studied in the laboratory. The objective of this proposal is to make significant breakthroughs in (1) moving cloaking from frontier research closer to practicality, (2) turning perfect imaging into a viable technology and (3) demonstrating the quantum physics of the event horizon in the laboratory. This project is at the cutting edge of a global communal effort in the research of metamaterials. The overarching theme of the project is to make abstract and seemingly fantastic ideas practical, by combining ideas from geometry and general relativity with the latest advances in optical metamaterials and integrated and ultrafast photonics.","2495399","2013-03-01","2018-02-28"
"TRANSFORMER","Structural transformations and phase transitions in real-time","Jens Alexander Egon BIEGERT","FUNDACIO INSTITUT DE CIENCIES FOTONIQUES","Chemical and material sciences are key drivers of our modern economy with transformative impact at all levels of society. In particular, the ability to synthesize and to tailor substances and materials with specific function is all-pervading into modern society. Vital is a firm understanding of structural transformations of molecules and phase transitions of solids as they are omnipresent, e.g. as formation and breakage of molecular bonds, proton motion and isomerization, and as collective phenomena in phase transitions. Gaining insight into the ultrafast correlated dynamics is highly challenging and requires revolutionary methodologies and innovative approaches to capture the dynamics from its onset.
TRANSFORMER will provide unprecedented insight into the real-time electronic and nuclear dynamics of molecular transformations and phase transitions with advanced new methodologies and a multi-faceted approach to the investigation. The project exploits our pioneering achievements in attosecond soft X-ray spectroscopy (XAFS) and laser-induced electron diffraction (LIED) to pinpoint in real-time which electronic states participate at which nuclear configuration. The proposal consists of three objectives: 

O1: We will establish the methodical boundaries of LIED for space-time imaging of isolated molecules. 

O2: We will extract simultaneous and real-time electronic and nuclear information, thus gain insight into the underlying many-body quantum correlations. 

O3: We will use our methodology to realize resolving both, molecular isomerization and a solid’s metal-to-insulator phase transition, in its electronic and nuclear degrees of freedom and in real time.  

If successful, TRANSFORMER would undoubtedly provide an unprecedented view into electronic and nuclear dynamics, thereby reaching far beyond the state of the art with clear potential to surpass current limits in molecular and material sciences.","2471749","2018-09-01","2023-08-31"
"TransQ","Mass, heat and spin transport in interlinked quantum gases","Tilman Holger ESSLINGER","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","The objective of the proposed project is to create a versatile experimental and methodological platform for exploring transport mechanisms with quantum gases. Our approach will enable studying the dynamics of mass, heat and spin transport between linked reservoirs with a unique degree of control and flexibility, and promises to open up a route to discovering yet-unknown transport phenomena.

Over the past two decades, ultracold atomic quantum gases have taken an increasingly lively role in the endeavour to understand quantum many-body systems, offering insights into a wide range of quantum phases and transitions between them. Recently, the approach has proven its potential to take us beyond the simulation of existing concepts and to provide a platform for probing the physics of quantum many-body systems in novel contexts. In particular, we have shown that measurements of directed transport through channels connecting atomic reservoirs not only emulate scenarios known form electronic transport in solid-state systems, but can test new experimental situations and give rise to new questions.

We now propose to establish a general quantum-gas platform for exploring a wide range of configurations for transport measurements. Specifically, we will study the non-equilibrium dynamics in systems consisting of connected fermionic quantum-gas nodes, which serve as particle reservoirs of different size, shape and dimensionality that can be individually initialized and coupled to one another using configurable links. Time-dependent drive or controlled dissipation can be applied to the nodes and links. Using such networks, we will study transport between reservoirs of different nature, probe superfluid samples with controlled particle currents, characterize transport processes at the interface of different quantum phases, search for superfluidity in driven systems, prepare and detect Majorana fermions and develop functionality in complex structures.","2500000","2017-10-01","2022-09-30"
"TRAVERSE","Transcending Reality   Activating Virtual Environment Responses through Sensory Enrichment","Melvyn Slater","UNIVERSITAT DE BARCELONA","The goal of this project is to provide the means for  transcending reality : to use virtual reality to transcend the self, whereby participants in an immersive virtual reality (IVR) may have the strong illusion of transforming their bodies, and therefore have experiences in VR that are not possible in physical reality. The goal is to use IVR for something that is genuinely new and uniquely possible within such systems, beyond the normal representational approach to VR which attempts mainly to faithfully simulate reality. The research is based on three fundamental ideas: (a) that our body image is highly malleable, and that virtual reality can be used to transform it; (b) that the brain has sufficient plasticity to remap tactile sensations, so that a sensation felt on one part of the body can be associated with an event on another part of the body   and that therefore this can be used in VR in order to endow virtual experiences with physicality; (c) that the brain can be fooled into inferring that something perceived in VR is  really happening  (even though the person knows for sure that it is not)   and that there are techniques that can be employed in VR to enhance the probability that this occurs. Hence the research is interdisciplinary, a novel combination of neuroscience and computer science, with the likelihood of benefits to both. The work is organised into three major Streams corresponding to (a)-(c), where specific hypotheses are derived from the general idea and tested empirically. Each set of experiments requires an engineering solution. A fourth Stream of work will be for integration   creating scenarios for the testing of the general hypotheses implied by (a)-(c). The main application will be in social virtual environments, where a human participant interacts with virtual characters, or a mixture of virtual characters and other online people represented in the VR. Applications would be in the fields of therapy, physical rehabilitation and entertainment.","2409768","2009-04-01","2015-03-31"
"TRENSCRYBE","TRapped ENSembles of Circular RYdBErg atoms for quantum simulation","Michel BRUNE","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","TRENSCRYBE proposes a groundbreaking concept for a quantum simulator (QS) of spin systems. QSs aim at an in-depth understanding of many-body physics, important for fundamental issues (quantum transport, phase transitions), but also for the development of engineered materials. A QS transcribes the system of interest into another with the same dynamics but with a fully controlled Hamitlonian parameter set. It provides a complete access to all the systems observables and allows the exploration of regimes, in which numerical simulations are difficult or impossible due to the huge size of the Hilbert space. QSs of spin systems are the focus of an intense activity, e.g. with trapped ions, superconducting devices, atoms in optical lattices or low-angular-momentum Rydberg atoms. I propose with TRENSCRYBE a disruptive QS using trapped circular Rydberg atoms. Their microwave spontaneous emission can be inhibited, extending their lifetime in the minute range. In contrast with ordinary Rydberg states, they can be trapped in optical lattices, allowing one to take full benefit from such exceptional lifetimes. I propose an innovative preparation method of ultra-cold defect-free chains with a few tens of atoms. The QS realizes a Nearest-Neighbor XXZ spin-1/2 Hamiltonian in a transverse field, the spin states being coded on circular levels. Its parameters are fully tunable by adjusting a static electric field and a microwave dressing applied on the atoms. I will first benchmark this QS by adiabatic evolutions through the phase diagram of a 1-D chain in a regime where numerical results are available. I will explore quenches through quantum phase transitions and the generation of defects in a regime where numerical or theoretical predictions are difficult. I will realize a first step towards 2-D arrays with spin ladders and explore their topological phases. The realization of TRENSCRYBE will open a whole realm of new possibilities for quantum simulation of spin systems.","2240000","2018-11-01","2023-10-31"
"TReX","Transient Relativistic eXplosions","Tsvi PIRAN","THE HEBREW UNIVERSITY OF JERUSALEM","The recent and upcoming deep and large field of view surveys has ascribed transient sources an ever-increasing role in 21st century astronomy. We propose to explore three relativistic transients: Compact binary mergers; Stellar disruption by massive black holes (TDEs) and Gamma-Ray Bursts (GRBs). Mergers are the prime targets of advanced Gravitational Waves (GW) detectors. Their detection will open a new window on the Universe. However localization, based on electromagnetic (EM) counterparts, that we propose to study here, is essential for GW Astronomy. TDEs provide a novel view on galactic centers’ massive black holes. However, TDE observations pose some puzzles, suggesting that a revision of the current tidal disruption theory is needed.  New observations provide a wealth of data on GRBs and this is the time to determine their inner workings and to obtain a clear model for the prompt emission mechanism – a long standing puzzle.  This project includes theoretical modeling of these events as well as phenomenology of the observations and even some data analysis and observations.  Mergers, TDEs and GRBs, are tightly interconnected and share similar physical mechanisms. The theory of merger radio flares and of TDE’s radio emission draws, for example, on GRBs’ afterglow theory and the interpretation of TDE high-energy emission is based on concepts borrowed from the prompt emission of GRBs. A coordinated theoretical study will reveal and utilize the commonalities of these phenomena and has a strong potential to obtain far reaching results beyond the current state of the art with possible implications to other high energy astrophysical phenomena. While this is a theoretical proposal we address at all stages directly observational issues. Hence the proposal is closely related to observations - interpreting existing puzzling observations, predicting new ones or suggesting strategies how to obtain them.","1449375","2016-10-01","2021-09-30"
"TRIGGDRUG","Reactions That Translate mRNA into Drug-like Molecules","Oliver Seitz","HUMBOLDT-UNIVERSITAET ZU BERLIN","How could a molecular cancer therapy look like in 2040? In cancer, gene expression is deregulated due to amplification, mutation and translocation of genes. Next generation RNA sequencing provides us with the opportunity to identify the number and identity of the gene products aberrantly expressed in a patient. But do we have methods that take advantage of the personalized sequence data? In this research project we propose the idea to use the RNA molecules expressed upon disease-type gene expression as instructors for the chemical synthesis of drug-like molecules that cure the disease. Accordingly, drug-like molecules would only be formed in those cells that express the disease-specific RNA molecules. Such a personalized molecular therapy would eliminate side effects caused by unwanted perturbation of healthy cells. The idea to use cellular RNA molecules as triggers for drug synthesis requires methods that couple RNA recognition with a change of chemical reactivity. Reactive molecules must be able to “read” and “translate” the sequence of a RNA molecule into a drug-like output. We will develop mRNA-triggered reactions that i) proceed with turnover in template to cope with low mRNA copy numbers and ii) allow the single-step synthesis of highly active drug-like molecules to address deregulated protein targets inside cancer cells. To achieve this aim, we will advance chemical acyl transfer and alkylidene transfer reactions. The reactions on disease-specific mRNA will form peptides/peptidomimetics/small molecule-based kinase inhibitors which will induce apoptosis in cancer cells. We will target validated drug targets. Synergy between the nucleic acid and protein worlds will be harnessed. Furthermore, we will develop a RNA-promoted reaction with turnover beyond product inhibition. This will enable a transcriptome-activated photodynamic therapy. In a nutshell, we will develop a chemistry-based tool to hijack disease mRNA and rewire the cell death program.","2470400","2016-01-01","2020-12-31"
"TripleSolar","Solar Energy Conversion in Molecular Multi-Junctions","René A. J. Janssen","TECHNISCHE UNIVERSITEIT EINDHOVEN","The project focuses on investigating and developing novel principles for solar-to-electricity and solar-to-fuel conversion using organic semiconductors and employing multiple photons in a process that mimics natural photosynthesis. The goal is to develop efficient solar energy convertors based on cheap and abundant materials that offer prospects to be employed on large scale and contribute to global conversion and storage of solar energy.

Presently, state-of-the-art polymer solar cells reach power conversion efficiencies of  ~10% in solar light. Projected efficiencies are as high as 20% when multi-junction solar cells can be employed. Closing this gap is a tremendous challenge that will require pushing every single step in the conversion process to its intrinsic limits, eliminating losses close to perfection. In addition to efficient conversion, storage of energy is crucial because solar electricity supply and demand are intermittent. Capturing solar energy in chemical bonds of molecular fuels is most effective in terms energy density and the first firm ideas are emerging on how this can be achieved. We will use our expertise in the area of polymer solar cells to create multi-junction molecular solar-to-electricity conversion devices with unprecedented power conversion efficiencies and develop new concepts for efficient solar-to-chemical conversion.

To reach these ambitious goals, the project focuses on investigating fundamental questions regarding charge generation and on developing new organic materials, electrocatalysts and devices for solar-to-electricity and solar-to-fuel conversion. The activities involve designing and synthesizing new materials, performing photophysical and morphological studies, analyzing charge and exciton transport in relation to morphology, developing new interfacial layers, electrocatalysis, and exploring the use of multi-junction configuration devices in solar cells and artificial leaves.","2493585","2014-03-01","2019-02-28"
"TROPGEO","Tropical Geometry","Grigory Mikhalkin","UNIVERSITE DE GENEVE","The goal of this project is to develop Tropical Geometry, a newly emerging kind of algebraic geometry. It is expected to be more powerful than Classical Geometry in a range of applications (particularly in Physics-minded applications). In the same time it is significantly simpler in several mathematical aspects. In the last decade a number of initial applications of this new geometry has appeared with a success, particularly in the framework of the so-called Gromov-Witten theory, based on curves, i.e. 1-dimensional algebraic varieties. The new subject became known as Tropical Geometry since algebraically it is a based on the so-called ``Tropical Calculus'' of Computer Science. In the tropical world the curves are metric graphs, sometimes enhanced with additional structure. Stepping forward from my recent successes in set-up and application of Tropical Geometry I plan to continue this work. Particularly I plan to advance the following challenging lines of research:
Solve several classical complex enumerative problems, particularly compute ZeuthenÕs characteristic numbers.
Develop tropical homology theories.
Advance the theory of amoebas and coamoebas (algae) of algebraic varieties.
Advance understanding of real algebraic geometry.
Establish direct relation between Feynman diagrams and tropical curves.
Break Òthe Gromov-Witten barrierÓ in Enumerative Geometry.
Develop birational tropical geometry in higher dimensions.
These directions are intrinsically related in their scope and suggested methodology. Some of the proposed goals are very ambitious, but even partial advances would mean a big step forward.","1928800","2010-01-01","2014-12-31"
"TUCLA","Towards a deepened understanding of combustion processes using advanced laser diagnostics","Lars Eric Marcus Aldén","LUNDS UNIVERSITET","The field of combustion is of utmost societal/industrial importance while at the same time posing outstanding scientific challenges. In order to handle these, it is extremely important to develop and apply non-intrusive laser-diagnostic techniques with high spatial and temporal resolution for measurements of key parameters such as species concentrations and temperatures. Such techniques have been developed and applied by the PI for more than thirty-five years and the home institute has one of the most advanced instrumentations in academia world-wide. 
The proposal activities will be divided into two areas including five main Work packages:

1. Development of new diagnostic techniques. We will concentrate on concepts based on structured illumination which will add a new dimension to present diagnostics based on temporal, intensity and spectral properties. It will allow for multiscalar measurements and efficient suppression of background light. Furthermore, we will work with femto/picosecond lasers for investigating the diagnostic applicability of filamentation, new aspects of non-linear techniques, and diagnostic aspects of photodissociation phenomena.

2. Phenomenological combustion studies using advanced laser diagnostics. A very important aspect of the project is to use the developed and available diagnostic techniques to assure experimental data in extremely challenging environments and together with modeling experts enhance the understanding of combustion phenomena. Studies will be carried out on three
different topics:
-  Flame structures in laminar flames at high pressure as well as turbulent flames at atmospheric/high pressure.
-  Biomass gasification, where complex fuels require new techniques to measure nitrogen, alkali, chlorine and sulfur compounds, as well as for measurements inside fuel particles.
-  Combustion improvement by electric activation which can be introduced to handle flame oscillations and instabilities.","2442000","2016-01-01","2020-12-31"
"TUNE","Testing the Untestable: Model Testing of Complex Software-Intensive Systems","Lionel, Claude, Laurent Briand","UNIVERSITE DU LUXEMBOURG","Software-intensive systems pervade modern society and industry. These systems often play critical roles from an economic, safety or security standpoint, thus making their dependability indispensible. Software Verification and Validation (V&V) is core to ensuring software dependability. The most prevalent V&V technique is testing, that is the automated, systematic, and controlled execution of a system to detect faults or to show compliance with requirements. Increasingly, we are faced with systems that are untestable, meaning that traditional testing methods are highly expensive, time-consuming or infeasible to apply due to factors such as the systems’ continuous interactions with the environment and the deep intertwining of software with hardware.
TUNE will enable testing of untestable systems by revolutionising how we think about test automation. Our key idea is to frame testing on models rather than operational systems. We refer to such testing as model testing. The models that underlie model testing are executable representations of the relevant aspects of a system and its environment, alongside the risks of system failures. Such models inevitably have uncertainties due to complex, dynamic environment behaviours and the unknowns about the system. This necessitates that model testing be uncertainty-aware.
We propose to develop scalable, practical and uncertainty-aware techniques for test automation, leveraging our expertise on model-driven engineering and automated testing. Our solutions will synergistically combine metaheuristic search with system and risk models to drive the search for critical faults that entail the most risk. TUNE is the first initiative with the specific goal of raising the level of abstraction of testing from operational systems to models. The project will bring early and cost-effective automation to the testing of many critical systems that defy existing automation techniques, thus significantly improving the dependability of such systems.","2307932","2016-09-01","2021-08-31"
"TWELVE LABOURS","Twelve Labours of Image Processing","Jean-Michel Morel","ECOLE NORMALE SUPERIEURE DE CACHAN","After a 15-year preparatory work, a group of mathematicians and computer scientists is ready to reconsider and formalize the main steps of image processing, and to devise a way to make them fully automatic. The project will conceive, revise or accelerate the dozen highly accurate algorithms necessary to establish a universal image processing chain, applicable to all raw digital images obtained from reflex or compact off-the-shelf cameras, and to all more specialized image generation systems, including a digital photon capture array (CCD or CMOS,...). The direct applications will be: - a fully autonomous image processing chain transforming any raw image into a distortion corrected and noise free visible image with optimal color and contrast. On line demo and C-code will be made available to the community; - a complete 3D image reconstruction system for an in-project Earth observation satellite (MISS project, Centre National d&apos;Etudes Spatiales). This Earth scanner will take stereo pairs of the Earth at 40 centimeters resolution, permitting to build highly accurate urban elevation models. - a camera based passive 3D reconstruction system with $\pi/40000$ angular precision competing with the best active short range 3D triangulation scanners. Getting a reproducible 1/20 pixel accurate image processing chain from raw photon sensor data to 3D reconstruction with fully unsupervised algorithms will be a major change of the discipline. The team has devised a new image statistical theory and invented several new mathematical processes dealing with image noise and projective invariant image matching. The team has strongly contributed to the world&apos;s most successful image processing company, DxO Labs, and to the image processing line of three Earth observation satellites.","1837568","2010-09-01","2015-08-31"
"TWISTS","Twists & more: the complex shape of light","Miles Padgett","UNIVERSITY OF GLASGOW","""My scientific career to date has centered around the phase and intensity shaping of light beams, specifically for the pioneering studies of Orbital angular momentum.

I wish to build upon this foundation applying twisted and shaped light to sensing and imaging in both the classical and quantum domains.

I will use orbital angular momentum as a new imaging modality and as the basis as a remote sensor of rotational motion.  I will used randomly shaped light beam as a new approach to 3D vision.  I will use the quantum correlations of orbital angular momentum and other spatial states to explore new demonstrations of quantum behaviour and deliver imaging performance beyond the classical limit.

To realize this project, funding is sought for 2 FTE postdoctoral research assistants for 5 years and 3 PhD students who will work in close conjunction with myself.  I will devote myself to this project, both technically in the laboratory and in promoting the results of the program to international scientific, industrial and political peers.""","1734570","2014-04-01","2019-03-31"
"UFSD","Ultra-Fast Silicon Detectors: Enabling Discoveries","Nicolo' Cartiglia","ISTITUTO NAZIONALE DI FISICA NUCLEARE","The goal of our project is to empower a broad range of research fields with a completely new particle detector, able to concurrently deliver time resolutions of the order of ~10 picoseconds and position resolutions of ~30 microns. 

In so doing, we will remove the constrains that many applications such as particle tracking, medical PET, mass spectroscopy, and beam monitoring have due to the lack of precise information on all 4 dimensions. 

Our analysis of state-of-the-art particle detectors has shown a dichotomy: specialized sensors measure very accurately either time or position, but not both: the ambitious goal of UFSD is to create a new family of detectors, based on controlled charge multiplication in silicon, which will remove this limitation. We will have to tackle significant challenges along this research path, but our simulations and prototypes indicate that this approach has the potentiality to radically transform present detectors and to enable many applications to reach their peak performances.  

Our ultimate goal -- a highly segmented detector with a space resolution of ~30 microns and a time resolution of ~10 picoseconds -- can be achieved only by developing full custom Very Large Scale Integrated chips that, matching the size of the read-out to the area of each pixel sensor, will deliver unprecedented timing resolution at the pixel level.

The PI, N. Cartiglia, is well known in the field of particle physics and detector development. He is currently in charge of several projects involving both new directions in physics, among which the LPCC forward physics group and the CT-PPS project at CERN, as well as new developments in instrumentation. He will lead the project leveraging on his past experience in detector development and group/grant management, his knowledge of the silicon research foundries and laboratories, and the expertise available in his home institution  (INFN, Torino, Italy).","1793313","2015-09-01","2021-08-31"
"ULTIMATE CERAMICS","Printed Electroceramics with Ultimate Compositions","Heli Maarit Jantunen","OULUN YLIOPISTO","The ultimate goal of this research is to make extremely advanced leap enabling processing of wide variety of ceramic materials at ultra low temperatures denoted as ULTIMATE CERAMICS(200-500 oC). The project has its risks, but advantages like utilization of pure ceramic materials on challenging substrates like plastic and paper could offer novel scientific results as well as business opportunities to European industry. Key issues based on scientific laws of matters forming the basic research methodology to succeed are • intelligent selection and development of starting materials • utilization of nano technology • management of dense and uniform packaging of powder particles • minimization of required activation energy during sintering • management of type, level and rate of diffusion in sintering • microwave sintering There are several reasons why this kind of ULTIMATE CERAMICS can now be seriously research. The main issues are that ano particle silver pastes sinterable at ~ 200 oC have recently become commercially available, and ceramics with nano particle size have been widely on the market. However, taking the high risk, ground-breaking challenge, ultimate novel materials and processes are available. ULTIMATE CERAMICS offer significance novel business opportunities for European industry not available in any other way since ceramic materials are able to perform e.g. as semiconductors, dielectric, non-linear dielectrics, sensors, and electrically or magnetically tunable devices. In the industrial point of view, the main issue is to enable printable structures on paper compatible with nano particle silver electrodes and printed organic materials. It is also obvious that novel scientific results will be created especially when several techniques like e.g. microwave sintering – nano particles – sintering aids –silver electrodes - are combined.","1933200","2012-02-01","2017-10-31"
"ULTRADYNE","Ultrafast dynamics of hydrogen bonded structures in condensed matter","Hans Thomas Elsaesser","FORSCHUNGSVERBUND BERLIN EV","Structure-function relationships in condensed matter will be unraveled by studying structural dynamics on atomic length and time scales. The most advanced structure-sensitive methods of ultrafast science such as time-resolved x-ray diffraction and multidimensional vibrational spectroscopy with a temporal resolution better than 100 fs will be applied for mapping fluctuating (macro)molecular structures and photoinduced structural transformations of solids in real-time. The following phenomena will be addressed: (i) Interactions between functional units of individual nucleic base pairs and deoxyribonucleic acid (DNA) oligomers will be determined by multidimensional vibrational spectroscopy. The transfer of excitations in a local geometry and along the double helix, energy dissipation along the helix and into the aqueous environment, as well as the mechanisms of local and global hydration will be studied via the ultrafast response of local vibrations. In this way, hydrogen bonding interactions of water with the DNA backbone and with base pairs are discerned. (ii) Photoinduced structural transformations in prototype hydrogen-bonded solids will be studied by ultrafast x-ray diffraction. The so far unknown dynamics and driving forces of phase transitions in molecular ferroelectrics such as ammonium sulfate and others will be determined by recording transient diffraction patterns after electronic and/or vibrational excitation. A key issue will be the so far unresolved change of the hydrogen bond pattern. This work will include the implementation of new forefront techniques of ultrafast science, e.g., ultrafast x-ray powder diffraction, for a broad range of future applications.","2490500","2010-03-01","2015-02-28"
"ULTRALASER","ULTRALONG FIBRE LASERS","Sergey Konstantinovich Turitsyn","ASTON UNIVERSITY","The project aims to develop a new research concept of ultra-long fibre lasers. The novelty of the concept is in the ground-breaking idea to use laser in the communication applications as a transmission medium rather than as a source of radiation.  Ultra-long Raman fibre lasers (first demonstrated in 2005 by the Aston Photonics Research Group led by the applicant) present an area of interdisciplinary research at the interface of high-speed communications, laser physics, optical signal processing, nonlinear science, and mathematical theories of wave turbulence and disordered systems. The project will explore a broad range of research directions emerging from our original proposal of using ultra-long Raman fibre laser technique for quasi-lossless optical transmission with a focus on the four major  areas: (i) communications - the development of advanced cross-domain (both in space and frequency) flat-gain, zero power excursion transmission schemes; development of long reach non-repeatered fibre transmission links based on  the concept of quasi-lossless fibre spans; (ii) secure communications   fundamentally new non-quantum approaches to key distributions using ultra-long laser; (iii) laser science   up-scaling of pulse energy to record levels in mode-locked lasers by substantial increase of fibre laser cavity; new types of lasers, including ultra-long random lasers and “modeless” ultra-long lasers; (iv) underlying physics and applications of ultra-long fibre laser   optical wave turbulence in fibre lasers; new distributed sensing techniques using ultra-long Raman fibre laser. The proposed program will be unique internationally and will combine the symbiotic development of new scientific ideas and techniques based on the concept of ultra-long fibre laser with the practical engineering design considerations of immediate and future technology applications.","1659771","2011-05-01","2016-04-30"
"UltraLVP","Chemistry and transport properties of bridgmanite controlling lower-mantle dynamics","Tomoo Katsura","UNIVERSITAET BAYREUTH","Seismic observations imply that slab descent and plume ascent are impeded in the mid-mantle (MM) (depths of 660–1000 km, pressures of 23–40 GPa). A recent evaluation of viscosity variation suggested the presence of a viscosity increase or maximum in the MM that could drag the slab and plume motions. The viscosity variation may be caused by a change in the rheology of bridgmanite (Brg), the dominant mineral in the lower mantle (LM). The absence of seismic anisotropy suggests the dominance of diffusion creep in the majority of the LM. Element diffusivities and grain size are two essential factors of diffusion creep, and defect chemistry controls diffusivity. Hence, this project will determine defect chemistry, diffusivity and the grain growth rate of Brg. Since plume ascent originates in deep parts in the LM, these three properties need to be determined at pressures up to 80 GPa. Although use of a large-volume press (LVP) is vital for obtaining reliable high-pressure experimental data on mineral and rock properties, conventional LVP with carbide anvils can only generate 27 GPa. Recent LVP technology can generate over 100 GPa using sintered diamond (SD) anvils, but the process is currently very difficult for practical use. We developed a method to generate 50 GPa using hard carbide (HWC) anvils that allows practical investigation of Brg properties at mantle temperatures. We will investigate the three properties of Brg up to 50 GPa using LVP with HWC. We will develop LVP technology with SD to reliably generate pressures up to 80 GPa at mantle temperatures, and we will investigate the Brg properties under these conditions. These data will enable numerical modelling of slab and plume dynamics to explain the seismic observations. Through such modelling, we will investigate how materials are transported between the surface and deep mantle reservoirs, which can provide insight into Earth’s evolution and surface habitability.","2642120","2018-10-01","2023-09-30"
"ULTRANMR","Ultrafast Hyperpolarized NMR and MRI in Multiple Dimensions","Lucio Frydman","WEIZMANN INSTITUTE OF SCIENCE LTD","Multidimensional nuclear magnetic resonance (nD NMR) plays a unique role in Science as a primary tool for the characterization of biomolecules, as part of drug-discovery processes, and in clinical imaging (MRI). Further progress in NMR is hampered by this spectroscopy s low sensitivity, arising from the weak interactions that it involves. The prospects of solving this problem by continuing with incremental bigger machines approaches are poor, given the high maturity reached by existing technologies. The present Project deals with this issue by departing from traditional concepts, and relying on two incipient but highly promising developments in the field. One of these pertains ex situ dynamic nuclear hyperpolarization, an approach capable of eliciting liquid state NMR signals that surpass those afforded by the highest-field spectrometers by factors e10,000. While capable of providing super-signals hyperpolarization has the drawback of involving irreversible changes in the physical state of the sample. This makes it incompatible with nD NMR technologies, requiring the collection of multiple scans identical to one another except for systematic delay variations. As second component in this high-risk/high-gain Project we propose merging hyperpolarization with &quot;ultrafast&quot; methods that we have recently developed for completing arbitrary nD NMR/MRI acquisitions within a single scan. The resulting synergy could increase sensitivity by orders of magnitude, while demanding negligibly small amounts of spectrometer/scanner time to complete nD acquisitions. This should provide an ideal starting point for the analysis of a variety of organic and structural biology problems, and provide new tools to explore in vivo metabolism focusing on cancer biomarkers.","2499780","2010-03-01","2015-02-28"
"UltraPal","Ultimate Paleo-Ocean Records from Biogenic Calcites","Anders MEIBOM","ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE","The ambition with this proposal is to create ultimate paleo-environmental records for the oceans. This is a fundamental scientific challenge and the motivation is simple: The oceans take up 70% of the Earth’s surface area and represent an immense sink/source of, e.g., heat and CO2, which makes them key to the evolution of Earth’s climate. Assessing and understanding anthropogenic global climate change requires that role of the oceans is understood in detail. Proxies are needed to establish past ocean conditions with high accuracy.
    Calcite has played a fundamental role in these efforts because isotopic and trace element compositions of limestone and calcitic fossils are related to ocean conditions at the time of their formation. Geochemical studies of calcite in the ocean sediment record have therefore contributed enormously to the understanding of Earth’s climatic evolution over the last several hundred million years. 
    However, our recent work (Bernard et al., Nature Communications, 2017) identifies a fundamental problem: Visually imperceptible, ultrastructure-level processes that occur during sediment diagenesis can introduce a very strong bias in these records, in particular those based on biogenic calcite; i.e., structures produced by living organisms such as foraminifera and brachiopods. Previously not investigated or taken into account, such ultrastructure-level diagenesis will (and does) create large errors in ocean paleo-environmental reconstructions, even under the close-to-ambient pressure and temperature conditions characterizing shallow sediment burial. This proposal offers a solution: An entirely new, interdisciplinary approach, including ultra-high-resolution isotopic imaging (NanoSIMS), is developed here to quantify these effects in a broad range of biogenic calcites, permitting genuinely non-biased, calcite-based paleo-ocean reconstructions to be created. The impact of this work on climate change research will be dramatic and immediate.","2435000","2019-01-01","2023-12-31"
"UltraPhase","Ultrafast Quantum Physics in Amplitude and Phase","Alfred Leitenstorfer","UNIVERSITAT KONSTANZ","Ultrafast phenomena related to and/or accessible only via the absolute temporal phase of electronic, vibrational and spin coherent excitations in condensed matter are studied via electromagnetic transients in the multi-terahertz regime. The project also includes innovative aspects of quantum optics, femtosecond lasers and terahertz technology. Four central objectives are as follows:
(i) Establishing rapid quantum oscillatory motion as the earliest regime in the dynamics and transport of electrons in solids. Fundamental phenomena like the temporal buildup of effective mass in semiconductors and Zitterbewegung in graphene are accessed directly.
(ii) Studying nonclassical light emission predicted to emerge after non-adiabatic perturbation of ultrastrongly coupled systems of light and matter. The quantum properties of radiation released by such processes are investigated at the uncertainty limit between amplitude and phase of the light field.
(iii) Observation and control of charge and spin electronic properties of solids under extremely high transient electric or magnetic bias provided by a novel source of phase-locked multi-terahertz pulses allowing analysis with a resolution significantly below half a cycle of light.
(iv) Field-resolved photon-echo studies in the mid infrared. Unprecedented insights into complex phenomena like the interplay between low-energy degrees of freedom in high-temperature superconductors and intermolecular motion in liquids are envisioned.
New developments in ultrabroadband terahertz technology will enable the experiments:
(a) Generation of phase-locked electromagnetic transients with precisely controlled shape of the electric field like quasi-monopolar terahertz shock waves or single-cycle pulses with field amplitudes up to 30 MV/cm.
(b) Coherent detection of electric fields with bandwidth up to 200 THz and sensitivity at the uncertainty limit, giving access to the quantum properties of electromagnetic waves in amplitude and phase.","2490000","2012-04-01","2017-03-31"
"ULTRAS","Ultra-luminous supernovae : understanding their nature and cosmic evolution","Stephen Smartt","THE QUEEN'S UNIVERSITY OF BELFAST","""Until the last few years, all of the exploding stars in the Universe were thought to be of two types : core-collapse of a massive star or thermonuclear explosion of a white dwarf. The advent of wide-field synoptic sky surveys has opened up a new parameter space which allows very large volumes of the Universe to be searched for explosive stars. A new class of """"ultra-luminous"""" supernovae have been discovered that challenge our physical understanding. These optical transients are typically 20-100 times brighter than normal supernovae and the physical mechanism that produces their huge luminosity is not well understood. A proportion of them may be “pair-instability” supernovae, which have been predicted only to exist in the early Universe and result from the evolution of Population III metal free stars. The existence of this ultra-luminous population of explosions in the nearby Universe is now certain, but what their nature is and what fraction really are “pair-instability” supernovae remains to be determined. The Pan-STARRS is a novel wide-field synoptic telescope survey  which sweeps the sky to find moving objects and optical transients. I have secured leadership roles in the survey which will allow me to quantify this population of ultra-luminous supernovae at low and high redshift and to uncover their true nature. This will pave the way for searching for them at the highest redshifts with future space missions, possibly pushing into the era of reonization at z ~ 6, and determining whether the first supernovae in the Universe can be observed. Theoretical calculations for the number of Population III supernovae (from the first stars) have predicted low numbers of detections.  The recent surprising discovery that these pair-instability supernovae may exist in the local Universe, and appear confined to low-metallicity galaxies, could potentially alter these predictions dramatically.""","2315044","2012-04-01","2017-03-31"
"ULTRASUPERTAPE","ULTRAfast growth of ultrahigh performance SUPERconducting TAPEs","Maria Teresa Puig Molina","AGENCIA ESTATAL CONSEJO SUPERIOR DEINVESTIGACIONES CIENTIFICAS","ULTRASUPERTAPE aims to demonstrate an unprecedented approach for fabrication of low cost / high throughput / high performance High Temperature Superconducting (HTS) tapes, or Coated Conductors, to push the emerging HTS industry to market. The breakthrough idea is the use of Transient Liquid Assisted Growth from low cost Chemical Solution Deposition of Y, Ba, Cu metallorganic precursors to reach ultrafast growth rates. The key concept relies on the discovery of a tool to control the ignition effect of the transient liquid formation through the decomposition of barium carbonate even for thick films. The capability to modulate the transient liquid state with composition variations and low cost capital investment equipment enriches its potentiality. Innovative Additive Manufacturing and Digital Printing methodologies are identified to devise an integrated system able to address the full manufacturing process from solution deposition by ink jet printing to ultrafast epitaxial crystallization of the superconducting phase. A combinatorial chemistry strategy ensures fast screening operation. Furthermore, ULTRASUPERTAPE will boost Coated Conductor performances up to outstanding limits at high and ultrahigh fields, by smartly designing and engineering the local strain and electronic state properties of nanocomposite superconducting films. The digital-printing additive-manufacturing approach developed will be cleverly adapted to create unique superconducting nanocomposites from nanoparticle colloids with unlimited concentrations. This new instrument is foreseen to be transferable to many other functional applications of advanced nanocoatings, where long length or large area production of functional epitaxial films or multilayer structures are required. Consequently, wise ideas and technology emerged from this proposal are foreseen to penetrate the new energy paradigm beyond the clean, efficient and smart limits that Superconductivity offers.","2496652","2015-12-01","2020-11-30"
"UMWP-CHIP","Universal microwave photonics programmable processor for seamlessly interfacing wireless and optical ICT systems","JOSE CAPMANY FRANCOY","UNIVERSITAT POLITECNICA DE VALENCIA","Information and communication technology (ICT) systems are expanding at an awesome pace in terms of capacity demand, number of connected end-users and required infrastructure. To cope with these rapidly increasing growth rates there is a need for a flexible, scalable and future-proof solution for seamlessly interfacing the wireless and photonic segments of communication networks. 
RF or Microwave photonics (MWP), is the best positioned technology to provide the required flexible, adaptive and future-proof physical layer with unrivalled characteristics. Its widespread use is however limited by the high-cost, non-compact and heavy nature of its systems. Integrated Microwave Photonics (IMWP) targets the incorporation of MWP functionalities in photonic chips to obtain cost-effective and reduced space, weight and power consumption systems. IMWP has demonstrated some functionalities in through application specific photonic circuits (ASPICs), yielding almost as many technologies as applications and preventing cost-effective industrial manufacturing processes. A radically different approach is based on a universal or general-purpose programmable photonic integrated circuit (PIC) capable of performing with the same hardware architecture the main required functionalities. The aim of this project is the design, implementation and validation of such processor based on the novel concept of photonic waveguide mesh optical core and its integration in a Silicon Photonics chip. Its three specific objectives are: (1) The architecture design and optimization of a technology-agnostic universal MWP programmable signal processor, (2) The chip mask design, fabrication and testing of the processor and (3) The experimental demonstration and validation of the processor. Targeting record values in bandwidth and footprint its potential impact will be very large by unlocking bandwidth bottlenecks and providing seamless interfacing of the fiber and wireless segments in future ICT systems.","2494444","2017-07-01","2022-06-30"
"UniChem","Unifiying Concepts for Acid-Base- and Redox-Chemistry: Development, Validation and Application of Absolute pH and pe Scales Culminating in the Protoelectric Potential Map PPM","Ingo Rainer Gerd Krossing","ALBERT-LUDWIGS-UNIVERSITAET FREIBURG","Acid-base and redox chemistry are the two most fundamental concepts in general chemistry. However, both concepts are usually limited to one medium/solvent, although they are related through the gaseous hydrogen atom delivering the proton for Brønsted acidity and the electron for redox chemistry.
Here we describe, how the ideal proton gas and the ideal electron gas form the reference states for our absolute pH Brønsted acidity scale (Angew. Chem. 2010) and the related absolute pe redox scale. Both can be combined to the Protoelectric Potential Map (PPM) that allows studying and understanding any redox or acid base reaction over solvent and even phase boundaries in dependence of the pHabs and peabs. First examples for this PPM are included with the proposal and ways how to establish anchor points for this two-dimensional PPM scale are delineated. It should be noted that this unifying concept is thermodynamically exact and makes full use of all hitherto available (experimental or calculated) pKa-values, Gibbs transfer energies and electrochemical standard potentials in either aqueous or non-aqueous solution. Even coupling to the gaseous or solid state is possible.
The research team shall include three subgroups: i) the principle problem conception and method development Methodology Group (MG), ii) the experimental validation and measurement Validation Group (VG) and last, but not least, iii) the preparative chemistry group that shall make use of the inherent potential of this unified view on Brønsted acidity and redox chemistry by preparing unusual protonated or oxidized species that present spectacular prototype problems to elucidate the frontiers of chemistry (Application Group AG).
By using this threefold approach, we envision to highlight the full potential of this new concept to those who could also make use of this approach, i.e. scientists from almost all areas of chemistry and the surrounding disciplines.","1866000","2012-04-01","2017-03-31"
"UNIQDS","Universal Framework for Charge Transport 
in Quantum Dot Systems","Jong Min Kim","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","The field of quantum dots (QDs) is one of the major growth areas in interdisciplinary field of physics, materials, chemistry, and engineering for the exploration of fundamental physical properties and potential/new functionalities. This will serve as a basis for creation of unique applications such as new display/lighting, photovoltaic device, TFTs and image sensors. However, there are serious impediments to the device performance such as high efficiency and longer life time due to the lack of understanding in charge transport and light-matter interaction mechanism in QD networks. Therefore, the proposed work is a comprehensive and fundamental understanding of underlying physics for charge transport in i) a single QD and surface, ii) QD/QD, iii) QD/interface/matrix, iv) QD/layer and /electrode, and v) bulk QD network systems and the creation of any real devices with new functionality. Enormous opportunities will arise from many unanswered questions of general nature/fundamental physical aspects of QDs related to charge transport that have still to be addressed. Thus, we will highlight and focus on strongly linked key themes and challenges that are at the heart of our proposed work. The main emphasis of proposed work will be on the understanding and control of charge transport dynamics in various QD systems, even though we explore the development of meaningful technologies and new devices based on QDs in the proposal. Our most intriguing issue is to expand the basic understanding of QDs for their potential applications. We will study interface dipole design/control, computational engineering for charge transport, analysis of the above five subsets, and will realise them into a full system with QD networks. Another challenge lies in integrating new QD materials with flexible/large-area substrates by monolayer-level control. We also propose the development of new synthetic routes for QDs with stable surface for supporting the above charge transport. This work will be underpinning research aimed at the development of the charge transport based QD devices with high efficiency and longer lifetime. These provide enormous opportunities to enable us not only to broaden and deepen our knowledge/experience in this area, but also to make rational predictions and open new device/system concepts unique to QD networks.","2454650","2013-11-01","2018-10-31"
"UNIVERSALEPTO","Test of Lepton Flavour Universality with Kaon Decays","John Bourke Dainton","THE UNIVERSITY OF LIVERPOOL","Physics at the high-mass scale can also be manifest in dynamical effects at lower, accessible energy. Thus, the existence of new high-mass-scale physics implies new mechanisms by which new interactions between constituents, quarks and gluons with leptons, must also exist. This proposal is for a precision measurement at the NA62 experiment at the European Laboratory for Particle Physics (CERN) of the ratio R(K) of the branching ratios (BR) of two rare, leptonic, decays of the K+, namely K+ to e+ neutrino and K+ to mu+ neutrino, with a precision of 0.2%. When compared with the prediction of the Standard Model (uncertainty 4E-4), the measurement will be sensitive to physics at the Terascale (TeV energy). The experiment is based on a sample of about 10E13 in-flight K+ decays, for which selection and background rejection exploit precision kinematic reconstruction and particle identification. The team will contribute a Cerenkov detector, which will time-stamp individual K +decays in the decay volume of the experiment at the 50 MHz K+ beam rate. When operating in the NA62 experiment, the sensitivity to K decay BRs will then be better than 10E-11. Such a measurement of R(K) will certainly constrain in a unique manner the nature of Terascale physics. The reality of measurements at this level of precision, which may point to lepton flavour violation, will make possible a number of definitive tests concerned with the violation of lepton flavour universality. The importance of the measurements is therefore compelling, and especially so when taken in the context of imminent measurements using long baseline neutrino beams.","2282255","2011-09-01","2016-08-31"
"URSAT","Understanding Random Systems via Algebraic Topology","Robert Joseph Adler","TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY","Over the past decade there has been a significant expansion of activity in applying the techniques and theory of algebraic topology to real world problems. The expression  `applied algebraic topology' is no longer an oxymoron! This expansion has generated new mathematical theory, new computational techniques, and even commercial startups.  However, there is still an important component of this topological approach that has not been treated in any depth, and this is the inherently stochastic nature of the world. Consequently, there is an urgent need to complement recent developments, which have been primarily deterministic, with sophisticated stochastic modelling and analysis. The current proposal aims to attack this issue   by applying algebraic topological thinking to random systems.

Over the past two years, the PI Adler and colleagues have organised workshops in Banff, Palo Alto and Chicago with tens of researchers from topology, probability, statistics, random networks, image analysis and other areas, with the aim of defining the important problems that  `random algebraic topology' should address. These brain trusts have born fruit in terms of setting some clearly defined goals, many of which help  motivate the core of the current proposal, which is by far the most ambitious of a number of earlier and current projects.

These endeavours are expected to have -- and are to a considerable part driven by -- applications to areas outside of mathematics, while at the same time having deep, intrinsic, mathematical interest. The multi-faceted aspect of the proposal, involving a number of areas within mathematics that do not usually appear together, is highly  novel and requires the setting up of a large and coordinated team of researchers. This will include the PI, graduate students and postdoctoral fellows, and short and medium term visiting scholars from a variety of disciplines","1904000","2013-03-01","2018-09-30"
"USMS","Ultra Strong Materials","Reinhard Pippan","OESTERREICHISCHE AKADEMIE DER WISSENSCHAFTEN","The theoretical strength of metals and ceramics is about 10% of their Young’s modulus. Although whiskers reach strength values close to this limit they cannot be used in the design of load bearing structures. Currently the typical strength of the structural materials in use is only in the range of few % of this theoretical limit. Premature plastic deformation and failure due to flaws are the main reasons for this distinctive lower limit. For engineering applications, adequate fracture toughness is required which permits a ductile behaviour and certain strength even in the presence of flaws or cracks. The strength of the strongest metallic materials is only 10 % of the theoretical limit. Increasing the strength of metallic high strength materials by a few percent is usually associated with an unacceptable decrease in fracture toughness and results in a very flaw sensitive strength similar to that known for ceramics. In pearlitic steel wires it was possible to overcome this 10% limitation significantly. In the last years for the first time a strength of 6.3 GPa was obtained for this material which is about 30% of the theoretical limit or 3 times stronger than other high strength steels. The group of the PI has shown that these wires have an exceptional toughness equivalent to a high damage tolerance. The proposed ERC-grant should permit the analysis of the phenomena for this superior combination of strength and ductility. The knowledge of the essential required architectural features of this nano-composite and the necessary properties of the individual phases as well as their interfaces will be used to design nano-architectures also in other materials to obtain such exceptional properties. The developed skill in the generation of nano-composites from coarse constituents will be used for the production of similar nano-composites, the proof of developed concepts, and the generation of new ultra strong materials.","2445000","2014-03-01","2019-02-28"
"VANDER","Search for New Phenomena, Materials and Applications Using Van Der Waals Assembly of Individual Atomic Planes","Andrey Geim","THE UNIVERSITY OF MANCHESTER","Many layered materials can be disassembled into isolated atomic planes, similar to graphite that splits into graphene layers. A few years ago, we demonstrated that such atomic planes – two-dimensional (2D) crystals – can be stacked on top of each other making so-called van der Waals (vdW) heterostructures. This has ignited a new field that is booming and has not disappointed in delivering interesting science. However, research efforts have so far involved only relatively simple structures that are assembled from 2 or 3 crystals and employ only a very limited number of 2D materials. 
This proposal is based on several recent experimental and technological advances by the applicant’s group. These include access to many new 2D crystals that have become available if their (dis)assembly is done in an inert atmosphere. Little remains known about these materials. We have also mastered the art of vdW assembly to make heterostructures containing dozens of different layers with control of their crystallographic alignment. Dedicated measurement techniques aimed at investigation of the resulting atomically thin structures and devices have been developed, too. 
The applicant plans to exploit these advances and push the research field forward in several new directions that include the search for 2D materials exhibiting unusual properties and novel functionalities, creation of multilayer vdW devices with designer characteristics and the assembly of artificial thin films with unique electronic spectra. Some of the proposed directions (e.g., studies of molecular transport through angstrom-scale capillaries) are nearly guaranteed to bring a large amount of new science and potential applications, whereas other directions (e.g., artificial films exhibiting enhanced superconductivity) are more adventurous. A unifying goal of all the proposed projects is to explore the myriad of exciting opportunities opened up within the field of 2D materials and vdW heterostructures.","2499580","2019-05-01","2024-04-30"
"VarCity","Variation & the City","Luc S. J. Van Gool","EIDGENOESSISCHE TECHNISCHE HOCHSCHULE ZUERICH","VarCity aims to drastically innovate 3D city modeling, moving it away from the traditional, bottom-up Lidar or structure-from-motion pipelines. Not only will large-scale city modeling be automated, but the results will look more realistic, will be more compact and will be semantically structured. Moreover, the dynamics of traffic flows will be added as animations (to safeguard anonymity) and videos of special events and landmarks can be retrieved with the 3D model as geographical context. These videos of flows and events are again automatically analysed and combined. For the combination of multiple event videos, a virtual editor will be implemented, ensuring compilations of high quality and providing an excellent overview whenever possible. The overall result will be an augmented, live Google street.
The modeling is based on three types of input data: 1) dedicated mobile mapping imagery taken from several synchronized cameras mounted on a van, 2) images of landmarks and corresponding Wikipedia pages mined from public Internet repositories, and 3) the large amount of videos people can expect to stream from their mobile phones in the near future (given e.g. the upcoming LTE). With respect to the latter VarCity thus takes a proactive stand, but such data are easy to simulate. The other data types are already available to the project, through spin-off companies.
For the modeling of the architectural structures, procedural methods will be used. Starting from the images, procedural models of the buildings are generated as instantiations of their architectural style, expressed in a style grammar. The key idea underlying much of the work is to let visual object class recognition and detection feed back into what traditionally are considered `low-level’ vision processes. This approach will enhance several aspects of the work, including the inverse procedural modeling, event detection, traffic flow analysis, inpainting, level-of-detail, super-resolution, etc.","2441893","2012-05-01","2017-04-30"
"VARIOGEO","The Geometric Calculus of Variations and its Applications","Jürgen Karl Theodor Jost","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","The project is concerned with the geometric calculus of variations and its applications in a wide range of fields. I start with fundamental examples of variational problems from geometry and physics, the Bernstein problem for minimal submanifolds of Euclidean spaces, non-abelian Hodge theory as a harmonic map approach to representations of Kähler groups, and Dirac harmonic maps as a mathematical version of the nonlinear supersymmetric sigma model of quantum field theory. These examples will motivate a general regularity and rigidity theory in  geometric analysis that will be based in a fundamental way on convexity properties. Convexity will then be linked to concepts of non-positive curvature in geometry, and it will lead me to a general theory of duality relations and convexity. That theory will encompass the formal structures of the new calculus of variations and statistical mechanics, information theory and statistics, and mathematical population genetics in biology. Also, the connection with symmetry principles as arising in high energy theoretical physics will be systematically explored. Further applications lie in material sciences, pattern recognition, and bioinformatics.
The project will thus achieve a novel integration of different disciplines from mathematics and the natural sciences.","1500000","2011-04-01","2016-03-31"
"VARIS","Variational Approach to Random Interacting Systems","Wilhelmus Theodorus Franciscus Den Hollander","UNIVERSITEIT LEIDEN","The goal of my mathematical research is to force a breakthrough in solving and understanding a number of long-standing open problems that are rooted in physics and chemistry. My objects of study are systems consisting of a large number of random components that interact locally but exhibit a global dependence. Typically, the components of these systems are subject to a simple microscopic dynamics. The challenge lies in understanding the complex macroscopic phenomena that may arise from this dynamics. Core to my proposal are macroscopic phenomena that are very hard to grasp with heuristic or numerical methods: pinning, localisation, collapse, porosity, nature vs. nurture, metastability, condensation, ageing, catalysis, intermittency and trapping. My main line of attack is to combine large deviation theory, which is a well-established technically demanding yet flexible instrument, with a number of new variational techniques that I have recently developed with my international collaborators, which are based on space-time coarse-graining. My goal is to apply this powerful combination to a number of complex systems that are at the very heart of the research area, in order to arrive at a complete mathematical description. The idea is to use the coarse-graining techniques to compute the probability of the possible trajectories of the microscopic dynamics, and to identify the most likely trajectory by maximising this probability in terms of a variational formula. The solution of this variational formual is what describes the macroscopic behaviour of the system, including the emergence of phase transitions. My proposal focuses on five highly intriguing classes of random interacting systems that are among the most challenging to date: (1) polymer chains; (2) porous domains; (3) flipping magnetic spins; (4) lattice gases; (5) evolving random media. The unique reward of the variational approach is that it leads to a full insight into why these systems behave the way they do.","1930000","2011-05-01","2016-04-30"
"VARMET","Variational Metadynamics","Michele Parrinello","UNIVERSITA DELLA SVIZZERA ITALIANA","We propose to extend the time scale accessible to atomistic-based simulation methods from the current   range to millisecond and beyond without special-purpose machines.  We shall do this by combining and extending two recent developments: a recent reformulation of the enhanced sampling problem into a powerful variational principle that opens a wealth of possibilities and provides a novel and fruitful standpoint for new developments; and a procedure for extracting rates from enhanced runs.  We shall apply the methods thus developed to two major problems of great practical interest. One is the lifetime of a ligand-protein bound state.  This quantity is not easily accessible experimentally and yet it is crucial in drug design to determine the potency of a drug.  We plan to develop a viable and widely applicable way to compute it.  The other is a study of crystallization from solution, for which we wish to determine the nucleation mechanism and nucleation rates and understand and control crystal growth.  These are all issues of great relevance in engineering, pharmacology and nanotechnology.  Besides being relevant on their own merits, these two applications present different challenges to the enhanced methods.  We also believe that new methods should not be developed in an abstract way but in close interaction with real-life applications.","2488827","2016-01-01","2020-12-31"
"VERDI","polyValent mEsopoRous nanosystem for bone DIseases","Maria VALLET-REGI","UNIVERSIDAD COMPLUTENSE DE MADRID","Finding simple solutions to complex problems has been a challenge for humankind for decades. VERDI aims at designing a multifunctional nanosystem to heal complex bone diseases. This is an engineering challenge that will be tackled through the use of building blocks designed on the basis of cutting-edge technology. These building blocks will be assembled into a versatile multifunctional nanosystem that can be adapted through slight variations for the treatment of three diseases of clinical relevance: bone infection, bone cancer and osteoporosis. The novelty of this proposal is the design of a nanosystem that may address several diseases using a unique, versatile and scalable strategy. Mesoporous silica nanoparticles are selected as the main component of the nanoplatform because of their biocompatibility, robustness, loading capacity and versatile surface modification. The nanosystem will be modified by rational selection of building blocks, with targeting and/or therapeutic abilities, to tackle either one or a combination of pathologies. These features will enable us to deliver a library of nanomedicines using a toolbox of building blocks, customizing a specific nanosystem depending on the disease to be treated. The risks associated to VERDI are numerous, such as the great complexity of producing completely asymmetrical nanoparticles (NPs), the risk that modifying a drug or therapeutic peptide will affect its therapeutic efficacy, and the difficulty of achieving effective in vivo bone targeted NPs. A contingency plan for each risk has been elaborated. The expertise and capacities of my research group guarantees successful results, which we expect to lead to a revolution in the therapy of bone cancer, bone infection and osteoporosis. Additionally, the application of a single technology for the treatment of three different but frequently associated diseases will favour industrial scale-up process, thereby promoting the transition of nanomedicine from bench to bedside.","2500000","2016-10-01","2021-09-30"
"VERIWARE","From Software Verification to Everyware Verification","Marta Zofia Kwiatkowska","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","In the words of Adam Greenfield, the age of ubiquitous computing is here: a computing without computers, where information processing has diffused into everyday life, and virtually disappeared from view . Conventional hardware and software has evolved into everyware sensor-enabled electronic devices, virtually invisible and wirelessly connected on which we increasingly often rely for everyday activities and access to services such as banking and healthcare. The key component of everyware is embedded software, continuously interacting with its environment by means of sensors and actuators. Ubiquitous computing must deal with the challenges posed by the complex scenario of communities of everyware , in presence of environmental uncertainty and resource limitations, while at the same time aiming to meet high-level expectations of autonomous operation, predictability and robustness. This calls for the use of quantitative measures, stochastic modelling, discrete and continuous dynamics and goal-driven approaches, which the emerging quantitative software verification is unable to address at present. The central premise of the proposal is that there is a need for a paradigm shift in verification to enable everyware verification, which can be achieved through a model-based approach that admits discrete and continuous dynamics, the replacement of offline methods with online techniques such as machine learning, and the use of game-theoretic and planning techniques. The project will significantly advance quantitative probabilistic verification in new and previously unexplored directions. I will lead a team of researchers investigating the fundamental principles of everyware verification, development of algorithms and prototype implementations, and experimenting with case studies. I will also provide continued scientific leadership in the area of ubiquitous computing.","2060360","2010-05-01","2016-04-30"
"VESCEL","Vascular Engineering on chip using differentiated Stem Cells","Albert van den Berg","UNIVERSITEIT TWENTE","Organs-on-chip hold great promise for the creation of complex and realistic disease models while having the potential to refine, reduce and (partly) replace existing animal models (3R principle). Of all organs, vasculature is extremely well-suited to realize on-chip since it pervades the whole organism, is present in all other organs, its malfunctioning plays a role in many diseases and finally is ideally suited to approach with microfabrication and microfluidic technologies. In the VESCEL program we propose the development of innovative technologies enabling the use of differentiated human induced pluripotent stem cells (hiPSC) to engineer blood vessels on chip that constitute realistic disease models for thrombosis and neurodegenerative (ND) diseases. The use of differentiated hiPSC allows the realization of blood vessels based upon patient-specific material, without the need for biopsies, while development of integrated microsensors for small molecules (pH, O2, NO) offers the possibility of on-line monitoring. To optimize the hiPSC differentiation conditions we propose the use of a microdroplet platform, that combines high-throughput capability (up to 1000 cells/s) with control of single cell microenvironment. We will also develop a new flexible technology for real 3D vasculature realization using advanced 3D printing technologies. These four innovative technology developments will be integrated in two biomedical applications to study two important classes of diseases, thrombosis and neurodegenerative (ND) diseases. For thrombosis we focus on the study of parameters such as blood pressure and stenosis as well as effects of drugs on thrombus formation, while for ND diseases study we will use a blood brain barrier (BBB) model to investigate nanoparticle and peptide transport across the BBB for a form of Alzheimer’s disease, as well as leukocyte extravasation for multiple sclerosis (MS).","2250000","2015-10-01","2020-09-30"
"VHIA","Vision and Hearing in Action","Patrice Horaud","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","The objective of VHIA is to elaborate a holistic computational paradigm of perception and of perception-action loops. We plan to develop a completely novel twofold approach: (i) learn from mappings between auditory/visual inputs and structured outputs, and from sensorimotor contingencies, and (ii) execute perception-action interaction cycles in the real world with a humanoid robot. VHIA will achieve a unique fine coupling between methodological findings and proof-of-concept implementations using the  consumer humanoid NAO manufactured in Europe. The proposed multimodal approach is in strong contrast with current computational paradigms influenced by unimodal biological theories. These theories have hypothesized a modular view, postulating quasi-independent and parallel perceptual pathways in the brain. VHIA will also take a radically different view than today's audiovisual fusion models that rely on clean-speech signals and on accurate frontal-images of faces; These models assume that  videos and sounds are recorded with hand-held or head-mounted sensors, and hence there is a human in the loop who intentionally supervises perception and interaction. Our approach deeply contradicts the belief that complex and expensive humanoids (often manufactured in Japan) are required to implement research ideas. VHIA's methodological program addresses extremely difficult issues: how to build a joint audiovisual space from heterogeneous, noisy,  ambiguous and physically different visual and auditory stimuli,  how to model seamless interaction, how to deal with high-dimensional input data, and how to achieve robust and efficient human-humanoid communication tasks through a well-thought tradeoff between offline training and online execution. VHIA bets on the high-risk idea that in the next decades, social robots will have a considerable economical impact, and there will be millions of humanoids, in our homes, schools and offices, which will  be able to naturally communicate with us.","2497296","2014-02-01","2019-01-31"
"VIBRATE","Self-sustaining vibration and mechanical resonance effects in stimuli responsive liquid crystal polymer coatings and membranes (Vibrate)","Dirk Jan BROER","TECHNISCHE UNIVERSITEIT EINDHOVEN","The amplification of collective molecular effects to macroscopic deformation is one of the most intriguing challenges  in materials science. Individual molecules can be reversibly activated by various means such as light, chemicals and external electrical or magnetic  fields. They may respond  by changing their orientation, their molecular conformation or by breaking /forming chemical bonds. Bringing these molecules in a matrix of an ordered molecular system, such as a liquid crystal polymer network, introduces molecular cooperativity and directionality. Dimensional changes of the individual molecules add up to large effects such as the formation of deformed surfaces in a coating and the formation of free volume (molecular voids) in membranes. The objective of the present proposal is to bring the molecular action, in conjunction with the macroscopic deformation, out of its equilibrium aiming a self-sustaining oscillation of the macroscopic response to a continuous trigger.  An example that will be investigated is a surface that continuously changes it topography when addressed by its trigger. Another example is a membrane that will oscillate its (localized) free volume thus providing an active transport mechanism for species through the membrane. Alternatively we will investigate the response to an oscillating trigger with a frequency matching the molecular action to find sweet spots for mechanical resonance thus enhancing the macroscopic effect. The research is challenging. Only, with a comprehensive and combined effort, we can expect the required progress needed to close the gap between materials science, optics, electronics and mechanics and to deliver routes to new applications. But when achieved it will undoubtfully lead to new applications in coating and film technology with an outlook to soft robotics, self-pumping membranes, mechanical communication at man-machine interfaces and energy harvesting.","2429323","2015-10-01","2020-09-30"
"VIDEO HOLOGRAPHY","video-rate holographic projection by novel meta-materials","Jan Genoe","INTERUNIVERSITAIR MICRO-ELECTRONICA CENTRUM","Today, despite many efforts by researchers world-wide, there are no holographic projectors that allow video-rate electronically controlled projection of complex holograms. Optically re-write-able holograms exist, but they are too slow; Acoustically-formed holograms can be switched fast but the image complexity is very limited.   We identify the essential roadblock as one that we intend to clear by a breakthrough innovation coming from a combination of electronics, optics and material science. 

We propose a radically novel way to make and control holograms, that will be based on the direct, analog, nanometer-resolution and nanosecond-speed control over the local refractive index of a slab waveguide core over several square centimetres. Holograms will be formed by leaky waves evanescent from the waveguide, and controlled by the refractive-index modulation profile in the core. That profile will be controlled and modulated by electrical fields applied with nano-precision through one of the cladding layers of the waveguide. To that end, a novel metamaterial is proposed for this cladding. Also novel driving schemes will be needed to control the new holographic projecting method.

With this combined radical innovation in architecture, materials and driving schemes, it is the goal of this project to fully prove the concept of video-rate electrically-controlled holographic projection. This will be the basis for many future innovations and applications, in domains such as augmented reality, automotive, optical metrology (LIDAR, microscopy, ...), mobile communication, education, safety, etc..., and result in a high economic and social impact.","2499074","2017-10-01","2022-09-30"
"VIDEOWORLD","Modeling, interpreting and manipulating digital video","Jean Ponce","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","Digital video is everywhere, at home, at work, and on the Internet. Yet, effective technology for
organizing, retrieving, improving, and editing its content is nowhere to be found. Models for video content, interpretation and manipulation inherited from still imagery are obsolete, and new ones must be invented. With a new convergence between computer vision, machine learning, and signal processing, the time is right for such an endeavor. Concretely, we will develop novel spatio-temporal models of video content learned from training data and capturing both the local
appearance and nonrigid motion of the elements---persons and their surroundings---that make up a dynamic scene. We will also develop formal models of the video interpretation process that leave behind the architectures inherited from the world of still images to capture the complex interactions between these elements, yet can be learned effectively despite the sparse annotations typical of video understanding scenarios. Finally, we will propose a unified model for
video restoration and editing that builds on recent advances in sparse coding and dictionary learning, and will allow for unprecedented control of the video stream. This project addresses fundamental research issues, but its results are expected to serve as a basis for groundbreaking technological advances for applications as varied as film post-production, video archival, and smart camera phones.","2454090","2011-01-01","2016-12-31"
"VIN","Video-rate Scanning Probe Microscopy Imaging  of Nanostructures on Surfaces","Flemming Besenbacher","AARHUS UNIVERSITET","The goal of this ERC proposal VIN is to develop the next generation of scanning probe microscopes (SPMs) The microscopes will set new standards in the field through their ability to acquire images at video-rate frequency, while retaining high (atomic) resolution capability. This new instrumental platform will be implemented both under ultra-high vacuum conditions, in a high-pressure gas cell, and under liquid-phase conditions. It will be utilized to create and explore novel research avenues for the study of physical, chemical, and biological surface processes at the single-atom/molecule level with the highest possible spatial and temporal resolution. In particular I will study dynamic phenomena in surface nanostructures, focusing on three mutually synergetic and interdisciplinary priority areas: i) Catalytic reactivity of nanostructures, ii) Self-organisation of organic molecules at surfaces, iii) Biomolecular structures, processes and interactions under physiological conditions. The adsorption, diffusion and interaction of molecules are the basic steps involved in reactions at surfaces. All of them are dynamic processes, where high temporal resolution can provide new groundbreaking insight into e.g. the mechanisms underlying catalysis. Video-rate SPMs will also facilitate investigations of the kinetic aspects of molecular self- organisation at surfaces such as diffusion, intra-molecular conformational dynamics, nucleation and growth of structures. The effort will build upon the world-leading expertise in design, construction and use of SPMs in my research group at the Interdisciplinary Nanoscience Center (iNANO) and the Department of Physics and Astronomy, University of Aarhus, Denmark. To achieve the ambitious research goals, I will bring together an interdisciplinary team of highly talented younger scientists.","1324983","2008-12-01","2013-11-30"
"VIRMETAL","Virtual Design, Virtual Processing and Virtual Testing of Metallic Materials","Francisco Javier Llorca Martinez","FUNDACION IMDEA MATERIALES","The project VIRMETAL is aimed at developing multiscale modeling strategies to carry out virtual design, virtual processing and virtual testing of advanced metallic alloys for engineering applications so new materials can be designed, tested and optimized before they are actually manufactured in the laboratory. The focus of the project is on materials engineering i.e. understanding how the structure of the materials develops during processing (virtual processing), the relationship between this structure and the properties (virtual testing) and how to select materials for a given application (virtual design). 

Multiscale modeling will be tackled using a bottom-up, hierarchical, modeling approach. Modeling efforts will begin with ab initio simulations and bridging of the length and time scales will be accomplished through different multiscale strategies which will encompass the whole range of length and time scales required by virtual design, virtual processing and virtual testing. Nevertheless, not everything can or should be computed and critical experiments are an integral part of the research program for the calibration and validation of the multiscale strategies. 

The research will be focused on two cast metallic alloys from the Al-Si-Mg and Mg-Al-Zn systems. The expected breakthrough is precisely to demonstrate that the structure and properties of two standard engineering alloys of considerable industrial interest can be obtained from first principles by bridging a cascade of modeling tools at the different length scales. Once this is proven, further research will lead to the continuous expansion of both the number and the capability of multiscale simulation tools, leading to widespread application of Computational Materials Engineering in academia and industry. This will foster the implementation of this new revolutionary technology in leading European industries from aerospace, automotive, rail transport, energy generation and engineering sectors.","2466250","2015-11-01","2020-10-31"
"VirtualSeis","Virtual Seismology: monitoring the Earth's subsurface with underground virtual earthquakes and virtual seismometers","cornelis WAPENAAR","TECHNISCHE UNIVERSITEIT DELFT","If it were possible to place seismometers and seismic vibrators anywhere below the ground in, for example, an induced-earthquake-sensitive area, we could measure the source mechanism of actual earthquakes, monitor the geomechanical state of the area over time, and quantify the ground motion caused by possible future earthquakes. Moreover, we could monitor fluid flow in aquifers, geothermal reservoirs or CO2 storage reservoirs, with unprecedented resolution. Unfortunately, placing seismic instruments anywhere below the ground is not practically feasible. 

I propose to develop groundbreaking methodology for creating virtual seismic sources (earthquakes or seismic vibrators) and virtual seismometers anywhere in the subsurface, from seismic reflection measurements carried out at the surface of the earth. I call this Virtual Seismology (VS). VS accurately mimics the responses to actual earthquakes that would be recorded by actual buried seismometers, including all multiple scattering effects.

In particular I will develop VS for:
(1) Investigating induced-earthquake problems. (a) I will develop high-density multi-component seismic acquisition methodology, using the latest technology of controllable seismic vibrators and seismic sensing with fibre-optic cables, and apply it in an actual induced-earthquake sensitive area. (b) I will use these data to create virtual sources and receivers in the subsurface to characterize induced earthquakes, quantify the ground motion of actual and possible future earthquakes, and monitor the geomechanical state of the area over time.
(2) Imaging and monitoring subsurface fluid flow. I will develop highly repeatable VS methodology for time-lapse 3D reflection data to monitor fluid-flow processes in the subsurface with excellent spatial and temporal resolution.

With my track record in pioneering seismic interferometry, I am in an excellent position to develop VS, which will have major impact on the field of seismic imaging and monitoring.","2499474","2017-09-01","2022-08-31"
"VISCHEM","Visualizing Molecular Change","Villy Sundström","LUNDS UNIVERSITET","What knowledge is needed to characterize function of a biomolecule, molecular material or a chemical reaction mechanism? Structure has long been considered to be the key information and several powerful techniques like X-ray crystallography and multidimensional NMR have been developed, following the notion that  seeing is believing,  to provide equilibrium (static) structures of molecules. At the same time function of a molecular system implies change of structure and in order to characterize and understand how a biomolecule or material works it is necessary to know how and why the structural changes occur. The How is related to finding out precisely which structural changes occur, which atoms are involved, how are they affected, which bonds are broken, how does energy and charge flow through the molecule, and what are the temporal characteristics of the changes. The Why is associated to energetics and interactions   what is the energy landscape that connects reactants, intermediates and products and how do molecules interact with each other and with their environment? Structural and dynamical information is today generally obtained in separate experiments   static structures from X-ray crystallography, cryo-electron microscopy and multidimensional NMR, while molecular time scale dynamics are obtained from e.g. ultrafast laser spectroscopy. Dynamics from such experiments does not provide direct information on structural changes, but has to be inferred from often sophisticated analysis of spectroscopic data. Thus, there is a great need for experiments that can provide direct information on structural changes occurring on the molecular time scale of picoseconds and femtoseconds. The goal of this project is to develop a table-top experimental setup for sub-ps X-ray spectroscopy to obtain molecular time scale geometrical and electronic structural information of chemical and biological processes for deeper insights into molecular reaction mechanisms.","2056000","2008-12-01","2013-11-30"
"VISIONSPACE","Visionary Space Systems: Orbital Dynamics at Extremes of Spacecraft Length-Scale","Colin Robert Mcinnes","UNIVERSITY OF STRATHCLYDE","This ground-breaking project will deliver radically new approaches to orbital dynamics at extremes of spacecraft length-scale to underpin new space-derived products and services for Europe. These include levitated geostationary orbits for large gossamer spacecraft to massively increase space telecommunications capacity, new displaced polar orbits for continuous environmental monitoring of the arctic and swarms of interacting micro-spacecraft for revolutionary new commercial and science applications. To pursue these ambitious goals, resources of order ¬2M are requested for 3 postdoctoral research assistants for 5 years to establish a European Research Council funded Advanced Space Concepts Laboratory. The University of Strathclyde will provide resources for a further 3 PhD students (¬250k) and a custom suite of offices to host the laboratory. Through extensive European and international links, the work of the laboratory will build on the demonstrated capability of the Principal Investigator to devise and effectively transfer radical new space concepts from academia through to industry and to influence policy-making at the highest levels. Dissemination will be through an international network of collaborators (e.g. ESA, NASA, Tsinghua University) and a public outreach project to stimulate debate on visionary, large-scale space engineering ventures. The project will underpin revolutionary new space technologies and applications and will champion the role of European space engineering as a venture at the forefront of technological and social advancement.","2012038","2009-02-01","2014-09-30"
"VISREC","Visual Recognition","Andrew Zisserman","THE CHANCELLOR, MASTERS AND SCHOLARS OF THE UNIVERSITY OF OXFORD","Our goal is to develop the fundamental knowledge to design a visual system that is able to learn, recognize and retrieve quickly and accurately thousands of visual categories, including materials, objects, scenes, human actions and activities. A ``visual google'' for images and videos -- able to search for the ``nouns'' (objects, scenes), ``verbs'' (actions/activities) and adjectives (materials, patterns) of visual content.  The time is right for making great progress in automated visual recognition: imaging geometry is well understood, image features are now highly developed, and relevant statistical models and machine learning algorithms are well-advanced.  Our goal is to make a quantum leap in the capabilities of visual recognition in real-life scenarios.  The outcomes of this research will impact any applications where visual recognition is useful, and will enable new applications entirely: effortlessly searching and annotating home image and video collections on their visual content; searching and annotating large commercial image and video archives (e.g. YouTube); surveillance; using an image, rather than text, to access the web and hence identify its visual content.","1872056","2009-01-01","2014-12-31"
"VOLDIES","Dynamics of volcanoes and their impact on the environment and society","Robert Stephen John Sparks","UNIVERSITY OF BRISTOL","Active volcanoes threaten 500 million people and vulnerability is increasing due to population growth, globalisation and increasing environmental stresses. The project will investigate key topics that will provide the understanding to increase societal risk and reduce vulnerability of communities. The project will: investigate how volcanoes work focussing on the nature and dynamics of magma chambers; construct a global database on volcanic hazards and risk; and develop new approaches to assessment of volcanic risk. The magma chamber is the fundamental control on the behaviour of most volcanoes, and so an advance in understanding of their physical nature and behaviour affects almost every other aspect of volcano behaviour and phenomena. Integrated models of the formation and behaviour of magma chambers will be take account of heat transfer, crustal deformation, magma properties, and internal chamber processes. Volcano behaviour will be investigated in terms of magma flows from chambers to the Earth s surface. The models will be informed by and tested against geophysical, geochemical and observational data at selected volcanoes and igneous intrusions characterised by superb datasets. An integrated model of magma chambers will improve interpretations of geophysical data and understanding of hazardous volcanic phenomena, such as debris avalanches, pyroclastic flows and lahars. The project will create a global database on volcanic eruptions, their hazards and key risk factors (such as population density), which will be analysed to provide robust data for hazard and risk assessment at global, regional and local scales. New methods of probabilistic risk assessment will be developed, which combine hard and soft data, take account of uncertainties and integrate information on vulnerability and hazard. The research will include a study of risk perception as a key factor in vulnerability by comparing communities in different volcanic settings and across cultures.","2488957","2009-01-01","2014-08-31"
"VSSC","Verifying and Synthesizing Software Compositions","Sagiv","TEL AVIV UNIVERSITY","One of the first things a programmer must commit to in developing any significant piece of software is the representation of the data. In applications where performance or memory consumption is important, this representation is often quite complex: the data may be indexed in multiple ways and use a variety of concrete, interlinked data structures. The current situation, in which programmers either directly write these data structures themselves or use a standard data structure library, leads to two problems:
1:The particular choice of data representation is based on an expectation of what the most common workloads will be; that is, the programmer has already made cost-benefit trade-offs based on the expected distribution of operations the program will perform on these data structures.
2: It is difficult for the programmer to check or even express the high-level consistency properties of complex structures, especially when these structures are shared. This also makes software verification in existing programming languages very hard.
We will investigate specification languages for describing and reasoning program data at a much higher level. The hope is that this can reduce the inherited complexity of reasoning about programs. In tandem, we will check if the high level specifications can be semi-automatically mapped specifications to efficient data representations.
A novel aspect of our approach allows the user to define global invariants and a restricted set of high level operations, and only then to synthesize a representation that both adheres to the invariants and is highly specialized to exactly the set of operations the user requires. In contrast, the classical approach in databases is to assume nothing about the queries that must be answered; the representation must support all possible operations.","1577200","2013-04-01","2018-03-31"
"WACSWAIN","WArm Climate Stability of the West Antarctic ice sheet in the last INterglacial (WACSWAIN)","Eric WOLFF","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","Recent papers predict the loss of most of the West Antarctic Ice Sheet (WAIS) by 2500 if CO2 emissions and rising global temperatures are not controlled. It is critical to test whether the models making such worrying predictions are realistic. I will do this by obtaining new data from the last interglacial (LIG, 130,000-115,000 years ago) to assess the response of the WAIS to comparable warmth. 

During the LIG, sea level reached 6-9 m higher than today. It is inferred that Antarctic ice sheets contributed several metres of sea level rise, under a climate similar to the one we could be committing ourselves to in the next few centuries.  Most authors assume that the lost ice came mainly from the WAIS.  Models that predict large ice loss in the future also produce a very significant retreat of the WAIS and loss of the Ross and Ronne ice shelves under LIG conditions.

Were the WAIS and Ronne Ice Shelf significantly smaller in the LIG? If so, what was the time course of their retreat and regrowth?  This project will remedy the surprising lack of direct evidence about these questions. I will examine data from ice cores that reach the LIG, drilled on the periphery of the WAIS. I will include retrieval of one new strategically-placed bedrock core, and obtain an isotope profile that will test the potential of another site.  The loss of much of the WAIS would have several effects on peripheral ice caps: glacio-isostatic (GI) uplift and a change in atmospheric circulation would cause a recognisable spatial and temporal pattern of symptoms.  The retreat of the Ronne Ice Shelf would leave a clear signature in marine aerosol concentrations in the ice. By examining changes in water isotopes, sea salt, air content and other proxies in all the cores, in comparison with different model outputs, I will estimate the timing and extent of WAIS retreat and regrowth during the LIG. This will support or question the use of sensitive models to predict future change in the WAIS.","2817554","2017-08-01","2022-07-31"
"WALKINGMOLS","Synthetic Molecules that Walk Down Tracks: The First Small-Molecule Linear Motors","David Alan Leigh","THE UNIVERSITY OF MANCHESTER","The goal of this research project is to make the first synthetic small-molecule structures that can walk down tracks, mimicking the types of movement exhibited by the biological motor proteins myosin, kinesin and dynein. We propose to construct the first synthetic, mechanically processive, chemical systems from first principles; i.e. to design, synthesize, operate and characterize wholly synthetic small molecule structures that progressively advance directionally along a molecular  track  in response to stimuli. Different principles ( passing leg  and  inchworm  mechanisms) for processive mechanical molecular-level motion will be developed and experimentally explored. With some designs it is envisaged that the  walker  units will be able to change direction or switch between pathways as a result of external signaling or the nature of the environment and, ultimately, be able to transport a cargo from one place to another on a surface.  Sequential processive movement is unprecedented for wholly synthetic molecular structures and is the key requirement for making translational/linear motors that can perform tasks (transport cargoes from place to place or progressively exert a force) at the molecular level. Its successful demonstration would be a landmark accomplishment and mark a major new direction for synthetic supramolecular chemistry and molecular nanotechnology.","2256401","2009-01-01","2013-12-31"
"WATER","Probing the Structure and Dynamics of Water in its Various States","Anders Nilsson","STOCKHOLMS UNIVERSITET","We propose to address some of the most important outstanding questions for a microscopic understanding of water: What is the structure and dynamics of the hydrogen-bonding network that give rise to all the unique properties of water? How is the structure and dynamics affected by temperature, pressure and by perturbation through interaction with solutes and interfaces? Here we point to the opportunity to exploit the completely new avenues that the novel x-ray free-electron lasers open up for probing both structure and dynamics of water from hot temperatures down to the deeply supercooled regime where the anomalous properties become extreme. We plan to further develop fast cooling and ultrafast x-ray probing allowing access to below the homogeneous ice nucleation limit, to probe equilibrium dynamics through probe-probe techniques based on x-ray correlation spectroscopy, to access low-energy vibrational mode dynamics through THz pump and x-ray scattering probe and to transfer x-ray spectroscopies into the time domain.
We will address one of the currently most debated issues related to a potential liquid-liquid transition and 2nd critical point in liquid water. The goal is to determine experimentally if water, as hypothesized in certain models, can really exist as two liquids, if there is reversible phase transition between the hypothesized liquids, evaluate if these hypothesized liquids can equilibrate on a time scale faster then the rate of ice nucleation and if there exists a critical point that can explain the fluctuations related to the diverging response functions. We will continue to critically investigate our proposed hypothesis that water at ambient temperature encompasses fluctuations around two local structures and that the dominating structure is a strongly distorted hydrogen bonded environment. We will investigate if these concepts can be used to describe the observed perturbations of water structure by solutes and interfaces.","2486951","2015-09-01","2020-08-31"
"WATERUNDERTHEICE","Where is the water under the Greenland ice sheet?","Dorthe Dahl-Jensen","KOBENHAVNS UNIVERSITET","Recent analysis of radar-depth sounder data has shown that many areas of the Greenland ice sheet have melt water under the base. The extent of the wet base and distribution of melt water are poorly known. Also lakes under the ice have not been discovered in contrast with those in Antarctica. The effect of the water beneath the ice, however, is well documented: it lubricates the bed and removes the friction between the basal ice and underlying bedrock. The ice with a wet bed flows faster, reacts rapidly to changes in climate and the basal-melt water contributes to the fresh-water supply to the ocean from the Greenland ice sheet. The primary objectives of the project are to map melt water extent of the Greenland ice sheet and its impact by tracing internal layers and analyzing bedrock returns from airborne radio-echo sounding data, and use mapping results in conjunction with ice-sheet and hydrostatic models for the movement of the basal water to predict the ice-sheet s response to climate change. The information derived from deep ice-cores that reach the bed will be used to constrain models. We will also study the basal material (dust, DNA and microbiological material) and bedrock properties from the deep-ice core sites. This will add a further dimension to the study and provide opportunities to look for life under the ice and constrain the age of the Greenland ice sheet. The proposed research is a high risk project because of the difficulty in accessing basal conditions under 3-km of ice with a potential for high payoff science. The team will consist of scientists and engineers with expertise in the palaeoclimate, radar sounding and signal processing, and ice-sheet models.","2499999","2010-01-01","2015-12-31"
"WAVETOMO","Imaging earth's internal structure using full waveform tomography","Barbara Romanowicz","INSTITUT DE PHYSIQUE DU GLOBE DE PARIS","Since January 2011, the PI holds a faculty position at the Collège de France, this proposal will facilitate transferring and re-establishing her research program at IPG in Paris. The goal of the proposed research program is to investigate earth’s deep structure and dynamics using advanced seismological forward and inverse modeling techniques. The primary focus is on global and continental scale mantle structure, with a secondary focus on the earth’s core. The primary objective is to develop high resolution three-dimensional models of the present day thermal and compositional structure of the mantle through the development of forward and inverse seismic waveform modeling approaches. This will be pursued along two directions that will eventually be combined: (a) Using a spectral-element-based seismic waveform modeling approach, develop high resolution seismic models of 3D elastic, isotropic and anisotropic , and anelastic structure of the earth’s mantle, with particular emphasis at the global scale on the lower mantle and, at the tectonic plate scale, on lithosphere-asthenosphere structure; (b) Develop an approach to invert full seismic waveforms, combined with other seismic constraints (such as travel times and normal mode eigenfrequencies) directly for 3D thermal and compositional structure of the mantle, using the best available constraints from mineral physics and geodynamics. A secondary objective is to constrain inner core structure and anisotropy using a combination of free oscillation splitting measurements and travel times and amplitudes of inner core sensitive body waves, with the goal of better characterizing the mantle versus inner core origin of observed anomalies currently attributed to inner core anisotropy.","2499198","2011-06-01","2017-05-31"
"WDTracer","White Dwarfs as Tracers of Stellar, Binary and Planetary Evolution","Boris Teddy Gaensicke","THE UNIVERSITY OF WARWICK","The vast majority of all stars in the Galaxy will evolve, or already have evolved, into white dwarfs, Earth-sized stellar embers sustained by electron degeneracy, and the study of white dwarfs is essential to many aspects of our understanding regarding the evolution of single and binary stars and of planetary systems. However, because of their intrinsic faintness, the known population of galactic white dwarfs is extremely fragmentary, in fact, we do not even have a complete census of white dwarfs within the nearest 20pc. The aim of this project is to produce the first deep, homogeneous all-sky sample of single and binary white dwarfs. I will achieve this ambitious goal by using the combined wealth of information that ongoing deep surveys, spanning the ultraviolet to mid-infrared, collect for about a billion objects in our Galaxy. My analysis and follow-up study of this new sample will advance our understanding of all aspects of the evolution of white dwarfs by at least an order of magnitude, and will produce answers to a wide range of fundamental questions:

* How do white dwarf binaries evolve?
* What is the population of Type Ia supernova progenitors?
* How many white dwarfs bear the remnants of planetary systems?
* What is the chemical composition of extra-solar minor planets?
* What is the star formation history of the Galaxy?
* How much mass do stars lose during their post-main sequence evolution?","2285977","2013-04-01","2018-03-31"
"WEAR3D","Wearable Augmented Reality 3D Displays","Hakan Urey","KOC UNIVERSITY","Wearable displays have advanced rapidly over the past few decades but they are limited in field-of-view due to optical constraints. Likewise, 3D displays have several technological and viewing discomfort limitations. These limitations result from the missing 3D depth cues in stereoscopic displays, which are essential for real 3D and for interactive augmented reality (AR) applications. Wear3D proposal aims to overcome the two fundamental scientific challenges of wearable displays and make them as natural as wearing a pair of eyeglasses: (i) Eliminate the relay lenses. We need to overcome the focusing problem of the eyes in order to completely eliminate the large relay lenses. As a result, miniaturization of wearable displays will be possible by taking full advantage of the advancements in micro-technologies; (ii) Provide all the essential 3D depth cues to avoid perceptual errors and viewing discomfort. We need to enable the two eyes to fixate at the correct depth of the objects rather than the display panel without losing resolution. Thereby, eliminating the conflict between the accommodation and convergence. Overcoming these challenges would enable a display which can provide natural looking and interactive 3D and very wide field-of-view (>100deg) in an eyeglasses form factor. Such a display goes far beyond the state-of-the art in wearable displays and open new research directions for intelligent human-computer interfaces and AR.","2496525","2014-01-01","2018-12-31"
"WEBDAM","Foundations of Web Data Management","Serge Abiteboul","INSTITUT NATIONAL DE RECHERCHE ENINFORMATIQUE ET AUTOMATIQUE","We propose to develop a formal model for Web data management. This model will open  new horizons for the development of the Web in a well-principled way, enhancing its functionality, performance, and reliability. Specifically, the goal is to develop a universally accepted formal framework for describing complex and flexible interacting Web applications featuring notably data exchange, sharing, integration, querying and updating. We also propose to develop formal foundations that will enable peers to concurrently reason about global data management activities and cooperate in solving specific tasks and support services with desired quality of service.  Although the proposal addresses fundamental issues, its goal is to serve as the basis for ground-breaking future software development for Web data management.","2415620","2008-12-01","2013-11-30"
"WHISPER","Towards continuous monitoring of the continuously changing Earth","Michel Campillo","UNIVERSITE JOSEPH FOURIER GRENOBLE 1","This project is focused on the use of the seismic ambient noise to monitor slight changes of properties in the solid Earth. Processing of noise records allow s to mimic a situation in which a perfectly repeatable source is activated at the location of a passive recorder. The implication is the detection of changes of strain at depth with applications in different contexts.  A major field of application is the monitoring of potentially dangerous structures like volcanoes or active fault zones prone to damaging earthquakes.  The project includes new methodological developments and field experiments.  Applications in regions where changes are induced by human activity are important both for the quantitative refinement of the method and for the important economic and social implications of these problems.","1700736","2009-07-01","2015-06-30"
"WIHM","Water Isotopes of Hydrated Minerals (WIHM)","David Hodell","THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE","""Water is fundamental to life as we know it, and the current strategy for the search of life on Mars is to “follow the water”. Much of the water on Mars exists today in the form of hydrated minerals that incorporate molecular water or hydroxyl into their crystalline structure. Hydrated minerals provide a tool for studying the past history of the hydrosphere on Mars as well as Earth. Oxygen and hydrogen isotopes of hydration water in minerals record information about the conditions under which the minerals form and subsequently interact with fluids after deposition. This proposal outlines the technological and scientific basis for a new approach to the isotopic study of hydration water in minerals and mineraloids. The recent availability of  tunable diode lasers now permits the measurement of water isotopes by laser absorption spectroscopy at a precision equal to or better than conventional methods. This development opens new opportunities for studying the isotope composition of mineral hydration water in novel ways, which hitherto has been difficult or impossible. I propose a five-year program to establish a research group at the University of Cambridge to study the nature of water isotopes in hydrated minerals: (1) how many different types of hydration states of water are present and how they are fixed into the mineral structure; (2) how the isotopic composition varies among the differently bound water in minerals; (3) how the type of bonding of water in minerals affects the rate of isotopic exchange with external water; and (4) whether or not any of the hydration water has retained its isotopic composition from the time of formation. The project will include both a theoretical (ab initio simulations) and empirical approach that has the potential to transform our understanding of mineral hydration water and its isotopic composition.  The research has wide ranging application for addressing fundamental problems across many disciplines in Earth and Planetary sciences.""","2495876","2014-02-01","2019-01-31"
"WIPFAB","Wideband Integrated Photonics For Accessible Biomedical Diagnostics","James Shafto Wilkinson","UNIVERSITY OF SOUTHAMPTON","Photonic technologies are set to revolutionise our access to chemical and biochemical information, driven by demand for fast, low-cost, automated chemical analysis in applications from food safety, water quality, security, personal and preventative medicine, pharmacogenetics to point-of-care diagnostics. The low cost and robustness of microfabrication approaches which enabled the mobile phone and digital camera are expected to lead to similarly widespread deployment of chemical and bioanalytical microsystems. Optical techniques play a major role in quantitative chemical analysis and are the mainstay of detection in “lab-on-chip” systems, but the degree of optical functionality integrated in these systems remains extremely limited, and they have yet to benefit fully from the recent massive growth in photonics communications technologies. Photonic technologies for telecommunications operate in the near infra-red (NIR) wavelength region from 800nm – 1800nm, driven by the spectral transmission window in silica optical fibre. However, the ideal molecular “fingerprint” region for biochemical analysis is dominated by the mid infra-red (MIR) spectral region. Biosensor and lab-on-chip research and commercialisation have both been severely hampered by the lack of an integrated photonic platform which can operate over both the NIR and MIR spectral ranges, and which would enable new opportunities for sensitive, selective, label-free biochemical analysis. This programme sets out to advance the frontiers of biophotonics research in MIR materials systems, integrated photonic components for biochemical analysis and nanostructured photonic materials for light control. New approaches to clinical point-of-care diagnostics will be enabled by realising a mass-manufacturable monolithically-integrated photonics/optofluidic technology for chemical and biochemical analysis in the near and mid-infrared, exploiting advanced spectroscopic techniques for accessible biomedical diagnostic","3062006","2012-04-01","2017-03-31"
"WoodNanoTech","Wood Nanotechnology for Multifunctional Structures","Lars BERGLUND","KUNGLIGA TEKNISKA HOEGSKOLAN","""Materials tend to be either structural or functional. Here focus is on biobased composites combining both aspects, using nanostructured wood templates. Wood is the most widely used biobased material for load-bearing structures, but the range of achievable properties and functions can still be increased. The objective is to develop scalable nanotechnology for wood structures, utilizing its nanocellulosic skeleton. Processing and materials design concepts are developed in the form of a wood nanotechnology toolbox. Focus is on transparent wood for engineering applications, a concept pioneered in my laboratory. Transparent wood can combine load-bearing properties with photonics functions and light weight.
    The cellulose nanofibril skeleton in wood is a sophisticated reinforcement structure. For transparent wood, processing to nanoporous but mechanically robust templates without chromophores, is needed. Templates are then further functionalized using in-situ polymerization and/or inorganic nanoparticle precipitation. Molecular dynamics simulations are used to design polymers and methods for cellulose surface modification. Optical property research on material effects on scattering, polarization properties will then generate new ideas in """"wood photonics"""". Device functions can be integrated in large structures, using anisotropy and the hierarchical structure in wood. Optically functional additives can be used to generate unique effects for applications such as lighting systems, LED panels, wood lasers, electrochromic windows or load-bearing and transparent panels with tailored combinations of transmittance and haze. Optical and mechanical properties are studied using experiments and modeling. The project team combines polymeric biocomposites competence with photonics expertise in a multidisciplinary effort.
""","2461947","2017-09-01","2022-08-31"
"WORDS","Words and Waring type problems","Aner Shalev","THE HEBREW UNIVERSITY OF JERUSALEM","Hilbert's solution to Waring problem in Number Theory shows that every positive integer is a sum of g(n) nth powers. Surprising non-commutative analogues of this phenomenon were discovered recently in Group Theory, where powers are replaced by general words. Moreover, the study of group words occurs naturally in important contexts, such as the Burnside problems, Serre's problem on profinite groups, and finite simple group theory. We propose a systematic study of word maps on groups, their images and kernels, as well as related Waring type problems. These include a celebrated conjecture of Thompson, problems regarding covering numbers and mixing times of random walks, as well as probabilistic identities in finite and profinite groups. This is a highly challenging project in which we intend to utilize a wide spectrum of tools, including Representation Theory, Algebraic Geometry, Number Theory, computational group theory, as well as probabilistic methods and Lie methods. Moreover, we aim to establish new results on representations and character bounds, which would be very useful in various additional contexts. Apart from their intrinsic interest, the problems and conjectures we propose have exciting applications to other fields, and the project is likely to shed new light not just in group theory but also in combinatorics, probability and geometry.","1197800","2010-01-01","2014-12-31"
"X-FIVE","Fifth Generation of Ultra Bright X Ray Beam","Victor, Armand Malka","CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS","The recently built X-ray Free Electron Lasers (FEL), the so-called fourth generation light sources, provide extremely intense coherent femtosecond pulses of energetic radiation. These FELs based on Linear Accelerator (Linac) technology require radio-protected facilities which are several kilometres in length and also require significant investment (more than a hundred million euros). They have already produced an impressive list of outstanding results in many scientific fields. As a consequence of these successes they are already significantly over-subscribed by the scientific community. The recent progress realized by laser plasma accelerators that can nowadays deliver high quality energetic particle beams in ultra short bunches (of a few femtoseconds) with very high peak currents (of a few kA) are very encouraging for the future. Thanks to the huge longitudinal electric fields they can produce, laser plasma accelerators appear to be a natural candidate to reduce the size and cost of future FELs. Among the many applications laser plasma accelerators sustain, the hope to build a compact Free Electron Laser, has been clearly identified by the scientific community as the near future grand challenge. The goal of the X-five project is then to demonstrate the feasibility of such fifth generation light sources - compact and low cost - that will satisfy the increasing demand of the scientific community. This next generation of FELs will deliver bright X ray beams at a repetition rate of 10 Hz, of interest for the many applications that do not require very high average brightness such as available from  the fourth light sources generation. The laser plasma accelerator research activities of the X-five project will also be of benefit to other fields, including medicine, radiation biology, chemistry, physics and material science, security, and of course accelerator science.","1703136","2014-04-01","2019-03-31"
"XCHEM","XUV/X-ray lasers for ultrafast electronic control in chemistry","Fernando Martin Garcia","UNIVERSIDAD AUTONOMA DE MADRID","Advances in generating controlled few-cycle laser pulses and novel ultrashort XUV/Xray sources, from free electron laser (FEL)-based to attosecond high harmonic generation (HHG)-based, have opened completely new avenues for imaging electronic and nuclear dynamics in molecules, with exciting applications in physics, chemistry and biology. Processes such as ionization and dissociation of simple diatomic molecules can now be monitored in real time, but the access to few-femtosecond or attosecond time scales in the XUV/X-ray domain may also allow one to uncover and control the dynamics of elementary chemical processes such as, e.g., ultrafast charge migration, proton transfer, isomerization or multiple ionization, and to address new key questions about the role of attosecond coherent electron dynamics in chemical reactivity. The success of current experimental efforts in explaining these phenomena, present in many biological processes, is seriously limited due to the difficulty in their interpretation. In this respect, the implementation by the applicant’s group of nearly exact theoretical methods in supercomputers has made it possible to guide experimental research on simple systems. Such theoretical methods lie outside the traditional quantum chemistry realm since, e.g., they must accurately reproduce the time evolution of the coupled electronic and nuclear motions in the electronic and dissociative continua, including electron correlation and non-adiabatic effects. The necessary extension to systems of chemical interest, the current bottleneck in this field, requires extensive and novel theoretical developments along a similar direction. The aim of this project is to study the electronic and coupled electronic-nuclear dynamics in complex molecules at the attosecond or few-femtosecond time-scales, developing concepts and accurate theoretical tools to interpret the new generation of time-resolved experiments and to achieve ultrafast electronic control in chemistry.","2447736","2012-01-01","2017-08-31"
"Xenoscope","Towards a multi-ton xenon observatory for astroparticle physics","Laura BAUDIS","UNIVERSITAT ZURICH","Dark matter is one of the greatest mysteries in the Cosmos, as its intrinsic nature is largely unknown. The identification and characterization of dark matter particles is a major endeavor in physics. XENOSCOPE will be a unique project focussed on essential, cutting-edge research towards a multi-ton dark matter detector using liquid xenon (LXe) as target material. With its low energy threshold, ultra-low backgrounds and excellent energy resolution, a LXe observatory will be highly sensitive to other rare interactions, such as from solar and supernova neutrinos, double beta decays of 136Xe, as well as from axions and axion-like particles. To design and construct a 50 t (40 t in the time projection chamber, TPC) detector, a number of critical technological challenges must first be addressed. Fundamental aspects are related to the design of the TPC, including the identification of new photosensors, the optimization of the light and charge collection (hence the energy threshold and resolution), and the minimization of radioactive backgrounds. XENOSCOPE will address all these aspects through a number of small, medium-size and a full-scale (in the z-coordinate of the TPC) prototypes. The goal is to specify the required input for the technical design of the 50 t detector, to be realized by the DARWIN consortium which the PI leads.  Arrays of VUV-sensitive SiPMs will be studied as novel light sensors, and a 4-π photosensor coverage TPC will be constructed for the first time. Signal detection will be optimized for both low and high-energy readout, thus drastically increasing the dynamic range of a LXe-TPC. Low-background materials will be identified and characterized not only for the photosensors and their read-out, but for all the components of the detector.  Finally,  a full scale TPC in the z-dimension, 2.6 m in height, will be designed, built and operated and electron drift and extraction into the vapor phase over such large distances for the first time demonstrated.","3344108","2017-10-01","2022-09-30"
"XLASERS","X-RAY LASERS, PHOTON SCIENCE, AND STRUCTURAL BIOLOGY","Janos Hajdu","UPPSALA UNIVERSITET","Theory predicts that with an ultra-short and very bright coherent X-ray pulse, a single diffraction pattern may be recorded from a large macromolecule, a virus, or a cell before the sample explodes and turns into a plasma. The over-sampled diffraction pattern permits phase retrieval and hence structure determination. The first free-electron lasers (FELs) capable to deliver ultra bright and very short X-ray pulses for such experiments have recently started operations. These are the most brilliant sources of X-rays to date, exceeding the peak brilliance of conventional synchrotrons by a factor of 10 billion. In the duration of a single flash, the beam focused to a micron-sized spot has the same power density as all the sunlight hitting the Earth, focused to a millimetre square. The interaction of an intense X-ray pulse with matter is profoundly different from that of an optical pulse. A necessary goal of the programme is to explore photon-material interactions in strong X-ray fields. Our aim in structural biology is to step beyond conventional damage limits and develop the science and technology required to enable high-resolution studies of single biological objects near the physical limits of imaging. Eligible targets include single virus particles, organelles, cells, nanocrystals, and isolated macromolecules. A particular aim of the planned work is to obtain high-resolution structures for giant viruses. The challenges engage an interdisciplinary approach, drawing upon structural sciences, biology, atomic and plasma physics, optics and mathematics. The potential for breakthrough science is great with impact not only in biology or physics but wherever dynamic structural information with high spatial and temporal resolution is valuable.  The overall relevance of the programme extends beyond basic science, to technologies of essential importance to a future Europe.","2500000","2011-12-01","2016-11-30"
"XLS","""New Frontiers for Computational Solid Mechanics based on eXtended Level Set representation. Applications to damage mechanics, contact mechanics and stress analysis.""","Nicolas Moës","ECOLE CENTRALE DE NANTES","""The present project intends to introduce new uses of the level set representation of surfaces in order to develop simulation methods able to solve problem currently unsatisfactorily addressed. Three main applications are targeted: localisation phenomena and very complex cracking patterns, precise determination of stresses around contact zones and, finally, introduction in reasonably coarse meshes of the mechanical influence of geometrically small but vitally important reinforcement for the structure safety (weld points,  cables, fillets, …). We plan to address them with a unified technology, parts of which are already being incorporated in codes worldwide because of the contribution of the PI and colleagues in the past ten years.

The PI is a specialist in the field of computational mechanics and has been the founder and among the major promoters of the eXtended Finite Element Method (X-FEM), now widely used for crack growth and material interfaces modelling. With the X-FEM, internal or external mechanical boundaries do not need to be explicitly meshed and may conveniently be stored as level sets.

The project proposes new algorithm based  an original and extended use of the level set concept in conjunction with the X-FEM :

*Thick Level Set (TLS):   We consider that the mechanical variable responsible for the localization of deformation in a softening behaviour (say damage for instance) is not local but tied to the movement of a degradation front located by a level set.
*Inequality Level Set (ILS):  We rephrase variational inequality formulation (such as contact) as shape optimization. The shape of the active contact zone is sought and represented by a level set. For any given level set location, a variational equality is solved and a sensitivity analysis is performed to update the level set location.
*Subgrid  Level Set (SLS):  The structural features (cables, fillets, …) are represented on a subgrid different from the mesh used to perform the computation.""","1957909","2012-04-01","2017-03-31"
"XMEMS","Towards Cost-Efficient Flexible Heterogeneous Integration for Micro- and Nanosystem Fabrication","Nils Göran Stemme","KUNGLIGA TEKNISKA HOEGSKOLAN","""This proposal targets the development of flexible heterogeneous integration schemes for combining best-of-class materials, components and manufacturing methods into economically viable micro- and nanosystem (MEMS) solutions.
Today, the IC industry drives the development of most micro- and nanofabrication technologies, which are characterized by standardized processes, very large production volumes of >10.000 wafers/month and enormous capital investments. In contrast, the vast majority of MEMS demand production volumes of <100 wafers/month and different manufacturing and integration processes for each type of device. The a-priori acceptance of IC manufacturing technologies for MEMS therefore leads to missed market opportunities for many moderate volume MEMS-based products and to sub-optimal material choices.
Instead, we aim for a new MEMS-specific integration and manufacturing paradigm, in which the technologies and tools are adapted to the production volumes and design variations of MEMS devices. Specifically, we will develop novel and enabling micro/nano fabrication and integration techniques with a focus on flexibility and cost-efficiency in the following areas:
"""" Heterogeneous Material Integration, where we incorporate high-performance materials into MEMS using unconventional and innovative technologies and tools, including serial integration, wafer-level integration and free-form fabrication of MEMS;
"""" Heterogeneous System Integration, where we develop new wafer level schemes to combine, process and interconnect components fabricated with different technologies such as MEMS, NEMS, ICs or photonics;
"""" Lab-on-Chip Integration, in which transducers, mass transport solutions, surface biochemistry and liquids are combined at the wafer level into high-performance systems.""","2279800","2011-03-01","2016-02-29"
"XNA","Development of an artificial information system","Piet Herdewyn","KATHOLIEKE UNIVERSITEIT LEUVEN","""Artificial genetic sequences have become an important tool for the development of new therapeutics but may also define a trait of technological innovation. The possibility to synthesize genes, plasmids and chromosomes combined with the possibilities of directed mutagenesis and genetically reprogramming organisms and directing their evolution will become a crucial issue in synthetic biology. The uniformity of genetic alphabets, the universality of the genetic code, the ubiquity of genetic interchanges and the risks of genetic pollution cannot be overlooked.  On the other hand, synthetic nucleic acids will become more and more important as potential new drugs.
It is proposed to develop an additional type of nucleic acids for the use as information system for the propagation of specific information of non-natural origin. It is the aim of the project to contribute to the development of an artificial genetic system orthogonal to the natural system that can be used as well in synthetic biology as in medicine.
Therefore we have to select & develop the appropriate chemical and enzymatic tools. This means (chemically) the selection of unnatural nucleic acids, their precursors and their modification for uptake in bacteria. Specialized polymerases as well as ligases will need to be developed for this purpose. The goal of the project is to design and synthesize a first orthogonal plasmid and new series of aptamers. A first application is the production of new therapeutics. This is a multidisciplinary project involving mainly chemistry and biotechnology. The general project architecture is to explore experimental progress in vivo and in vitro to reach the final assembly of an XNA episome.""","2500000","2013-04-01","2018-03-31"
"XRAYonACTIVE","An X-ray spectroscopy view on active sites: removing the obscuring silent majority","Franciscus Martinus Frederikus De Groot","UNIVERSITEIT UTRECHT","One of the holy grails in catalytic research is the determination of the structure of the active site. Information on the catalytically active site is notoriously difficult to obtain as it concerns a small minority of states in a sea of other silent states. This silent majority of states obscures the action that takes place on the active states. Because the core hole localizes the final state, X-ray absorption spectroscopy (XAS) is a powerful local probe of the electronic structure and XAS provides detailed information on catalysts under working conditions. A major limitation in the present experiments is the fact that the signal from the majority of non-active sites overwhelms the details from the active sites.

In this proposal I will develop a new idea to solve this problem and allow x-ray spectroscopy  to unveil the nature of active sites. The idea is based on a detailed knowledge of the resonant x-ray emission (RXES) process that allows the detection of RXES spectra that are specific for the active state only. Model calculations predict an enhancement of the active sites over the silent sites of approximately a factor 50, allowing the clear detection of active sites above 1 %  presence.

This method will be suitable to study active sites in heterogeneous catalysts based on transition metal ions. However, the approach is also suitable to study transition metal active sites in homogeneous catalysis and biocatalysis. In addition, the  proposed research will have an impact on first principles x-ray spectroscopy calculations in theoretical chemistry and theoretical physics.","2500000","2014-04-01","2019-03-31"
"XRayProton","Ultrafast Structural Dynamics of Elementary Water-Mediated Proton Transport Processes","Erik Theodorus Johannes NIBBERING","FORSCHUNGSVERBUND BERLIN EV","How acids and bases react in water is a question raised since the pioneering days of modern chemistry. Recent decades have witnessed an increased effort in elucidating the microscopic mechanisms of proton exchange between acids and bases and the important mediating role of water in this. With ultrafast spectroscopy it has been shown that the elementary steps in aqueous proton transfer occur on femtosecond to picosecond time scales. Aqueous acid-base neutralization predominantly proceeds in a sequential way via water bridging acid and base molecules. These ultrafast experiments probing molecular transitions in the ultraviolet, visible and mid-infrared spectral ranges, though, only provide limited insight into the electronic structure of acids, bases and the water molecules accommodating the transfer of protons in the condensed phase. Soft-x-ray absorption spectroscopy (XAS), probing transitions from inner-shell levels to unoccupied molecular orbitals, is a tool to monitor electronic structure with chemical element specificity. The aim is now to develop steady-state and time-resolved soft-x-ray spectroscopy of acids and bases in water-poor and water-rich solutions. Here novel liquid flatjet technology is utilized with soft-x-ray sources at synchrotrons as well as table-top laser-based high-order harmonic systems, to elucidate the electronic structural evolution of proton transfer pathways. Questions to be solved are electronic structural changes upon hydrogen bond formation, the nature of hydrated proton species, and the impact of conversion from acid to conjugate base (or base to conjugate acid) in aromatic alcohols, carboxylic and amine compounds, and ultimately the oxygen oxidation state in hydrated protons. Resolving the electronic structural dynamics of elementary steps of aqueous proton transport will furthermore elucidate the role of mediating water in bulk solution, and in specific conditions such as hydrogen fuel cells or trans-membrane proteins.","2482500","2018-09-01","2023-08-31"
"XUV-COMB","High Resolution Extreme Ultraviolet Laser Spectroscopy","Thomas Rudolf Udem","MAX-PLANCK-GESELLSCHAFT ZUR FORDERUNG DER WISSENSCHAFTEN EV","Discrepancies between theory and experiments have been fuelling the development of physics from the discrete line spectrum of hydrogen, where classical physics fails, to the Lamb shift, which is unexplained by the Dirac equation. At the precision frontier, comparisons between theory and experiment have been performed almost exclusively with atomic hydrogen. To progress from there, it very good accuracy. The spectroscopic investigation of muonic hydrogen, which was successful recently, was the first step in that direction and has brought up a serious challenge to Quantum Electrodynamics (QED).  Even though we could solve this problem in the meantime, there might be others waiting to be discovered when leaving the beaten track of ordinary atomic hydrogen. Besides anti-hydrogen, high resolution laser spectroscopy of singly ionized helium is the next logical item on the list. In addition of representing a hitherto unexplored system, He+ allows for a far better test than atomic hydrogen due to the QED power series expansion of its energy levels in terms of ZWith the nuclear charge Z and the fine structure constant the disputed terms are of the order of (Z)6. Unlike ordinary hydrogen, He+ can be readily stored in an ion trap and sympathetically cooled by co-stored ions with an accessible cooling transition. This approach eliminates essentially all dominating experimental uncertainties that we face with ordinary hydrogen today. The 1S-2S resonance is the sharpest and hence most interesting transition. Its observation requires highly coherent extreme ultraviolet radiation at 60.8 nm which can be generated through high order harmonics from a mode locked laser. The resulting frequency comb is most suitable for a two-photon transition as photons from pairs of modes combine to deliver the excitation energy. Other applications of such new laser source are foreseeable.","1968125","2017-06-01","2022-05-31"
"ZOOMecular","Read the fine print: Zooming into paleoenvironmental and biogeochemical processes through molecular imaging of biomarker distributions in sediments","Kai-Uwe Hinrichs","UNIVERSITAET BREMEN","Lipid biomarkers provide unique information to disciplines such as paleoceanography, paleoecology and biogeochemistry. Factors limiting their scope include high sample demand and analytical complexity, constraining resolution of time and space to decadal and centimeter scales, respectively. However, dynamic interactions between physical, chemical and biological processes are recorded within sedimentary matrices at finer scales; lipid biomarkers could decode this sedimentary fine print if the limitations of resolution could be overcome. In a recent PNAS paper, we have demonstrated that this can be done and shown that µm-scale molecular images of paleoenvironmental and geobiological processes can be obtained directly on surfaces of cut sediment cores via laser desorption ionization coupled to mass spectrometry. The project ZOOMecular will build on this innovation by interrogating laminated sediment archives of Late Quaternary climate change and dissecting the complex environmental and ecological responses at subannual resolution. Through analysis of spatial associations of lipid biomarkers with the sedimentary matrix, we will provide a new view of the mechanisms underlying delivery to and preservation of molecular signals in sedimentary records. ZOOMecular will seek to examine the microbial habitat niches at sedimentary interfaces that are home to globally important biogeochemical processes but that are largely known from studies of cm3-scale samples. To enable these pioneering studies, we will develop innovative analytical protocols for a suite of informative biomarkers and for the acquisition of congruent molecular and elemental maps of geological samples. ZOOMecular will unlock otherwise inaccessible information of broad geoscientific relevance; its goals go far beyond the state-of-the-art and its outcome has the potential to transform biomarker research. Such a project can be successfully realized only within a frontier research scheme as provided by the ERC.","3000000","2015-11-01","2020-10-31"
"µTHALYS","Micro-Technologies and Heterogeneous Advanced Platforms for Implantable Medical Systems","Robert M.O. Puers","KATHOLIEKE UNIVERSITEIT LEUVEN","The μTHALYS project aims to create a technology platform that enables a next revolution by bringing microsystem technology to the next level in terms of integration, miniaturization and multifunctionality and applying this development to address pending needs in health care.
Several breakthrough materials, basic concepts and fabrication techniques will be developed based on silicon or going far beyond silicon: At the wafer scale integration level, integration of advanced polymers (optics, conductive polymers, ionic polymer-metal composites) will be studied. These will be applied in several novel subminiature actuator and sensor devices with broad application potential, amongst which microfluidic systems, pressure sensing arrays,
In order to come to complex 3D systems combining modalities as optics, microfluidics, actuators and electronics, advanced device level fabrication and hybrid assembly technologies will be studied as well. Furthermore, the methods for packaging implants (flex/stretch interconnect technology, advanced interposers,…) will be pushed far beyond the current state of the art. The adoption of soft, and even
bioresorbable materials for packaging and interconnects will spectacularly improve the human-implant interface.
Another important research line pursued is the study of ultra-low power electronics for medical implants: sensor interfacing, A/D conversion, signal processing, data communication and power transfer.
These fundamental research activities will lead to many applied projects and valorization activities during and long afterwards the end of this grant. In the project itself, two main medical applications are targeted directly: a urinary pacemaker to prevent incontinence, and a new generation of implantable electrodes for neurology.","2452885","2014-03-01","2019-02-28"
